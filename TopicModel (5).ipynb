{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AR8pHwOXpi5",
        "outputId": "067341e8-c23e-4939-b225-eaccffea8536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘ISOcodes’, ‘fastmatch’, ‘Rcpp’, ‘RcppParallel’, ‘SnowballC’, ‘stopwords’, ‘RcppArmadillo’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘iterators’, ‘foreach’, ‘shape’, ‘RcppEigen’, ‘glmnet’, ‘lda’, ‘matrixStats’, ‘quadprog’, ‘slam’\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "install.packages(\"tidyverse\")\n",
        "install.packages(\"quanteda\")\n",
        "install.packages(\"RColorBrewer\")\n",
        "install.packages(\"stm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQIEel4nZehh",
        "outputId": "7b99968f-d57d-4cf3-e7c5-7daa617f7183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Package version: 3.2.1\n",
            "Unicode version: 10.0\n",
            "ICU version: 60.2\n",
            "\n",
            "Parallel computing: 2 of 2 threads used.\n",
            "\n",
            "See https://quanteda.io for tutorials and examples.\n",
            "\n",
            "Warning message in system(\"timedatectl\", intern = TRUE):\n",
            "“running command 'timedatectl' had status 1”\n",
            "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
            "\n",
            "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.6     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
            "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.7     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.9\n",
            "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
            "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.2     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
            "\n",
            "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
            "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
            "\n",
            "stm v1.3.6 successfully loaded. See ?stm for help. \n",
            " Papers, resources, and other materials at structuraltopicmodel.com\n",
            "\n"
          ]
        }
      ],
      "source": [
        "library(quanteda); library(tidyverse); library(RColorBrewer); library(stm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usHcj8mmZgnV",
        "outputId": "0ae68fc3-97aa-4a69-9c87-03cb6238cc79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mRows: \u001b[22m\u001b[34m15\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m1\u001b[39m\n",
            "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
            "\u001b[1mDelimiter:\u001b[22m \",\"\n",
            "\u001b[31mchr\u001b[39m (1): description\n",
            "\n",
            "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
            "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        }
      ],
      "source": [
        "dataset <- read_csv(\"/content/FINAL JBS DATASET.csv\", locale = readr::locale(encoding=\"ISO-8859-1\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "60ifsM4laPZz",
        "outputId": "bb693991-c9d8-4f15-894e-7698b2fdd571"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".dl-inline {width: auto; margin:0; padding: 0}\n",
              ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
              ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
              ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
              "</style><dl class=dl-inline><dt>text1</dt><dd><span style=white-space:pre-wrap>'Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 \\nhttps://doi.org/10.1186/s13326-019-0218-0\\nRESEARCH Open Access\\nCombining lexical and context features\\nfor automatic ontology extension\\nSara Althubaiti1,2, S¸enay Kafkas1,2 , Marwa Abdelhakim1,2 and Robert Hoehndorf1,2*\\nAbstract\\nBackground: Ontologies are widely used across biology and biomedicine for the annotation of databases. Ontology\\ndevelopment is often a manual, time-consuming, and expensive process. Automatic or semi-automatic identification\\nof classes that can be added to an ontology can make ontology development more efficient.\\nResults: We developed a method that uses machine learning and word embeddings to identify words and phrases\\nthat are used to refer to an ontology class in biomedical Europe PMC full-text articles. Once labels and synonyms of a\\nclass are known, we use machine learning to identify the super-classes of a class. For this purpose, we identify lexical\\nterm variants, use word embeddings to capture context information, and rely on automated reasoning over\\nontologies to generate features, and we use an artificial neural network as classifier. We demonstrate the utility of our\\napproach in identifying terms that refer to diseases in the Human Disease Ontology and to distinguish between\\ndifferent types of diseases.\\nConclusions: Our method is capable of discovering labels that refer to a class in an ontology but are not present in an\\nontology, and it can identify whether a class should be a subclass of some high-level ontology classes. Our approach\\ncan therefore be used for the semi-automatic extension and quality control of ontologies. The algorithm, corpora and\\nevaluation datasets are available at https://github.com/bio-ontology-research-group/ontology-extension.\\nKeywords: Disease ontology, Embeddings, Neural network\\nBackground\\nThe biomedical community has spent significant\\nresources to develop biomedical ontologies which con-\\ntain and define the basic classes and relations that occur\\nwithin a domain. Biomedical ontologies are developed by\\ndomain experts and are often developed in conjunction\\nwith the needs arising in literature-based curation of\\nbiological databases.\\nManual curation of databases based on literature is a\\nvery time-consuming task due to the massive amounts of\\nliterature, and automated methods have been developed\\nearly on to aid in curation [1]. One of the key tasks in\\ncomputational support for literature curation is the auto-\\nmatic concept recognition of mentions of ontology classes\\nin text [2]. An ontology class is an intensionally defined\\n*Correspondence: robert.hoehndorf@kaust.edu.sa\\n1Computational Bioscience Research Center, King Abdullah University of\\nScience and Technology, 23955-6900 Thuwal, Saudi Arabia\\n2Computer, Electrical and Mathematical Sciences and Engineering Division,\\nKing Abdullah University of Science and Technology, 23955-6900 Thuwal,\\nSaudi Arabia\\nentity that has a formal descriptionwithin an ontology and\\naxioms that determine its relation with other classes [3]. In\\nnatural language, multiple terms and phrases can be used\\nto refer to an ontology class [4], and the formal depen-\\ndencies within an ontology further determine whether a\\nterm refers to a class or not (i.e., whether a term refers to\\na particular class may depend on background knowledge,\\nin particular subclass relations, contained in an ontol-\\nogy). For example, the Disease Ontology (DO) [5] declares\\nPrediabetes syndrome (DOID:11716) to be a subclass\\nof Diabetes mellitus (DOID:9351), and based on this\\ninformation we assume that any reference to, or mention\\nof, Prediabetes syndrome is also a reference to Diabetes\\nmellitus (with respect to DO).\\nThere are several text mining systems designed for\\nontology concept recognition in text. These methods are\\neither based on lexical methods and therefore applicable\\nto a wide range of ontologies [6, 7] or they are domain-\\nspecific and rely on machine learning [8]. Text mining\\n© The Author(s). 2020 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the\\nCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver\\n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 2 of 13\\nbased-methods can also be used to automatically or semi-\\nautomatically construct and extend ontologies [9, 10]. For\\nexample, Lee et al. [11] focus on text mining of relations\\nthat are asserted in text between mentions of ontology\\nclasses that has been used to refine ontology classes in the\\nGene Ontology (GO) [12]. Text mining can also be used to\\nsuggest new subclasses and sibling classes in ontologies,\\nfor exampleWächter and Schroeder [13] carried out a text\\nmining based-system from different text sources which is\\nused for extending OBO ontologies by semi-automatically\\ngenerating terms, definitions and parent\\u0096child relations.\\nXiang et al. [14] have developed a pattern-based system\\nfor generating and annotating a large number of ontology\\nterms, following ontology design patterns and providing\\nlogical axioms that may be added to an ontology. Recently,\\nclustering based on statistical co-occurrence measures\\nwere also used to extend ontologies [15].\\nHere, we introduce a novel method relying on machine\\nlearning to identify whether a word used in text refers\\nto a class that could be included in a particular ontol-\\nogy. Essentially, our method classifies terms to determine\\nif they are usually mentioned in the same context as the\\nlabels and synonyms of classes in an ontology (which are\\nused as seeds to train the classifier); this classifier can then\\nbe applied to unseen terms. Furthermore, our method can\\nalso be used to expand ontologies by suggesting terms that\\nare mentioned within the same context as specific classes\\nin an ontology.\\nWe demonstrate the utility of our method in identi-\\nfying words referring to diseases from DO in full text\\narticles. We select the DO because the labels and syn-\\nonyms of DO classes are relatively easy to detect in text\\nand a large number of computational methods rely on\\naccess to a comprehensive disease ontology [16\\u009619]. Our\\nmethod achieves highly accurate (F-score &gt; 90%) and\\nrobust results, is capable of recognizing multiple different\\nclasses including those defined formally through logical\\noperators, and combines dictionary-based and context-\\nbased features; therefore, our method is also capable of\\nfinding new words that refer to a class. We manually\\nevaluate the results and suggest several additions to the\\nDO.\\nMethods\\nBuilding a disease dictionary\\nWe built a dictionary from the labels and synonyms\\nof classes in the Disease Ontology (DO), downloaded\\non 5 February 2018 from http://disease-ontology.org/\\ndownloads/. The dictionary consisted of 21,788 terms\\nbelonging to 6,831 distinct disease classes from DO. We\\nutilized the dictionary with the Whatizit tool [20] and\\nannotated the ontology class mentions along with their\\nidentifiers in approximately 1.6 million open access full-\\ntext articles from the Europe PMC database [21] (http://\\neuropepmc.org/ftp/archive/v.2017.06/) and generated a\\ncorpus annotated with mentions of classes in DO. We\\npreprocessed the corpus by removing stop words such as\\n\\u0093the\\u0094, \\u0093a\\u0094, and \\u0093is\\u0094 as well as some punctuation characters.\\nGenerating context-based features\\nWe use Word2Vec [22] to generate word embedding.\\nSpecifically, we use a skip-gram model which aims to find\\nword representations that are useful for predicting the\\nsurrounding words in a given sentence or a document\\nconsisting of sequence of words; w1,w2, ...,wK . The objec-\\ntive is to maximize the average log probability using the\\nfollowing formula:\\nV (w) = 1K\\nK?\\nk=1\\nK?\\n?c?j?c;j \\003=0\\nlog p(wK+j|wK ) (1)\\nwhere word vectors V (w) are computed by averaging over\\nthe number of words K and c is the size of the training\\ncontext. We generated the word embedding by using the\\ndefault parameter settings of theWord2Vec gensim imple-\\nmentation: vector size (dimensionality) of 100, window\\nsize 5, minimum occurrence count of 5, and we use a\\nskip-gram (sg) model.\\nSupervised training\\nWe carried out a set of experiments to choose the optimal\\ntraining algorithm to design our model. In our experi-\\nments we used default parameters for the training algo-\\nrithms but different hidden layers for Artificial Neural\\nNetworks (ANNs) [23]. Our experiments show that the\\nANN model outperforms an SVM model [24] (see Addi-\\ntional file 1: Table 1 for full details), and our model\\nperforms best with 200 neurons in a single hidden layer\\n(we tested a single hidden layer with a size of 10, 50,\\n100, and 200 neurons). We report results accordingly to a\\nmodel with 200 neurons in the remainder of this work. In\\nANNs, multiple neurons are organized in layers. Typically,\\ndifferent layers perform different kinds of transforma-\\ntions on their inputs [25]. In our experiments, we used\\nan ANN with an input layer of different sizes, a single\\nhidden layer that uses a sigmoid activation function, and\\nan output layer that differs based on the experiment. We\\ntrain each classifier in a supervised manner, using 10-fold\\nstratified cross-validation. Additionally, we report testing\\nperformance on an independent 20% testing set which\\nwe generated by randomly removing data points before\\ntraining.\\nRecognizing ontology classes in text\\nWe used two approaches to recognize the mention of\\nontology classes in text. Our first approach relies solely on\\nlabels and synonyms of the classes within a given ontol-\\nogy O and can be used to determine whether a word refer\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 3 of 13\\nto a class in O. We first obtain an ontology O in the Web\\nOntology Language (OWL) [26] format and extract a list\\nof class labels and synonyms L from O; we further utilize\\na text corpus T as input to our method. Then, we gener-\\nate word embeddings (i.e., vector-space encodings of the\\ncontexts in which a word occurs) for all words in our text\\ncorpus T and train a supervised machine learning model\\nto classify whether a word refers to a class in O or not\\n(using the L\\u0092s words as positive training instances and all\\nothers as negative instances).\\nFigure 1 illustrates the workflow of our first approach.\\nOur method is generic and can, in principle, be applied\\nto any ontology as long as the ontology provides labels\\n(or synonyms), these labels can be identified in text, and\\nthe ontology from which the labels are extracted is more\\nor less limited to a single domain. For example, refer-\\nence ontologies in the OBO Foundry [27] are usually\\nsingle domain ontologies and therefore suitable for our\\nmethod. Ontologies that would not be suitable are appli-\\ncation ontologies that cover multiple domains, such as the\\nExperimental Factor Ontology (EFO) [28] (although our\\nmethods can be applied to parts of it). It is most useful to\\nextend an existing ontology with new labels, synonyms, or\\nclasses.\\nIn our second approach, we rely on annotations from\\nthe Whatizit tool [20] to identify the mention of ontology\\nclasses in text and determine their specific superclasses in\\nan ontology. Our approach takes an ontology O in OWL\\nformat, a set of ontology classes S = {C1, ...,Cn}, and a\\ncorpus of text T as inputs.\\nThis approach first uses Whatizit as a named entity\\nrecognition and normalization tool to normalize class\\nlabels and synonyms in text by replacing all mentions\\nof a class with the class identifier (i.e., the class URI).\\nWe annotate 15,183 distinct terms using Whatizit; the\\ntotal dictionary consists of 21,788 terms (derived from\\nthe labels and synonyms of classes in DO). We then train\\nWord2Vec model that captures the context of the men-\\ntion of the class and generates a vector space embedding\\nfor that class. Given such vector space embeddings for\\na set of classes in O, we use the vector space embed-\\ndings as input to a machine learningmethod that classifies\\nwhether another class appears in a similar context. We\\nuse this method to determine if a class should belong the\\nsuperclass of C in O. Figure 2 illustrates the workflow of\\nthis approach.\\nThe main difference between the two approaches is that\\nthe first approach broadly identifies terms or words that\\nrefer to classes within a domain (as defined by the sum\\nof classes within an ontology) while the second approach\\ncan determine whether a term or word refers to a class\\nthat should appear as a subclass of a more specific ontol-\\nogy class. Both methods generate \\u0093seed\\u0094 words in text and\\nthen use these seeds first to generate context-based fea-\\ntures (through Word2Vec) and use these context-based\\nfeatures in a supervised machine learning classifier.\\nManual analysis process\\nWe manually evaluate some of our findings. The manual\\nevaluation is based on the medical expert knowledge of\\nthe evaluator who is a trained clinician, and supplemented\\nby literature search to validate some findings or resolve\\nconflicts. Mainly, results were confirmed by searching\\nfor review papers that characterize a condition. Overall,\\nFig. 1 Label-based workflow. The workflow describes how words (in red) are classified as disease or \\u0093other\\u0094\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 4 of 13\\nFig. 2 Annotation-based workflow. In this workflow, we first normalize the mentions of disease classes in the corpus and then apply Word2Vec to\\ngenerate embeddings for classes, not merely words\\nmanual curation following the suggestions by our classi-\\nfier took 10-15 min per sample (which included identify-\\ning related classes in the DO and drafting an explanation\\nfor cases which disagree with the DO).\\nResults\\nBroad classification of domain-specific terms: application\\nto diseases\\nOur method is a workflow that can be used to identify\\nwhether a term or phrase commonly refers to a class\\nthat may be included in a domain-specific ontology as a\\nlabel, synonym, or a new class. To achieve this goal, we\\nuse the existing labels and synonyms within a domain-\\nontology as \\u0093seeds\\u0094 to train a machine learning classifier\\nthat determines whether a new term is sufficiently similar\\nto an existing label or synonym and may therefore also be\\nincluded in the ontology. We represent terms primarily by\\nthe context in which they occur within a large corpus of\\ntext; we useWord2Vec [22] for this purpose.We then train\\nan Artificial Neural Network classifier in a supervised\\nmanner to distinguish between the terms already included\\nwithin a domain ontology (and therefore expected to refer\\nto a particular kind of phenomena) and randomly chosen\\nterms not included in the ontology (and therefore most\\nlikely not referring to a phenomenon within the domain\\nof the ontology).\\nWe demonstrate our method using the Human Dis-\\nease Ontology (DO) [5] and applying it to the terms\\noccurring in a large corpus of full-text biomedical articles\\n(see \\u0093Methods\\u0094). First, we tested whether our approach is\\ncapable of identifying words that refer to the Disease class\\n(DOID:4), i.e., whether our method can detect terms\\nthat refer to a disease. We generated word embeddings\\nfor every disease terms and other words in our corpus of\\nfull-text articles.\\nFigure 3 illustrates the distribution of the terms refer-\\nring to a diseases in DO and other words mentioned in\\nour corpus which do not belong to DO using the t-SNE\\ndimensionality reduction [29]. We can see that the terms\\nare clearly different and should be separable through a\\nmachine learning system.\\nTherefore, we trained a machine learning model to\\nrecognize whether a word refers to the disease or not\\nusing the word embeddings as input. We split the vec-\\ntor space embeddings into a training and testing dataset\\nand consider all embeddings referring to disease as pos-\\nitive instances and all others as negatives. We do not\\napply any filtering before selecting the positive or negative\\nsamples. We randomly select negatives equal to the num-\\nber of positives (7,932 positives and 7,932 negatives). We\\nwithhold 20% of randomly chosen positive and negative\\ninstances for testing, train a model on the remaining 80%\\nthrough 10-fold cross validation, and report the perfor-\\nmance results on the 20% test set. Evaluated on the testing\\nset, we can distinguish between disease and non-disease\\nterms with an F-score of 95% and AUC of 96% (see Table 1\\nand Figure 4).\\nTo better understand the source of errors and whether\\nour approach can be used to reliably extend ontolo-\\ngies (either with additional labels and synonyms, or new\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 5 of 13\\nFig. 3 a) The visualization of the embeddings using the t-SNE for binary-classification task b) The visualization of the embeddings using the t-SNE\\nfor classifying infectious diseases. c) The visualization of the embeddings using the t-SNE for classifying anatomical diseases. d) The visualization of\\nthe embeddings using the t-SNE for classifying the combination of infectious and anatomical diseases\\nclasses), we performed a manual analysis on a set of 20\\nfalse positive samples out of 197 which are not the label or\\nsynonym of a disease class DO but are classified as disease\\nby our classifier (see Table 2). We found that the majority\\nof the 20 false positive samples refer to either diseases or\\nphenotypes (where phenotypes are the observable char-\\nacteristics of an organism that may occur manifestations,\\nor signs and symptoms, of a disease, but do not constitute\\na disease on its own). For example, Aphthosis is a pre-\\ndiction of our method which refers to a human disorder\\nthat is not currently in the DO; the majority of false pos-\\nitives are disease-related terms that do not explicitly refer\\nto a disease. For example, we predictedmal-absorption as\\na disease term which may refer to a phenotype in some\\ncontexts. Our findings indicate that an ANN classifier\\ncan identify known terms referring to diseases, and can\\nfurther suggest novel terms which may prove useful for\\nontology development and extension.\\nFine-grained classification: distinguishing between groups\\nof diseases\\nAs our method showed capability to identify terms refer-\\nring to a disease, we next tested whether our method can\\nalso distinguish between different types of diseases. For\\nthis purpose, we used the embeddings generated from a\\npre-processed corpus in which we normalize all mentions\\nTable 1 F-score and AUC for our four experiments using different hidden layer sizes\\nClassification Hidden layer sizes 10 50 100 200\\nNumber of classes F-score AUC F-score AUC F-score AUC F-score AUC\\nDiseases 2 94.65% 95.31% 94.83% 95.97% 95.32% 96.06% 94.49% 95.99%\\nInfectious disease 5 95.65% 95.01% 96.01% 95.74% 95.43% 95.22% 95.68% 96.42%\\nAnatomical disease 13 69.18% 77.22% 70.15% 80.24% 70.20% 76.98% 72.00% 85.11%\\nInfectious + anatomical diseases 17 71.07% 84.75% 73.13% 84.03% 72.61% 84.98% 72.67% 83.66%\\nThe values in bold represent the highest AUC and F-score within each experiments\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 6 of 13\\nFig. 4 ROC curves for each experiment (Diseases, Infectious disease, Anatomical disease and a combination of Infectious disease + Anatomical disease)\\nof a disease in our corpus using Whatizit tool. The dis-\\nease dictionary that we utilized with Whatizit includes a\\ntotal of 21,788 terms (labels and synonyms) from DO. We\\nfound that 15,183 of these 21,788 terms appeared in our\\ncorpus and we generate an embedding vector for each of\\nthem. We then first trained a neural network model to\\nrecognize whether a disease-term refers to the Infectious\\nDisease (DOID:0050117) class or not, and furthermore\\nwhether our method is able to distinguish between the\\nfour different types of infectious disease in DO (i.e., bac-\\nterial, fungal, parasitic, or viral infectious disease). As\\ntraining data, we used the word embeddings generated for\\nDO classes, and we used the Elk reasoner to split them\\ninto four types of infectious diseases, and an additional\\nclass for diseases that are not a subclass of Infectious Dis-\\nease in DO. We randomly select 20% of the disease in\\nDO as validation set and train the neural network classi-\\nfier using 10-fold cross-validation on the remaining 80%\\nto separate diseases into one of the five classes (non-\\ninfectious, bacterial, fungal, parasitic and viral infections).\\nTable 1 shows the performance achieved on the validation\\nset.While the performance is less than predicting whether\\na term refers to a disease, our classifier can distinguish\\nbetween specific disease classes.\\nWe manually analyzed a set of 20 false positive samples\\nout of 38 which are not a subclass of Infectious disease in\\nthe DO but are classified as infectious by our classifier (see\\nTable 3). We found that 7 of these 20 cases can be sug-\\ngested to be subclasses of the specific infectious disease\\nthey have been classified with but do not have a subclass\\nrelation asserted or inferred in DO. For example, the term\\nsyphilitic meningitis (DOID:10073) is a disease that our\\nmethod classify as a bacterial infectious disease but it is\\nnot classified as infectious in the DO.\\nMoreover, to test the strength of our method to distin-\\nguish between disease classes, we further trained a neural\\nnetwork model to distinguish between the 12 different\\nsubclasses of Disease of anatomical entity (DOID:7), as\\nwell as an additional class for diseases not classified as\\nsubclasses of Disease of anatomical entity. We used the\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 7 of 13\\nTable 2 Manually analyzed disease terms predicted as disease\\nTerm Manual analysis result Explanation for the suggested diseases\\nFACTO other -\\nleucoencephalopathy other -\\nAphthosis Disease A disease refers to a condition with repetitive mucosal ulcers\\n[30, 31].\\nDesmoid other -\\nmetapneumovirus other -\\nTracheobronchomalacia Disease A rare condition with abnormal flaccidity of both the trachea\\nand the bronchi which results in possibility of narrowing or\\ncollapse of the airway [32\\u009634].\\nRESLES Disease A rare condition characterized by transient lesions in the cen-\\ntral part of the splenium of the corpus callosum (SCC), followed\\nby complete reversibility on follow-up magnetic resonance\\nimaging (MRI) after a variable period. It coincides with different\\ndiseases [35, 36].\\nmal-absorption other -\\nacroparesthesias other -\\nlimb-shaking other -\\npineocytomas Disease A rare disease that has an Orphanet ID: ORPHA:251912. It is\\none of the pineal parenchymal tumors and is considered the\\nleast aggressive one [37, 38].\\nhypomineralisation other -\\nneurognathostomiasis Disease It is a severe form of human gnathostomiasis, DOID:11379,\\nwhich can lead to disease and death, it involves the nervous\\nsystem [39\\u009641].\\nMetastasis other -\\nmyelomatosis Disease A type of cancer that begins in plasma cells that produce anti-\\nbodies. It could be one of the synonyms of multiple myeloma\\nDOID:9538 [42, 43].\\nAMRF Disease An OMIM disease, OMIM:254900 [44].\\narthralgia other -\\nfibrodentinoma Disease Fibrodentinoma is a benign odontogenic tumor that occurs\\nin children and young adults. The disease name usually is\\nrepresented as \\u0093Ameloblastic Fibrodentinoma\\u0094 [45, 46].\\ninfantile-ataxia other -\\nknowlesi other -\\nThe terms in bold represent the correctly validated terms (by a clinician) that classified as diseases terms using our method (in Diseases classification experiment).\\nsame method to split the classes in training and test set as\\nbefore. Results are shown in Table 1 and demonstrate that\\nour method can also be useful to classify diseases in their\\nanatomical sub-systems.\\nWe manually analyzed a set of 20 false positive samples\\nout of 127 which are not a subclass of Anatomical dis-\\nease in the DO but are classified as being a subclass of a\\nparticular anatomical system disease by our classifier (see\\nTable 4). We found that 12 of the 20 false positives can be\\nsuggested to be subclasses of the specific anatomical sys-\\ntem disease they have been classified with but do not have\\nsuch a subclass relation asserted or inferred in DO. For\\nexample, we classify Narcolepsy (DOID:8986) as a Ner-\\nvous system anatomical disease, and this may be added as\\na new subclass axiom to DO.\\nAs it is often inconvenient to train separate classifiers,\\nwe also combined both tasks and trained a multi-class\\nclassifier to classify disease classes either as infectious or\\nanatomical, or as other disease. We evaluate the perfor-\\nmance of this combined model (see Table 1), and our\\nmachine learning system achieves an AUC up to 84% (see\\nFigure 4). These results demonstrate it may be possible to\\nidentify new subclasses, although the performance drops\\nwhen we increase the complexity of the classification\\nproblem by distinguishing between more subclasses.\\nDiscussion\\nWe developed a method to automatically expand ontolo-\\ngies in the biomedical domain with new classes, syn-\\nonyms, or axioms. We demonstrate the utility of our\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 8 of 13\\nTable 3 Sample of manually analyzed disease terms predicted as infectious disease\\nDisease terms Ontology class assigned\\nby ANN\\nManual analysis result Suggested additional\\nclassification\\nDOID Explanation\\nPelizaeus-Merzbacher\\ndisease\\nViral infectious disease Non-infectious (inherited\\ndisorder)\\n- - -\\nKaposi\\u0092s sarcoma Viral infectious disease Viral infectious disease herpes simplex DOID:8566 The disease is caused by\\nHuman herpesvirus 8\\nwhich is Herpesviridae\\ninfection.\\nmaxillary sinusitis Bacterial infectious disease Bacterial infectious disease\\n(usually start viral and\\nprogress to either\\nbacterial or fungal)\\n- - It is an infection in the\\nmaxillary sinuses which\\ncould be due to different\\netiology, one of them is\\nbacterial [47].\\nkeratosis follicularis Bacterial infectious disease Non-infectious (genetic\\ndisease)\\n- - -\\nchronic rheumatic\\npericarditis\\nViral infectious disease The condition is triggered\\nby autoimmune reaction\\nto infection, mainly group\\nA streptococci.\\n- - -\\ngastroparesis Viral infectious disease In most cases the nerve is\\ndamaged by diabetes or\\nsurgery, however, a viral\\ninfection might be a cause\\n- - A condition in which the\\nstomach suffers from\\nparesis that affects the\\nfood movement to the\\nsmall intestine [48, 49].\\nosmotic diarrhea Bacterial infectious disease symptom - - -\\nfamilial cold\\nautoinflammatory\\nsyndrome\\nViral infectious disease Non-infectious (inherited\\ndisease)\\n- - -\\nangular cheilitis Fungal infectious disease Etiology is controversial,\\nmost commonly fungal or\\nbacterial.\\n- - Ambiguous.\\nBinder syndrome Viral infectious disease Congenital disease - - -\\nhypohidrosis Bacterial infectious disease Multi-causal - - -\\nSjogren\\u0092s syndrome Viral infectious disease autoimmune disease - - -\\nmedian rhomboid\\nglossitis\\nFungal infectious disease Etiology is controversial,\\nhowever it is considered\\nas a variant of orallesion\\nassociated with candida\\ninfection [50].\\n- - Ambiguous.\\nGoodpasture syndrome Viral infectious disease autoimmune disease - - -\\nsyphilitic meningitis Bacterial infectious disease Bacterial infectious disease syphilis DOID:4166 Considering the same\\nconcept of etiology, both\\ndiseases are caused by\\nbacterial infection\\n(Treponema pallidum).\\nacute diarrhea Viral infectious disease symptom - - -\\nWHIM syndrome Bacterial infectious disease Congenital disease - - -\\nerythrasma Fungal infectious disease Bacterial infection disease - - -\\nchronic wasting disease Parasitic infectious disease Neurodegenerative\\ndisorder\\n- - -\\nscarlet fever Bacterial infectious disease Bacterial infectious disease rheumatic fever DOID:1586 The disease is caused by\\nGroup A bacteria of the\\ngenus Streptococcus,\\nsame causative agent for\\nRheumatic fever.\\nThe terms in bold represent the correctly validated terms (by a clinician) that classified as infectious diseases terms using our method (in Infectious disease classification\\nexperiment).\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 9 of 13\\nTable 4 Sample of manually analyzed disease terms classified as affecting particular anatomical systems\\nDisease terms Ontology\\nclass\\nOntology\\nclass\\nassigned by\\nANN\\nManual analysis\\nresult\\nSuggested\\nadditional\\nclassification\\nDOID Explanation\\nTimothy\\nsyndrome\\ngenetic\\ndisease\\ncardiovascular\\nsystem\\ndisease\\nCannot specify\\n(affect multiple\\nparts)\\n- - -\\nFamilial periodic\\nparalysis\\ndisease of\\nmetabolism\\ncardiovascular\\nsystem\\ndisease\\nmusculoskeletal\\nsystem disease\\n- - -\\nHyperprolactinemiadisease of\\nmetabolism\\nendocrine\\nsystem\\ndisease\\nendocrine system\\ndisease\\npituitary gland\\ndisease\\nDOID:53 The pituitary gland is\\nthe endocrine gland\\nresponsible for\\nsecreting prolactin.\\nAngiokeratoma\\ncircumscriptum\\ndisease of\\ncellular\\nproliferation\\ngastrointestinal\\nsystem\\ndisease\\ncardiovascular\\nsystem disease\\n- - -\\nZollinger-\\nEllison\\nsyndrome\\nsyndrome gastrointestinal\\nsystem\\ndisease\\ngastrointestinal\\nsystem disease\\npeptic ulcer disease DOID:750 It is a disease that\\naffects either\\npancreas, duodenum,\\nor both of them. Both\\norgans are pats of the\\nGIT system. The\\ndisease pathology\\nis mainly excessive\\ngastrin secretion with\\nsubsequent peptic\\nulcers.\\nPolycystic liver\\ndisease\\ngenetic\\ndisease\\ngastrointestinal\\nsystem\\ndisease\\ngastrointestinal\\nsystem disease\\nliver disease DOID:409 It is a genetic disorder\\nthat affects primarily\\nthe liver.\\nBilirubin\\nmetabolic\\ndisorder\\ndisease of\\nmetabolism\\nhematopoietic\\nsystem\\ndisease\\nhematopoietic\\nsystem disease\\nkernicterus due to\\nisoimmunization\\nDOID:12043 Bilirubin disorder\\ncould be a result of\\nblood pathology,\\nsame as for the\\nmentioned\\nclassification\\nDOID:12043.\\nAlpha\\nthalassemia\\ngenetic\\ndisease\\nhematopoietic\\nsystem\\ndisease\\nhematopoietic\\nsystem disease\\nhemoglobinopathy DOID:2860 The disease is mainly a\\nhemoglobin\\ndisorder with\\nhematological\\nphenotypes.\\nKabuki syndrome syndrome immune sys-\\ntem disease\\nNot anatomical\\n- multisystems\\n- - -\\nAmyloidosis disease of\\nmetabolism\\nimmune sys-\\ntem disease\\nNot anatomical -\\nmultisystems\\n- - -\\nFatty liver disease disease of\\nmetabolism\\nmusculoskeletal\\nsystem\\ndisease\\ngastrointestinal\\nsystem disease\\n- - -\\nRenal-hepatic-\\npancreatic\\ndysplasia\\nphysical\\ndisorder\\nmusculoskeletal\\nsystem\\ndisease\\nCannot specify\\n(affect multiple\\nparts)\\n- - -\\nRadioulnar syn-\\nostosis\\nphysical\\ndisorder\\nmusculoskeletal\\nsystem\\ndisease\\nmusculoskeletal\\nsystem disease\\nbone development\\ndisease/Synostosis\\nDOID:0080006/\\nDOID:11971\\nThere is already an\\nentity in the DO for\\nsynostosis under\\nbone development\\ndisease.\\nHypophosphatasia genetic\\ndisease\\nmusculoskeletal\\nsystem\\ndisease\\nmusculoskeletal\\nsystem disease\\nbone remodeling\\ndisease\\nDOID:0080005 We could suggest\\nan additional\\nclassification based\\non the main affected\\nsystem. Our\\nsuggestive\\nclassification is\\nmusculoskeletal since\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 10 of 13\\nTable 4 Sample of manually analyzed disease terms classified as affecting particular anatomical systems (Continued)\\nthe disease is mainly\\naffecting\\nmineralization of\\nthe bone with\\nphenotypes similar to\\nthose of Rickets\\nDOID:10609.\\nNarcolepsy disease of mental\\nhealth\\nnervous\\nsystem\\ndisease\\nnervous system\\ndisease\\n* * *\\nAceruloplasminemia disease of metabolism nervous\\nsystem\\ndisease\\nnervous system\\ndisease\\nneurodegeneration\\nwith brain iron\\naccumulation\\nDOID:0110734 The disease main\\npathophysiology is\\neither the absence\\nor dysfunction of\\nceruloplasmin with\\nsubsequent iron\\naccumulation in\\nvarious organ, mainly\\nthe brain.\\nGlomangiomatosis disease of cellular pro-\\nliferation\\nnervous\\nsystem\\ndisease\\ncardiovascular\\nsystem disease\\n- - -\\nDeafness-dystonia-\\noptic neuronopathy\\nsyndrome\\ndisease of metabolism nervous sys-\\ntem disease\\nnervous system\\ndisease\\nnervous system\\ndisease; since it\\ncovers many\\nsubclasses to\\nwhich we can map\\nmany aspects of\\nthis disease\\nDOID:863 The disease\\u0092s\\nphenotypes reflect\\nneurological affection\\nofmultiple parts in the\\nnervous system.\\nTrophoblastic\\nneoplasm\\ndisease of cellular\\nproliferation\\nreproductive\\nsystem\\ndisease\\nreproductive\\nsystem disease\\nFemale\\nreproductive organ\\ncancer\\nDOID:120 The term refers to the\\ngroup of\\nmalignant neoplasms\\nthat consist of\\nabnormal\\nproliferation of\\ntrophoblastic tissues\\nsimilar to\\nchoriocarcinoma\\nDOID:3596 and\\ngestational\\ntrophoblastic\\nneoplasia\\nDOID:3590.\\nCryptorchidism physical disorder reproductive\\nsystem\\ndisease\\nreproductive\\nsystem disease\\ntesticular disease DOID:2519 The term refers to\\nundescended testicle.\\n*Nacrolepsy: is classified as a sleep disorde'</span></dd><dt>text2</dt><dd>'RESEARCH Open Access\\nTemporal information extraction from\\nmental health records to identify duration\\nof untreated psychosis\\nNatalia Viani1*, Joyce Kam1, Lucia Yin1, André Bittar1, Rina Dutta1,2, Rashmi Patel1,2, Robert Stewart1,2 and\\nSumithra Velupillai1\\nAbstract\\nBackground: Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health,\\nas longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about\\nwhen psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic\\nhealth records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information\\nunderlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research.\\nNatural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant\\ninformation in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are\\nlikely to include the information of interest. Next, temporal information extraction methods are needed to identify'</dd><dt>text3</dt><dd><span style=white-space:pre-wrap>'RESEARCH Open Access\\nDisclosing Main authors and Organisations\\ncollaborations in bioprinting through\\nnetwork maps analysis\\nLeonardo Azael García-García1* and Marisela Rodríguez-Salvador2\\nAbstract\\nBackground: Scientific activity for 3D bioprinting has increased over the past years focusing mainly on fully functional\\nbiological constructs to overcome issues related to organ transplants. This research performs a scientometric analysis on\\nbioprinting based on a competitive technology intelligence (CTI) cycle, which assesses scientific documents to establish\\nthe publication rate of science and technology in terms of institutions, patents or journals. Although analyses of\\npublications can be observed in the literature, the identification of the most influential authors and affiliations has not\\nbeen addressed. This study involves the analysis of authors and affiliations, and their interactions in a global framework.\\nWe use network collaboration maps and Betweenness Centrality (BC) to identify of the most prominent actors in\\nbioprinting, enhancing the CTI analysis.\\nResults: 2088 documents were retrieved from Scopus database from 2007 to 2017, disclosing an exponential growth\\nwith an average publication increase of 17.5% per year. A threshold of five articles with ten or more cites was\\nestablished for authors, while the same number of articles but cited five or more times was set for affiliations. The\\nauthor with more publications was Atala A. (36 papers and a BC = 370.9), followed by Khademhosseini A. (30\\ndocuments and a BC = 2104.7), and Mironov (30 documents and BC = 2754.9). In addition, a small correlation was\\nobserved between the number of collaborations and the number of publications. Furthermore, 1760 institutions with a\\nmedian of 10 publications were found, but only 20 within the established threshold. 30% of the 20 institutions had an\\nexternal collaboration, and institutions located in and close to the life science cluster in Massachusetts showed a strong\\ncooperation. The institution with more publications was the Harvard Medical School, 61 publications, followed by the\\nBrigham and Women\\u0092s hospital, 46 papers, and the Massachusetts Institute of Technology with 37 documents.\\nConclusions: Network map analysis and BC allowed the identification of the most influential authors working on\\nbioprinting and the collaboration between institutions was found limited. This analysis of authors and affiliations and\\ntheir collaborations offer valuable information for the identification of potential associations for bioprinting researches\\nand stakeholders.\\nKeywords: Network map analysis, Betweenness centrality, Bioprinting, Text mining, Collaboration analysis,\\nscientometrics, competitive technology intelligence\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: L.A.Garcia-Garcia@sussex.ac.uk\\n1University of Sussex, School of Engineering and Informatics, Falmer,\\nBrighton, UK\\nFull list of author information is available at the end of the article\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics\\n           (2020) 11:3 \\nhttps://doi.org/10.1186/s13326-020-0219-z\\nBackground\\nResearch articles are public documents that report scien-\\ntific advancements to share knowledge and promote devel-\\nopment in science. These documents contain fundamental\\ninformation regarding not only to research but also to the\\norganizations and authors involved. This data is of interest\\nto identify leading organizations and to map scientific\\ncollaborations.\\nScientometric tools such as co-citation analysis, biblio-\\ngraphic coupling, or co-author analysis can help to achieve\\nthese goals. Co-citation analysis and bibliographic coupling\\nare mainly used to measure the flow of information based\\non the documents selected by authors, while co-author\\nanalysis is more focused on the analysis of collaboration be-\\ntween authors, taking into consideration the social aspect\\nof the research collaboration. Furthermore, co-author ana-\\nlysis has been proved to be useful to determine the multi\\nand interdisciplinary of the institutions and their collabora-\\ntions [1]. Co-author analysis requires information related to\\nauthors\\u0092 aliases, affiliations, publications, areas of research,\\nand their collaborations. This information can be obtained\\nfrom digital libraries (DL) aimed to create systems for the\\nidentification of authors such as ORCID, which was created\\nby non-profit organizations, or ResearcherID, Scopus,\\nPubMED or Web of Science, which are companies that are\\ndeveloping their unique identifiers for authors [2\\u00964]. When\\nevaluating advances in science and technology, names of\\nauthors and affiliations become major indicators, as 1) their\\nnumber of citations by peers correlates to their acknow-\\nledgment as influential on their area of research [5] and 2)\\ncontributes to determining the specific disciplines involved\\nin the research [1], both are important elements to nurture\\nthe decision-making process. In this sense, Competitive\\nIntelligence (CI) acquires a relevant role, through the defin-\\nition, collection, analysis, and presentation of relevant infor-\\nmation [6]. The CI process can be further enhanced by\\nincorporating feedback form experts to validate the infor-\\nmation obtained [7]. CI is fundamental to research and de-\\nvelopment (R&amp;D), including products or processes with\\nradical novelty, such as bioprinting.\\nBioprinting is an emerging technology, a variant of addi-\\ntive manufacturing that involves the fabrication of 3D con-\\nstructs for living tissues and organs [8, 9]. This discipline\\nis growing at an accelerated pace, involving branches of\\nknowledge such as biology and engineering. Bioprinting\\nhas been developed to assist the needs of a fast-growing\\npopulation. This technique has potential social and eco-\\nnomic impacts [10, 11], including a huge effect in organ\\ntransplants, where one of the main objectives is the print-\\ning of functional biological structures to help in the short-\\nage of organs, thus overcoming long waiting lists and\\nissues related to the transplanted organs such as rejection\\n[10\\u009612]. Although there have been significant signs of pro-\\ngress in the past years, there are some areas of research to\\nbe explored in this incipient technology [11]. Since acad-\\nemy and industry have acknowledged that bioprinting will\\nhave a significant impact on the health-care sector in the\\nfollowing years, the identification of technology trends in\\n3D bioprinting [13, 14], including potential printing tech-\\nniques [15], becomes crucial to stay competitive and to\\ndevelop new technologies in this field. With this aim,\\nRodriguez-Salvador et al. [7] performed a patentometric\\nand scientometric analysis in bioprinting to identify trends\\nand to explore the knowledge landscape of this technology.\\nIn addition, they also identified the most prolific institu-\\ntions, being the MIT (113 publications) the number one,\\nfollowed by Nanyang Technology University (103 publica-\\ntions), and Tsinghua University (93 publications); They\\nalso found that the three first countries with more publica-\\ntions were USA with 1491, followed by China with 744,\\nand Germany with 377 [7]. These analyses are mostly\\nbased on the frequency of documents by affiliation and\\ncountry, and no inclusion or exclusion terms were set. The\\ninsights obtained can be enhanced with the identification\\nof the leading scientists and their field of expertise, thus\\ndistinguishing the principal areas of current research and\\ndetermining potential opportunities for R&amp;D.\\nIn order to unveil scientific and technological trends, it is\\nimportant to face big volumes of information using text\\nmining. This activity can be applied to identify and extract\\npotentially useful information from texts. It combines tools\\nsuch as machine learning, artificial intelligence, and statis-\\ntics to analyse large amounts of both structured and un-\\nstructured data. The information obtained can contribute\\nto understanding patterns in data by making use of tools\\nsuch as text categorization, text clustering, information ex-\\ntraction, among others [16]. Information retrieval, word\\nfrequency analysis, word distribution, pattern recognition,\\nand visualisation techniques are some of the most frequent\\npractices [17]. As a conclusion, text mining adds important\\nvalue to the pattern recognition by structuring the content\\nof data from textual sources for research, data analysis,\\nbusiness or competitive intelligence (CI) [17\\u009619].\\nA fundamental topic for the CI is the determination of\\nkey players, such as the main organizations and authors in-\\nvolved in scientific advancements. Network analysis can be\\nused to identify the collaboration in a visual pattern, where\\neither the authors or affiliations are represented by nodes\\nand their collaborations can be seen as the connection\\namong them. Moreover, the nodes with common attributes\\nof interest for the analysis can be grouped using clusteriza-\\ntion. Clusterization allows to group components with simi-\\nlar characteristics, such as research topics or techniques.\\nWhen clustering collaborations, the closer the nodes in au-\\nthors or affiliations network maps, the more similarities\\nthey share [5, 20]. Furthermore, collaboration analysis can\\nbe strengthened with the assessment of the Betweenness\\nCentrality (BC) to determine the level of association of the\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 2 of 13\\nnodes according to their position in the network. A\\nstraightforward measurement of the association level can\\nbe the connectivity, but it fails to disclose the importance\\nof a node. To overcome this, BC measure can be calculated\\nto evaluate the importance of a node and its social inter-\\naction in a network as this measure counts the number of\\nregions in the map connected by each element, setting\\ntheir importance in the flow of information [21, 22].\\nScientometric and patentometric techniques have been\\nused recently to analyse the number publications per year,\\nthe main authors, and organizations to determine the main\\nadvancements in bioprinting (methods, materials, etc.)\\n[23\\u009625]. Scientometric and text mining can be used to de-\\ntect the authors and affiliations with more publications\\nand more influence in bioprinting. This information can\\nbe an input for people looking for well-known experts in\\nbioprinting or state-of-the-art developments in the field.\\nTo achieve the main goal of this paper, a customised\\nsearch query was used to gather documents from Scopus.\\nThe query included keywords highly used in the most\\ncited papers on bioprinting. Two network maps of collab-\\norations, one for authors and one for affiliations, were\\ngenerated and analysed. Further analyses were carried out\\nto estimate the BC measurement, and the relationship be-\\ntween number of publications and the number of collabo-\\nrations. These parameters were used for the identification\\nof the most prolific (those with more publications on this\\ntopic) and important authors and institutions involved in\\nthe publications of advances in bioprinting.\\nThis analysis is the first attempt to undertake a quantita-\\ntive analysis using a network analysis approach and the\\ncalculation of centrality measurements to strengthen the\\nCI methodologies. The findings enhance the perception of\\nthe importance of collaborations among institutions for\\nthe generation of high-quality scientific outcomes and for\\nthe dissemination of the knowledge generated, helping\\nboth researchers and stakeholders in the identification of\\npotential opportunities for research and collaboration.\\nMethods\\nThis paper is focused on the network analysis of authors\\nand institutions from scientific publications in bioprinting.\\nThe analysis comprises both, a network analysis on the\\ncollaboration among institutions and one that deals with\\nthe collaboration among authors. The network maps were\\ngenerated in Gephi, an open-source software for network\\nanalysis [26\\u009634]. Betweenness centrality was calculated\\nfor both, authors and institutions\\u0092 collaborations.\\nThe adequate identification of specific keywords on the\\ntopic of interest is a determining step in the search strat-\\negy, as they contribute to the appropriate establishment of\\nthe search queries. A preliminary search in Scopus using\\nonly the term bioprinting with no period of time defined\\nwas the first stage of this research. Scopus was selected for\\nthe information retrieval as this is a major scientific data-\\nbase that includes information from more than 20,000 sci-\\nentific journals [35]. The ten most cited papers identified\\nthrough this search were selected, as they have been\\nacknowledged as referents for the topic. Table 1, García-\\nGarcía[36], shows the ten articles that formed the first set\\nof documents. These papers were used to identify the key-\\nwords to form the search queries. A text mining program\\nwas specially coded to carry out the text-mining of these\\npublications, thus determining the most relevant keywords\\non the topic. With a broader range of terms and their syn-\\nonyms we guarantee the inclusion of a wide range of pub-\\nlications compared to searches performed using only the\\nterm bioprinting. Three different types of keywords were\\nsearched in the whole text of the papers, being 1) the most\\nfrequent terms, 2) terms containing the word bio, and 3)\\nthe collocations, which are the juxtapositions of two words\\nwith a greater frequency. A cleaning of terms was accom-\\nplished manually afterward to sort them out according to\\nspecialized language of the subject. The identified key-\\nwords were separated by subtopics (i. e. technology,\\nprocess, and application) to form the search queries. A set\\nof 23 searches were performed with the selected termin-\\nology prior to the development of the definite query.\\nThese searches were used to identify the correct grouping\\nof terms and the exclusion terms.\\nThe search query was formed using the keywords previ-\\nously identified in combination with Boolean and proxim-\\nity operators, and exclusion terms. For this stage, the\\ndefinite search was carried out by defining the period of\\ntime, from 1 January 2000 to 15 November 2017 (when\\nthe information collection was concluded). The main\\nquery is observed in the appendix A1. The collection activ-\\nity involved the use of the query to search in title, abstract\\nand keywords. A quick review of titles and abstracts of the\\ndocuments found was carried out to discard those not\\nrelated to bioprinting.\\nThe bibliographic information of the documents identi-\\nfied in Scopus was retrieved and exported in a CSV format\\nto be cleaned and analysed. A cleaning process and the\\ncomplete normalization of the data was carried out to\\nstandardize authors and affiliations names. We performed\\na manual name disambiguation for both authors and affili-\\nations. The two authors analysed manually all the names\\non each one of the publications gathered. Every time a\\nsimilar name was observed, name disambiguation was car-\\nried out by looking to the full name, affiliation, and e-mail.\\nThe level of agreement on the disambiguation performed\\nby the authors was measured using Cohen\\u0092s kappa [37].\\nCo-author analysis was limited exclusively to the informa-\\ntion of the publications gathered and we did not require\\nfurther information from available DLs.\\nThe analysis to identify the most influential authors\\nand affiliations was carried out by setting a threshold for\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 3 of 13\\nTa\\nb\\nle\\n1\\nC\\nom\\npa\\nris\\non\\nof\\nth\\ne\\nto\\np\\nte\\nn\\nci\\nte\\nd\\npa\\npe\\nrs\\nfro\\nm\\nSc\\nop\\nus\\nob\\nta\\nin\\ned\\nfro\\nm\\nth\\ne\\nse\\nar\\nch\\nof\\n`b\\nio\\npr\\nin\\ntin\\ng\\u0092\\nan\\nd\\nth\\ne\\nde\\nve\\nlo\\npe\\nd\\nse\\nar\\nch\\nqu\\ner\\ny\\nin\\ntit\\nle\\ns,\\nab\\nst\\nra\\nct\\ns,\\nor\\nke\\nyw\\nor\\nds\\nTo\\np\\nte\\nn\\nre\\nsu\\nlts\\nus\\nin\\ng\\nth\\ne\\nke\\nyw\\nor\\nd\\nbi\\nop\\nrin\\ntin\\ng\\n[3\\n6]\\nTo\\np\\nte\\nn\\nar\\ntic\\nle\\ns\\nus\\nin\\ng\\nth\\ne\\nde\\nve\\nlo\\npe\\nd\\nse\\nar\\nch\\nst\\nrin\\ng\\nTi\\ntle\\nA\\nut\\nho\\nrs\\nYe\\nar\\nSo\\nur\\nce\\nC\\nite\\ns\\nTi\\ntle\\nA\\nut\\nho\\nr\\nYe\\nar\\nSo\\nur\\nce\\nC\\nite\\ns\\n1\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nof\\ntis\\nsu\\ne\\nan\\nd\\nor\\nga\\nns\\n[3\\n8]\\n.\\nM\\nur\\nph\\ny,\\nS.\\nV.\\n,\\nA\\nta\\nla\\n,A\\n.\\n20\\n14\\nN\\nat\\nur\\ne\\nBi\\not\\nec\\nhn\\nol\\nog\\ny\\n32\\n(8\\n),\\npp\\n.7\\n73\\n\\u00967\\n85\\n14\\n98\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nof\\ntis\\nsu\\nes\\nan\\nd\\nor\\nga\\nns\\n[3\\n8]\\n.\\nM\\nur\\nph\\ny\\nS.\\nV.\\n,A\\nta\\nla\\nA\\n.\\n20\\n14\\nN\\nat\\nur\\ne\\nBi\\not\\nec\\nhn\\nol\\nog\\ny\\n32\\n(8\\n),\\npp\\n.\\n77\\n3\\u0096\\n78\\n5.\\n14\\n98\\n2\\nSc\\naf\\nfo\\nld\\n-fr\\nee\\nva\\nsc\\nul\\nar\\ntis\\nsu\\ne\\nen\\ngi\\nne\\ner\\nin\\ng\\nus\\nin\\ng\\nbi\\nop\\nrin\\ntin\\ng\\n[3\\n9]\\n.\\nN\\nor\\not\\nte\\n,C\\n.,\\nM\\nar\\nga\\n,\\nF.\\nS.\\n,N\\nik\\nla\\nso\\nn,\\nL.\\nE.\\n,\\nFo\\nrg\\nac\\ns,\\nG\\n.\\n20\\n09\\nBi\\nom\\nat\\ner\\nia\\nls\\n30\\n(3\\n0)\\n,p\\np.\\n59\\n10\\n\\u00965\\n91\\n7\\n60\\n0\\nM\\nic\\nro\\nsc\\nal\\ne\\nte\\nch\\nno\\nlo\\ngi\\nes\\nfo\\nr\\ntis\\nsu\\ne\\nen\\ngi\\nne\\ner\\nin\\ng\\nan\\nd\\nbi\\nol\\nog\\ny\\n[4\\n0]\\n.\\nKh\\nad\\nem\\nho\\nss\\nei\\nni\\nA\\n.,\\nLa\\nng\\ner\\nR.\\n,B\\nor\\nen\\nst\\nei\\nn\\nJ.,\\nVa\\nca\\nnt\\niJ\\n.P\\n.\\n20\\n06\\nPr\\noc\\n.N\\nat\\nl.\\nAc\\nad\\n.\\nSc\\ni.\\nU\\n.S\\n.A\\n.,\\n10\\n3\\n(8\\n),\\npp\\n.2\\n48\\n0\\u0096\\n24\\n87\\n.\\n11\\n63\\n3\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nof\\nva\\nsc\\nul\\nar\\niz\\ned\\n,\\nhe\\nte\\nro\\nge\\nne\\nou\\ns\\nce\\nll-\\nla\\nde\\nn\\ntis\\nsu\\ne\\nco\\nns\\ntr\\nuc\\nts\\n[2\\n4]\\n.\\nKo\\nle\\nsk\\ny,\\nD\\n.B\\n.,\\nTr\\nub\\ny,\\nR.\\nL.\\n,G\\nla\\ndm\\nan\\n,A\\n.S\\n.,\\nH\\nom\\nan\\n,K\\n.A\\n.,\\nLe\\nw\\nis\\n,J\\n.A\\n.\\n20\\n14\\nAd\\nva\\nnc\\ned\\nM\\nat\\ner\\nia\\nls\\n26\\n(1\\n9)\\n,p\\np.\\n31\\n24\\n\\u00963\\n13\\n0\\n58\\n8\\nC\\nlin\\nic\\nal\\ntr\\nan\\nsp\\nla\\nnt\\nat\\nio\\nn\\nof\\na\\ntis\\nsu\\ne-\\nen\\ngi\\nne\\ner\\ned\\nai\\nrw\\nay\\n[4\\n1]\\n.\\nM\\nac\\nch\\nia\\nrin\\niP\\n.,\\nJu\\nng\\neb\\nlu\\nth\\nP.\\n,\\nG\\no\\nT.\\n,A\\nsn\\nag\\nhi\\nM\\n.A\\n.,\\nRe\\nes\\nL.\\nE.\\n,\\nC\\nog\\nan\\nT.\\nA\\n.,\\nD\\nod\\nso\\nn\\nA\\n.,\\nM\\nar\\nto\\nre\\nll\\nJ.,\\nBe\\nlli\\nni\\nS.\\n,P\\nar\\nni\\ngo\\ntt\\no\\nP.\\nP.\\n,\\nD\\nic\\nki\\nns\\non\\nS.\\nC\\n.,\\nH\\nol\\nla\\nnd\\ner\\nA\\n.P\\n.,\\nM\\nan\\nte\\nro\\nS.\\n,C\\non\\nco\\nni\\nM\\n.T\\n.,\\nBi\\nrc\\nha\\nll\\nM\\n.A\\n.\\n20\\n08\\nTh\\ne\\nLa\\nnc\\net\\n37\\n2\\n(9\\n65\\n5)\\n,p\\np.\\n20\\n23\\n\\u00962\\n03\\n0.\\n10\\n14\\n4\\nPr\\nin\\ntin\\ng\\nan\\nd\\npr\\not\\not\\nyp\\nin\\ng\\nof\\ntis\\nsu\\nes\\nan\\nd\\nsc\\naf\\nfo\\nld\\ns\\n[2\\n3]\\n.\\nD\\ner\\nby\\n,B\\n.\\n20\\n12\\nSc\\nie\\nnc\\ne\\n33\\n8\\n(6\\n10\\n9)\\n,\\npp\\n.9\\n21\\n\\u00969\\n26\\n51\\n0\\nM\\nec\\nha\\nni\\nca\\nlp\\nro\\npe\\nrt\\nie\\ns\\nan\\nd\\nce\\nll\\ncu\\nltu\\nra\\nlr\\nes\\npo\\nns\\ne\\nof\\npo\\nly\\nca\\npr\\nol\\nac\\nto\\nne\\nsc\\naf\\nfo\\nld\\ns\\nde\\nsi\\ngn\\ned\\nan\\nd\\nfa\\nbr\\nic\\nat\\ned\\nvi\\na\\nfu\\nse\\nd\\nde\\npo\\nsi\\ntio\\nn\\nm\\nod\\nel\\nlin\\ng\\n[4\\n2]\\n.\\nH\\nut\\nm\\nac\\nhe\\nr\\nD\\n.W\\n.,\\nSc\\nha\\nnt\\nz\\nT.\\n,\\nZe\\nin\\nI.,\\nN\\ng\\nK.\\nW\\n.,\\nTe\\noh\\nS.\\nH\\n.,\\nTa\\nn\\nK.\\nC\\n.\\n20\\n01\\nJo\\nur\\nna\\nlo\\nf\\nBi\\nom\\ned\\nic\\nal\\nM\\nat\\ner\\nia\\nls\\nRe\\nse\\nar\\nch\\n55\\n(2\\n),\\npp\\n.2\\n03\\n\\u00962\\n16\\n.\\n93\\n9\\n5\\nA\\ndd\\niti\\nve\\nm\\nan\\nuf\\nac\\ntu\\nrin\\ng\\nof\\ntis\\nsu\\nes\\nan\\nd\\nor\\nga\\nns\\n[4\\n3]\\n.\\nM\\nel\\nch\\nel\\ns,\\nF.\\nP.\\nW\\n.,\\nD\\nom\\nin\\ngo\\ns,\\nM\\n.A\\n.N\\n.,\\nKl\\nei\\nn,\\nT.\\nJ.,\\nBa\\nrt\\nol\\no,\\nP.\\nJ.,\\nH\\nut\\nm\\nac\\nhe\\nr,\\nD\\n.W\\n.\\n20\\n12\\nPr\\nog\\nre\\nss\\nin\\nPo\\nly\\nm\\ner\\nSc\\nie\\nnc\\ne\\n37\\n(8\\n),\\npp\\n.\\n10\\n79\\n\\u00961\\n10\\n4\\n49\\n5\\nSo\\nlid\\nfre\\nef\\nor\\nm\\nfa\\nbr\\nic\\nat\\nio\\nn\\nof\\nth\\nre\\ne-\\ndi\\nm\\nen\\nsi\\non\\nal\\nsc\\naf\\nfo\\nld\\ns\\nfo\\nr\\nen\\ngi\\nne\\ner\\nin\\ng\\nre\\npl\\nac\\nem\\nen\\nt\\ntis\\nsu\\nes\\nan\\nd\\nor\\nga\\nns\\n[4\\n4]\\n.\\nLe\\non\\ng\\nK.\\nF.\\n,C\\nhe\\nah\\nC\\n.M\\n.,\\nC\\nhu\\na\\nC\\n.K\\n.\\n20\\n03\\nBi\\nom\\nat\\ner\\nia\\nls\\n24\\n(1\\n3)\\n,p\\np.\\n23\\n63\\n\\u00962\\n37\\n8.\\n73\\n9\\n6\\n25\\nth\\nan\\nni\\nve\\nrs\\nar\\ny\\nar\\ntic\\nle\\n:E\\nng\\nin\\nee\\nrin\\ng\\nhy\\ndr\\nog\\nel\\ns\\nfo\\nr\\nbi\\nof\\nab\\nric\\nat\\nio\\nn\\n[4\\n5]\\n.\\nM\\nal\\nda\\n,J\\n.,\\nVi\\nss\\ner\\n,J\\n.,\\nM\\nel\\nch\\nel\\ns,\\nF.\\nP.\\n,G\\nro\\nll,\\nJ.,\\nH\\nut\\nm\\nac\\nhe\\nr,\\nD\\n.W\\n.\\n20\\n13\\nAd\\nva\\nnc\\ned\\nM\\nat\\ner\\nia\\nls\\n25\\n(3\\n6)\\n,p\\np.\\n50\\n11\\n\\u00965\\n02\\n8\\n46\\n5\\nSt\\nem\\nce\\nll-\\nba\\nse\\nd\\ntis\\nsu\\ne\\nen\\ngi\\nne\\ner\\nin\\ng\\nw\\nith\\nsi\\nlk\\nbi\\nom\\nat\\ner\\nia\\nls\\n[4\\n6]\\n.\\nW\\nan\\ng\\nY.\\n,K\\nim\\nH\\n.-J\\n.,\\nVu\\nnj\\nak\\n-\\nN\\nov\\nak\\nov\\nic\\nG\\n.,\\nKa\\npl\\nan\\nD\\n.L\\n.\\n20\\n06\\nBi\\nom\\nat\\ner\\nia\\nls\\n27\\n(3\\n6)\\n,p\\np.\\n60\\n64\\n\\u00966\\n08\\n2.\\n65\\n7\\n7\\nA\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nsy\\nst\\nem\\nto\\npr\\nod\\nuc\\ne\\nhu\\nm\\nan\\n-s\\nca\\nle\\ntis\\nsu\\ne\\nco\\nns\\ntr\\nuc\\nts\\nw\\nith\\nst\\nru\\nct\\nur\\nal\\nin\\nte\\ngr\\nity\\n[4\\n7]\\n.\\nKa\\nng\\n,H\\n.-W\\n.,\\nLe\\ne,\\nS.\\nJ.,\\nKo\\n,I\\n.K\\n.,\\nYo\\no,\\nJ.J\\n.,\\nA\\nta\\nla\\n,A\\n.\\n20\\n16\\nN\\nat\\nur\\ne\\nBi\\not\\nec\\nhn\\nol\\nog\\ny\\n34\\n(3\\n),\\npp\\n.3\\n12\\n\\u00963\\n19\\n46\\n6\\nSc\\naf\\nfo\\nld\\n-fr\\nee\\nva\\nsc\\nul\\nar\\ntis\\nsu\\ne\\nen\\ngi\\nne\\ner\\nin\\ng\\nus\\nin\\ng\\nbi\\nop\\nrin\\ntin\\ng\\n[3\\n8]\\n.\\nN\\nor\\not\\nte\\nC\\n.,\\nM\\nar\\nga\\nF.\\nS.\\n,\\nN\\nik\\nla\\nso\\nn\\nL.\\nE.\\n,F\\nor\\nga\\ncs\\nG\\n.\\n20\\n09\\nBi\\nom\\nat\\ner\\nia\\nls\\n30\\n(3\\n0)\\n,p\\np.\\n59\\n10\\n\\u00965\\n91\\n7\\n60\\n0\\n8\\nPr\\nin\\ntin\\ng\\nth\\nre\\ne-\\ndi\\nm\\nen\\nsi\\non\\nal\\ntis\\nsu\\ne\\nan\\nal\\nog\\nue\\ns\\nw\\nith\\nde\\nce\\nllu\\nla\\nriz\\ned\\nex\\ntr\\nac\\nel\\nlu\\nla\\nr\\nm\\nat\\nrix\\nbi\\noi\\nnk\\n[4\\n8]\\n.\\nPa\\nti,\\nF.\\n,J\\nan\\ng,\\nJ.,\\nH\\na,\\nD\\n.-H\\n.,\\nKi\\nm\\n,D\\n.-H\\n.,\\nC\\nho\\n,D\\n.-W\\n.\\n20\\n14\\nN\\nat\\nur\\ne\\nCo\\nm\\nm\\nun\\nic\\nat\\nio\\nns\\n53\\n,9\\n35\\n41\\n2\\nO\\nrg\\nan\\npr\\nin\\ntin\\ng:\\nTi\\nss\\nue\\nsp\\nhe\\nro\\nid\\ns\\nas\\nbu\\nild\\nin\\ng\\nbl\\noc\\nks\\n[4\\n9]\\n.\\nM\\niro\\nno\\nv\\nV.\\n,V\\nis\\nco\\nnt\\niR\\n.P\\n.,\\nKa\\nsy\\nan\\nov\\nV.\\n,F\\nor\\nga\\ncs\\nG\\n.,\\nD\\nra\\nke\\nC\\n.J.\\n,M\\nar\\nkw\\nal\\nd\\nR.\\nR.\\n20\\n09\\nBi\\nom\\nat\\ner\\nia\\nls\\n30\\n(1\\n2)\\n,p\\np.\\n21\\n64\\n\\u00962\\n17\\n4.\\n59\\n4\\n9\\nTi\\nss\\nue\\nen\\ngi\\nne\\ner\\nin\\ng\\nby\\nse\\nlf-\\nas\\nse\\nm\\nbl\\ny\\nan\\nd\\nbi\\no-\\npr\\nin\\ntin\\ng\\nof\\nliv\\nin\\ng\\nce\\nlls\\n[5\\n0]\\n.\\nJa\\nka\\nb,\\nK.\\n,N\\nor\\not\\nte\\n,C\\n.,\\nM\\nar\\nga\\n,F\\n.,\\nVu\\nnj\\nak\\n-\\nN\\nov\\nak\\nov\\nic\\n,G\\n.,\\nFo\\nrg\\nac\\ns,\\nG\\n.\\n20\\n10\\nBi\\nof\\nab\\nric\\nat\\nio\\nn\\n2\\n(2\\n),0\\n22\\n00\\n1\\n29\\n0\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nof\\nva\\nsc\\nul\\nar\\niz\\ned\\n,\\nhe\\nte\\nro\\nge\\nne\\nou\\ns\\nce\\nll-\\nla\\nde\\nn\\ntis\\nsu\\ne\\nco\\nns\\ntr\\nuc\\nts\\n[2\\n4]\\n.\\nKo\\nle\\nsk\\ny\\nD\\n.B\\n.,\\nTr\\nub\\ny\\nR.\\nL.\\n,\\nG\\nla\\ndm\\nan\\nA\\n.S\\n.,\\nBu\\nsb\\nee\\nT.\\nA\\n.,\\nH\\nom\\nan\\nK.\\nA\\n.,\\nLe\\nw\\nis\\nJ.A\\n.\\n20\\n14\\nAd\\nva\\nnc\\ned\\nM\\nat\\ner\\nia\\nls\\n26\\n(1\\n9)\\n,p\\np.\\n31\\n24\\n\\u00963\\n13\\n0\\n58\\n8\\n10\\n3D\\nBi\\nop\\nrin\\ntin\\ng\\nof\\nhe\\nte\\nro\\nge\\nne\\nou\\ns\\nao\\nrt\\nic\\nva\\nlv\\ne\\nco\\nnd\\nui\\nts\\nw\\nith\\nal\\ngi\\nna\\nte\\n/g\\nel\\nat\\nin\\nhy\\ndr\\nog\\nel\\n[5\\n1]\\n.\\nD\\nua\\nn,\\nB.\\n,H\\noc\\nka\\nda\\ny,\\nL.\\nA\\n.,\\nKa\\nng\\n,K\\n.H\\n.,\\nBu\\ntc\\nhe\\nr,\\nJ.T\\n.\\n20\\n13\\nJo\\nur\\nna\\nlo\\nfB\\nio\\nm\\ned\\nic\\nal\\nM\\nat\\ner\\nia\\nls\\nRe\\nse\\nar\\nch\\n-\\nPa\\nrt\\nA\\n10\\n1\\nA\\n(5\\n),\\npp\\n.\\n12\\n55\\n\\u00961\\n26\\n4\\n24\\n4\\nBi\\nnd\\nin\\ng\\nan\\nd\\nco\\nnd\\nen\\nsa\\ntio\\nn\\nof\\npl\\nas\\nm\\nid\\nD\\nN\\nA\\non\\nto\\nfu\\nnc\\ntio\\nna\\nliz\\ned\\nca\\nrb\\non\\nna\\nno\\ntu\\nbe\\ns:\\nTo\\nw\\nar\\nd\\nth\\ne\\nco\\nns\\ntr\\nuc\\ntio\\nn\\nof\\nna\\nno\\ntu\\nbe\\n-b\\nas\\ned\\nge\\nne\\nde\\nliv\\ner\\ny\\nve\\nct\\nor\\ns\\n[5\\n2]\\n.\\nSi\\nng\\nh\\nR.\\n,P\\nan\\nta\\nro\\ntt\\no\\nD\\n.,\\nM\\ncC\\nar\\nth\\ny\\nD\\n.,\\nC\\nha\\nlo\\nin\\nO\\n.,\\nH\\noe\\nbe\\nke\\nJ.,\\nPa\\nrt\\nid\\nos\\nC\\n.D\\n.,\\nBr\\nia\\nnd\\nJ.-\\nP.\\n,P\\nra\\nto\\nM\\n.,\\nBi\\nan\\nco\\nA\\n.,\\nKo\\nst\\nar\\nel\\nos\\nK.\\n20\\n05\\nJo\\nur\\nna\\nlo\\nft\\nhe\\nAm\\ner\\nic\\nan\\nCh\\nem\\nic\\nal\\nSo\\nci\\net\\ny\\n12\\n7\\n(1\\n2)\\n,p\\np.\\n43\\n88\\n\\u00964\\n39\\n6.\\n57\\n4\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 4 of 13\\neach analysis. A threshold of five documents cited at\\nleast ten times was set for authors, while for the institu-\\ntions we selected those with five documents cited at least\\nfive times. These inclusion parameters were based on\\nthe median number of cites for the whole set of docu-\\nments, which was 10.33, and the median number of pub-\\nlications per author was 5.76. For institutions, the mean\\nnumber of publications was 10 with the same median of\\ncites for the documents, 10.33; however, only three affili-\\nations were within the threshold, hence the median for\\ncitations and documents was reduced to 5 to include\\nmore affiliations.\\nTo identify the most prolific authors, the top ten au-\\nthors with more frequency within the threshold defined\\nwere selected and a Pearson correlation was computed to\\ndetermine the relationship existing between the number\\nof publications and the number of co-authors. The au-\\nthors were clustered by the similarity of areas of research\\nin the network maps and those with higher networking\\nwere identified by BC calculation. The number of times a\\nnode is taken as a connection for the shortest paths\\nbetween two other nodes can be estimated through BC,\\nwhich measures the node\\u0092s connection to different groups\\non a network map, being of a higher value the one who\\nconnects more groups [53]. The BC is obtained using the\\nequation [53]:\\nCB vð Þ ¼\\nX\\nv?s?t\\n?st vð Þ\\n?st\\nWhere ?st is the total of shortest paths from node s to\\nnode t, and ?st (v) is the number of those paths that go\\nthrough v.\\nThe information within the threshold was imported\\ninto VOSviewer, a software for data analysis and visual-\\nisation [54, 55], to perform the network map analysis.\\nThe authors or institutions are represented by nodes or\\nvertex in the network maps, and their connections are\\nrepresented by links or edges; in this document, the\\nterms are used indistinctly to refer to authors or affilia-\\ntions and their connections. Two undirected network\\nmaps were constructed from two matrices, representing\\nonly the correlation and not causality. A matrix of au-\\nthors and a matrix of affiliations were generated using\\nthe visualisation of singularities (VOS) of the VOSviewer\\nsoftware [55]. The clustering was performed in VOS-\\nviewer, computed using the default Field Independent\\nClustering Model (FICM) [55]. The statistical analysis to\\ndetermine the BC of the nodes forming both maps was\\nperformed in Gephi. The final step of the analysis was\\nthe consultation with experts in 3D bioprinting to valid-\\nate the results. Experts from UK and Asia were selected\\nbased on their international presence and impact in the\\nfield considering elements such as their number of cites,\\npublications, projects, and their availability. Instead of\\nproviding the experts with a list of authors found on the\\nresults of this research, we asked them to provide a list\\nof authors working on bioprinting according to their\\nown criteria. This method was used to reduce bias in\\ntheir selection, as they provided a list acknowledging\\ntheir peers based on their own experience. Is it worth\\nmentioning that the experts requested anonymity, there-\\nfore, we can only provide professional details of three of\\nthem at the time they were consulted. One of the ex-\\nperts was affiliated to the School of materials at the Uni-\\nversity of Manchester and had more than 10,000 Scopus\\ncitations. A second expert was affiliated to the Faculty of\\nEngineering at the University of Nottingham and had\\nmore than 760 citations. A third one was affiliated to the\\nSingapore Centre for 3D printing at Nanyang Technol-\\nogy University with more than 14,000 citations.\\nResults\\nFrom the initial search, where the ten most cited articles in\\nbioprinting from Scopus were considered, the top-cited\\narticle is 3D bioprinting of tissue and organs [37]. This is a\\nreview of different techniques used in bioprinting cited\\n1498 times, as seen in Table 1; the second most cited art-\\nicle is Scaffold-free vascular tissue engineering using bio-\\nprinting [38]. This article describes a fully biological\\nmethod to fabricate tubular vascular grafts and has been\\ncited less than 50% of the first author, 600 times; the third\\npaper, entitled 3D bioprinting of vascularized heterogeneous\\ncell-laden tissue constructs [24] was cited 446 times and\\ndescribes methods to generate vascularized tissue con-\\nstructs. The second and third papers are focused on one of\\nthe biggest challenges faced to print fully functional organs,\\nthe fabrication of scaffold-free blood vessels with mechan-\\nical properties close to the naturally grown vessels. Five of\\nthe ten articles were published in journals related to mate-\\nrials, four of them in general science journals (Nature Bio-\\ntechnology, Nature Communications, and Science), and\\none in the journal of Biofabrication, as can be observed in\\nTable 1. The results of the searches in Scopus using only\\nthe term bioprinting and those obtained using the search\\nquery developed are listed and compared in Table 1. It can\\nbe observed that the paper entitled 3D bioprinting of tissue\\nand organs still in first place in both results. The second\\npaper listed in the results from the search string is Micro-\\nscale technologies for tissue engineering biology by Khadem-\\nhosseini et al. [39] with 77% of the cites of the most first\\npublication, 1163, followed by the paper Clinical trans-\\nplantation of a tissue-engineered airway by Macchiarini\\net al. [40], published in the Lancet. Interestingly, the first\\nthree papers are published in three major journals covering\\nbiology and medical-related science, and six out of the 10\\npapers on this search were published in journals related to\\nmaterials and one in chemical engineering.\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 5 of 13\\nUsing the previously defined search query a total of\\n2088 publications were found from 2007 to November\\n15 of 2017 (when information collection activity ended).\\nFigure 1 shows the number of publications per year,\\nthere is a remarkable growth, where the highest number\\nof publications is 339 for 2017.\\nAfter the data selection and cleaning, a total of 228\\nauthors were found within the threshold of at least 5 docu-\\nments with 10 or more citations. 89 of the authors were\\nfound with repeated surnames. A Cohen\\u0092s kappa (?) of\\n0.62 was obtained for the agreement on the author name\\ndisambiguation. Values from 0.61 to 0.8 are ranked as\\nGood [36]. A collaboration was observed in 93% of the au-\\nthors, being 792 the total number of connections in the\\nmap. Regarding affiliations, a total of 20 organizations fall\\ninto the inclusion threshold, from which only 30% had an\\nexternal collaboration.\\nFrom the analysis, only ten authors were found to have\\nmore than 18 documents, as seen in Fig. 2, where the\\nnumber of documents and the number of co-authors for\\neach of them are shown. The author with more documents\\nfalling in the threshold defined is Atala A. with 36 docu-\\nments and 13 co-authors. The following author, Khadem-\\nhosseini A., had a total of 30 documents and more than\\ndouble of collaborations for the first author, 27 co-authors,\\nbeing the one with more connections. Mironov V. was in\\nthird place with 30 documents, and 20 co-authors. A Pear-\\nson correlation analysis was performed to determine the\\nrelationship between documents and co-authors, and a\\nweak positive correlation was observed, as the Pearson cor-\\nrelation coefficient had a value r = 0.29 for the top ten au-\\nthors, stating a lack of relationship between the number of\\nco-authors and the number of publications.\\nFigure 3 shows the network map of the author\\u0092s col-\\nlaboration, where the nodes\\u0092 size is proportional to their\\nBC value. The nodes representing the authors were\\ngrouped in a total of 17 clusters. From the BC calcula-\\ntion, the most prolific author, Atala A., was at the Wake\\nForest Institute for Regenerative Medicine from the\\nWake Forest University School of Medicine, Winston\\nSalem, United States when the information was gathered\\n(15 November 2017). According to Scopus altmetrics,\\nthis author had an h-index of 89, 850 documents pub-\\nlished, and a total of 17,376 citations working with 150\\nco-authors at the time of the analysis (see Table 2). On\\nthe other hand, under the inclusion terms, this author\\npublished a total of 36 documents on the topic analysed,\\nhaving 13 connections, 2851 citations, and a between-\\nness centrality value of 370.9.\\nThe second most prolific author found is Khademhos-\\nseini Ali L.I., affiliated with the Brigham and Women\\u0092s\\nHospital, Department of Medicine, Boston, United States,\\nwhen the data was collected. This author had an h-index\\nof 88, a total of 645 papers, with a total of 16,704 citations\\nand 150 co-authors, as stated in the Scopus altmetrics.\\nConsidering the inclusion terms, this author accounted\\nfor 30 documents, 27 connections, 3047 citations, and a\\nbetweenness centrality of 2104.9 (see Table 2).\\nFig. 1 Publications per year in bioprinting\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 6 of 13\\nThe third author was Mironov V., from the Laboratory\\nfor Biotechnological Research \\u00913D bioprinting solutions\\u0092,\\nMoscow, Russian federation. The Scopus altmetrics showed\\nthat this author had 105 papers, 3231 cites, and an h-index\\nof 31, co-authoring with 150 people. In this analysis, the au-\\nthor accounted for 30 documents, 20 links, 1009 citations,\\nand a betweenness centrality of 2754.9 (see Table 2).\\nAccording to the network map and the BC calculations,\\nMironov V. was stated as the author with a higher influ-\\nence in the knowledge flow of the collaboration network, as\\nit had the higher BC, followed by Khademhosseini A. While\\nMironov was affiliated to a biotechnological research\\nlaboratory, Atala and Khademhosseini were associated to\\ntwo of the top ten research departments in bioprinting\\nfound on this analysis.\\nThe authors ranked by the experts were compared\\nwith the most influential authors disclosed in this study,\\nas it can be seen from Table 3.\\nThree of the ten top authors in this scientometric study\\nwere considered as influential by the experts consulted,\\nAtala A., Mironov V., and Wei Sun; who were listed among\\nthe top five authors in both cases. The top three authors\\nfrom this study, who are listed in Table 3, are also the main\\ninfluential authors with a higher BC (see Table 2).\\nInstitutions\\u0092 research efforts can be better estimated by\\nthe number and the quality of their publications, therefore\\nthe affiliations wit'</span></dd><dt>text4</dt><dd><span style=white-space:pre-wrap>'Keet Journal of Biomedical Semantics            (2020) 11:4 \\nhttps://doi.org/10.1186/s13326-020-00224-y\\nDATABASE Open Access\\nThe African wildlife ontology tutorial\\nontologies\\nC. Maria Keet\\nAbstract\\nBackground: Most tutorial ontologies focus on illustrating one aspect of ontology development, notably language\\nfeatures and automated reasoners, but ignore ontology development factors, such as emergent modelling guidelines\\nand ontological principles. Yet, novices replicate examples from the exercise they carry out. Not providing good\\nexamples holistically causes the propagation of sub-optimal ontology development, which may negatively affect the\\nquality of a real domain ontology.\\nResults: We identified 22 requirements that a good tutorial ontology should satisfy regarding subject domain, logics\\nand reasoning, and engineering aspects. We developed a set of ontologies about African Wildlife to serve as tutorial\\nontologies. A majority of the requirements have been met with the set of African Wildlife Ontology tutorial ontologies,\\nwhich are introduced in this paper. The African Wildlife Ontology is mature and has been used yearly in an ontology\\nengineering course or tutorial since 2010 and is included in a recent ontology engineering textbook with relevant\\nexamples and exercises.\\nConclusion: The African Wildlife Ontology provides a wide range of options concerning examples and exercises for\\nontology engineering well beyond illustrating just language features and automated reasoning. It assists in\\ndemonstrating tasks concerning ontology quality, such as alignment to a foundational ontology and satisfying\\ncompetency questions, versioning, and multilingual ontologies.\\nKeywords: Ontology engineering, Tutorial ontology, African wildlife\\nBackground\\nThe amount of educational material to learn about ontolo-\\ngies is increasing gradually, and there is material for dif-\\nferent target audiences, including domain experts, applied\\nphilosophers, computer scientists and software develop-\\ners, and practitioners. These materials may include a tuto-\\nrial ontology to illustrate concepts and principles and may\\nbe used for exercises. There are no guidelines as to what\\nsuch a tutorial ontology should be about and should look\\nlike. The two most popular tutorial ontologies are about\\nwine and pizza, which are not ideal introductory subject\\ndomains on closer inspection (discussed below), they are\\nlimited to OWLDL only, and are over 15 years old by now,\\nCorrespondence: mkeet@cs.uct.ac.za\\nDepartment of Computer Science, University of Cape Town, 18 University\\nAvenue, Rondebosch, Cape Town, South Africa\\nhence, neither taking into consideration the more recent\\ninsights in ontology engineering nor the OWL 2 standard\\nwith its additional features.\\nConsidering subject domains in the most closely related\\narea, conceptual modelling for relational databases, there\\nis a small set of universes of discourse that are used in\\nteaching throughout the plethora of teaching materials\\navailable: the video/DVD/book rentals, employees at a\\ncompany, a university, and, to a lesser extent, flights and\\nairplanes. Neither of these topics for databases lend them-\\nselves well for ontologies, for the simple reason that the\\ntwo have different purposes. It does raise the question as\\nto what would be suitable and, more fundamentally, what\\nit is that makes some subject domain suitable but not\\nanother, and, underlying that, what the requirements are\\nfor an ontology to be a good tutorial ontology.\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 2 of 11\\nTable 1 Summary of main extant tutorial ontologies\\nOntology Year Stated aim Content Language Modelling issues Automated\\nreasoning\\nCurrent OE (e.g.,\\nODP, FO)\\nwine 2001 novice \\u0091all aspects\\u0092\\n(methodology,\\nmodelling, reasoning)\\nfor OE\\nsomewhat generic,\\nrepetitive, limited\\nextensibility\\nframes; the\\nwine.owl in\\nOWL DL is\\nbased on it\\nmultiple (e.g., class vs\\ninstance, hasX)\\nyes no\\npizza 2004 Protégé user guide,\\nalso illustrate OWL\\nand reasoning\\nsomewhat generic,\\nrepetitive, limited\\nextensibility\\nOWL DL multiple (e.g., hasX,\\nFO commitment, lack\\nof domain &amp; range)\\nyes no\\nuniversity 2005 illustrate OWL and\\nreasoning\\ngeneric, CDM (cf.\\nontology) scope,\\nvery small\\n&lt;OWL DL\\n(ALCIN)\\nmultiple (e.g., XorY,\\nnaming of\\nindividuals)\\nyes no\\nzooAnimals 2011 illustrate DL&amp;OWL\\nand some GoodOD\\nmodelling guidelines\\ngeneric, a lot of\\ndetail, easily\\nextensible\\n&lt;OWL 2 DL\\n(SHO)\\nfew yes partially\\n(BioTopLite)\\nfamily\\nhistory\\n2013 illustrate OWL 2 DL\\nand reasoning\\nspecific to author\\u0092s\\nfamily, not extensible\\nOWL 2 DL multiple (e.g., hasX,\\nFO commitment, lack\\nof domain &amp; range)\\nerror no\\nshirt 2015 illustrate the design of\\nthe FMA\\ngeneric, structure\\nspecific to FMA,\\nrepetitive, not\\nextensible\\n&lt;OWL 2 DL\\n(ALCIQ)\\nfew (lack of domain\\n&amp; range)\\nnone very limited\\n(reference\\nontology)\\nAbbreviations: OE: ontology engineering; ODP: ontology design pattern; FO: foundational ontology; CDM: conceptual data model; FMA: foundational model of anatomy;\\nOWL DL is SHOIN(D) and OWL 2 DL is SROIQ(D) in DL notation\\nIn this paper, we will first analyse existing tutorial\\nontologies and highlight some issues. We then proceed\\nto formulate a preliminary, first, list of requirements that\\ntutorial ontologies should meet. The African Wildlife\\nOntology (AWO) tutorial ontologies are then introduced\\nbriefly and held against the requirements. The scope of\\nthis paper is thus to introduce the AWO tutorial ontolo-\\ngies and to frame it in that context. Finally, we discuss and\\nconclude.\\nTutorial ontologies: issues and comparison\\nThere are several tutorial ontologies, which are sum-\\nmarised in Table 1 and discussed in this subsection; the\\nnext subsection that summarises the problems.\\nOf the six tutorial ontologies considered in detail, two\\nare popular, being the Wine Ontology and the Pizza\\nontology, since they are part of the W3C OWL guide\\nand designed for the most popular ontology development\\nenvironment (Protégé), respectively. They have various\\nshortcomings as tutorial ontologies, however, especially\\nconcerning modelling practices or styles (see also [1]).\\nThe Wine ontology in its current form emanates from\\nthe \\u0093Ontology development 101\\u0094 tutorial [2] with its\\nframes and slots that was subsequently transferred into\\nOWL1 and used in the \\u0093OWL guide\\u0094 [3], which is a W3C\\nRecommendation. While the guide contains some good\\nsuggestions, such as that \\u0093Synonyms for the same con-\\ncept do not represent different classes\\u0094 [2], there are also\\n1http://www.w3.org/TR/2003/PR-owl-guide-20031209/wine\\nmodelling issues, notably that the ontology is replete with\\nthe class-as-instance error that is promoted by the incor-\\nrect statement in the tutorial \\u0093Individual instances are the\\nmost specific concepts represented in a knowledge base.\\u0094\\n[2] (e.g., TaylorPort as instance of Port and MalbecGrape\\nas instance of Grape instead of as subclass of ), and the\\nsub-optimal object property naming scheme of \\u0091hasX\\u0092 ,\\nsuch as adjacentRegion between two Regions rather than\\nthe reusable and generic adjacent. Further, it uses differ-\\nent desiderata in the direct subclassing of wine such as the\\nlikes of Bordeaux and Loire (region-based) and Chardon-\\nnay and Cabernet Sauvignon (grape-based), and then\\nthere are other criteria, like DessertWine (food pairing-\\nbased grouping) and \\u0091wine descriptor\\u0092 ones (DryWine,\\nRedWine, TableWine), This does make it interesting for\\nshowing classification reasoning (except the undesirable\\ndeduction that DryWine ? TableWine), but is not ideal\\nfrom amodelling viewpoint. Further, from a tutorial view-\\npoint: there are many repetitions, such as very many\\nwineries, which distract from the principles, and it lacks\\nannotations.\\nThe Pizza ontology tutorial was created for the Pro-\\ntégé user manual and OWL DL ontology language [4].\\nIt reflects the state of the art at that time, yet much has\\nhappened over the past 15 years. For instance, there are\\nnew OWL 2 features and there are foundational ontolo-\\ngies that provide guidance for representing attributes (cf.\\nPizza\\u0092s ValuePartition). Pizza\\u0092s DomainConcept throws a\\nlearner straight into philosophical debates, which may not\\nbe useful to start with, and, for all practical purposes,\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 3 of 11\\nduplicates owl:Thing. Like the Wine ontology, it has\\nthe \\u0091hasX\\u0092 naming scheme for object properties, such as\\nhasTopping, including the name of the class it is sup-\\nposed to relate to, which is a quirk that is a combination\\nof a workaround for not having qualified number restric-\\ntions (anOWL 1 artefact) and of a sub-optimal ontological\\nanalysis of the relation (in casu, of how the toppings really\\nrelate to the rest of the pizza) that reduces chance of\\nontology reuse and alignment. Also, this propagates into\\nstudents\\u0092 modelling approaches: students\\u0092 ontologies in\\nearlier instances of the author\\u0092s course on ontology engi-\\nneering included, among others, a sandwich ontology with\\nhasFilling, an electrical circuit board ontology with hasIso-\\nlator, furniture with hasHeadboard. Modelling issues\\nare compounded by the statement \\u0093we generally advise\\nagainst doing [domain and range declarations]\\u0094 in the\\ntutorial documentation. When one aims to get novices\\nto use Protégé and OWL so as not get too many error\\nwith the automated reasoners, that might make sense,\\nbut ontologically, fewer constraints make an ontology less\\ngood because it admits more unintended models. Finally,\\nit has repetitive content to show features, which may be\\ndistracting, and, as with Wine, there is only one \\u0091final\\u0092\\nontology, despite that multiple releases are common in\\npractice.\\nOther tutorial ontologies include Family History,\\nzooAnimals, University, and Shirt. Family History [5] is\\ndeveloped by the same group as Pizza and aims to teach\\nabout advanced OWL 2 features and maximise the use\\nof inferencing. Loading it in Protégé 5.2 results in three\\npunning errors, since it mixes three object properties with\\nannotation properties (affecting 32 axioms), which is dis-\\nallowed, and trying to classify it without the three anno-\\ntation properties returned an OutOfMemoryError (on a\\nMacBookPro, 2.6 GHz and 8GB of memory), which is not\\nideal to start a tutorial with. Concerning modelling issues,\\nParentOfRobert illustrates one can use individuals in class\\nexpressions, but just that the language allows it, does not\\nmean it is ontologically a good idea that must be taught.\\nIt also has the \\u0091hasX\\u0092 semantic weakness, very few anno-\\ntations, DomainEntity being subsumed by owl:Thing,\\nand multiple data properties. In contrast to Pizza and\\nWine, all the declared instances are instances and the\\nontology has different versions as one goes along in the\\nchapters. It has some subject domain aspects descending\\ninto politics, which would render it unsuitable for teach-\\ning in several countries, such as stating that Sex? Female\\nunionsq Male (enforcing a gender binary) and that Person \\004\\n? 2 hasParent.Person (multiple constructions are possible\\nbiologically, societally, and legally).\\nThe remaining tutorial ontologies have been developed\\nby different \\u0091schools\\u0092 of views on ontology engineering\\n(OE), which is readily apparent in their scope and con-\\ntent. The zooAnimals tutorial ontology [6] comes closest\\nto our aims for a versatile tutorial ontology, demonstrat-\\ning multiple OWL features, avoiding modelling issues\\nsuch as class vs instance, and it is informed by a top-\\ndomain ontology (BioTop) as well as deep philosophical\\nnotions such as dispositions. It puts them all together\\ninto one ontology instead of gradual extensions, how-\\never, which is off-putting at a novice stage. One may\\nquibble about some of the content, such as simplifica-\\ntions that Plant ? ?hasProperPart.Chloroplast (notably,\\nsome parasitic plants and all myco-heterotrophic plants\\ndo not have chloroplasts) and there are unintended unde-\\nsirable deductions\\u0097i.e., logically implied, but incorrect\\nontologically\\u0097such as marineAnimal \\004 Omnivore since\\nnot all such animals are omnivores. Any simplified \\u0091com-\\nmon generic subject domain\\u0092 is likely to have some short-\\ncuts that are not 100% scientifically accurate, and it may be\\na fine line between tutorial approximation and modelling\\nmistake.\\nThe University ontology focuses on illustrating OWL\\nfeatures and automated reasoning, rather than modelling.\\nFor instance, it has AcademicStaff with sibling NonAca-\\ndemicStaff where a \\u0093non-X\\u0094 complement class is sub-\\noptimal, especially when there is a term for it. The repre-\\nsentation of Student \\004 Person is an advanced modelling\\naspect that can be improved upon with a separate branch\\nfor roles played by an object. The Computer Science\\nOntology was based on the University Ontology tuto-\\nrial and contains artificial classes, like unions of classes\\n(ProfessorinHCIorAI) and underspecified or incorrect indi-\\nviduals like AI and HCI (e.g., some course instance would\\nbe CS_AI-sem1-2018 instead).\\nThe Shirt ontology is a tutorial ontology to explain the\\nstructure and organisation of the Foundational Model of\\nAnatomy in a simpler way2 and therefore does not have\\nthe hasX naming scheme for object properties, it has no\\ndata properties and no instances. It has many annotations\\nwith explanations of the entities. There are no inferences.\\nRegarding suitability of the subject domains of the\\nontologies assessed, they are mixed. Wine misses many\\nwine-producing regions in the Americas (e.g., Chile), in\\nEurope (e.g. Spain, Bulgaria), and elsewhere (e.g., South\\nAfrica) and Pizza lacks varieties beyond Italian and Amer-\\nican ones, and both are served regularly in a relatively\\nsmall part of the world, therewith reducing their appeal\\ninternationally. Family history and a university as subject\\ndomains veer too easily into the area of database design for\\na single application, rather than application-independent\\ngeneric knowledge for an ontology. Shirts and zoo animals\\ndo not have these shortcomings.\\nFinally, more or less related textbooks were consid-\\nered [7\\u009611]. Only the \\u0093Semantic Web for the working\\n2http://xiphoid.biostr.washington.edu/fma/shirt_ontology/shirt_ontology_1.\\nphp\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 4 of 11\\nOntologist\\u0094 (2nd ed.) has sample files for the book\\u0092s many\\nsmall examples3 with two reoccurring subject domains,\\nbeing English literature and products.\\nProblems to address\\nThe previous section described several problems with\\nexisting tutorial ontologies. Notably, the recurring short-\\ncomings are that\\ni) good modelling practices are mostly ignored in\\nfavour of demonstrating language features,\\nautomated reasoning, and tools\\nii) when good modelling practices and at least some\\nrecent ontology engineering advances are included, it\\nfalls short on language features and gradual\\nextensions.\\nThis has a negative effect on learning about ontology\\ndevelopment, for tutorial ontology practices are nonethe-\\nless seen by students as so-called \\u0091model answers\\u0092 even if\\nit were not intended to have that function.\\nThe ontology survey does not reveal what may be the\\ncharacteristics of a good tutorial ontology and, to the best\\nof our knowledge, there is no such list of comprehen-\\nsive criteria for tutorial ontologies specifically. Schober\\net al. [6] propose a partial list with seven high-level\\ncontent requirements indeed, such as a common sense\\nknowledge subject domain that lends itself well to demon-\\nstrate the \\u0093classic\\u0094 modelling challenges, but it omits the\\nessential components of logic, reasoning, and engineering\\nrequirements. Scoping it more broadly, one could con-\\nsider modelling guidelines and automated checkers for\\nproduction level ontologies, such as [12\\u009615]. They can\\ninform the development of tutorial ontologies, in partic-\\nular to avoid such issues as the class vs. instance error in\\nthe provided sample ontology, but that is different from\\neducating students about the foundations and reasons for\\nsuch guidelines starting from a basic level of modelling\\nto more advanced issues. For instance, disjointness and\\ncovering constraints among subclasses of a parent class is\\nindeed desirable together with coherent criteria to declare\\na taxonomy [15], but that does not let students observe or\\nexperience mistakes, i.e., learn what is suboptimal or does\\nnot work and why. A tutorial ontology also would have\\nto be able to accommodate common pitfalls and grad-\\nual quality improvements, among other things, which are\\nnot covered by the general guidelines. Also, general guide-\\nlines tend to follow one commitment over another\\u0097e.g.,\\nthe GoodOD guidelines favour a realist approach with\\nthe BFO foundational ontology\\u0097but for teaching OE in\\ngeneral, students need learn to be cognisant of multiple\\npossible commitments, their consequences when choos-\\ning one or the other, and have at least one practical\\n3http://www.workingontologist.org/Examples.zip; Last accessed: 26-11-2018.\\nexample of such a difference to illustrate it, which general\\nguidelines do not provide.\\nPotential benefits of the African wildlife ontology tutorial\\nontologies\\nIn order to address these problems, we introduce the\\nAfrican Wildlife Ontology (AWO). The AWO has been\\ndeveloped and extended over 8 years. It meets a range of\\ndifferent tutorial ontology requirements, notably regard-\\ning subject domain, use of language features and auto-\\nmated reasoning, and its link with foundational ontologies\\non the one hand and engineering on the other. It aims to\\ntake a principled approach to tutorial ontology develop-\\nment, which thereby not only may assist a learner, but,\\nmoreover from a scientific viewpoint, it might serve as a\\nstarting point for tutorial ontology creation or improve-\\nment more broadly, and therewith in the future contribute\\nto an experimental analysis of tutorial ontology qual-\\nity. This could benefit educational material for ontology\\ndevelopment.\\nAlso, educationally, there is some benefit to \\u0091reusing\\u0092\\nthe same ontology to illustrate a range of aspects, rather\\nthan introducing many small ad hoc examples, for then\\nlater in a course, it makes it easier for the learners to see\\nthe advances they have made. This is also illustrated with\\noffering multiple versions of the ontology, which clearly\\nindicate different types of increments.\\nFinally, the AWO can be used on its own or together\\nwith the textbook \\u0093An Introduction to Ontology Engineer-\\ning\\u0094 [16], which contains examples, tasks and exercises\\nwith the AWO.\\nConstruction and content\\nThe construction of the AWO tutorial ontologies has gone\\nthrough an iterative development process since 2010. This\\ninvolved various extensions and improvements by design,\\nmainly to address the increasing amount of requirements\\nto meet, and maintenance issues, such as resolving link\\nrot of an imported ontology. Rather than describing the\\nprocess of the iterative development cycles, we present\\nhere a \\u0091digest\\u0092 version of it. First, a set of tutorial ontol-\\nogy requirements are presented together, then a brief\\noverview of the AWO content is described, and subse-\\nquently we turn to which of these requirements are met\\nby the AWO.\\nOE tutorial ontology requirements\\nTutorials on ontologies may have different foci and it\\nis unlikely that an ontology used for a specific tutorial\\nwill meet all requirements. The ontology should meet the\\nneeds for that tutorial or course, and that should be stated\\nclearly. As such, this list is intended to serve as a set of con-\\nsiderations when developing a tutorial ontology. Each item\\neasily can take up a paragraph of explanation. We refrain\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 5 of 11\\nfrom this by assuming the reader of this paper is suffi-\\nciently well-versed in ontology engineering and seeking\\ninformation on tutorial ontologies. For indicative purpose,\\nthe requirements are categorised under three dimensions:\\nthe subject domain of the ontology, logics &amp; reasoning,\\nand engineering factors.\\nSubject domain\\nThe tutorial ontology\\u0092s subject domain, also called uni-\\nverse of discourse, should be versatile to be able to cater\\nplausibly for a range of modelling aspects. We specify\\nseven requirements for it, as follows.\\n1. It should be general and commonsensical domain\\nknowledge, so as to be sufficiently intuitive for\\nnon-experts to be able to understand content and\\nadd knowledge. Optionally, it may be an enjoyable\\nsubject domain to make it look more interesting and,\\nperhaps, also uncontroversial4 to increase chance of\\nuse across different settings and cultures.\\n2. The content should be not wrong ontologically,\\nneither regarding how things are represented (e.g.,\\nno classes as instances) nor the subject domain\\nsemantics (e.g., whales are mammals, not fish).\\n3. It needs to be sufficiently international or\\ncross-cultural so that experimentation with a\\nscenario with multiple natural languages for\\nmultilingual ontologies is plausible.\\n4. Its contents should demonstrate diverse aspects\\nsuccinctly when illustrating a point (in contrast to\\nbeing repetitive in content).\\n5. It needs to be sufficiently versatile to illustrate the\\nmultiple aspects in ontology development (see\\nbelow), including the use of core relations (e.g.,\\nmereology).\\n6. It should permit extension to knowledge that\\nrequires features beyond Description Logics-based\\nOWL species, so as to demonstrate representation\\nlimitations and pointers to possible directions of\\nsolutions (e.g., temporal aspects, non-monotonicity,\\nfull first-order logic).\\n7. The subject domain should be able to possibly be\\nused in a range of use case scenarios (database\\nintegration, science, NLP, and so on).\\nLogics &amp; reasoning\\nSince a core feature of ontologies is their logic under-\\npinning, a tutorial ontology thus also will need to meet\\ncriteria for the representation language and automated\\nreasoning over it. They are as follows.\\n4Recent political issues include complaints with subject domains of exercises\\nthat perpetuate stereotypes and simplifications, such as, but not limited to, the\\ngender binary, who can marry whom, and gendered subject domains\\nperceived to reside at the edges of the spectrum.\\nI. The ontology should be represented in a logic that has\\ntool support for modelling and automated reasoning.\\nII. The ontology should be represented in a logic that\\nhas tool support for \\u0091debugging\\u0092 features that\\n\\u0091explain\\u0092 the deductions, meaning at least showing\\nthe subset of axioms involved in a deduction.\\nIII. It should permit simple classification examples and\\neasy examples for showing unsatisfiability and\\ninconsistency, such that it does not involve more\\nthan 2-3 axioms in the explanation, and also longer\\nones for an intermediate level.\\nIV. The standard reasoning tasks should terminate fairly\\nfast (&lt; 5 s) for most basic exercises with the\\nontology, with the \\u0091standard\\u0092 reasoning tasks being\\nsubsumption/classification, satisfiability, consistency,\\nquerying and instance retrieval.\\nV. The representation language should offer some way\\nof importing or linking ontologies into a network of\\nontologies.\\nVI. The language should be expressive enough to\\ndemonstrate advanced modelling features (e.g.,\\nirreflexivity and role composition).\\nVII. The logic should be intuitive for the modelling\\nexamples at least at the start; e.g., if there is a need for\\nternary relations, then the logic should permit\\nn-aries with n ? 3 so that it can be represented as\\nsuch, rather than as an approximation with a\\nreification and a workaround pattern.\\nEngineering and development tasks\\nAn ontology is an artefact, which has to be built and\\nmaintained. To this end, there are multiple approaches,\\nmethodologies, methods, and software tools of which at\\nleast a subset will have to become part of an ontologist\\u0092s\\ntoolbox. We identified eight broad requirements:\\nA. At least some ontology development methods and\\ntools should be able to use the ontology, be used for\\nimprovement of the ontology, etc.\\nB. The ontology needs to permit short/simple\\ncompetency questions (CQs) and may permit long\\nand complicated CQs, which are formulated for the\\nontology\\u0092s content and where some can be answered\\non the ontology and others cannot.\\nC. At least some of the top-level classes in the hierarchy\\nshould be straight-forward enough to be easily linked\\nto a leaf category from a foundational ontology (e.g.,\\nAnimal is clearly a physical object, but the ontological\\nstatus of Algorithm is not immediately obvious).\\nD. It should be relatable to, or usable with, or else at\\nleast amenable to the use of, ontology design\\npatterns, be they content patterns or other types.\\nE. It is beneficial if there is at least one ontology\\nsufficiently related to its contents, so that it can be\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 6 of 11\\nused for tasks such as comparison, alignment, and\\nontology imports.\\nF. It is beneficial if there are relevant related\\nnon-ontological resources that could be used for\\nbottom-up ontology development.\\nG. It should be able to show ontology quality\\nimprovements gradually, stored in different files.\\nH. It should not violate basic ontology design principles\\n(e.g., classes and relations vs. implementation\\ndecisions with data properties and data types when\\nrepresenting qualities of entities, such as an animal\\u0092s\\nweight).\\nWhile this list may turn out not to be exhaustive in the\\nnear future, it is expected to be sufficient for introduc-\\ntory levels of ontology development tutorials and courses.\\nEither way, this list of requirements are already hard\\nto meet in one single ontology. For instance, simplicity\\n(Items 3, III, and B) vs. complicated extensions and onto-\\nlogical precision (Items 6 and C) cannot be fully met\\nat once. On the flip side, some requirements are closely\\nrelated or overlap, such as design principles (Item H) and\\nnot being wrong ontologically (Item 2) since some of the\\nformer are informed by the latter.\\nContent of the AWO \\u0096 at a glance\\nThe principal content of the AWO is, in the first stage\\nat least, \\u0091intuitive\\u0092 knowledge about African wildlife.\\nThis subject domain originated from an early Semantic\\nWeb book ([8], Section 4.3.1) that was restructured and\\nextended slightly for its first, basic version; see Table 2\\nand Fig. 1. It has descriptions of typical wildlife animals,\\nsuch as Lion and Elephant, and what they eat, including\\nImpala (a type of antelope), and Twig or Leaf, respectively.\\nBasic extensions in the simple version of the ontology\\n(v1) include plant parts, so as to demonstrate parthood\\nand its transitivity, and carnivore vs. herbivore, which\\nmake it easy to illustrate disjointness, subsumption rea-\\nsoning, and unsatisfiable classes, and carnivorous plants\\nto demonstrate logical consequences of declaring domain\\nand range axioms 5. Most elements have been annotated\\nwith informal descriptions, and several annotations link to\\ndescriptions on Wikipedia.\\nLike the aforementioned Family History ontology, there\\nare several versions of the AWO that reflect different\\nstages of learning. In the case of the AWO, this is not\\nspecifically with respect to OWL language features, but\\none of notions of ontology quality and where one is in\\nthe learning process. For instance, version 1a contains\\nanswers to several competency questions\\u0097i.e., quality\\n5in casu, declaring eats too restrictively with as domain only Animal: then\\neither it will result in an unsatisfiable CarnivorousPlant (if Animal and Plant are\\ndeclared disjoint) or it will result in the undesirable deduction that\\nCarnivorousPlant \\004 Animal\\nrequirements that an ontology ought to meet [17]\\u0097that\\nwere formulated for Exercise 5.1 in the \\u0093Methods and\\nmethodologies\\u0094 chapter of [16]. Versions 2 and 3, on\\nthe other hand, have the AWO aligned to the DOLCE\\nand BFO foundational ontologies, respectively, whose dif-\\nferences and merits are discussed in Chapter 6 of the\\ntextbook, ensuring discussion of refinements in ontologi-\\ncal precision with, e.g., processes and dispositions (e.g., an\\nEating class with participating objects cf. an eats object\\nproperty). Their respective versions with the answers to\\nthe related exercises have the name appended with an \\u0091a\\u0092\\nas well. Version 4 has some contents \\u0091cleaned up\\u0092, par-\\ntially based on what the OOPS! tool [14] detected; it uses\\nmore advanced language features; and takes steps in the\\ndirection of adhering to science more precisely with finer\\ngranularity, such as type of carnivores and distinguishing\\nbetween types of roots.\\nThere are also four versions in different natural lan-\\nguages, being in isiZulu, Afrikaans, Dutch, and Spanish,\\nwhich mainly serve the purpose of illustrating issues with\\nmultilingual settings of ontology use, which relates to\\ncontent in Chapter 9 of the textbook.\\nAWO against the requirements\\nThe AWO meets most of the requirements (see Table 3).\\nConcerning the subject domain, the content is general,\\nversatile, not wrong, sufficiently international, and not\\nrepetitive (Items 1-4). The AWO includes the core rela-\\ntion of parthood for, especially, plants and their parts, with\\noptional straightforward extensions with the participation\\nrelation (e.g., animals participating in a Chasing event)\\nand membership (animal collectives, such as Herd; see v4\\nof the AWO), therewithmeeting Item 5. Representation of\\nrelevant domain knowledge beyond Description Logics-\\nbased OWL species (Item 6) could include information\\nabout temporal segregation of foraging or commensal-\\nism, inclusion of species with distinct successive phases\\nwith substantial morphological changes (e.g., Caterpil-\\nlar/Butterfly), and the notion of rigidity between what an\\nobject is and the role it plays (e.g., Lion playing the role of\\nPredator; see v4 of the AWO). The subject domain is also\\nfertile ground for exceptions that may be represented with\\nnon-monotonic logics; typical examples are that, gener-\\nally, birds fly and plants have chlorophyl, but not all of\\nthem (e.g., the penguin and the dodder, respectively). Use\\ncase scenarios (Item 7) may be, among others, science\\nof African wildlife, activism on endangered species, and\\napplications such as a database integration and manage-\\nment system for zoos and for tourism websites.\\nRegarding the logics and reasoning, the AWO is rep-\\nresented in OWL [19], and thus has ample tooling sup-\\nport for knowledge representation, reasoning, and basic\\ndebugging/explanation, with ontology development envi-\\nronment tools such as Protégé (Items I-III). The AWO\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 7 of 11\\nTable 2 AWO ontologies, with their main differences\\nFile name Difference\\nAfricanWildlifeOntology.xml This is the file from http://www.iro.umontreal.ca/~lapalme/ift6281/OWL/AfricanWildlifeOntology.xml,\\nthat was based on the description in [8]\\nAfricanWildlifeOntologyWeb.owl AfricanWildlifeOntology.xml + changed the extension to .owl and appended the name\\nwith Web. This ontology gave at the time (in 2010) a load error in the then current version of Protégé\\ndue to the use of Collection in the definition of Herbivore\\nAfricanWildlifeOntology0.owl AfricanWildlifeOntologyWeb.owl + that section on Collection removed\\nAfricanWildlifeOntology1.owl AfricanWildlifeOntology0.owl + several classes and object properties were added (up to\\nSRI DL expressiveness), more annotations, URI updated (described in Example 4.1 in [16])\\nAfricanWildlifeOntology1a.owl AfricanWildlifeOntology1.owl + new content for a selection of the CQs in Exercise 5.1 in\\n[16] (its CQ5, CQ8) and awo_12 of the CQ dataset [18])\\nAfricanWildlifeOntology2.owl AfricanWildlifeOntology1.owl + OWL-ised DOLCE (Dolce-Lite.owl) was imported\\nand aligned\\nAfricanWildlifeOntology2a.owl AfricanWildlifeOntology2.owl + answers to the questions in Example 6.2 in [16] on\\nfoundational ontology alignment\\nAfricanWildlifeOntology3.owl AfricanWildlife'</span></dd><dt>text5</dt><dd><span style=white-space:pre-wrap>'Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 \\nhttps://doi.org/10.1186/s13326-020-00221-1\\nRESEARCH Open Access\\nNeural side effect discovery from user\\ncredibility and experience-assessed online\\nhealth discussions\\nVan-Hoang Nguyen* , Kazunari Sugiyama, Min-Yen Kan and Kishaloy Halder\\nAbstract\\nBackground: Health 2.0 allows patients and caregivers to conveniently seek medical information and advice via\\ne-portals and online discussion forums, especially regarding potential drug side effects. Although online health\\ncommunities are helpful platforms for obtaining non-professional opinions, they pose risks in communicating\\nunreliable and insufficient information in terms of quality and quantity. Existing methods in extracting user-reported\\nadverse drug reactions (ADRs) in online health forums are not only insufficiently accurate as they disregard user\\ncredibility and drug experience, but are also expensive as they rely on supervised ground truth annotation of\\nindividual statement. We propose a NEural ArchiTecture for Drug side effect prediction (NEAT), which is optimized on\\nthe task of drug side effect discovery based on a complete discussion while being attentive to user credibility and\\nexperience, thus, addressing the mentioned shortcomings. We train our neural model in a self-supervised fashion\\nusing ground truth drug side effects from mayoclinic.org. NEAT learns to assign each user a score that is\\ndescriptive of their credibility and highlights the critical textual segments of their post.\\nResults: Experiments show that NEAT improves drug side effect discovery from online health discussion by 3.04%\\nfrom user-credibility agnostic baselines, and by 9.94% from non-neural baselines in term of F1. Additionally, the latent\\ncredibility scores learned by the model correlate well with trustworthiness signals, such as the number of \\u0093thanks\\u0094\\nreceived by other forum members, and improve credibility heuristics such as number of posts by 0.113 in term of\\nSpearman\\u0092s rank correlation coefficient. Experience-based self-supervised attention highlights critical phrases such as\\nmentioned side effects, and enhances fully supervised ADR extraction models based on sequence labelling by 5.502%\\nin terms of precision.\\nConclusions: NEAT considers both user credibility and experience in online health forums, making feasible a\\nself-supervised approach to side effect prediction for mentioned drugs. The derived user credibility and attention\\nmechanism are transferable and improve downstream ADR extraction models. Our approach enhances automatic\\ndrug side effect discovery and fosters research in several domains including pharmacovigilance and clinical studies.\\nKeywords: Online health communities, Drug side effect discovery, Credibility analysis, Deep learning, Natural\\nlanguage processing\\n*Correspondence: vhnguyen@u.nus.edu\\nSchool of Computing, National University of Singapore, 13 Computing Drive,\\n117417 Singapore, Singapore\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 2 of 16\\nBackground\\nSeeking medical opinions from online health commu-\\nnities has become popular: 71% of adults aged 18\\u009629\\n(equivalent to 59% of all U.S. adults) reported consulting\\nonline health websites for opinions [1]. These opinions\\ncome from an estimated twenty to one hundred thousand\\nhealth-related websites [2], inclusive of online health com-\\nmunities that network patients with each other to pro-\\nvide information and social support [3]. Platforms such\\nas HealthBoards1 and MedHelp2 feature users report-\\ning their own health experiences, inclusive of their self-\\nreviewed drugs and medical treatments. Hence, they are\\nvaluable sources for researchers [4, 5].\\nAlthough patients use these platforms to access valuable\\ninformation about drug reactions, there are challenges\\nto their effective, large-scale use. There is lexical varia-\\ntion where users describe the same side effect differently.\\nFor example, dizziness can be expressed as giddiness or\\nmy head is spinning, posing difficulty to most feature-\\nbased or keyword matching approaches. Separately, there\\nare valid concerns regarding credibility of user-generated\\ncontents to be harvested at large in which research has\\nshown to be of variable quality and should be approached\\nwith caution [6\\u00969]. One proxy indicator for information\\nquality is the author\\u0092s trustworthiness [10]. In the con-\\ntext of social media or online forums, user trustworthiness\\nis often approximated via ratings from other users, i.e.,\\nnumber of thanks or upvotes [11], or via their consistency\\nof reporting credible information [12, 13]. In addition to\\ncredibility, forum members also offer expertise thanks to\\ntheir own experience \\u0096 with prescriptions in particular \\u0096\\nand facilitate responses to drug queries [14]. For instance,\\nwhile reporting expected side effects for a specific treat-\\nment, patients with long-term use of certain drugs can be\\na complementary source of information:\\nWhile my experience of 10 years is with Paxil, I expect that Zoloft will be\\nthe same. You should definitely feel better within 2 weeks. One way I found to\\nmake it easier to sleep was to get lots of exercize [sic]. Walk or run or whatever\\nto burn off that anxiety. \\u0096 User 3690.\\nThe above is an answer to a thread asking for expected\\nside effects for depression treatment with Zoloft.\\nUser 3690\\u0092s history of active discussion on other anti-\\ndepressants such as Lexapro and Xanax lends credibil-\\nity to them being an authority on depression treatments.\\nWe noticed that Zoloft (mentioned in the thread)\\nshares many common side effects with the other two\\nanti-depressants: \\u0093changed behavior,\\u0094 \\u0093dry mouth,\\u0094 and\\n\\u0093sleepiness or unusual drowsiness.\\u0094 as illustrated in Table 1.\\nMany such examples suggest that drugs which are often\\nprescribed together for the same treatment, such as anti-\\ndepressants, are likely to be discussed within a same\\n1https://www.healthboards.com/\\n2https://medhelp.org/\\nTable 1 Side effects of anti-depressants\\nDrugs Side effects\\nLexapro chills, constipation, cough, decreased appetite, decreased\\nsexual desire, diarrhea, drymouth, joint pain, muscle\\nache, tingling feeling, sleepiness or unusual\\ndrowsiness, unusual dream, sweating, ...\\nXanax abdominal or stomach pain, muscle weakness , changed\\nbehavior, chills, cough, decreased appetite, decreased\\nurine, diarrhea, difficult bowel movement, cough, dry\\nmouth, tingling feeling, sleepiness or unusual\\ndrowsiness, slurred speech, sweating, yellow eye,..\\nZoloft changed behavior, decreased sexual desire, diarrhea,\\ndrymouth, heartburn, sleepiness or unusual\\ndrowsiness, sweating,..\\nThe Drugs and Side effects columns respectively list the anti-depressants and their\\nside effects extracted from a drug\\u0096side effect database. Side effects in common\\namong those listed are bold\\nthread and share common side effects. In addition, users\\nwho have experienced certain drug reactions are more\\noutspoken and active on those discussions involving drugs\\nof similar side effects. These signals arise from the rich\\ncontext of online health information; hence, we expect\\nsystems to explore beyond individual statements. Specif-\\nically, they should consider the complete discussion con-\\ntent as well as the global experience of each involved users,\\nin order to discover drug side effects or extract adverse\\ndrug reactions (ADRs).\\nWe argue that modeling user expertise from experi-\\nenced side effects is more robust compared against gen-\\neral user profile and engagement features [13, 14], as user\\nexpertise provides more meaningful signals for side effect\\ndiscovery. To the best of our knowledge, there is no pre-\\nvious work that incorporates user expertise in side effect\\ndiscovery in discussion forums at either the thread or\\npost level. In this work, given online health discussions,\\nwe propose a novel end-to-end neural architecture that\\njointly models each author\\u0092s credibility, their global expe-\\nrience and their post\\u0092s textual content to discover the\\nside effect of unseen drugs. We optimize the model on\\na self-supervised task of predicting side effect of men-\\ntioned drugs for complete threads, where ground truth\\nis accessible. Our key observation is that users can be\\ngrouped into clusters that share the same expertise or\\ninterest in certain drugs, possibly due to their common\\ntreatment or medical history. We incorporate this crit-\\nical observation into our user model in representing a\\npost\\u0092s content via a cluster-sensitive attention mechanism\\n[15]. We also follow general definition of truth discov-\\nery and let the model learn a credibility score that is\\nunique to every user and descriptive of their trustworthi-\\nness. Our experiments include an overall ablation study\\nto validate the significance of each model component.\\nThis paper extends our former work [16] by conducting\\na correlation study that analyzes the representativeness of\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 3 of 16\\nlearned credibility scores and a comparison between our\\nself-supervised attention-based approach and traditional\\nsupervised sequence labeling approaches on side effect\\nmention extraction.\\nWe summarize our contributions as follows:\\n\\u0095 We propose a NEural ArchiTecture, NEAT, that\\ncaptures 1) user expertise and 2) credibility, 3) the\\nsemantic content of individual posts and 4) the\\ncomplete discussion thread, to improve side effect\\ndiscovery from online health discussions. NEAT\\u0092s\\nmain means of user credibility and experience\\nassessment can be easily adopted by various neural\\nattentional encoders [17, 18].\\n\\u0095 We formulate a self-supervised task of side effect\\nprediction of mentioned drugs for the proposed\\nnetwork to jointly optimize its components.\\n\\u0095 We conduct experiments to verify the validity of our\\nlearned credibility and the robustness of\\nself-supervised attention-based extraction,\\ncomparing against traditional supervised sequence\\nlabeling baselines.\\nRelated work\\nWe first review existing approaches to drug side effect\\ndiscovery from health forums and social media. Next, we\\nexamine how these works incorporate user credibility and\\nexpertise in their learning objective. Finally, we justify\\nour choice of neural architecture by discussing its mod-\\neling capability of context-rich structures such as online\\ndiscussion.\\nDrug Side Effect Discovery. Existing methods for drug\\ndiscovery from online content extract drugs at post and\\nstatement level. ADR mining systems typically include a\\nnamed entity recognition (NER) model and a relationship\\nor semantic role labeling model [19, 20]. Recent neu-\\nral approaches address lexical variation in user-generated\\ncontent \\u0096 the difficulty faced by traditional keyword\\nmatching and rule-based approaches \\u0096 to improve recog-\\nnition and labeling components [21, 22]. Distributed word\\nrepresentations [23, 24] constructed from context can\\ncapture semantics based on the hypothesis that syn-\\nonyms often share similar contextual words. For example,\\n\\u0093headache\\u0094 and \\u0093cephalea\\u0094 will have close representations\\nif they share contextual words such as \\u0093head\\u0094 or \\u0093pain\\u0094.\\nApproaches to sub-word embedding [25, 26] model the\\nmorphology of words by leveraging sub-word or charac-\\nter information. These representations are naturally inte-\\ngrated into neural sequential models [17, 18, 27] that are\\nsensitive to syntactic order. However, supervised sequence\\nlabeling or mention extraction approaches require labo-\\nrious annotations at the word (token) level, and are\\nonly capable of discovering side effects that are explic-\\nitly present in the text. Expert supervision or additional\\nsemantic matching models are also required to map such\\nrecognized text segments to standardized vocabularies or\\nthesaurii [28]. In contrast, our proposed self-supervised\\ntask formulation discovers the aggregated side effects of\\nmentioned drugs for each community discussion by con-\\nsidering the whole thread\\u0092s content. The list of discussed\\ndrugs are tagged by forummoderators or obtained by pat-\\ntern matching. This learning design not only effectively\\nalleviates the need for expensive, finer-grained annota-\\ntions but also allows for the prediction of side effects not\\nexplicitly mentioned in the discussion.\\nUser Credibility and Expertise Integration. Credi-\\nbility is of the utmost concern in large-scale knowl-\\nedge harvesting [8, 29, 30]. Previous work on side effect\\ndiscovery from individual statements or posts derive\\ninformation credibility by verifying a statement\\u0092s men-\\ntioned side effects against ground truth drug side effect\\ndatabases, and assess associated user credibility by mea-\\nsuring the percentage of a user\\u0092s credible statements\\n[13, 31]. In contrast, our approach to side effect discov-\\nery from discussions by jointly modeling multiple posts\\nand authors eschews the assessment of statement cred-\\nibility and derives user credibility differently. We assign\\neach user a positive score that is used to weight their\\npost content in representing the discussion\\u0092s holistic con-\\ntent. Suchweighted summation is detailedmathematically\\nin Appendix 1 to conform to the general principle of\\ntruth discovery, where sources providing credible infor-\\nmation should be assigned higher credibility scores, and\\nthe information that is supported by credible sources will\\nbe regarded as true [10]. Although our dataset does not\\nprovide any ground truth for user trustworthiness, we fol-\\nlowed the previous usage of ratings or upvotes in online\\nforums and adopted the number of \\u0093thanks\\u0094 received from\\nother forum members [11] as our proxy for user trust-\\nworthiness. Previous works have modeled user expertise\\nbased on user profiles such as demographics; activity\\nfeatures such as posting frequency and posting pattern\\nthrough time series and network analysis [13, 14]. As\\nshown in an earlier example in Section 1, modeling user\\nexpertise from previously experienced side effects better\\ncaptures author authoritativeness for certain side effects.\\nIt is also universally applicable to any online platform.\\nModeling Online Discussion Content and Structure\\nAs our work makes use of the rich topographical proper-\\nties of online communities, we briefly review approaches\\nfor modeling textual content and post-thread discus-\\nsion structure. Previous works use probabilistic graphical\\nmodels implicitly to represent textual content (especially,\\ntopicmodeling) as bags-of-words [28, 32] or inventories of\\nstylistic and linguistic features [13]. Such lightweight rep-\\nresentation are well-suited in moderately short contexts,\\ni.e., sentences or posts. However, in terms of modeling\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 4 of 16\\nlong discussions consisting of multiple posts, state-of-the-\\nart models for Community Question Answering (CQA)\\nfeature hierarchical neural architectures [33\\u009635]. In term\\nof encoding text, sequential encoders such as Long Short-\\nTermMemory (LSTM) [36] or Convolutional Neural Net-\\nworks (CNN) [18] are capable of encoding long-term\\ndependencies and semantic expressiveness by leverag-\\ning word embeddings. In terms of encoding hierarchical\\nstructures such as community discussions consisting of\\npost- and thread-level features, neural architectures allow\\nfor straightforward and efficient integration of multiple\\nlearning objectives. In addition, our neural architecture,\\nNEAT, incorporates attention mechanism that focuses on\\nessential phrases while encoding post content, and joint\\nuser credibility learning while optimizing for the side\\neffect discovery objective.\\nMethods\\nBasic Terminology. To ensure a consistent representa-\\ntion, we define some terms and formalize them as follows:\\n\\u0095 A drug d has a set of side effects,\\nSd = {s1, s2, . . . , s|Sd|}\\u0095 A post p is a message in online forums and contains a\\nsequence of words. Each post p belongs to the set of\\nall online forum posts P and is written by a user u\\nand belongs to a thread t.\\n\\u0095 A user u is a member of an online forum and\\nparticipates in a list of threads, i.e.,\\nTu = {t1, t2, . . . , t|Tu|} by writing at least one post in\\neach thread. We use the terms user and author, as\\nwell as user experience and user expertise\\ninterchangeably. Each user belongs to the set of all\\nonline forum users U and is characterized by their\\ncredibility and expertise. Credibility wu of user u\\nreflects the probability of user u provide trustworthy\\nor helpful information, and is approximated from the\\nnumber of \\u0093thanks\\u0094 given from other forummembers.\\n\\u0095 A thread t (see Table 2) is an ordered collection of\\npost\\u0096user pairs,\\nQt = {(p1,u1) , (p2,u2) , . . . ,\\n(\\np|Qt |,u|Qt |\\n)}.\\nEach thread discusses the treatment for a particular\\ncondition and entails a list of prescribed drugs\\nDt = {d1, d2, . . . , d|Dt |}. Hence, every thread has a list\\nof aggregated potential side effects defined as\\nSt = Sd1 ? Sd2 · · · ? Sd|Dt | .\\nTask Definition. Drug side effect discovery from discus-\\nsions is the task of assigning the most relevant subset of\\npotential side effects to threads discussing certain drugs,\\nfrom a large collection of side effects. We view the drug\\nside effect discovery problem as a multi-label classifica-\\ntion task. In our setting, an instance of item\\u0096label is a\\ntuple\\n(xt , y\\n)\\nwhere xt is the feature vector of thread t\\nderived from its list of post\\u0096user pairs Qt and y is the side\\neffect label vector i.e., y ? {0, 1}|S|, where |S| is the number\\nof possible side effect labels. Given training instances, we\\ntrain our classifier to predict the list of drug side effects in\\nunseen threads discussing unseen drugs.\\nFormal Hypothesis. Given a thread t with Qt , we\\nhypothesize that considering the credibility and experi-\\nence of user u ? (p,u) ? Qt improves the quality of feature\\nrepresentation in thread t, resulting in better drug side\\neffect discovery performance.\\nSelf-supervised Drug Side Effect Discovery. We pro-\\npose a self-supervised learning objective. Instead of\\nrelying on the identical and independently distributed\\nassumption of fully supervised learning, we construct\\nthe dataset from threads that can discuss a set of com-\\nmon drugs. We look up the side effects of these men-\\ntioned drugs via a drug\\u0096side effect medical database\\nobtained from Mayo Clinic portal. Our self-supervised\\ntask explores discussion-based side effect discovery which\\nalleviates the need for finer-grain annotation compared\\nagainst existing approach of statement-based side effect\\ndiscovery. We also propose our neural architecture,\\nTable 2 A sample discussion thread from an online health community\\nUser IDs Posts Mentioned drugs Aggregated side effects\\n3690 While my experience of 10 years is with Paxil,\\nI expect that Zoloft will be the same. You\\nshould definitely feel better within 2 weeks.\\nOne way I found to make it easier to sleep\\nwas to get lots of exercize. Walk or run or\\nwhatever to burn off that anxiety.\\nZoloft, Paxil changed behavior, decreased sexual desire,\\ndiarrhea, dry mouth, heart-burn, sleepiness\\nor unusual drowsiness,...\\n26521 I\\u0092ve heard of people going \\u0093cold turkey\\u0094 and\\nhaving withdrawal at 6 months! Please, get\\nin contact with a doctor ASAP! \\u0093common\\nsymptoms include dizziness, electric shock-\\nlike sensations, sweating, nausea, insomnia,\\ntremor, confusion, nightmares and vertigo\\u0094\\nThe User IDs and Posts columns respectively list the IDs of users involved in the discussions and their messages. The Mentioned drugs and Aggregated side effects columns\\nrespectively list the explicitly discussed drugs and their combined side effects\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 5 of 16\\nNEAT, that jointly models user credibility, expertise and\\ntext content with attention while optimizing for the self-\\nsupervised objective. The network has three major com-\\nponents: 1) user expertise representation with rich multi-\\ndimensional vectors; 2) cluster-sensitive attention being\\ncapable of focusing on relevant phases for post con-\\ntent encoding improvement; and 3) credibility weighting\\nmechanism which effectively learns to assign credibility\\nscore to each user, based on their content. We discuss its\\nimplementation in the following sections. Figure 1 shows\\nthe detailed network architecture of our model.\\nUser Expertise Representation (UE). We embed each\\nuser u ? U as a vector vu so that the vector captures user\\nu\\u0092s experience with certain side effects. As each user u par-\\nticipates in the threads Tu, entailing a list of experienced\\nside effects, we derive user side effect experience vector\\nv?u ? R|S| where S is the set of all possible side effects\\nand v?ui = nui where user u has discussed ith side effect\\nin nui threads. We obtain a user drug experience matrix\\nM? ? R|U|×|S| where jth row of M? denotes user side\\neffect experience vector of jth user. To avoid learning from\\nsparse multi-hot encoded representations and to improve\\nFig. 1 The neural architecture of our proposed NEAT. The wu and vu boxes denote Credibility Weight (CW) component and User Expertise (UE)\\ncomponent. The yellow boxes and blue boxes denote Cluster Attention (CA) component and neural text encoders with attention. The highlighted\\nwords in red denoted the text segments that are being attended by the encoder. The ×, ?, and ? symbols denote the multiplication, summation,\\nand sigmoid, respectively\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 6 of 16\\nthe model\\u0092s scalability with the number of side effects, we\\nperform dimensionality reduction, specifically principal\\ncomponent analysis (PCA) [37], to our experience matrix\\nM? obtained from training set. Figure 2 shows percentage\\nof variance explained versus number of included principal\\ncomponents. Since our PCA plots do not show signifi-\\ncant improved percentage of variance explained beyond\\n100 components, we use g = 100 components, reduc-\\ning our original M? ? R|U|×|S| to user expertise matrix\\nM ? R|U|×g .\\nUser Cluster Attention (CA).We make an assumption\\nvia observations that users in online health communities\\ncan be effectively grouped into clusters based on their\\nprevious side effect experience. The advantages of clus-\\ntering users is twofold: First, since users in the same\\nclusters share certain parameters, they are jointly mod-\\neled and more active forum members leverage less active\\nones. Second, clustering efficiently reduces the number\\nof parameters to learn and improves optimization and\\ngeneralization. We apply K-means \\u0096 a distance-based\\nunsupervised clustering algorithm [38] \\u0096 to binary-valued\\nuser experience vectors v?u after normalization. By using\\ncosine similarity, the algorithm effectively groups users\\nwith a high number of co-occurred side effects in the same\\ncluster. To determine the number of clusters c, we plot\\nthe silhouette scores against the number of clusters and\\nobserve the sharp drop after c = 7 (Fig. 3). The average\\nsilhouette score is 0.57 for our choice of c = 7, indi-\\ncating that users are moderately matched to their own\\ngroups and separated from other groups. The top 5 most\\ncommon side effects in each clusters are shown in Table 3.\\nIn the larger domain of natural language processing,\\nattention has become an integral part for modeling text\\nsequences [39, 40]. By learning to focus on essential text\\nsegments, attention allows text encoders to capture long\\nterm semantic dependencies with regard to auxiliary con-\\ntextual information [41, 42]. In our related task of ADR\\nmentions extraction, attention has been adopted recently\\nin neural sequence labelling models [21, 43], resulting\\nin promising improvement. Inspired by the concept, we\\nenhance text encoding with user expertise attention. Even\\nthough the attention is adjusted to the non-extractive self-\\nsupervised task of thread-level drug side effect discovery,\\nwe hypothesize that our model learns to highlight the\\nmentioned accurate side effects, and can be used as a\\nself-supervised baseline for side effect extraction. Based\\non the previously obtained clustering results, we assign a\\nlearnable cluster attention vector for each user group and\\nincorporate their expertise into the text encoding process.\\nPost Content Encoding. NEAT takes the content of a\\nthread t as input, which is a list of post\\u0096user pairs Qt .\\nPost pi of pair (pi,ui) ? Qt consists of a sequence of\\nwords xpi = {w1, . . . ,wn} with length n. We seek to rep-\\nresent a post pi as a vector vp that effectively captures\\nits semantics through an encoding function f (xpi) mod-\\neled by a neural text encoding module (the blue boxes\\nin Fig. 1). We embed each word into a low dimensional\\nvector and transform the post into a sequence of word\\nvectors {vw1 , vw2 , . . . , vwn}. Each word vector is initialized\\nusing pre-trained GloVe [24] embeddings, and each out-\\nof-vocabulary word vector is initialized randomly. We\\nmake use of modularity \\u0096 a major advantage of neural\\nFig. 2 Principal component analysis on user experience vectors. The horizontal axis denotes the number of principal components chosen for PCA,\\nwhile the vertical axis denotes their percentage of variance explained. We notice that the percentage of variance explained does not increase\\nsignificantly after 100 principal components\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 7 of 16\\nFig. 3 Silhouette scores for User Clustering. The horizontal axis denotes the number of clusters chosen for K-means clustering, while the vertical axis\\ndenotes their the silhouette scores. We notice that the silhouette scores drop sharply after 7 clusters.\\narchitectures \\u0096 and design the post content encoder as\\na standalone component that can be easily updated with\\nany state-of-the-art text encoder. In this work, we pro-\\nvide two neural text encoders: long-short term memory\\n(LSTM, see Fig. 4) [36] and convolutional neural net-\\nworks (CNN, see Fig. 5) [18], both of which incorporates\\nattention mechanism.\\nA bi-directional LSTM encodes the word vector\\nsequence and outputs two sequences of hidden states: a\\nforward sequence, Hf = hf1,hf2, . . . ,hfn that starts from\\nthe beginning of the text; and a backward sequence,Hb =\\nhb1,hb2, . . . ,hbn that starts from the end of the text. Formany\\nsequence encoding tasks, knowing both past (left) and\\nfuture (right) contexts has proven to be effective [44]. The\\nstates hfi ,hbj ? Re of the forward and backward sequences\\nare computed as follows:\\nhfi = LSTM(hfi?1, vwi), hbj = LSTM(hbj+1, vwj),\\nwhere e is the number of encoder units, and hfi ,hbj are the\\nith and jth hidden state vector of the forward (f ) and back-\\nward (b) sequence. We derive the cluster attention vector\\nas vai ? Re for each user ci, from which the weights of\\neach hidden state hfj and hbj based on their similarity with\\nthe attention vector are:\\nwaj =\\nexp(vaihj)?n\\nl=1 exp(vaihl)\\n. (1)\\nThe intuition behind Eq. (1), inspired by Luong et al.\\n[39], is that hidden states which are similar to the atten-\\ntion vector vai should be paid more attention to; hence\\nare weighted higher during document encoding. vai is\\nadjusted during training to capture hidden states that are\\nsignificant in forming the final post representation. waj\\nis then used to compute forward and backward weighted\\nfeature vectors:\\nhf =\\nn?\\nj\\nwajh\\nf\\nj , hb =\\nn?\\nj\\nwajhbj . (2)\\nWe concatenate the forward and backward vectors to\\nobtain a single vector, following previous bi-directional\\nLSTM practice [45].\\nTable 3 Most common experienced side effects for each user\\ncluster ci (i = 1 to 7)\\nCluster Most common experienced side effects\\nc1 vision blurred, yellow skin, vision double, yellow eye,\\nnose stuffy\\nc2 headache, itch, stomach pain, weak, nausea\\nc3 itch, irritate, headache, pain abdominal, stomach\\ncramp\\nc4 bad taste, nausea, tiredness, irritate, mouth ulcer\\nc5 skin red, itch, rash skin, skin peeling, burning skin\\nc6 sneezing, nose runny, nose stuffy, decrease sexual\\ndesire, pain breast\\nc7 nausea, stomach pain, vomit, diarrhea, pain\\nabdominal\\nThe left column lists the names of 7 clusters, and the right column describes the\\nmost common experienced side effects of users in each cluster\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 8 of 16\\nFig. 4 LSTM-based encoder with cluster attention. The × and + cells denote the attention-weighted summation described in Eq. (2). The C cell\\ndenotes the concatenation of the forward, hf , and backward, hb , hidden states\\nOur choice of CNN-based encoder is based on prior\\nwork [18, 46]. A convolution block k consists of two sub-\\ncomponents: a convolution layer and a cluster attention\\nlayer. In the convolution layer, a kernel of window s\\n(0 &lt; s &lt; n) of weight W is used to generate\\nthe hidden representation hkj for the word embeddings\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 9 of 16\\nFig. 5 CNN-based Encoder with Cluster Attention. The × and + cells denote the attention-weighted summation described in Eq. 2. The C cell\\ndenotes the concatenation of the final hidden states of K convolution blocks\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 10 of 16\\n{vwi?s+1 , · · · , vwi} as:\\nhkj = CONV (W , {vwi?s+1 , · · · , vwi}) (3)\\nwhere CONV (·) is the convolution operation described\\nin [18]. In the cluster attention layer, we first derive the\\nattention weight waj for each hidden representation hkj\\nsimilarly to the LSTM-based encoder. Attention weighted\\npooling is used to obtain the convolution block output as\\nfollows:\\nhk =\\nn?\\nj\\nwajhkj (4)\\nSince we use multiple convolution blocks of different\\nkernel sizes, the final post representation is the concate-\\nnation of K block outputs hk .\\nThread Content Encoding with Credibility Weights\\n(CW). For every post\\u0096user pair (pi,ui) at thread t, we\\nfirst compute feature vector vpi for post pi. NEAT then\\nconcatenates this post\\u0096user representation with user ui\\u0092s\\nexpertise vector vui to form post\\u0096user complex vector vpui .\\nThis post\\u0096user complex is weighted by a user credibil-\\nity ewui , where wui initially set to 0 per user and updated\\nwhile training for the self-supervised side effect discovery\\nobjective. We implement credibility learning according to\\nthe general intuition from the truth discovery literature:\\nusers who give quality posts, on which the model can\\nsolely base to make correct predictions, are given a higher\\ncredibility. We also exploit this credibility score to encode\\nthe thread representation by placing emphasis on the con-\\ntent of credible users. A representation of a thread that\\nmeets the above description is the weighted sum of each\\npost\\u0096user complex vector:\\nvt =\\nn?\\ni=1\\nvp?ui =\\nn?\\ni=1\\newui vpui (5)\\nMulti-label Prediction:NEAT feeds the thread content\\nrepresentation vt through a fully connected layer whose\\noutputs can be computed as follows:\\nst = W tanh(vt) + b, (6)\\nwhere W and b are weights and biases of the layer. The\\noutput vector st ? R|S| is finally passed through a sigmoid\\nactivation function ?(·), and trained using cross-entropy\\nloss L defined as follows:\\nL = 1|T |\\n|T |?\\nt=1\\n{yt · log(? (st)) + (1 ? yt) · log(1 ? ?(st))}\\n+ ?1\\n??\\nu\\nv2u + ?2\\n?\\ni\\n|wui |\\n(7)\\nWe adopt regularization that penalizes the training loss\\nwith the user experience matrix\\u0092s L2 norm by a fac-\\ntor of ?1 and the user credibility vector wu\\u0092s L1 norm\\nby a factor of ?2. The loss function is differentiable,\\nthus trainable with the Adam optimizer [47]. During\\nour gradient-based learning, user ui\\u0092s credibility score\\nwui is updated by calculating ?L?wui by back-propagation\\n(see Appendix 1).\\nResults\\nWe conduct experiments to validate the effectiveness of\\nour proposed model. We design an ablation study to high-\\nlight the effectiveness of each component of NEAT in\\nour self-supervised side effect prediction. In addition, we\\nexpand our previous work [16]. More specifically,\\n1. We verify the representativeness of the learned\\ncredi'</span></dd><dt>text6</dt><dd><span style=white-space:pre-wrap>'Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 \\nhttps://doi.org/10.1186/s13326-020-00223-z\\nRESEARCH Open Access\\nEnabling ad-hoc reuse of private data\\nrepositories through schema extraction\\nLars Christoph Gleim1* , Md Rezaul Karim1,2, Lukas Zimmermann3, Oliver Kohlbacher3,4,5,6,7,\\nHolger Stenzhorn3,8, Stefan Decker1,2 and Oya Beyan1,2\\nAbstract\\nBackground: Sharing sensitive data across organizational boundaries is often significantly limited by legal and ethical\\nrestrictions. Regulations such as the EU General Data Protection Rules (GDPR) impose strict requirements concerning\\nthe protection of personal and privacy sensitive data. Therefore new approaches, such as the Personal Health Train\\ninitiative, are emerging to utilize data right in their original repositories, circumventing the need to transfer data.\\nResults: Circumventing limitations of previous systems, this paper proposes a configurable and automated schema\\nextraction and publishing approach, which enables ad-hoc SPARQL query formulation against RDF triple stores\\nwithout requiring direct access to the private data. The approach is compatible with existing Semantic Web-based\\ntechnologies and allows for the subsequent execution of such queries in a safe setting under the data provider\\u0092s\\ncontrol. Evaluation with four distinct datasets shows that a configurable amount of concise and task-relevant schema,\\nclosely describing the structure of the underlying data, was derived, enabling the schema introspection-assisted\\nauthoring of SPARQL queries.\\nConclusions: Automatically extracting and publishing data schema can enable the introspection-assisted creation of\\ndata selection and integration queries. In conjunction with the presented system architecture, this approach can\\nenable reuse of data from private repositories and in settings where agreeing upon a shared schema and encoding a\\npriori is infeasible. As such, it could provide an important step towards reuse of data from previously inaccessible\\nsources and thus towards the proliferation of data-driven methods in the biomedical domain.\\nKeywords: Semantic web, Linked data, RDF, SPARQL, Schema extraction, Privacy, Data access, Distributed systems,\\nQuery design, Personal health train, FAIR data\\nBackground\\nData-driven methods play an increasingly important role\\nfor cost-efficient and timely research results and effec-\\ntive decision support [2] throughout numerous domain\\nsuch as economics [3], education [4], manufacturing [5],\\nhealthcare and life sciences [6\\u00968].\\nAt the same time, the data that build the founda-\\ntion of these models oftentimes underlies strict sharing\\n*Correspondence: gleim@cs.rwth-aachen.de\\nThis work is an extended version of a paper previously published at the\\nSeWeBMeDA-2018 workshop [1].\\n1Informatik 5, RWTH Aachen University, Ahornstr. 55, 52062 Aachen, Germany\\nFull list of author information is available at the end of the article\\nrequirements. For example, in the sensitive healthcare\\ndomain, although first responders, hospitals, and many\\nother stakeholders already collect valuable data for data-\\ndriven research and treatment today, large portions of this\\ndata remain inaccessible to the majority of stakeholders\\n\\u0096 largely due to ethical, administrative, legal and political\\nhurdles that render data sharing infeasible [9]. In prac-\\ntice, this leads to an inability to access large amounts of\\ndata crucial for a variety of tasks such as the optimiza-\\ntion of decision support systems, first response systems\\nand data-driven research. At the core of this issue lies the\\nlack of an effective mechanism to allow for data access\\nin a legally certain, sustainable and cost-efficient manner\\nwithout extensive delays.\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 2 of 15\\nFor example, learning health systems, allowing for data-\\ndriven research on sensitive data such as electronic health\\nrecords (EHRs), have long been said to bear the poten-\\ntial to \\u0093fill major knowledge gaps about health care costs,\\nthe benefits and risks of drugs and procedures, geo-\\ngraphic variations, environmental health influences, the\\nhealth of special populations, and personalized medicine.\\u0094\\n[10]. While a variety of such systems have been proposed\\n[10\\u009613], practical implementation has so far not become\\na reality, likely due to the aforementioned hurdles.\\nIn order to enable data economy in privacy-sensitive\\ndomains and effective reuse of existing data and research,\\nnovel approaches are emerging to overcome these limi-\\ntations. One of those approaches is the Personal Health\\nTrain (PHT) framework [14], which aims to bring algo-\\nrithms and statistical models to data sources, rather than\\nsharing data with the third parties such as researchers.\\nThe main benefit of this approach is its ability of utilizing\\nall the data, including the sensitive and private informa-\\ntion, without data having to leave the original data source.\\nA key challenge of this approach is that data users (such as\\nresearchers) are required to develop their models without\\nhaving a grasp of the actual data. Unless there are univer-\\nsally agreed information models and data set descriptions,\\nthere is a need to create and communicate a schema \\u0096 that\\nis information about the structure of the data \\u0096 to enable\\nwriting queries for heterogeneous data resources.\\nThis work is embedded in our ongoing efforts support-\\ning data reuse in healthcare environments and conducted\\nas part of the SMITH [15] and DIFUTURE [16] projects.\\nThe key contributions of this paper consist of an auto-\\nmated approach for extracting task-relevant schema from\\nRDF data sources for the efficient formulation of data\\nselection and integration queries without direct access to\\nthe data and a corresponding integration with an infor-\\nmation system architecture that allows for the subsequent\\nevaluation of that query in a secure enclave.\\nIn the following, we describes some related work and\\nthe basic foundations of our approach. Subsequently, we\\noutline the motivation of our research, as well as the key\\nchallenges of schema extraction from sensitive data with-\\nout sacrificing privacy, followed by the description of our\\nproposed schema extraction approach from existing data\\nin the methods section. We then present a number of\\nevaluation results of the proposed data selection and inte-\\ngrationmethodology, based on the schema extracted from\\na sample use case. After a discussion of our results, we fin-\\nish with a conclusion of our results and a short outlook of\\ndirections for future work.\\nRelated work\\nIn order to facilitate knowledge discovery for both\\nhumans and machines, the FAIR data principles [17]\\nhave been proposed: A set of guiding principles to make\\nresearch and scientific data Findable, Accessible, Interop-\\nerable, and Re-usable. These guidance principles promise\\nto help in the discovery, access, integration and analysis of\\ntask-appropriate scientific data and associated algorithms\\nandworkflows. Thus, FAIR is gaining a lot of attention and\\nincreasing adoption.\\nCore to realizing these principles are Semantic Web\\nTechnologies [18], which provide a framework for data\\nsharing and reuse by making the semantics of data\\nmachine interpretable. Particularly the directed, graph-\\nbased data model RDF [19\\u009621] (built entirely upon the\\nnotion of statements, i.e. data in the form of subject\\npredicate object triples) in conjunction with formal\\nconceptualizations of information models, semantics and\\nencoding conventions in RDF vocabularies and ontologies\\ntakes an important role.\\nAs such, RDF Schema (RDFS) [22] and the Web Ontol-\\nogy Language (OWL) [23] provide a proven framework in\\norder to describe (but not necessarily enforce) the struc-\\nture and semantics of data. Substantially, RDFS introduces\\nthe concepts of classes and properties as well as basic rela-\\ntions between them. OWL \\u0096 a computational logic-based\\nlanguage \\u0096 extends upon these concepts in order to repre-\\nsent rich and complex knowledge about things, groups of\\nthings, and relations between them.\\nIn the context of this work, we use the term \\u0091schema\\u0092\\nto refer to the semantic and structural annotation of data\\nusing especially these two vocabularies.\\nOn the other hand, the classical notion of schema as\\nthe formal definition of the shape that data needs to com-\\nply with in order to be valid (i.e. schema validation and\\nenforcement) also exists in the Semantic Web with the\\nShape Expression Language (ShEx) [24] and the Shapes\\nConstraint Language (SHACL) [25]. At this time, there\\nare however no established ways of sharing data shapes\\nthrough public repositories and as such, in practice, they\\nare only adapted in isolated deployments.\\nNevertheless, using RDFS and OWL, it is possible to\\ncreate domain-specific, optionally interoperable vocabu-\\nlaries and ontologies, which may declare e.g. term or con-\\ncept equivalences and dependencies between each other\\nand subsequently enable interoperability across individual\\nencodings.\\nKey to realizing the semantics described in RDFS\\nand OWL vocabularies is the inference or entailment\\nof implicit knowledge (inferred triples) that follow from\\nexplicit knowledge (dataset triples) via the semantics\\ndescribed in the corresponding vocabularies. Figure 1\\nillustrates some of the inferred triples that follow from\\nthe formal RDFS and OWL entailment semantics [26].\\nHere we assume the namespaces ex and snomed to be\\ndefined1.\\n1Likely the definitions would be http://example.org/ and http://purl.\\nbioontology.org/ontology/SNOMEDCT/\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 3 of 15\\nFig. 1 Illustration of several effects of entailment support of a SPARQL endpoint\\nIn this example, only a small number of triples is con-\\ntained in the actual dataset, while the majority of knowl-\\nedge is inferred using RDFS and OWL semantics. Notably\\neach resource is either a class, a property or an individual,\\ni.e. either schema or data.\\nPopular examples of RDFS and OWL vocabularies\\ninclude the Ontology for Biomedical Investigation (OBI)\\n[27] in the biology and healthcare domain, the GoodRela-\\ntions ontology [28] in eBusiness and theDCAT vocabulary\\n[29], which is used for the general purpose metadata\\nannotation of datasets and data catalogs.\\nIn the context of eHealth systems, support for the\\nSemantic Web is becoming more and more promi-\\nnent with candidates such as the multilingual thesaurus\\nSNOMEDCT [30], ongoing research efforts into an RDF\\nspecification of HL7 FHIR [31], as well as the establish-\\nment of clear guidelines for dataset descriptions such as\\nthe HCLS Community Profile [32].\\nVarious high-quality catalogs of freely reusable vocabu-\\nlaries exist, allowing for the easy discovery of suitable ter-\\nminology to semantically annotate data. Examples include\\nthe Linked Open Vocabulary (LOV) [33\\u009635] and the Bio-\\nPortal [36\\u009638] project.\\nThe related idea of using schema export and import for\\nfederated data access date back to as early as 1985 [39] but\\nit is only recently that the idea has receivedmore attention\\nin the context of the Semantic Web.\\nKellou-Menouer et al. [40] propose a schema discov-\\nery approach based on hierarchical clustering instead of\\ndata annotations thus leading to an approximate schema.\\nFlorenzano et al. [41], Lohmann et al. [42, 43] and\\nDudá\\u009a et al. [44] introduce approaches focused on schema\\nextraction for visualization of the data structure but do\\nnot consider publishing or reuse of the extracted schema.\\nBenedetti et al. [45, 46] propose an interesting related\\napproach for schema extraction, visualization and query\\ngeneration but do not consider interoperability issues and\\nrely on custom mechanisms for schema storage.\\nMotivation\\nRecently, Jochems et al. [47] and Deist et al. [48]\\nintroduced two related promising Semantic Web-based\\napproaches in the context of the PHT initiative, founded\\non the key concept of bringing research to the data rather\\nthan bringing data to the research. As such the underly-\\ning information system architecture enables learning from\\nprivacy sensitive data without the data ever crossing orga-\\nnizational boundaries, maintaining control over the data,\\npreserving data privacy and thereby overcoming legal and\\nethical issues common to other forms of data exchanges.\\nThe general approach of this underlying system can be\\noutlined as follows:\\n1 Initially, both the client and data provider agree upon\\na set of attributes or features, such that all\\nparticipating data providers have corresponding\\nsources of (privacy sensitive) data.\\n2 Then each data provider encodes their data using an\\n(also agreed upon) ontology or vocabulary,\\nconverting it into RDF representation. This process\\nyields proper Linked Data [49] and thus enables\\nsemantic interoperability [50].\\n3 The resulting RDF data is deployed to a private triple\\nstore at each location, providing a private SPARQL\\n[51] query endpoint, which is not directly accessible\\nby the client.\\n4 A SPARQL data query is then formulated based on\\nthe previously agreed upon encoding and a\\ncorresponding distributable processing algorithm\\ndefined.\\n5 The shared query is then executed locally at each\\ndata provider against their respective triple stores\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 4 of 15\\nand the returned data processed using the\\ncorresponding algorithm.\\n6 The local results are then combined into a global one.\\n7 Depending on the approach, steps 5 and 6 may be\\nfurther iterated.\\nWhile these approaches \\u0096 introduced in the context\\nof the PHT initiative \\u0096 work well when multiple parties\\nagree on jointly collecting, encoding and evaluating data\\nin advance \\u0096 such as is the case for conducting individual\\ncoordinated studies \\u0096 they solve the issue of interoperabil-\\nity by agreeing on a single shared knowledge representa-\\ntion and encoding methodology a priori (steps 1-3 in the\\nabove process). In an optimal setting where agreeing on a\\nsingle shared and global information model and encoding,\\nreuse of diverse and existing data could always be directly\\naccomplished with this approach.\\nHowever, to our knowledge, so far all corresponding\\nefforts have been unsuccessful. At the time of writing\\nthe popular https://fairsharing.org/ portal indexes 1084\\ndatabases using 1183 standards, suggesting that in prac-\\ntice, each collected dataset and domain much rather tends\\nto introduce its own encoding methodology.\\nAdditionally, RDF datasets de facto often combine terms\\nfrom multiple vocabularies and ontologies, sometimes\\ndeviating from the originally intended information mod-\\nels and encodings.\\nThus when trying to reuse diverse existing data, a proper\\nunderstanding of the real structure of the available data \\u0096\\ni.e. the schema of the data \\u0096 is indispensable. For a client\\nwithout direct access to the data, this information is how-\\never typically not available, since its acquisition inherently\\nrelies upon inspection of the structure of the data.\\nApproaches, such as the PHT, depend upon ad-hoc\\ndata selection and integration facilities (step 4 of the\\nPHT approach, corresponding to the first two steps of\\nthe classical Knowledge Discovery in Databases (KDD)\\nprocess [52]) for the efficient and effective extraction of\\nknowledge from private data sources. In order to enable\\nthe usage of such an approach with diverse existing\\ndata, suitable methods for the extraction and distribution\\ntask-specific schema, tailored specifically for the purpose\\nof enabling ad-hoc data selection and integration, are\\nneeded.\\nMethods\\nIn this section, we propose an automated approach for\\nextracting task-specific schema from RDF data sources in\\norder to enable the efficient formulation of SPARQL data\\nselection and integration queries without direct access to\\nthe data. First, we describe basic requirements for the\\nextracted schema, as well as the fundamental idea of the\\nschema extraction technique before subsequently intro-\\nducing a number of extensions, in order to support for\\nmore generally applicable schema extraction methodol-\\nogy. We discuss the trade-offs to be made between differ-\\nent versions of the schema extraction approach and finally\\nshow how the extracted schema can be used further for\\nthe data selection and integration.\\nIn the context of RDF data, the fundamental knowl-\\nedge required for the creation of SPARQL queries for\\ndata selection and integration consists of the various\\nrdf:type objects, the rdf:Property predicates and the struc-\\ntural relations between them. This information can itself\\nbe represented using Semantic Web Standards, such as\\nRDFS, OWL, ShEx or SHACL.\\nWhile shape languages such as ShEx and SHACL\\nare natural candidates for representing prescriptive data\\nschema, they are designed specifically for the validation of\\nclearly structured individual data shapes and to commu-\\nnicate explicit graph patterns. As such they are however\\nnot equally well suited for the formalization of the flexible\\nschema of entire semi-structured datasets.\\nRDFS on the other hand provides a simple and descrip-\\ntive structural annotation of the relationships between\\nproperties and classes and as such serves as a promising\\ncandidate for the task at hand.\\nWhile OWL further extends RDFS with a pow-\\nerful set of description logic-based modeling prim-\\nitives, the corresponding semantic complexity adds\\nsignificant overhead to the schema extraction pro-\\ncess. Especially since the extracted schema is only\\nmeant to be used for query authoring and explic-\\nitly not for reasoning, in the context of this work\\nwe generally restrict our effort to extracting schema\\nusing RDFS and the OWL owl:equivalentClass,\\nowl:equivalentProperty and owl:sameAs pred-\\nicates, which we deem most relevant in order to enable\\ninteroperability and the effective formulation of selection\\nand integration queries.\\nEspecially in order to ensure interoperability with\\nexisting Semantic Web technologies and compatibility\\nwith standard Semantic Web tools, such as schema-\\nintrospection-assisted SPARQL query builders, the\\nextracted schema should thus be available as a simple\\nRDFS and OWL vocabulary via a SPARQL endpoint.\\nSchema-introspection refers to the process of examin-\\ning the schema definition to determine which types of\\nentities exist, which properties are defined upon them and\\nsubsequently, what can be queried for. Since the schema\\nneeded to create data queries (e.g. using SPARQL) only\\ncontains basic structural information about the original\\ndata, it also conveys far less privacy critical information\\nthan exposing the actual data. As such it can be published\\npublicly without privacy concerns in many scenarios.\\nIn the following, we describe an automated approach for\\nschema extraction fromRDF data which allows for the for-\\nmulation of data selection and integration queries without\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 5 of 15\\ndirect access to the data and the subsequent evaluation of\\nthat query in a secure enclave.\\nSchema extraction\\nWe propose an approach for schema extraction based\\non exploiting key characteristics of RDF, RDFS, and\\nOWL. RDF data encoded in compliance with correspond-\\ning vocabularies inherently include metadata about their\\nsemantics and structural relationships.\\nFor the schema extraction, the rdf:type relation plays\\nthe key role, as it declares data points to be instances of\\nspecific data types or, according to RDFS terminology and\\nsemantics [53], classes. Anything that is a type in the sense\\nof occurring as the target of this relation should thus auto-\\nmatically becomes part of the schema as an entity of type\\nrdfs:Class. Additionally, any property relation (that\\nis any identifier occurring in the predicate position of a\\nsubject-predicate-object triple) which occurs in the data\\nshould be included as an entity of type rdf:Property.\\nFinally all directly describing properties of these classes\\nand properties should be included as well. For the scope\\nof this work, we assume that all data in the private data\\nrepository is sensitive and should remain private.\\nEntailment supported schema extraction Assuming\\nperfect conditions, namely proper inclusion of all used\\nvocabularies into the triple store, correct usage of those\\nvocabularies, as well as OWL entailment [26] support of\\nthe SPARQL endpoint providing access to the data, the\\nentire schema of a given RDF data set can be extracted\\nusing a single simple SPARQL CONSTRUCT query as\\ndepicted in Listing 1.\\n\\002\\nCONSTRUCT {?s ?p ?o}\\nWHERE {\\n{[] ?s []}\\nUNION {[] a ?s} .\\n?s ?p ?o .\\n}\\n\\003 \\004\\nListing 1 SPARQL schema extraction query relying on proper\\nentailment support of the endpoint.\\nNote that we explicitly define the relevant subset of all\\navailable schema information to be that which is actually\\nused in the data, i.e. the instantiated schema, and thus only\\nextract that.\\nThe preceding query constructs an RDF graph (line 1)\\ncontaining all the directly describing triples ?s ?p ?o\\nthat occur in the tripe store but having only the following\\nsubjects:\\n1 Instantiated RDF properties ?s (line 3) which\\naccording to RDF 1.1 Semantics [53] are any IRI used\\nin predicate position (c.f. rdfD2).\\n2 Instantiated RDFS classes ?s (line 4) via their\\noccurrence as the object of a triple with rdf:type\\nas the predicate. The fact that these are RDFS classes\\nfollows directly from the RDFS axiomatic triple\\nrdf:type rdfs:range rdfs:Class . in\\nconjunction with RDFS entailment pattern rdfs3\\n[53].\\nAccording to the SPARQL entailment regime, all the\\nsubclass relationships, transitive properties, equivalences\\netc. used in the data are automatically materialized (i.e.\\nincluded in the dataset as inferred knowledge as illus-\\ntrated in Fig. 1) and thus resolved and included too (c.f.\\n[53, 54]).\\nIt should be noted that the query only extracts direct\\nproperties (i.e. triples ?s ?p ?o directly related to the\\nsubject ?s) and as such, some complex constraints such\\nas OWL disjointness axioms are not included in the\\nextracted schema. However, as stated before, for the task\\nof query formulation we consider this to be sufficient.\\nDirectly instantiated schema Since in practice few\\nSPARQL endpoints actually support any kind of entail-\\nment and usually do not materialize implicit triples, the\\napplicability of this basic approach is limited. While the\\noriginal query can theoretically also be executed with-\\nout entailment support, it does not guarantee that all\\nused properties and classes are annotated accordingly\\nas rdf:Property and rdf:Class and completely\\nignores any resource ?s that lacks further describing\\ntriples ?s ?p ?o.\\nThus, in the following we introduce several revisions\\nof the initial extraction query 1 that allow us to reintro-\\nduce the missing triples without relying upon entailment\\nsupport. Additionally, many datasets de facto employ\\nterms from a number of different vocabularies and ontolo-\\ngies and deviate from the originally intended informa-\\ntion model. Since the availability of information about\\ndomain and range of the different properties employed\\nin the dataset is especially relevant in order to assist the\\nquery creation process, we further explicitly construct\\nrdfs:domain and rdfs:range statements according\\nto the property\\u0092s respective usage in the dataset.\\nIn scenarios where it is sufficient to consider only those\\ntypes and properties that are directly used in the dataset\\nor where no information whatsoever about the employed\\nvocabularies is available, it can be reasonable to disregard\\nthe inference generalizations and equivalences entirely.\\nListing 2 proposes a SPARQL query for the extraction\\nof a corresponding schema, which closely reflects the\\nstructure of the underlying data and works even if the\\ndefinitions of the employed ontologies are unavailable.\\nFor this and all further queries, we assume standard\\nSPARQL namespace and prefix definitions as specified by\\nthe World Wide Web Consortium\\u0092s OWL and SPARQL\\nspecifications [53, 55].\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 6 of 15\\n\\002\\nCONSTRUCT {\\n?predicate ?a ?b; a rdf:Property;\\nrdfs:domain ?pDomain; rdfs:range ?\\npRange.\\n?concept ?c ?d; a rdfs:Class.\\n} WHERE {\\n?s ?predicate ?o.\\nOPTIONAL {?s a ?pDomain}\\nOPTIONAL {?o a ?pRange}\\nOPTIONAL {?predicate ?a ?b}\\n[] a ?concept\\nFILTER(!isBlank(?concept))\\nOPTIONAL {?concept ?c ?d}\\n}\\n\\003 \\004\\nListing 2 Basic SPARQL schema extraction query which only\\ndiscovers RDFS classes and properties directly instantiated in the\\nqueried dataset.\\nAnalogously to query 1, we detect predicates as any\\nInternationalized Resource Identifier (IRI) used in predi-\\ncate position (line 5) and classes as IRIs used as objects of\\nRDF type triples (line 9). We also include any additional\\ninformation directly relating to those subjects that might\\nbe available in the dataset (lines 8 and 11). To explicitly\\nconstruct rdfs:domain and rdfs:range information\\nof the predicates, we further determine the rdf:type\\nof each subject (line 6) and object (line 7), if available.\\nAdditionally we filter out any class declarations without\\nan own identifier (line 10) to avoid potential referenc-\\ning issues with the extracted schema. Lastly we construct\\nthe schema graph as all discovered predicates (explicitly\\ntyped as rdf:Property) and their related informa-\\ntion (line 2) and all discovered classes (explicitly typed as\\nrdfs:Class) and their related information (line 3).\\nWhen applying this extraction approach to the dataset\\ndepicted in Fig. 1, we end up with the schema depicted in\\nFig. 2 where classes are highlighted in blue and properties\\nin green (i.e. with implicit rdf:type triples).\\nSubsequently, in this exemplary use case, following the\\nextracted schema closely one could query for instances\\nof the ex:Patient class and their corresponding prop-\\nerty ex:treatedAt, which however perfectly reflects\\nthe available dataset without inferred knowledge.\\nIt should be noted, that this extracted schema is explic-\\nitly not suited for triple entailment according to RDFS\\nsemantics, due to the conjunctive nature of multiple\\nrdfs:domain and rdfs:range definitions on prop-\\nerties (c.f. RDFS entailment patterns rdfs2 and rdfs3\\n[53]). A semantically correct alternative would be the\\nusage of Schema.org\\u0092s schema:domainIncludes and\\nschema:rangeIncludes properties in line 2, instead\\nof their RDFS equivalents. However, since RDFS domain\\nand range semantics are implemented in a variety of tools\\nfor schema exploration, visualization and assisted query\\nauthoring [56\\u009658], while schema.org semantics are not\\nequally well supported, we deliberately defer semantic\\ncorrectness to a closer representation of the underlying\\ndata\\u0092s structure.\\nLocally inferred schema In order to re-include previ-\\nously inferred information such as additional types and\\nclasses due to sub-property, subclass, domain, range or\\nequivalence relationships, we can extract the relevant\\nschema directly from the data and the full definitions of\\nthe employed ontologies using the SPARQL 1.1 Property\\nPaths [59] feature, independent of entailment support or\\nstatement materialization on the endpoint.\\nA corresponding SPARQL query is depicted in Listing 3.\\n\\002\\nWHERE {\\n?s ?x ?o. OPTIONAL {?s a ?pDomain}\\nOPTIONAL {?o a ?pRange}\\n?x (rdfs:subPropertyOf|owl:\\nequivalentProperty|^owl:\\nequivalentProperty\\n|owl:sameAs|^owl:sameAs)* ?predicate\\nOPTIONAL {?predicate ?a ?b}\\n{?predicate (rdfs:range|rdfs:domain) ?y}\\nUNION {[] a ?y}\\n?y (rdfs:subClassOf|owl:equivalentClass|^\\nowl:equivalentClass|owl:\\nsameAs|^owl:sameAs)* ?concept\\nFILTER(!isBlank(?concept))\\nOPTIONAL {?concept ?c ?d}}\\n\\003 \\004\\nListing 3 Extended WHERE clause of schema extraction query 2\\nemploying SPARQL 1.1 Property Paths to emulate RDFS\\nspecialization, domain and range semantics, as well as OWL\\nequivalence entailment.\\nThe query constructs a graph, which in addition to all\\ninstantiated RDFS classes and RDF properties (and their\\ndirect properties) includes generalizations and equivalent\\nresources of those via RDFS and OWL semantics.\\nFor both properties and classes, we resolve corre-\\nsponding generalizations directly using the relevant\\nRDFS entailment patterns (rdfs5, rdfs7, rdfs9, rdfs11)\\n[53] and concept equivalences using OWL\\u0092s owl:\\nequivalentClass, owl:equivalentProperty\\nand owl:sameAs predicates [54] in lines 5 and 9. While\\nowl:sameAs is only supposed to be used for the decla-\\nration of equivalence between individuals, it is commonly\\nmisused in practice and as such deliberately included in\\nthis query.\\nrdfs:Class annotations are further inferred follow-\\ning RDFS entailment rules rdfs2 and rdfs3 [53] from\\nrdfs:domain and rdfs:range properties declared\\non instantiated rdf:Property resources (line 7).\\nWhen applying this extraction approach to the dataset\\ndepicted in Fig. 1, we end up with the relevant schema\\ndepicted in Fig. 3. As before, classes are highlighted in blue\\nand properties in green.\\nFollowing the extracted schema, it is now also possible\\nto query for instances of the hospital and person classes, as\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 7 of 15\\nFig. 2 Directly instantiated schema extracted from Example 1\\nwell as a number of equivalent SNOMEDCT vocabulary\\nterms.\\nEmploying terminology services\\nIn practice, individual SPARQL endpoints providing\\naccess to individual datasets cannot be (and are not) bur-\\ndened with serving all vocabularies and terminologies\\nused in the dataset and related to those. That is the pur-\\npose of specialized terminology services and vocabulary\\ncatalogs, such as the aforementioned LOV and BioPortal\\nprojects.\\nIn order to resolve equivalences and generalizations\\nacross vocabularies, it is thus possible to make use of the\\nSPARQL 1.1 Federated Query protocol [60, 61] in order\\nto entail additional schema triples using external termi-\\nnology services. The query depicted in Listing 4 employs\\nfederated queries to the SPARQL endpoint http://\\nexample.org/terminology in order to accomplish\\nthis. The query further explicitly filters out all subject that\\nare blank nodes in order to avoid renaming and resolu-\\ntion issues between blank nodes from different sources\\n(c.f. [60]).\\nWhile the approach follows the same principles as the\\npreviously introduced local inference (c.f. Listing 3), here\\neach inference step also includes results from the exter-\\nnal terminology service. As such, following the exam-\\nple from before, the extracted schema would now also\\ninclude all inferred knowledge from the SNOMEDCT\\nvocabulary as well as any vocabulary known to the\\nterminology service that declares equivalences with\\nSNOMEDCT.\\nIn some cases, such as with rare diseases, even the\\nlimited communication with remote terminology services\\nmight affect data privacy, since the instantiation of cer-\\ntain very rare classes or predicates might in itself reveal\\nprivate data. In such cases a local terminology service can\\nbe employed, i.e. by creating a local deployment of the\\nLOV service or by providing local copies of the relevant\\nfull vocabularies. Nevertheless, sharing of the extracted\\nschema in such cases may still require additional consid-\\nerations.\\nUnfortunately, current implementations of federated\\nSPARQL queries still typically incur large performance\\npenalties by using suboptimal resolution strategies. As\\nsuch, in practice, it is often helpful tomanually decompose\\nthe single query into multiple query steps. An exem-\\nplary four-step approach using the SPARQL 1.1 UPDATE\\nconstruct [62, 63] can be found in the supplementary\\nFig. 3 Locally inferred relevant schema extracted from example 1\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 8 of 15\\nmaterials2, which also includes performance optimized\\nreformulations of the other queries.\\n\\002\\nWHERE {\\n?s ?x ?o.\\nOPTIONAL {?s a ?pDomain}\\nOPTIONAL {?o a ?pRange}\\n{?x (rdfs:subPropertyOf|owl:'</span></dd><dt>text7</dt><dd><span style=white-space:pre-wrap>'Grabar et al. Journal of Biomedical Semantics            (2020) 11:7 \\nhttps://doi.org/10.1186/s13326-020-00225-x\\nRESEARCH Open Access\\nCAS: corpus of clinical cases in French\\nNatalia Grabar1,2*\\u0086, Clément Dalloux3\\u0086 and Vincent Claveau3\\u0086\\nAbstract\\nBackground: Textual corpora are extremely important for various NLP applications as they provide information\\nnecessary for creating, setting and testing those applications and the corresponding tools. They are also crucial for\\ndesigning reliablemethods and reproducible results. Yet, in some areas, such as themedical area, due to confidentiality\\nor to ethical reasons, it is complicated or even impossible to access representative textual data. We propose the CAS\\ncorpus built with clinical cases, such as they are reported in the published scientific literature in French.\\nResults: Currently, the corpus contains 4,900 clinical cases in French, totaling nearly 1.7M word occurrences. Some\\nclinical cases are associated with discussions. A subset of the whole set of cases is enriched with morpho-syntactic\\n(PoS-tagging, lemmatization) and semantic (the UMLS concepts, negation, uncertainty) annotations. The corpus is\\nbeing continuously enriched with new clinical cases and annotations. The CAS corpus has been compared with\\nsimilar clinical narratives. When computed on tokenized and lowercase words, the Jaccard index indicates that the\\nsimilarity between clinical cases and narratives reaches up to 0.9727.\\nConclusion: We assume that the CAS corpus can be effectively exploited for the development and testing of NLP\\ntools and methods. Besides, the corpus will be used in NLP challenges and distributed to the research community.\\nKeywords: Medical area, Natural language processing, Corpus with clinical cases, Morpho-syntactic and semantic\\nannotation, Sustainability, Reproducibility\\nBackground\\nTextual corpora are central for various NLP applications\\nas they provide information necessary for creating, set-\\nting, testing and validating these applications, the cor-\\nresponding tools, and the results. Yet, in some areas,\\ndue to confidentiality or to ethical reasons, it is compli-\\ncated or even impossible to access representative textual\\ndata typically created and used by the actors of these\\nareas. For instance, medical and legal areas are concerned\\nwith these issues: in the legal area, information on law-\\nsuits and trials remains confidential, while in the medical\\narea, medical confidentiality must be respected by the\\nmedical staff. In both situations, personal data cannot\\nbe made publicly available, which prevents corpora from\\n*Correspondence: natalia.grabar@univ-lille.fr\\n\\u0086Natalia Grabar, Clé Dalloux and Vincent Claveau contributed equally to this\\nwork.\\n1CNRS, UMR 8163, F-59000 Lille, France\\n2Univ. Lille, UMR 8163 - STL - Savoirs Textes Langage, F-59000 Lille, France\\nFull list of author information is available at the end of the article\\nbeing released and makes experiments non-reproducible\\nby other researchers and with other methods. To face such\\nsituations, Natural Language Processing (NLP) proposes\\nspecific methods and tools. Hence, for several years now,\\nanonymization and de-identification methods and tools\\nhave been made available and provide competitive and\\nreliable results [1\\u00964] reaching up to 90% precision and\\nrecall. But it may still be difficult to access de-identified\\ndocuments and use them for research. One reason is that\\nthere is a risk of re-identification of people, and more\\nparticularly of patients [5, 6] because medical histories\\ncan be unique. In consequence, the application of de-\\nidentification tools on personal data often does not permit\\nto make the data freely available and usable within the\\nresearch context.\\nYet, there is a real need for the development of methods\\nand tools for several applications suited for such restricted\\nareas. For instance, in the medical area, it is impor-\\ntant to design suitable tools for information retrieval and\\nextraction, for recruiting patients for clinical trials, for\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nGrabar et al. Journal of Biomedical Semantics            (2020) 11:7 Page 2 of 10\\nperforming several other important tasks such as index-\\ning, study of temporality, negation, etc. [7\\u009613]. Another\\nimportant issue is related to the reliability of tools and\\nto the reproducibility of study results across similar data\\nfrom different sources. The scientific research and clin-\\nical communities are indeed increasingly coming under\\ncriticism for the lack of reproducibility in the biomedical\\narea [14\\u009616], but notice that, for instance, psychology is\\nconcerned with this issue as well [17\\u009619]. The first step\\ntowards the reproducibility of results is the availability of\\nfreely usable tools and corpora. In the current contribu-\\ntion, we are mainly concerned with the construction of\\nfreely available corpora for the medical domain. Yet, we\\nare aware that sharing tools and methods is also impor-\\ntant. We assume that availability of corpora may boost the\\ndesign and dissemination of other resources, methods and\\ntools for biomedical tasks and applications.\\nThe purpose of our work is to introduce the CAS cor-\\npus, that contains clinical cases in French such as those\\npublished in scientific literature or used in the education\\nand training of medical students. In what follows, we first\\npresent some existing studies onmedical corpora creation\\n(\\u0093Existing work: freely available clinical corpora\\u0094), high-\\nlighting corpora which are freely available for research.\\nWe then present the methods used for building, annota-\\ntion and analysis of the CAS corpus with clinical cases in\\nFrench (\\u0093Methods\\u0094). The results are presented in \\u0093Results\\u0094\\nand discussed in \\u0093Discussion\\u0094. We conclude with some\\ndirections for future work (\\u0093Conclusion\\u0094 sections). The\\nwork presented in this article is an extended and updated\\nversion of our previous publication [20].\\nExisting work: freely available clinical corpora\\nWithin the medical area, we can distinguish two main\\ntypes of medical corpora: scientific and clinical.\\n\\u0095 Scientific corpora are issued from scientific\\npublications and reporting. Such corpora are\\nbecoming increasingly available to researchers thanks\\nto recent and less recent initiatives dedicated to open\\npublication, such as those promoted by the NLM\\n(National Library of Medicine) through the PUBMED\\nportal1 and specifically dedicated to the biomedical\\narea, and by the HAL2 and ISTEX3 initiatives, which\\nprovide generic portals for accessing scientific\\npublications from various areas, including medicine.\\nSuch corpora contain scientific publications that\\ndescribe research studies: motivation, methods,\\nresults and issues on precise research questions.\\nOther portals may also provide access to scientific\\nliterature aimed at specific purposes, namely indexing\\n1https://www.ncbi.nlm.nih.gov/pubmed\\n2https://hal.archives-ouvertes.fr/\\n3https://www.istex.fr/\\nreliable literature, such as proposed by HON [21],\\nCISMEF [22], and other similar initiatives [23]. Some\\nexisting scientific corpora also provide annotations\\nand categorizations, such as PoS-tagging [24] and\\nnegation [25]. These are often built for the purposes\\nof shared tasks [26, 27].\\n\\u0095 Clinical corpora are related to hospital and clinical\\nevents of patients. Such corpora typically contain\\ndocuments that describe medical history of patients\\nand the medical care they are undergoing. This kind\\nof corpora is typically created and used in clinical\\ncontext as part of the healthcare process. Even after\\nde-identification, it is complicated to obtain free\\naccess to this kind of medical data and, for this\\nreason, there are very few clinical corpora freely\\navailable for research.\\nIn our work, we are mainly interested in clinical cor-\\npora: the proposed literature review of the existing work\\nis aimed at clinical corpora that are freely available for\\nresearch. We present here the main existing clinical cor-\\npora:\\n\\u0095 MIMIC (Medical Information Mart for Intensive\\nCare), now available in its third version, provides the\\nlargest available set of structured and unstructured\\nclinical data in English. MIMIC III is a single-center\\ndatabase comprising information pertaining to\\npatients admitted in critical care units at a large\\ntertiary care hospital. Those data include vital signs,\\nmedications, laboratory measurements, observations\\nand notes charted by care providers, fluid balance,\\nprocedure codes, diagnostic codes, imaging reports,\\nhospital length of stay, survival data, and more. The\\ndatabase supports applications including academic\\nand industrial research, quality improvement\\ninitiatives, and higher education coursework [28].\\nThose data are widely used by researchers, for\\ninstance for predicting mortality [29, 30], for\\ndiagnosis identification and encoding [31, 32], for\\nstudies on temporality [33] or for identifying similar\\nclinical notes [34], to cite just a few existing studies.\\nData from these corpora are also used in challenges,\\nsuch as i2b2, n2c2 and CLEF-eHEALTH.\\n\\u0095 i2b2 (Informatics for Integrating Biology and the\\nBedside)4 is an NIH-funded initiative promoting the\\ndevelopment and test of NLP tools for\\nEnglish-language documents with the purpose of\\nhealthcare improvement. In order to enhance the\\nability of NLP tools to process fine-grained\\ninformation from clinical records, i2b2 challenges\\nprovide sets of fully de-identified clinical notes\\nenriched with specific annotations [9, 11, 35], such as:\\n4https://www.i2b2.org/NLP/DataSets/Main.php\\nGrabar et al. Journal of Biomedical Semantics            (2020) 11:7 Page 3 of 10\\nde-identification, smoking status, medication-related\\ninformation, semantic relations between entities, or\\ntemporality. The clinical corpora and their\\nannotations built for the i2b2 NLP challenges are\\navailable now for general research purposes.\\n\\u0095 n2c2 (National NLP Clinical Challenges),5 held in\\n2018 and 2019, also address the processing of\\nEnglish-language clinical documents. These\\nchallenges are dedicated to other typical tasks when\\nhandling clinical documents: inclusion of patients in\\nclinical trials, detection of adverse-drug events,\\ncomputing of textual semantic similarity, concept\\nnormalization, and extraction of family history.\\n\\u0095 CLEF-eHEALTH challenges6 held in 2013 and 2014\\nprovide annotations for disorder detection and\\nabbreviation normalization. In 2016 the focus was on\\nstructuring Australian free-text nurse notes. Finally,\\nin 2016 and 2017 death reports in French, provided\\nby the CépiDc,7 have been processed for death cause\\nextraction.\\n\\u0095 eHealth-KD 2019 challenge8 targets human language\\nmodelling in a scenario in which electronic health\\ndocuments in Spanish could be machine readable\\nfrom a semantic point of view. The two proposed\\ntasks are: identification and classification of key\\nphrases, and detection of semantic relations between\\nthese key phrases.\\nFinally, medical data, close to those handled in the clini-\\ncal context, can be found in clinical trials protocols. One\\nexample is the corpus of clinical trials annotated with\\ninformation on numerical values in English [36], and on\\nnegation in French and Brazilian Portuguese [37, 38].\\nMethods\\nWe first describe the specificity of the sources and clinical\\ncases from which the CAS corpus was created (\\u0093Building\\nthe corpus\\u0094), then the annotation rationale (\\u0093Annotation\\nof the corpus\\u0094), and the principles of its comparison with\\nsimilar clinical narratives from Rennes University Hospi-\\ntal (\\u0093Comparison with clinical narratives\\u0094 sections).\\nBuilding the corpus\\nThe CAS corpus in French contains clinical cases as\\npublished in scientific literature, legal or training mate-\\nrial. Hence, it is built using material freely available\\nin online sources. The collected clinical cases are pub-\\nlished in different journals and websites from French-\\nspeaking countries in various continents. Those clinical\\n5https://n2c2.dbmi.hms.harvard.edu/\\n6https://sites.google.com/site/shareclefehealth/\\n7http://www.cepidc.inserm.fr/\\n8https://knowledge-learning.github.io/ehealthkd-2019\\ncases are related to various medical specialties (e.g. cardi-\\nology, urology, oncology, obstetrics, pulmonology, gastro-\\nenterology...).\\nThe purpose of clinical cases is to describe clinical sit-\\nuations for real de-identified or fake patients. Common\\nclinical cases are typically part of education programs\\nused for training medical students, while rare cases are\\nusually shared through scientific publications to illustrate\\nless common clinical situations. As for clinical cases which\\ncan be found in legal sources, they usually report on situ-\\nations which became complicated due to various reasons\\nemanating from different healthcare levels: medical doc-\\ntor, healthcare team, institution, health system and their\\ninteractions.\\nSimilarly to clinical documents, the content of clinical\\ncases depends on the clinical situations that are illustrated,\\nand on the disorders, but also on the purpose of the pre-\\nsented cases: description of diagnoses, treatments or pro-\\ncedures, evolution, family history, adverse-drug reactions,\\nexpected audience, etc.\\nData in published clinical cases are de-identified by the\\nauthors prior to their publication. Besides, publication\\nis usually done with the written permission of patients.\\nThe case reports can be related to any medical situa-\\ntion (diagnosis, treatment, procedure, follow-up...), to any\\nspecialty and to any disorder. The typical structure of\\nscientific publications with clinical cases starts by intro-\\nducing the clinical situation, then one or more clinical\\ncases are presented to support the situation. Schemes,\\nimaging, examination results, patient history, lab results,\\nclinical evolution, treatment, etc. can also be provided for\\nthe illustration of clinical cases. Finally, those clinical cases\\nare discussed. Hence, such cases may present an exten-\\nsive description of medical problems. Such publications\\ngather medical information related to clinical discourse\\n(clinical cases) and to scientific discourse (introduction'</span></dd><dt>text8</dt><dd><span style=white-space:pre-wrap>'RESEARCH Open Access\\nStructuring, reuse and analysis of electronic\\ndental data using the Oral Health and\\nDisease Ontology\\nWilliam D. Duncan1,2* , Thankam Thyvalikakath2,3, Melissa Haendel4, Carlo Torniai5, Pedro Hernandez6, Mei Song7,\\nAmit Acharya8, Daniel J. Caplan9, Titus Schleyer2,10\\u0086 and Alan Ruttenberg11\\u0086\\nAbstract\\nBackground: A key challenge for improving the quality of health care is to be able to use a common framework to\\nwork with patient information acquired in any of the health and life science disciplines. Patient information\\ncollected during dental care exposes many of the challenges that confront a wider scale approach. For example, to\\nimprove the quality of dental care, we must be able to collect and analyze data about dental procedures from\\nmultiple practices. However, a number of challenges make doing so difficult. First, dental electronic health record\\n(EHR) information is often stored in complex relational databases that are poorly documented. Second, there is not\\na commonly accepted and implemented database schema for dental EHR systems. Third, integrative work that\\nattempts to bridge dentistry and other settings in healthcare is made difficult by the disconnect between\\nrepresentations of medical information within dental and other disciplines\\u0092 EHR systems. As dentistry increasingly\\nconcerns itself with the general health of a patient, for example in increased efforts to monitor heart health and\\nsystemic disease, the impact of this disconnect becomes more and more severe.\\nTo demonstrate how to address these problems, we have developed the open-source Oral Health and Disease\\nOntology (OHD) and our instance-based representation as a framework for dental and medical health care\\ninformation. We envision a time when medical record systems use a common data back end that would make\\ninteroperating trivial and obviate the need for a dedicated messaging framework to move data between systems.\\nThe OHD is not yet complete. It includes enough to be useful and to demonstrate how it is constructed. We\\ndemonstrate its utility in an analysis of longevity of dental restorations. Our first narrow use case provides a\\nprototype, and is intended demonstrate a prospective design for a principled data backend that can be used\\nconsistently and encompass both dental and medical information in a single framework.\\n(Continued on next page)\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: wdduncan@gmail.com\\n\\u0086Titus Schleyer and Alan Ruttenberg are joint senior authors\\n1National Center for Ontological Research, Buffalo, NY, USA\\n2Center for Biomedical Informatics, Regenstrief institute, Inc., Indianapolis, IN,\\nUSA\\nFull list of author information is available at the end of the article\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 \\nhttps://doi.org/10.1186/s13326-020-00222-0\\n(Continued from previous page)\\nResults: The OHD contains over 1900 classes and 59 relationships. Most of the classes and relationships were\\nimported from existing OBO Foundry ontologies. Using the LSW2 (LISP Semantic Web) software library, we translated\\ndata from a dental practice\\u0092s EHR system into a corresponding Web Ontology Language (OWL) representation based\\non the OHD framework. The OWL representation was then loaded into a triple store, and as a proof of concept, we\\naddressed a question of clinical relevance \\u0096 a survival analysis of the longevity of resin filling restorations. We\\nprovide queries using SPARQL and statistical analysis code in R to demonstrate how to perform clinical research\\nusing a framework such as the OHD, and we compare our results with previous studies.\\nConclusions: This proof-of-concept project translated data from a single practice. By using dental practice data, we\\ndemonstrate that the OHD and the instance-based approach are sufficient to represent data generated in real-\\nworld, routine clinical settings. While the OHD is applicable to integration of data from multiple practices with\\ndifferent dental EHR systems, we intend our work to be understood as a prospective design for EHR data storage\\nthat would simplify medical informatics. The system has well-understood semantics because of our use of BFO-\\nbased realist ontology and its representation in OWL. The data model is a well-defined web standard.\\nKeywords: Ontology, Dental health, Informatics, Electronic heath record, OWL, SPARQL\\nBackground\\nA key challenge for improving the quality of healthcare\\nis to be able to use a common framework to work with\\npatient information acquired in any of the health and life\\nscience disciplines. The patient information collected\\nduring dental care exposes many of the challenges that\\nconfront a wider scale approach. Within dentistry, a key\\naspect for improving the quality of care is the ability to\\ncollect and analyze data about oral health conditions\\nand procedures, such as the longevity of fillings, the fre-\\nquency of patient checkups, and incidence of tooth loss.\\nRecent reports estimate that 73.8% of solo practitioners\\nand 78.7% of group practitioners in the U.S. use a com-\\nputer to manage some, and 14.3 and 15.9%, respectively,\\nall patient information on a computer [1] . In conse-\\nquence, we now have the opportunity to study dental\\nhealth services and perform outcomes research using\\nlarge amounts of secondary data obtained from geo-\\ngraphically dispersed dental practices [2].\\nLarge secondary datasets could help us more easily\\nstudy diseases in a sizable samples with increased statis-\\ntical power, track patients for an extended period of\\ntime, provide valid and representative samples, supply\\ncorrelates not commonly collected in an oral health set-\\nting, collect data in real time and ascertain potential\\nconfounders [2].\\nAnalyzing data from electronic health records (EHR),\\nhowever, presents a number of challenges. First, dental\\nEHR information is often stored in relational databases\\nthat are poorly documented and have complex relations\\nbetween tables. This makes extracting and analyzing\\ndata from even a single practice\\u0092s system difficult. Sec-\\nond, dental EHR database schemas vary depending on\\nthe vendor who developed the system. This adds diffi-\\nculty when integrating data from multiple practices.\\nThird, information is not always encoded in the same\\nway. For example, a tooth encoded as number (e.g.,\\ntooth \\u00916\\u0092) or as a character array in which the index pos-\\nition of a character represents the tooth (e.g., the \\u0091Y\\u0092 in\\nthe character array \\u0091NNNNNYNNNNNNNNNNNN\\nNNNNNNNNNNNNNN\\u0092 represents a right upper sec-\\nondary canine tooth, i.e., tooth \\u00916\\u0092). Last, dental EHR sys-\\ntems are typically only loosely specified. So, outside of a\\ncommon core of structures for the oral cavity and its\\nparts, there is wide variation in how information such as\\nspecific types of materials, details of methods, instru-\\nments, and general patient health is represented. Much\\nof this information is either semi-or unstructured text.\\nWhile we focus here on dental EHRs, these same prob-\\nlems are endemic in other EHR systems.\\nTo demonstrate how to address some of these problems,\\nwe have developed the Oral Health and Disease Ontology\\n(OHD) as a common framework for representing dental\\nhealth information embedded in a larger framework ad-\\nequate to accommodate structured representation that\\ngoes beyond that in current dental EHR systems and ex-\\ntends into general medicine. The OHD contains terms for\\nrepresenting anatomical structures (e.g., distal surface of\\ntooth), dental procedures (e.g., tooth extraction), and oral\\nconditions (e.g., caries), as well as relations between terms\\n(e.g., distal surface is part of tooth). The OHD\\u0092s structure\\nprovides a common representation of the entities that\\nEHR data is about, without being designed in a way that\\nunintentionally limits it to only dental health data. This\\nmakes it possible to use the OHD as framework for inte-\\ngrating inhomogeneous data from disparate database sys-\\ntems and support representations for future systems.\\nUsing the OHD\\u0092s terms and relations, information from\\nmultiple dental EHRs can now be translated into OWL 2\\n[3] statements, stored in a semantic database or triple\\nstore, and queried using SPARQL [4] to extract informa-\\ntion for analysis.\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 2 of 19\\nAs a proof of concept, we have translated dental EHR\\ndata from a single dental practice, and performed a sur-\\nvival analysis of the longevity of resin filling restorations.\\nThe proof of concept demonstrates both aspects of the\\nOHD that are specific to dentistry (e.g. teeth, restora-\\ntions and other procedures) as well as aspects that would\\nremain unchanged in a general medical context, such as\\ndemographic correlates. The output of this analysis is\\ndiscussed in the Results section. In the Discussion sec-\\ntion we describe the potential for wider application.\\nRelated work\\nThe work in this paper expands upon previous work de-\\nveloping the OHD [5, 6], and provides a more detailed\\nexplication of the OHD\\u0092s structure. It differs from previ-\\nous ontology work, such as PeriO [7] and BigMouth [8],\\nin two respects.\\nFirst, it focuses on the domain of dental anatomy and\\nprocedures rather than genomic information. Second,\\nthe OHD\\u0092s use of the Basic Formal Ontology and Ontol-\\nogy of Biomedical Investigations as an upper-level\\nframework sets the stage for seamlessly extending it to\\ngeneral medical information. Moreover, the OHD is not\\na data repository, such as BigMouth [8], but a semantic\\nframework for representing data that may be used in the\\ndesign of repositories \\u0096 such as our semantic\\ntechnology-based repository of information translated\\nfrom (for now) a single dental practice.\\nWe considered using SNOMED and its dental subset\\nSNODENT, but there are problems that make these\\nstandards, at the moment, unusable for our purposes.\\nFirst, their licenses restrict modification of substantial\\nparts of the standard. This prevents us from reorganiz-\\ning content according to realist principles, adding defini-\\ntions, or adding or correcting axioms. Not all countries\\nlicense to use SNOMED, and this would prevent our\\nwork from being replicable worldwide.\\nSecond, there are serious quality issues with\\nSNOMED, and SNODENT in particular [9]. A major\\nissue is the question of ontological commitment \\u0096 what\\nterms mean. The vast majority of terms in SNODENT\\nand SNOMED come without textual definitions [10],\\nand the question of what SNOMED terms actually rep-\\nresent is still up for debate.\\nThird, use of these terminologies typically is within a\\nlayered framework that brings unnecessarily complica-\\ntion [11]. In common usage these resources are bound\\nto data models of medical records [12]. That means that\\none needs to separately understand the data models and\\nthe ontology. By contrast, in our approach the ingredi-\\nents for a representation are simple \\u0096 an OWL ontology\\nand high-quality SPARQL, OWL and RDF W3C specifi-\\ncations. Those logic-based specifications are substan-\\ntially clearer than HL7 specifications.\\nThe Open Biological and Biomedical Ontology (OBO)\\nFoundry approach is to have, for any given class, a single\\nidentifier, if necessary coordinating with developers of\\nother ontologies. The realism-based approach empha-\\nsizes that classes are collections of instances, that the in-\\nstances are things in the world, and that documentation\\nshould make clear what those instances are. The OHD\\nand the semantic technologies used to implement the\\nontology make it relatively easy to merge data. The data\\nis just added together, untransformed. It is possible to\\ndo this, in theory, because all parts of the representation\\nare clearly understood, the types of entities are shared,\\nand the choice to represent particulars using the stand-\\nard methods provided by semantic web standards allow\\nfor little creativity in how concrete representations are\\nconstructed. Because our focus is on showing how a uni-\\nfied representation system works, we consider out of\\nscope general methods for harmonizing or interchanging\\ndata with different representations, as is the focus of\\nHL7.\\nRecently, authors AR and WD have started participat-\\ning in the review and development of SNODENT. It is\\nentirely possible that in the future that SNOMED and\\nSNODENT might be used in the same manner that we\\nuse OHD here. The OHD and the source code used for\\ntranslation and analysis are available in full at https://\\ngithub.com/oral-health-and-disease-ontologies/ohd-\\nontology and in part in the Additional file 1.\\nMethods\\nOntology development\\nThe OHD was developed in a collaborative effort be-\\ntween dental researchers, practicing dentists, statisti-\\ncians, informatics experts, and ontologists. Our first task\\nwas to identify which dental entities would be repre-\\nsented. To guide this process, we developed a set of re-\\nsearch questions. For example, for the research question,\\n\\u0093What is the time from one restoration to its replace-\\nment on the same tooth?\\u0094, we determined that we would\\nneed to represent restoration procedures, the dates of\\nthe procedures, patients, patients\\u0092 teeth, surfaces of\\nteeth, and the restoration materials used to restore teeth.\\nWe provide the list of driving research questions in\\nAdditional file 1.\\nOnce our domain of focus was identified, our next\\nstep was to catalog the terms1 we would need in the\\nontology. We imported the Basic Formal Ontology\\n(BFO) and the Ontology for General Medical Science\\n(OGMS) as a whole and otherwise extracted terms from\\nexisting OBO Foundry ontologies that represented en-\\ntities relevant to our dental health domain using custom\\n1In this paper, we use the word \\u0091term\\u0092 as a unique natural language\\nexpression for a class, instance, or relation in our ontology.\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 3 of 19\\nprograms as well as the OntoFox2 web tool [13]. The\\nOntoFox tool implements the Minimum Information to\\nReference an External Ontology Term (MIREOT)\\nprinciple [14]. MIREOT is a practice by which one im-\\nports a selected set of terms from another ontology ra-\\nther than including the whole ontology, as importing in\\nOWL would do. Where relevant terms were not present\\nin an existing ontology, we created new terms. Each new\\nterm was assigned an Internationalized Resource Identi-\\nfier (IRI) [15], a human-readable label, a definition or\\ndocumentation, and the name of the term\\u0092s editor(s).\\nWhen appropriate, other metadata was included, such as\\nthe reference source for a definition and comments\\nabout a term\\u0092s definition such as its rationale, scope, and\\nusage. Throughout the ontology development process,\\nthe definitions were reviewed multiple times by team\\nmembers. In the following sections, we discuss the\\nmethods for acquiring the necessary terms.\\nOntology architecture\\nThe OHD is constructed in line with a number of OBO\\nFoundry principles. The OBO Foundry [16] is a collect-\\nive of ontology developers who are committed to collab-\\noration and adherence to shared principles. The mission\\nof the OBO Foundry is to develop a family of interoper-\\nable ontologies that are both logically well-formed and\\nscientifically accurate. OBO Foundry principles include\\nuse of the Basic Formal Ontology (BFO) [17], an upper-\\nlevel ontology, use of a standard IRI identifier space, re-\\nuse, where possible, of other Foundry ontologies, and\\nthe inclusion of a textual and, where feasible, logical def-\\ninition for each class and relation.\\nOntology reuse\\nThe OHD uses BFO as its upper-level ontology. BFO is de-\\nsigned as a domain-independent ontology based on princi-\\nples of ontological realism [18] As an upper-level ontology,\\nBFO establishes categories such as material entities, pro-\\ncesses, time, space, and realizable entities (properties), as well\\nas relations among them, such as the relation between a par-\\nticipant and a process they participate in.\\nWe reuse a number of classes and relations from exist-\\ning OBO Foundry ontologies, such as the Foundational\\nModel of Anatomy (FMA) [19] and the Ontology for\\nBiomedical Investigations (OBI) [20].\\nThis construction methodology serves two purposes.\\nFirst, it allows us to leverage the experience of the devel-\\nopers of OBO ontologies. Second, adhering to OBO\\nstandards and precedents makes the OHD more easily\\ninteroperable with other OBO ontologies [16], and this\\nallows developers to reuse our classes and provide\\nfeedback on how to improve the OHD. A summary of\\nthe reused ontologies is provided in Table 1.\\nClasses from the ontologies listed in Table 1 are then\\nextended to encompass entities in the oral health do-\\nmain. At present, this includes classes for representing\\nteeth and tooth surfaces, dental procedures, patients,\\nproviders, restoration materials, dental findings, and bill-\\ning codes. Each of these classes is discussed in the fol-\\nlowing sections.\\nAnatomical structures\\nWe use the FMA\\u0092s classes to represent anatomical struc-\\ntures, such as jaws, teeth, and tooth roots. However, in\\nour initial construction of the OHD, we found that the\\nFMA was not adequate for representing surfaces of\\nteeth. The FMA\\u0092s class surface of tooth is used to repre-\\nsent the two-dimensional curved plane that forms the\\nouter boundary of a tooth. This is not suitable for repre-\\nsenting the portions of enamel into which restoration\\nmaterial is placed. Thus, we added the class surface en-\\namel of tooth to represent the portions of enamel that\\nconstitute a tooth\\u0092s anatomical crown. The need for this\\nclass was reported to the FMA\\u0092s curators, and the FMA\\nnow includes the class surface layer of tooth3 to address\\nthis.\\nUntil recently, the FMA was authored in a representa-\\ntion system called Protégé Frames. In order to use it\\nwithin the OBO framework we needed to translate from\\nthe native frames version to a version that integrates\\nwith OBO ontologies. As part of that translation, classes\\nin FMA were placed as children of the appropriate BFO\\nor OBO classes. Second, we needed to translate the\\nframes expressions [25] to OWL before we could use it\\nwith the other classes OBO classes.\\n2http://ontofox.hegroup.org 3http://purl.org/sig/ont/fma/fma290055 (accessed August 2018)\\nTable 1 Summary of ontology reuse in OHD\\nOntology Classes/relations reused or specialized\\nBasic Formal Ontology (BFO) upper-level ontology used to\\ncoordinate other OBO ontologies\\nOntology for General Medical\\nScience (OGMS) [21]\\nhealth care entities; e.g., patient role,\\nvisit, disorder\\nFoundational Model of\\nAnatomy (FMA)\\nanatomical entities; e.g., jaw, tooth,\\ntooth surface\\nOntology for Biomedical\\nInvestigations (OBI)\\nrelations between processes to\\nentities; e.g., restoration procedure has\\nspecified input some tooth\\nInformation Artifact Ontology\\n(IAO) [22]\\ninformation entities in the dental\\nhealth care domain; e.g., billing codes,\\ngoals of dental procedures\\nOntology of Medically Related\\nSocial Entities (OMRSE) [23]\\ngender of patient\\nCommon Anatomy Reference\\nOntology (CARO) [24]\\nmale and female organism\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 4 of 19\\nPatients and health care providers\\nA given dental procedure (such as an oral evaluation or\\nrestoration procedure) will minimally involve a patient\\nand the dental health care provider. We define for the\\npatient and health care provider roles that characterize\\nthe way in which patients and providers participate in\\ndental procedures.\\nIn BFO, roles are realizable entities which are in\\nturn dependent entities. A dependent entity is one\\nthat cannot exist unless the entity bearing the role\\nexists. For example, a particular patient role cannot\\nexist unless the organism that bears the role (i.e., the\\npatient) exists. A role is optional in the sense that an\\nentity may gain or lose a role without its physical\\nmakeup being changed. For instance, a person may\\ncease to be patient at some practice due to the prac-\\ntice going out of business. The practice\\u0092s going out of\\nbusiness is an event that is external to the person,\\nand, thus, does not necessitate that the person is\\nsomehow physically changed. Roles are realizable in\\nthe sense that their existence can be manifested in a\\ncorrelated process. For instance a dental hygienist role\\nis realized when the hygienist engages in processes\\nrelated to their profession, such as plaque removal\\nand application of fluoride treatment. Roles and other\\ndependent continuants inhere, or are borne by, mater-\\nial entities.\\nEmploying this distinction between roles and their\\nbearers, we define the types dental health care provider\\nand human dental patient by first defining the appropri-\\nate roles for each kind of entity, and then defining pro-\\nviders and patients as being bearers of the roles4:\\nA dental health care provider role is a role that\\ninheres in a person who is licensed to provide den-\\ntal health care and is realized in a health care\\nprocess.\\nA dental health care provider is a human being who\\nbears a dental health care provider role.\\nA patient role is a role that inheres in a person and\\nis realized by the process of being under the care of\\na physician or health care provider. (OGMS)\\nA dental patient role is a patient role that is realized\\nby the process of being under the care of a dental\\nhealth care provider.\\nA human dental patient is a human being who\\nbears a dental patient role.\\nIn order to define the patient\\u0092s gender, we use the gen-\\nder role types from the Ontology of Medically Related\\nSocial Entities (OMRSE). The OMRSE is a realist repre-\\nsentation of medically related social entities developed\\nto cover demographics data and common roles of people\\nin healthcare encounters for reuse in the context of the\\nOBO Foundry. The gender role types are defined as\\nfollows:\\nA gender role is a human social role borne by a hu-\\nman being that is realized in behavior which is con-\\nsidered socially appropriate for individuals of a\\nspecific sex in the context of a specific culture.\\n(OMRSE)\\nA female gender role is a gender role borne by a hu-\\nman being that is realized in behavior which is con-\\nsidered socially appropriate for individuals of the\\nfemale sex in the context of the culture in question.\\n(OMRSE)\\nA male gender role is a gender role borne by a hu-\\nman being that is realized in behavior which is con-\\nsidered socially appropriate for individuals of the\\nmale sex in the context of the culture in question.\\n(OMRSE)\\nFemale and male dental patients are then simply de-\\nfined by relating the patient to the female and male gen-\\nder roles:\\nA female dental patient is a human dental patient\\nwho bears a female gender role.\\nA male dental patient is a human dental patient\\nwho bears a male gender role.\\nUsing roles to define patients and dental health care\\nproviders has two advantages. First, because roles are\\nformally defined, they represent the semantics for how\\nan entity participates in a procedure. That is, for a given\\ndental procedure, the patient participant is the entity\\nwhose participation realizes the dental patient role, and\\nthe provider participant is the entity whose participation\\nrealizes the dental health care provider role. In contrast,\\nfield names and values in relational databases are purely\\nsyntactic.\\nSecond, by using gender roles instead of anatomical\\nsex to represent male and female dental patients, we\\nallow for the possibility that the gender a patient assigns\\nto himself or herself may differ from the patient\\u0092s ana-\\ntomical sex (at birth), matching the common practice of\\nrecording patient-reported gender in clinical systems. In\\nthose cases in which biological sex needs to be4Classes/relations are defined in the OHD unless indicated otherwise.\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 5 of 19\\nrepresented, the OHD includes CARO\\u0092s types female or-\\nganism and male organism:\\nA female organism is a gonochoristic organism that\\ncan produce female gametes. (CARO)\\nA male organism is a gonochoristic organism that\\ncan produce male gametes. (CARO)\\nUsing these classes, a patient\\u0092s biological sex can then\\nbe defined according to biological criteria rather than\\ngender selection.\\nDental procedures\\nWe define the class dental procedure as a subclass of\\nOGMS\\u0092 health care encounter class:\\nA health care encounter is a temporally-connected\\nhealth care process that has as participants an\\norganization or person realizing the health care pro-\\nvider role and a person realizing the patient role.\\nThe health care provider role and patient role are\\nrealized during the health care encounter. (OGMS)\\nA dental procedure is a health care encounter that\\nrealizes a dental patient role in which the patient\\nundergoes a diagnostic or therapeutic process.\\nAs illustrated in Fig. 1, specific dental procedures are\\nthen defined by specializing the dental procedure class.\\nFor instance, endodontic procedure, surgical dental pro-\\ncedure, and tooth restoration procedure are defined as\\nfollows:\\nAn endodontic procedure is a dental procedure that\\nis performed on the pulp chamber and/or root canal\\nof a tooth, or a part thereof.\\nA surgical dental procedure is a dental procedure in\\nwhich there is structural alteration of soft tissue or\\nbone in or around the oral cavity by incision or\\ndestruction of tissues or by manipulation with in-\\nstruments causing localized alteration or transporta-\\ntion of tissue, including lasers, ultrasound, ionizing\\nradiation, scalpels, probes, and needles.\\nA tooth restoration procedure is dental procedure in\\nwhich either a whole tooth or a part of a tooth is re-\\nplaced by dental restoration material in order to re-\\nestablish the tooth\\'s anatomical and functional form\\nand function.\\nMore specific surgical and restoration procedures are\\nthen defined as subclasses of these terms. For example, a\\nnon-exhaustive set of surgical and restorative procedures\\ndefined in the OHD include:\\nA tooth extraction procedure is a surgical dental\\nprocedure that removes a tooth from the oral cavity.\\nA crown restoration procedure is a tooth restoration\\nprocedure whereby an artificial crown replaces all or\\npart of the natural dental crown.\\nA direct restoration procedure is a tooth restoration\\nprocedure in which the dental restoration material is\\nplaced in the tooth via some direct dental material\\ninsertion process.\\nAn indirect restoration procedure is a tooth restor-\\nation procedure in which the dental restoration ma-\\nterial is placed in the tooth via some dental material\\ntooth attachment process.\\nAn intracoronal restoration procedure is a tooth res-\\ntoration procedure in which a dental restoration ma-\\nterial is placed into a site that is located in the\\ncrown of the tooth.\\nA veneer restoration procedure is a tooth restoration\\nprocedure in which a thin layer of material (i.e., a\\nveneer) is placed over one or more surfaces of the\\nFig. 1 A portion of the hierarchy of health care encounters in OHD. Numbers represent the number of direct subclasses for a class, some not\\nshown for reasons of space\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 6 of 19\\ntooth for purposes such as improving the aesthetics\\nof the tooth or protecting the tooth\\'s surface from\\ndamage.\\nPatients and providers are related to dental procedures\\nusing BFO\\u0092s has participant and realizes relations. The\\nhas participant relation is a general way of relating pro-\\ncesses to the entities involved in them. For example, an\\noral evaluation (minimally) has as participants the pa-\\ntient undergoing the evaluation and the provider doing\\nthe evaluation. The realizes relation holds between a\\nprocess and a realizable entity such as a role. A role is\\ndefined in terms of what bears it, what process realizes\\nit, and the manner in which the bearer participates in\\nthe process. As an example, consider the aforementioned\\ndental patient role and dental health care provider role.\\nWhen a dental procedure is performed, the procedure\\nrealizes the roles of the patient and provider. The person\\nupon whom the procedure is performed acts (or be-\\nhaves) as the dental patient and the person doing the\\nprocedure acts (or behaves) as the provider. In this way,\\nthe dental procedure realizes the dental patient role of\\nthe patient and the dental health care provider role of\\nthe provider.\\nBFO defines temporal regions and a relation occupies\\ntemporal region that defines the temporal span of a\\nprocess. However, we don\\u0092t use this representation for\\ntwo reasons. First, there isn\\u0092t yet an established OBO\\npractice for specifying concrete dates. Second, the time\\nof a procedure is recorded only to the granularity of a\\nday. Pending development of representations that ac-\\ncommodate these issues, we defined a date property, oc-\\ncurrence date, that relates a process to an xsd:dateTime\\nsome time during which the process occurred. Another\\ndata property birth date was defined to relate a patient\\nto their date of birth.\\nTo characterize the way in which a tooth participates\\nin a specific dental procedure, we define roles that are\\nborne by the tooth and realized in the appropriate corre-\\nsponding procedure. For example, in order to represent\\nthat a tooth undergoes a root canal treatment, we specify\\nthat a tooth bears a particular tooth to undergo endodon-\\ntic procedure role and this role is then realized in a par-\\nticular endodontic procedure.\\nFor procedures that involve restorative materials, we de-\\nfine a dental restoration material role that is borne by (i.e.\\npossessed by) the restoration material. The role helps de-\\nfine the material in a domain-neutral way. All gold is\\nmetal, but not all gold is used in dental restorations, just\\nthose that bear the dental restoration material role.\\nThis role is then realized by the corresponding restor-\\nation procedure. For instance, an intracoronal restor-\\nation procedure (see above) realizes the dental\\nrestoration material role of the material that is placed\\ninside the crown of the tooth. In procedures that involve\\na specific kind of material, we use OBI\\u0092s has_specified_\\ninput relation to express that a procedure uses that ma-\\nterial. For example, an amalgam filling restoration is de-\\nfined as follows:\\nAn amalgam filling restoration is an intracoronal\\nrestoration procedure that uses amalgam to restore\\nthe tooth.\\nAs part of the logical framework of the OHD, we then\\ninclude the axiom that an amalgam filling restoration\\nhas_specified_input some portion of amalgam restor-\\nation material.\\nRestoration materials, restored teeth, and prosthetics\\nFor dental procedures that involve the use of restoration\\nmaterials (e.g., amalgam), we define the restoration ma-\\nterials in terms of the role the material has in replacing\\nportions of the tooth. In general, dental restoration ma-\\nterial has the role of serving as a prosthetic, that is, the\\nmaterial has the role of replacing a missing body part.\\nHowever, not all prosthetics replace the function of the\\nmissing body part, for example, a prosthetic eye cannot\\nsee, although it still functions to maintain the shape of\\nthe skull near the eyes. To address this, we define the\\nterm functional prosthetic role to represent a prosthetic\\nthat performs the function of the replaced body part.\\nSince dental restoration materials perform the function\\nof parts of the tooth they replace, we define dental res-\\ntoration material role as a subtype of functional pros-\\nthetic role:\\nA functional prosthetic role is a prosthetic role that\\nis realized by activities in which the material entity\\n(bearing the role) is used a manner that is similar to\\nhow the body part that the prosthesis replaces\\nwould be used.\\nA dental restoration material role is a functional\\nprosthetic role that is borne by a portion of dental\\nrestoration material and is realized in a tooth restor-\\nation procedure in which the restoration material\\nbecomes part of a restored tooth.\\nFunctional prosthetic role is not a term that is sp'</span></dd><dt>text9</dt><dd><span style=white-space:pre-wrap>'RESEARCH Open Access\\nIdentifying disease trajectories with\\npredicate information from a knowledge\\ngraph\\nWytze J. Vlietstra1* , Rein Vos1,2, Marjan van den Akker3,4, Erik M. van Mulligen1 and Jan A. Kors1\\nAbstract\\nBackground: Knowledge graphs can represent the contents of biomedical literature and databases as subject-\\npredicate-object triples, thereby enabling comprehensive analyses that identify e.g. relationships between diseases.\\nSome diseases are often diagnosed in patients in specific temporal sequences, which are referred to as disease\\ntrajectories. Here, we determine whether a sequence of two diseases forms a trajectory by leveraging the predicate\\ninformation from paths between (disease) proteins in a knowledge graph. Furthermore, we determine the added\\nvalue of directional information of predicates for this task. To do so, we create four feature sets, based on two\\nmethods for representing indirect paths, and both with and without directional information of predicates (i.e.,\\nwhich protein is considered subject and which object). The added value of the directional information of predicates\\nis quantified by comparing the classification performance of the feature sets that include or exclude it.\\nResults: Our method achieved a maximum area under the ROC curve of 89.8% and 74.5% when evaluated with\\ntwo different reference sets. Use of directional information of predicates significantly improved performance by 6.5\\nand 2.0 percentage points respectively.\\nConclusions: Our work demonstrates that predicates between proteins can be used to identify disease trajectories.\\nUsing the directional information of predicates significantly improved performance over not using this information.\\nKeywords: Knowledge graph, Disease trajectories, Predicates, Temporal relationships, Directionality of predicates,\\nProtein-protein interactions\\nBackground\\nKnowledge graphs can be used to represent the biomed-\\nical knowledge published in literature and databases [1].\\nKnowledge is formalized as subject-predicate-object tri-\\nples, where pairs of entities are related to each other by\\npredicates [2]. By integrating triples from a variety of\\nsources, knowledge graphs can be used to perform com-\\nputational analyses on the comprehensive body of bio-\\nmedical knowledge [3]. Previous work has used such\\nanalyses to identify new relationships between pairs of\\nentities, e.g., between drugs and diseases [4, 5], genes\\nand phenotypes [6, 7], or between diseases [8, 9].\\nMuch research has been performed with knowledge\\ngraphs that only consist of proteins, commonly referred\\nto as protein-protein interaction networks. Through the\\ninvolvement of proteins in metabolic, signaling, immune,\\nand gene-regulatory networks, protein-protein inter-\\naction networks can help to mechanistically explain dis-\\nease and physiological processes [10\\u009612]. Even though\\npredicates further specify the types of interactions be-\\ntween proteins, thereby providing additional information\\nthat can be analyzed, protein-protein interaction net-\\nworks usually do not use them. Instead, most methods\\nanalyze the network topology of proteins [12]. However,\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: w.vlietstra@erasmusmc.nl\\n1Department of Medical Informatics, Erasmus University Medical Center, Dr.\\nMolewaterplein 50, 3015 GE Rotterdam, the Netherlands\\nFull list of author information is available at the end of the article\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 \\nhttps://doi.org/10.1186/s13326-020-00228-8\\nwe have recently shown that analyses that are performed\\non protein knowledge graphs benefit from predicate in-\\nformation [13].\\nBy using the predicates that specify the mechanisms\\nby which proteins interact, temporal pathobiological re-\\nlationships may also be identified, although this has not\\nbeen demonstrated yet. A key application for such tem-\\nporal analyses is the identification of disease trajectories,\\nwhich are commonly occurring temporal sequences of\\ndiseases diagnosed in patients [14, 15]. An example of a\\ndisease trajectory found in a study by Jensen et al. [14] is\\nrheumatoid arthritis-precedes-heart failure, where pre-\\ncedes is defined as \\u0093occurs earlier in time. [\\u0085]\\u0094 [16]. The\\noccurrence of the reverse, heart failure-precedes-\\nrheumatoid arthritis, was found to occur significantly\\nless frequently in the same study, and therefore was not\\nclassified as a trajectory.\\nIdentifying relationships between diseases is an im-\\nportant and popular research topic for protein-protein\\ninteraction networks (see Related work section). In such\\nanalyses diseases are represented by so-called disease\\nproteins, which are proteins encoded by genes that are\\nassociated with a disease [17, 18]. Often cited benefits\\ninclude an improved understanding of the biological\\nmechanisms underlying disease interactions [8, 19, 20],\\nand the ability to anticipate the next disease, thereby\\nproviding the knowledge necessary to improve treatment\\nplans and interventions [14, 21]. However, the temporal\\naspects of relationships between diseases still require\\nfurther investigation. We therefore aim to automatically\\ndetermine whether a given sequence of two diseases\\nforms a trajectory. We do so by leveraging the predicate\\ninformation from paths between (disease) proteins in a\\nknowledge graph. We also determine whether there is\\nadded value in using directional information of predi-\\ncates for this task.\\nRelated work\\nPrevious authors have mostly focused on identifying un-\\ndirected relationships between diseases with protein net-\\nworks [19\\u009623]. For example, Kontou et al. created a\\ndisease-disease graph, where an edge between diseases\\nindicated that they shared at least one disease gene [23].\\nSun et al. calculated the similarity between diseases\\nbased on their shared disease proteins, shared physio-\\nlogical processes associated with these proteins, or the\\ngraph structures between the proteins [20]. Li and Agar-\\nwal identified which biological pathways were associated\\nwith diseases via their disease proteins, and identified re-\\nlationships between diseases based on the number of\\nshared pathways [19]. Menche et al. identified so-called\\ndisease modules, which are clusters of closely interre-\\nlated disease proteins [22]. They found that short dis-\\ntances between the modules of diseases were predictive\\nfor pathobiological relationships. Contrary to Kontou\\net al., they demonstrated that sharing disease proteins is\\nnot a requirement for diseases to be related to each\\nother.\\nTo our knowledge, Bang et al. were the only ones to\\nuse a directed protein-protein interaction network to\\nidentify disease trajectories [21]. The disease proteins of\\npairs of diseases were used to identify shared biomolecu-\\nlar pathways, after which the locations of the disease\\nproteins within these pathways were determined. The\\ndisease with most upstream disease proteins was classi-\\nfied as the first within the sequence of diseases. Add-\\nitionally, 13 million Medicare records were used to\\ncalculate two relative risk scores for each pair of dis-\\neases, corresponding with the two possible temporal se-\\nquences of the disease pair. If the sequence determined\\nwith the protein pathways concurred with the sequence\\nthat generated the largest relative risk, that sequence\\nwas identified as a trajectory. Between a total of 2604\\ndiseases, their method suggested 61 trajectories. These\\nwere evaluated with the biomedical literature, where fur-\\nther leads were found for 16 of them. Because the au-\\nthors only evaluated the trajectories that were suggested\\nby their method, it is unclear how many trajectories the\\nmethod failed to identify.\\nMaterials &amp; methods\\nReference sets\\nThe ability of our method to identify disease trajectories\\nwas evaluated with two reference sets, which have iden-\\ntified disease trajectories by different means. The first\\nreference set consisted of statistically-derived disease tra-\\njectories from a large retrospective study of Danish hos-\\npital data, while the second set consisted of literature-\\nvalidated disease trajectories that were based on a small\\nprospective study of Dutch general-practitioner data.\\nJensen reference set\\nThe first reference set was based on a study of Jensen\\net al. [14]. They retrospectively identified 4014 disease\\ntrajectories from 6.2 million electronic patient records of\\nDanish hospitals based on diagnoses assigned over 14.9\\nyears. All diagnoses in these patient records were repre-\\nsented as International Statistical Classification of Dis-\\neases and Related Health Problems 10th Revision (ICD-\\n10) codes. Jensen used the hierarchy within the ICD-10\\nto aggregate all diagnoses to a high abstraction level,\\nresulting in 681 two-digit codes, such as \\u0093Malignant neo-\\nplasm of breast\\u0094 (C50) or \\u0093Type 2 diabetes mellitus\\u0094\\n(E11).\\nJensen derived the disease trajectories from the Danish\\nhospital data in a two-step process. First, they identified\\nsequences of two diseases that were diagnosed within 5\\nyears from each other in at least 10 patients, and which\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 2 of 11\\nhad a relative risk higher than 1. Subsequently, the direc-\\ntion of each sequence had to be corroborated by a bino-\\nmial test that compared the frequency of the sequence to\\nthe frequency of its reversed sequence. Sequences that ful-\\nfilled both criteria were classified as disease trajectories.\\nTo represent the diseases in the Jensen set on the pro-\\ntein level, we used the expert-annotated associations be-\\ntween proteins and diseases from the manually curated\\nsubset of DisGeNet [18]. The Unified Medical Language\\nSystem (UMLS) MRCONSO table was used to map the\\nICD-10 codes of the Jensen trajectories to the UMLS\\nidentifiers that are used in DisGeNet. Two diseases, \\u0093Ac-\\ncidental poisoning by and exposure to other gases and\\nvapours\\u0094 (E47) and \\u0093Influenza due to identified zoonotic\\nor pandemic influenza virus\\u0094 (J09), were lost because\\ntheir ICD-10 codes could not be mapped to a UMLS\\nidentifier. Because only 25% of the high-level diseases in\\nthe Jensen set were represented within DisGeNet, we\\nused the \\u0093narrower\\u0094 and \\u0093child\\u0094 relationships from the\\nUMLS MRREL table to identify subclasses of all diseases.\\nBy expanding the diseases with their subclasses, the per-\\ncentage of diseases to which disease proteins could be\\nassigned was increased to 68% (465 of 679 diseases).\\nFrom the 4014 disease trajectories in the Jensen set,\\nthere were 2530 trajectories where disease proteins\\ncould be assigned to both diseases (63%). These 2530\\ntrajectories, which were used as positive cases in this ref-\\nerence set, contained 453 of the 465 diseases to which\\ndisease proteins could be assigned (97%). On average,\\ndiseases had 90 disease proteins assigned to them (me-\\ndian: 29, interquartile range: 7\\u009694). Disease proteins\\nwere on average assigned to 6.2 diseases (median: 3,\\ninterquartile range: 2\\u00968).\\nA set of 168,870 non-trajectories was constructed by\\ncreating all possible sequences of the 453 included dis-\\neases, minus the trajectories that were described by Jen-\\nsen. The set of non-trajectories thereby included\\nrandom pairs of diseases, the reversed temporal se-\\nquences of these random pairs, as well as the reversed\\ntemporal sequences of the trajectories. In the following,\\nwe will refer to the trajectories and non-trajectories as\\npositive and negative cases to align with common ter-\\nminology in the machine learning field.\\nVan den Akker reference set\\nThe second reference set was based on a prospective co-\\nhort study on disease susceptibility by Van den Akker\\net al. [24]. They followed a Dutch cohort of 3460\\npatients over 2 years, during which their general practi-\\ntioner notes were examined for sequences of Inter-\\nnational Classification of Primary Care (ICPC) codes\\nthat represent chronic, permanent, and recurrent dis-\\neases. In the Netherlands, each citizen is registered with\\na general practitioner, who acts like a gatekeeper for\\nsecondary and tertiary medical care, and is responsible\\nfor maintaining a complete medical history of the\\npatient.\\nA total of 473 unique sequences of diseases were\\nfound in this cohort, containing 122 distinct diseases.\\nEach sequence was manually evaluated using the pub-\\nlished biomedical literature and medical handbooks.\\nThere were 65 sequences of diseases where the literature\\nstated that the first disease increased the susceptibility of\\nacquiring the second disease, and 408 sequences where\\nno evidence of increased susceptibility was found. To\\nmaintain consistent terminology, we will refer to se-\\nquences with increased susceptibility as trajectories or\\npositives and to sequences without increased susceptibil-\\nity as non-trajectories or negatives.\\nTo assign disease proteins to these 122 diseases we\\nfollowed the same procedure as for the Jensen set by\\nusing the MRCONSO table to map the ICPC codes to\\nUMLS identifiers, after which the MRREL table was used\\nto group them with their subclasses. Disease proteins\\ncould be assigned to 97 diseases, which formed 55 tra-\\njectories and 316 non-trajectories. On average, diseases\\nhad 137 disease proteins assigned to them (median: 49,\\ninterquartile range: 17\\u0096167). Disease proteins were on\\naverage assigned to 3 diseases (median: 2, interquartile\\nrange: 1\\u00964).\\nTo determine whether our method could also identify\\nthe correct temporal sequence of the trajectories, 54\\nadditional non-trajectories were created by reversing the\\nsequence of the diseases in the literature-supported tra-\\njectories (the reverse sequence of one trajectory was\\nalready included as a non-trajectory in the data from the\\ngeneral practitioners).\\nKnowledge graph\\nThe predicates between proteins were extracted from\\nthe Euretos Knowledge Platform (EKP), a commercially\\navailable knowledge graph (http://www.euretos.com). In\\nthe EKP, information from more than 200 knowledge\\nsources from a wide variety of domains in the life sci-\\nences is represented as triples. The biomedical entities\\nsuch as proteins, drugs, or diseases that form the sub-\\njects and objects of these triples are represented in the\\nknowledge graph as vertices, each of which has one or\\nmore identifiers associated with it from external data-\\nbases. Mappings between the entities described in the\\ndifferent knowledge sources underlying the knowledge\\ngraph were made by matching their identifiers. The\\npredicate and provenance of each triple are specified as\\npart of an edge between the two vertices that represent\\nthe subject and object. The direction of the predicate\\ngoes from subject to object. The predicates in the under-\\nlying knowledge sources were matched to a standardized\\nset of 203 predicates. If an exact match was not\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 3 of 11\\navailable, a predicate was manually mapped. If there\\nwere no explicit predicates in a database that was used\\nas a knowledge source, the predicates were manually de-\\nrived from the database schema. A path between two\\nvertices is defined as a sequence of triples, or possibly a\\nsingle triple, connecting the vertices.\\nThe contents of the EKP are represented as documents\\nin a NoSQL database, which allows them to be flexibly\\nmodelled and indexed. The EKP can be run on a\\nreasonably-powered server, requiring an 8-core proces-\\nsor and 60GB of memory as a minimum. It has previ-\\nously been used in pre-clinical research for drug efficacy\\nscreening [13], prioritizing existing drugs as repurposing\\ncandidates for autosomal dominant polycystic kidney\\ndisease [25], and pathway enrichment [26].\\nFeature sets &amp; machine learning\\nThe paths between the disease proteins were extracted\\nfrom the EKP. To keep our graph comprehensible, we\\nonly extracted paths that consisted of one or two triples,\\ni.e., paths where two disease proteins are connected by\\nat most one intermediate protein. Within this range,\\nthree scenarios for the paths between the disease proteins\\nof two diseases A and B were distinguished (Fig. 1.):\\n1) Overlap, where A and B share a disease protein,\\noptionally with a path to itself, e.g. a disease protein\\nof which two copies bind with each other\\n(homodimerization).\\n2) Direct path, where a disease protein of A and a\\ndisease protein of B are part of one triple.\\n3) Indirect path, where one intermediate protein\\nconnects the disease proteins of A and B, requiring\\na sequence of two triples.\\nTwo different methods to represent indirect paths be-\\ntween disease proteins were compared. The first method\\nconstructed so-called metapaths [5], where the sequence\\nof predicates in an indirect path was used as single feature.\\nThe second method, which we refer to as split paths, con-\\nsidered each predicate in the indirect paths as a separate\\nfeature [13]. Each method was tested both with and with-\\nout directional information of predicates. Predicates were\\neither considered to all be undirected, or predicates were\\ncategorized as being directed or undirected based on ex-\\npert assessment (described in the Assessment of predicate\\ndirectionality section below), which we refer to as the\\nMixed variation. In the overlap scenario, where the subject\\nand the object were the same protein, predicates were al-\\nways considered to be undirected.\\nAll features were binary. Figure 2 shows the four\\nfeature sets that are derived from the example\\nshown in Fig. 1. We also experimented with feature\\nsets where all predicates were directed as indicated\\nby the subject and object of the triple in the EKP.\\nHowever, because some predicates are explicitly de-\\nfined as being undirected, using any directional in-\\nformation from triples with these predicates would\\ncontradict these definitions. Nonetheless, for the\\nsake of completeness we have chosen to present\\nthese results in Additional file 1.\\nRandom forests were trained to classify the sequences of\\ndiseases as positive or negative. Classification performance\\nFig. 1 Schematic overview of the overlap, direct, and indirect scenarios that were extracted from the knowledge graph. Both diseases A and\\ndisease B have three disease proteins (DP) associated with them according to the manually curated subset of DisGeNet. DisGeNet describes that\\nDP1 is known to be associated with both diseases, while the knowledge graph describes that it has a \\u0093binds with\\u0094 relationship to itself. DP2 and\\nDP4 have a direct \\u0093inhibits\\u0094 relationship, and DP3 and DP5 are connected through an indirect path, by an intermediate protein (IP). The arrows\\nbetween the proteins indicate which protein is the subject of the \\u0093inhibits\\u0094 predicate, and which one its object. The \\u0093binds with\\u0094 predicate was\\nconsidered to be undirected by the experts, and therefore does not have a direction. Based on the paths in the knowledge graph, four feature\\nsets are created, based on two methods to represent indirect paths, and both with and without the directional information of predicates\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 4 of 11\\nwas measured with the area under the receiver operator\\ncharacteristic curve (AUC) of a 10-fold cross-validation\\nexperiment [27, 28]. We report the mean and standard de-\\nviation of the AUCs of 10 repeated cross-validation exper-\\niments. The same folds that were used in the experiments\\nwith undirected predicates were also used in the experi-\\nments with directed predicates, after which the differences\\nbetween the test folds were tested for significance with a\\ntwo-sided, paired t-test.\\nTo control for the differences in prevalence and num-\\nber of cases between the two reference sets, we also re-\\nport the classification performance after undersampling\\nthe number of positive and negative cases in the Jensen\\nset to match those in the Van den Akker set.\\nFor the best performing classifiers we also report sensi-\\ntivity and specificity at the probability cutoff for which the\\nYouden index (sensitivity + specificity \\u0096 1) is largest [29].\\nMachine learning and evaluation of results were per-\\nformed in R [30] with the packages caret [31], ranger\\n[32], and pROC [33].\\nAssessment of predicate directionality\\nThree experts with a strong biomedical background and\\nfamiliarity with knowledge graphs assessed the direction-\\nality of 47 distinct predicates that were found in the ex-\\ntracted paths. They were provided with definitions of\\nthese predicates which were obtained from the Pathway\\nCommons resource [34]. If not available, definitions\\nwere sought through the National Library of Medicine\\n[35], or the OBO foundry [36]. The assessors could\\ncategorize each predicate as \\u0093directed\\u0094, \\u0093undirected\\u0094, or\\n\\u0093don\\u0092t know\\u0094. To establish directionality, a predicate had\\nto be categorized as directed or undirected by a majority\\n(i.e., two or three) of the assessors. Predicates that con-\\ntain a negation (e.g., \\u0093does not interact with\\u0094) were auto-\\nmatically categorized the same as the corresponding\\npredicate without negation (\\u0093interacts with\\u0094), and there-\\nfore not presented to the assessors. For some predicates\\nthe categorization was straightforward. For example,\\nPathway Commons defines the predicate \\u0093interacts with\\u0094\\nas \\u0093This is an undirected relation between participant\\nFig. 2 The four feature sets that were derived from the paths between the disease proteins in Fig. 1. All features are binary: Black fields indicate a\\n\\u0093True\\u0094 value, while empty fields indicate a \\u0093False\\u0094 value. For the \\u0093Mixed\\u0094 feature sets, the \\u0093Binds with\\u0094 predicate is assessed to be undirected by\\nexperts, while the \\u0093Inhibits\\u0094 predicate is assessed to be directed\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 5 of 11\\nproteins of a molecular interaction. [\\u0085]\\u0094 , and the predi-\\ncate \\u0093catalysis precedes\\u0094 as \\u0093This relation defines di-\\nrected interactions between proteins. [\\u0085]\\u0094 [34]. Six\\npredicates did not reach a majority in the first round\\nand were anonymously commented upon by the asses-\\nsors to motivate their categorization. These comments\\nwere shared between the assessors, after which they\\ncould update their initial choice. Each predicate was\\nthen categorized with a majority.\\nTable 1 shows the 12 predicates that were categorized\\nas undirected by the three experts. The other 35 predi-\\ncates were categorized as directed. A complete overview\\nof the predicates can be found in Additional file 2.\\nResults\\nExtracted paths\\nIn total, 6859 distinct disease proteins were assigned to\\nthe diseases in both reference sets, three of which could\\nnot be mapped to the EKP. Another 430 (6.3%) of the\\ndisease proteins were not found in any of the extracted\\npaths. From these disease proteins, 314 had no relation-\\nship to any other protein in the EKP.\\nThe remaining 6426 disease proteins were involved in\\n1,338,310 direct paths and 833,546,575 indirect paths,\\nwhile 2581 disease proteins had 7354 paths to them-\\nselves. All paths were based on 2,015,738 distinct triples,\\nwhich contained 17,132 different proteins and 47 differ-\\nent predicates.\\nThe overlap scenario, where the two diseases in the\\ntrajectory share at least one disease protein (scenario 1,\\nFeature sets &amp; Machine learning section), occurred in\\n58% of the positive cases of the Jensen set, and 95% of\\nthe positive cases of the Van den Akker set. No indirect\\npaths (scenario 3, Feature sets &amp; Machine learning sec-\\ntion) were found between the disease proteins of 119\\npositive cases (4.7%), and 18,217 negative cases of the\\nJensen set (10.8%), and one positive case (1.8%) and 15\\nnegative cases (4.1%) of the Van den Akker set.\\nClassification results\\nThe classification performance for both reference sets is\\nshown in Table 2. Mixed metapaths performed best,\\nachieving mean AUCs of 89.8% for the Jensen set and\\n74.5% for the Van den Akker set. Overall, classification\\nperformance on the Van den Akker set was 9.9 to 15.3\\npercentage points lower than on the Jensen set, while\\nstandard deviations were 9.6 to 11.3 percentage points\\nhigher. Metapaths performed 4.1 to 7.0 percentage\\npoints better than split paths. The performance of the\\nmixed feature sets was 1.9 to 6.5 percentage points\\nhigher than the undirected feature sets. All differences\\nbetween mixed and undirected feature sets were signifi-\\ncant (p-values for Jensen metapaths and split paths: &lt;\\n0.001; Van den Akker metapaths: 0.02, split paths 0.001).\\nTo quantify how much of the difference in AUC be-\\ntween the two reference sets could be attributed to their\\ndifference in size, the Jensen set was undersampled to\\nthe same number of positive and negative cases as the\\nVan den Akker set. With the exception of the mixed\\nmetapaths, performance dropped below the performance\\nthat was achieved with the Van den Akker set. The\\nstandard deviations also increased from 0.9\\u00961.7% to 8.4\\u0096\\n12.3%. The latter values are comparable to the standard\\ndeviations on the Van den Akker set.\\nFigure 3 shows the receiver operating characteristic\\n(ROC) curves of the mixed metapath classifiers that per-\\nformed best. For the Jensen set, sensitivity and specificity\\nat the maximum Youden index were 79.2% and 82.4%,\\nrespectively, while for the Van den Akker set these were\\n73.6% and 64.3%.\\nError analysis\\nFor our best classifier (mixed metapath features, trained\\non the Jensen set), we analyzed the top-15 false-positive\\nand the top-15 false-negative cases, searching the litera-\\nture for information that might explain the errors. The\\nresults of our analysis of the false positives are shown in\\nTable 3. Overall, the first 10 out of the top 15 false posi-\\ntives appear to be omissions from the Jensen set rather\\nthan misclassifications. For two false-positive cases, po-\\ntential mechanisms have been suggested, but the current\\nevidence is inconclusive on whether those mechanisms\\nare valid. For the remaining three false-positive cases no\\nliterature could be found, which may therefore be inter-\\nesting leads for further investigation.\\nTable 4 shows the results for the top-15 false nega-\\ntives. For six false negatives, the second disease was\\nlikely to be caused by the treatment of the first disease.\\nFor example, the radiation that is used to treat the ma-\\nlignant neoplasm of the larynx may compromise the\\nTable 1 Predicates categorized as undirected as a result of the\\nassessment process\\nUndirected Predicates\\nbinds with\\ncoexists with\\ndoes not coexist with\\nforms protein complex with\\ninteracts with\\ndoes not interact with\\nis associated with\\nis compared with\\nis functionally related to\\nis spatially related to\\nis the same as\\northolog is associated with\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 6 of 11\\nimmune system around the throat and mouth, thereby\\nincreasing susceptibility to oropharyngeal candidiasis\\n[54]. Two false-negative trajectories are likely to have\\nmechanical causes, rather than molecular pathways. The\\ntrajectory from malignant neoplasms of the ovary to nu-\\ntrient deficiency can be explained by the blocking of the\\nintestines by the ovarian tumor, thereby blocking the en-\\ntire digestive system [53]. For four of the false-negative\\ntrajectories, no description could be found in the litera-\\nture, making their assessment impossible. Assessment of\\nthe three remaining false negatives is speculative. For ex-\\nample, the trajectory from transient ischemic attacks\\n(TIA) to vitamin B12 deficiencies may be an artifact of\\nthe medical record keeping. Vitamin B12 is known to\\nprotect against TIAs [52], so what may often happen is\\nthat a vitamin B12 deficiency is only diagnosed after the\\nmore acute TIA has been treated in an emergency room.\\nDiscussion\\nOur work demonstrates that disease trajectories can be\\nidentified with the predicates between proteins in a know-\\nledge graph. To do so, our machine-learning based meth-\\nodology needed to successfully identify both the correct\\npairs of diseases, as well as their correct temporal se-\\nquences. Overall, representing indirect paths as metapaths\\nperformed superior as compared to representing them as\\nsplit paths. Using the directional information of predicates\\nsignificantly improved performance over not using this\\ninformation. Undersampling the Jensen set to the same\\nnumber of positive and negative cases as the Van den\\nAkker set showed that its lower performance and higher\\nstandard deviations could partially be explained by its small\\nsize.\\nIn previous work, Bang et al. [21] identified disease trajec-\\ntories by calculating the relative risk between two diseases\\nand combining this with the relative position of disease pro-\\nteins in biomolecular pathways. Their method is fully\\ndependent on shared disease proteins between the two dis-\\neases, whereas our method also works when there are only\\n(in) direct paths between disease proteins. In the Jensen set,\\nthis holds for 42% of the trajectories. Performance compari-\\nson of the methods is difficult because Bang et al. only vali-\\ndated the disease trajectories that were suggested by their\\nmethod, but did not validate the non-trajectories. Thus,\\nonly the precision of their method can be calculated but no\\ninsight is provided in the number of false-negative trajec-\\ntories. A final complication for the comparison between the\\ntwo methods is the claim of Bang et al. to discover causal\\nrelationships between diseases, rather than only temporal\\nones. Unfortunately, they refer to an example to define\\ncausal relationships between diseases, making it difficult to\\npinpoint how these differ from disease trajectories.\\nAlthough we do not foresee direct clinical application\\nof our work, our high performance may persuade ex-\\nperts to further examine the protein paths underlying\\nsome positively classified trajectories, either known or\\nTable 2 Classification results for the four feature sets for both reference sets\\nJensen set Jensen set - undersampled Van den Akker set\\nMetapaths Split paths Metapaths Split paths Metapaths Split paths\\nUndirected 83.3 (1.7) 78.3 (1.7) 64.2 (12.1) 61.9 (12.3) 72.5 (11.8) 68.4 (13.0)\\nMixed 89.8 (0.9) 82.8 (1.2) 82.3 (8.4) 69.6 (13.1) 74.5 (10.5) 70.3 (11.4)\\nThe values in the columns indicate the mean AUC and its standard deviation in % of 10 cross-validation experiments\\nFig. 3 ROC curves of the mixed metapaths classifiers for the Jensen set and the Van den Akker set\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 7 of 11\\nnewly suggested. Interpreting these protein paths might\\nprovide additional clues about the mechanism through\\nwhich the first disease leads to the second. Identifying\\nand understanding these mechanisms is likely to im-\\nprove prevention, prediction of disease progression,\\nintervention, and drug development, thereby indirectly\\nsupporting clinical practice and health-care policy. For\\nnow, such detailed examinations of the protein paths\\nwere beyond the scope of this project.\\nA downside of working on the protein level was that not\\nall disease trajectories could be studied. More than a third\\nof the trajectories of the Jensen set, and a fifth of the Van\\nden Akker set was lost because disease proteins could not\\nbe assigned to one or both of the diseases in a trajectory.\\nEven when disease proteins could be assigned to both dis-\\neases, alternative explanations were sometimes more\\nplausible. For example, our analysis of the false-negative\\ncases suggested that some trajectories could be explained\\nmechanically, or were likely due to a side effect of the\\ntreatment for the first disease. To determine the true per-\\nformance of our method, a validated set of trajectories that\\nare caused by biomolecular mechanisms would be needed.\\nAlternatively, the range of trajectories that can be analyzed\\nmay be broadened by linking diseases to other types of\\ndisease information available in the EKP, e.g., information\\nabout drugs or physiological processes.\\nThe two reference sets that were used in this research\\nwere both based on patient'</span></dd><dt>text10</dt><dd><span style=white-space:pre-wrap>'Moen et al. Journal of Biomedical Semantics           (2020) 11:10 \\nhttps://doi.org/10.1186/s13326-020-00229-7\\nRESEARCH Open Access\\nAssisting nurses in care documentation:\\nfrom automated sentence classification to\\ncoherent document structures with subject\\nheadings\\nHans Moen1*\\u0086 , Kai Hakala1,2\\u0086, Laura-Maria Peltonen3, Hanna-Maria Matinolli3, Henry Suhonen3,4,\\nKirsi Terho3,4, Riitta Danielsson-Ojala3,4, Maija Valta4, Filip Ginter1, Tapio Salakoski1 and Sanna Salanterä3,4\\nAbstract\\nBackground: Up to 35% of nurses\\u0092 working time is spent on care documentation. We describe the evaluation of a\\nsystem aimed at assisting nurses in documenting patient care and potentially reducing the documentation workload.\\nOur goal is to enable nurses to write or dictate nursing notes in a narrative manner without having to manually\\nstructure their text under subject headings. In the current care classification standard used in the targeted hospital,\\nthere aremore than 500 subject headings to choose from, making it challenging and time consuming for nurses to use.\\nMethods: The task of the presented system is to automatically group sentences into paragraphs and assign subject\\nheadings. For classification the system relies on a neural network-based text classification model. The nursing notes\\nare initially classified on sentence level. Subsequently coherent paragraphs are constructed from related sentences.\\nResults: Based on a manual evaluation conducted by a group of three domain experts, we find that in about 69% of\\nthe paragraphs formed by the system the topics of the sentences are coherent and the assigned paragraph headings\\ncorrectly describe the topics. We also show that the use of a paragraph merging step reduces the number of\\nparagraphs produced by 23% without affecting the performance of the system.\\nConclusions: The study shows that the presented system produces a coherent and logical structure for freely written\\nnursing narratives and has the potential to reduce the time and effort nurses are currently spending on documenting\\ncare in hospitals.\\nKeywords: Patient care documentation, Nursing documentation, Electronic health records, Text classification,\\nNatural language processing, Neural networks, Model interpretation\\n*Correspondence: hnsmoen@gmail.com\\n\\u0086Hans Moen and Kai Hakala contributed equally to this work.\\n1Department of Future Technologies, University of Turku, Vesilinnantie 5,\\n20500 Turku, Finland\\nFull list of author information is available at the end of the article\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 2 of 12\\nBackground\\nCare documentation is important for supporting the con-\\ntinuity of care in hospitals. According to literature, nurses\\nspend up to 35% (with an average of 19%) of their working\\ntime on documentation [1]. Naturally, if we can reduce the\\ntime that nurses spend on documentation, more time will\\nbe available for direct patient care.\\nTo support tasks such as navigation, planning and sta-\\ntistical analysis, nurses in many countries are required\\nto perform structuring of the information they write [2].\\nSuch structuring approaches include the use of documen-\\ntation standards, classifications and standardized termi-\\nnologies [3]. However, this usually adds certain restric-\\ntions and requirements to the documentation process\\ncompared to writing the information in an unstruc-\\ntured narrative manner. In Finland, nurses are nowa-\\ndays expected to structure the information they write\\nby using subject headings from the Finnish Care Clas-\\nsification (FinCC) standard [4]. This includes selecting\\nthe correct subject headings and writing the associated\\ninformation underneath. In this way, each subject head-\\ning forms a paragraph in the nursing note. As an example,\\nif a nurse wants to write something about administrated\\nwound care, he/she will first have to select an appropri-\\nate heading, e.g. \\u0093Wound\\u0094. FinCC consists primarily of two\\ntaxonomy resources, the Finnish Classification of Nurs-\\ning Diagnoses (FiCND) and the Finnish Classification of\\nNursing Interventions (FiCNI), and both of these have a\\nthree-level hierarchy. For example, one branch in FiCND\\nis: \\u0093Tissue integrity\\u0094 (level 1), \\u0093Chronic wound\\u0094 (level 2)\\nand \\u0093Infected wound\\u0094 (level 3). Another example, a branch\\nfrom FiCNI is: \\u0093Medication\\u0094 (level 1), \\u0093Pharmacotherapy\\u0094\\n(level 2) and \\u0093Pharmaceutical treatment, oral instructions\\u0094\\n(level 3). However, FinCC consists of more than 500 sub-\\nject headings, covering both interventions and diagnoses.\\nThis makes it potentially challenging and time consuming\\nfor nurses to use since they are required to memorize, use\\nand structure the information they write according to a\\nlarge number of subject headings [5].\\nWhat we are aiming for is to develop a system that can\\nassist nurses in selecting suitable subject headings and in\\nstructuring the text accordingly.We hypothesize that such\\na system has the potential to save time and effort required\\nfor documentation and ultimately free up more time for\\nother tasks. We see two use-cases for such a system: One\\nis where the system assists nurses in selecting appropri-\\nate headings when they write, in a suggestive manner, e.g.,\\nper sentence or paragraph; A second use-case is where\\nnurses are allowed to write or dictate (by voice to text)\\nin a fully unstructured narrative manner, without having\\nto take into consideration the structure or the use of sub-\\nject headings. Instead the system assigns subject headings\\nafterwards and restructures the text into paragraphs. In\\nthis study we focus on the second use-case.\\nThis is the continuation of a previously reported study\\nthat focused on assessing how an earlier version of the\\nsystem performs on the level of sentences [6]. The main\\nconclusion of that study is that a sentence classification\\nmodel trained on semi-structured nursing notes can be\\napplied on unstructured free nursing narratives without a\\nsubstantial decline in accuracy.\\nThis time we focus on paragraph-level assessment,\\nwhere we also explore a post-processing step aimed at\\nreducing the number of paragraphs initially generated\\nby the system. To evaluate our system, a team of three\\ndomain experts (aka evaluators) conduct a manual evalu-\\nation to assess both the grouping of sentences into para-\\ngraphs and the correctness of the assigned headings. In\\naddition we analyze the classification model in an attempt\\nto identify conflicts between the actual use of the sub-\\nject headings and the intended use according to the FinCC\\ntaxonomy.\\nAt the core of our system is a text classification model\\nbased on a bidirectional long short-termmemory (LSTM)\\nrecurrent neural network architecture [7, 8]. As train-\\ning data we use a large collection of nursing notes from\\na Finnish hospital which contain subject headings and\\nwhich are structured accordingly. Further, to acquire the\\ntype of narrative text that we would like to use as input\\nto the system, without a bias towards a particular struc-\\nture and subject headings, we made a set of nursing notes\\nbased on artificial patients that we use for testing.\\nRelated work\\nAs we focus on classifying individual sentences, the work\\nis closely related to other short text classification studies.\\nHowever, most of the prior work focuses on texts col-\\nlected from social media or other online sources [9\\u009611].\\nInterestingly, Zhang et al. [12] conclude that the optimal\\ntext classification method is strongly dependent on the\\nselected task, warranting domain specific research on this\\ntopic.\\nIn the clinical domain, a common objective for text\\nclassification has been the automated assignment of ICD\\ncodes to the target documents [13\\u009615]. For instance Xie et\\nal. [16] use a neural model for mapping diagnosis descrip-\\ntions extracted from discharge notes to the corresponding\\nICD codes. Similarly Koopman et al. [17] assign ICD-10\\ncodes to death certificates, but limit the scope to various\\ncancer types.\\nFor cases where available training data is scarce, Wang\\net al. [18] propose a system for producing weakly labeled\\ntraining data, where simple rules are initially used to\\nlabel a large set of unlabeled clinical documents and\\nthese labels are subsequently used as training targets for\\nmachine learning based classifiers. The approach is eval-\\nuated on smoking status and hip fracture classification,\\nbut shows mixed results, with a rule-based baseline being\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 3 of 12\\nthe strongest model in some cases. As our training dataset\\ninherently contains the used classification labels, we have\\nnot considered such weak supervision in our research.\\nTo our knowledge the most recent systematic review\\non clinical text classification was conducted by Mujtaba\\net al. [19]. In addition to comparing the classification\\napproaches utilized in different studies, the review focuses\\non the differences in the selected datasets. Their study\\nindicates that along with the medical literature, clinical\\ntext classification research mostly focuses on pathology,\\nradiology and autopsy reports, whereas other clinical doc-\\numents such as nursing care records are far less stud-\\nied. Moreover, the vast majority of the reviewed studies\\nonly evaluate their methods on English data, leading to\\nMujtaba et al. suggesting wider range of languages to be\\nincluded in these studies.\\nAs an additional note, Mujtaba et al. also conclude that\\ndeep learning methods are still relatively poorly studied in\\nthis domain. However, lately neural approaches have been\\nsuggested for a wide range of medical text classification\\npurposes [20\\u009622].\\nMore related to our research are prior studies on clinical\\nnote segmentation. Denny et al. [23] present an approach\\nfor detecting section headers in clinical notes based on the\\nfree text. More precisely, they focus on history and phys-\\nical examination documents where the goal is to identify\\nand normalize section headers as well as to detect section\\nboundaries. Li et al. [24] present a system that catego-\\nrizes sections in clinical notes into one of 15 pre-defined\\nsection labels for notes already split into sections. Their\\napproach relies on modelling the dependencies of consec-\\nutive section labels with Hidden Markov Models. In [25]\\ncoarse topics are assigned to the sections found in clin-\\nical notes. These topics are here seen as separate from\\nthe section headings used by the clinicians when writing,\\nthus the section headings are considered as input to the\\nclassifier along with the free text.\\nA distinction between our study and the prior work\\nis that we operate with an order of magnitude larger\\nset of section labels. Additionally, we rely on semi-\\nstructured nursing notes as training data with the devel-\\noped method subsequently being applied on unstruc-\\ntured notes. Thus, we do not utilize any prior knowledge\\nabout paragraphs/sections. Grouping the text into sensi-\\nble paragraphs is instead a task for the presented system \\u0096\\ntogether with assigning subject headings.\\nMethods\\nOur ultimate goal is to develop a system that is able\\nto automatically identify and classify, on sentence level,\\ninterventions and diagnoses mentioned in nursing narra-\\ntives, and further capable of grouping the text into sensible\\nparagraphs with subject headings reflecting their topics.\\nIn other words, we are aiming for a system that can let\\nnurses simply write or dictate in a narrative manner with-\\nout having to plan and structure the text with respect to\\nparagraphs and subject headings. In pursuing this goal\\nwe have implemented a prototype system with a neural\\nnetwork-based text classification model at its core. In this\\nsection we describe the data and methods used in the\\nimplementation and evaluation.\\nData\\nThe data set used for training is a collection of approxi-\\nmately 0.5 million patients\\u0092 nursing notes extracted from a\\nuniversity hospital in Finland. The selection criteria were\\npatients with any type of heart-related problem in the\\nperiod 2005 to 2009 and nursing notes from all units vis-\\nited during their hospital stay are included. The data is\\ncollected during a transition period between two classi-\\nfication standards, the latter being the mentioned FinCC\\nstandard. This means that our training data contains a\\nmixture of headings from these two. We only use sen-\\ntences occurring in paragraphs with subject headings,\\nwhich amounts to approximately 5.5 million sentences,\\n133,890 unique tokens and approximately 38.5 million\\ntokens in total. We exclude all subject headings used less\\nthan 100 times, resulting in 676 unique subject headings,\\nwhere their frequency count range from 100 to 222,984,\\nwith an average of 4,896. The individual sentences are\\nused as a training example with the corresponding subject\\nheading as the target class to be predicted. The average\\nsentence length is 7 tokens1 and the average number of\\nsentences per paragraph is 2.1. The data set is split into\\ntraining (60%), development (20%) and test (20%) sets and\\nfurther used to train and optimize the text classification\\nmodel.\\nText classification model\\nThe classification task is approached as a sentence-level\\nmulticlass classification task, where each sentence is\\nassumed to have one correct subject heading (label). Our\\ntext classification model is based on a bidirectional short-\\nterm memory (LSTM) recurrent neural network archi-\\ntecture [7, 8]. The model receives a sequence of words\\nas its input and encodes them into latent feature vectors\\n(dimensionality 300). These vectors are subsequently used\\nas the input for a bidirectional LSTM layer (dimensional-\\nity 600 per direction). As the final layer a fully connected\\nlayer with the dimensionality corresponding to the num-\\nber of target subject headings is used. The word embed-\\ndings are pretrained with Word2vec [26]. The model is\\noptimized for categorical cross-entropy with Adam opti-\\nmizer [27], stopping early based on the development set\\nperformance. As machine learning tools we primarily use\\n1Space separated units.\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 4 of 12\\nthe Keras deep learning library [28], with TensorFlow\\nlibrary [29] as backend.\\nWe want to emphasize that the focus of this paper is not\\nto find the optimal text classificationmethod and parame-\\nter settings for this task. This has instead been the focus of\\nanother study [30], where a range of different state-of-the-\\nart and baseline text classification methods are tested and\\ncompared. Results from the mentioned study indicate that\\na bidirectional version of LSTM networks performs best\\nwhen compared to other classification methods/models,\\nincluding convolutional neural networks, support vector\\nmachines and random forests [31\\u009633].\\nOn the test set, when the classifier is allowed to suggest\\none subject heading per sentence, it suggests the correct\\nheading for 54.35% of the sentences according to auto-\\nmated evaluation. When allowed to suggest 10 headings\\nper sentence, the correct one is among these 89.54% of the\\ntime (see [30] for more details).\\nSubject heading prediction and grouping into paragraphs\\nSince our prototype system relies primarily on a sentence-\\nlevel classification model, it starts by classifying each sen-\\ntence individually before grouping them into paragraphs.\\nHowever, this might arguably be the opposite order of how\\na human would approach this task. The system\\u0092s opera-\\ntion can be described as a four-step process. Step 1: First\\nthe text is split into sentences. For this we rely on a com-\\nbination of the NLTK tokenizers for Finnish [34] and a set\\nof regular expressions tailored for the clinical text. Step 2:\\nNext the classification model is used to classify each sen-\\ntence individually and assign the top predicted heading\\n(the one with the highest confidence value). Step 3: As\\na third step the sentences with the same assigned sub-\\nject heading are grouped into paragraphs. Step 4: The\\nfourth step focuses on merging paragraphs whose content\\nand assigned headings are close to each other in terms of\\nmeaning. This fourth step is included to potentially reduce\\nthe number of paragraphs to more closely simulate how\\nnurses document. Below we explain in more detail how\\nthis fourth step is done.\\nParagraphmerging explained: In the previous study [6],\\nthe evaluators reported that the system showed a ten-\\ndency to assign subject headings with a high level of\\nspecificity, and sometimes even too specific to be prac-\\ntical. For example, for two or more sentences describing\\ndifferent aspects of pain management in the same nurs-\\ning note, such as treatment and medication, the system\\nwould in some cases assign these to different subject\\nheadings, possibly headings of different level of speci-\\nficity/abstraction. This meant that, in some cases, unnec-\\nessarily many unique headings, thus paragraphs, were\\nassigned to each nursing note.\\nIn an attempt to reduce the number of paragraphs cre-\\nated, to more closely simulate how nurses document, we\\nhave implemented an experimental post-processing step\\nthat enables the system to merge paragraphs (within a\\nnursing note) that it finds to have similar subject head-\\nings. For this we primarily rely on the confidence values\\nof the classification model, as well as extracted vec-\\ntor representations of each subject heading. The LSTM\\nlayer outputs 600 dimensional sentence encodings for\\nboth directions of the input sequence, resulting in 1200\\ndimensional vectors representations for the subject head-\\nings. These we use to calculate heading similarity by\\napplying the cosine distance metric. See the \\u0093Data anal-\\nysis\\u0094 section for further description of these heading\\nvectors.\\nFirst a paragraph-to-paragraph similarity matrix is\\nformed reflecting how each paragraph would consider the\\nsubject headings from the other paragraphs (from step\\n3) as a likely candidate heading. To this end we define\\na simple asymmetric similarity function which measures\\nhow inclined a paragraph (source) is towards the head-\\ning of another paragraph (target) in the same nursing\\nnote. For each sentence in a given source paragraph we\\ntake the classifier\\u0092s confidence of the sentence belong-\\ning to the target heading and subtract the difference in\\nthe confidence between predicting the source heading\\nand the target heading. The individual sentence scores\\nare averaged and further summed with the cosine dis-\\ntance between the source and target headings and the\\nrelative size of the target paragraph (compared against\\nthe whole nursing note). The first component, relying\\non the confidence values of the classifier, describes how\\nwell the sentences fit in the target paragraph. The sec-\\nond component measures how semantically similar the\\ncompared paragraph headings are, more similar headings\\nbeing more likely to be merged. The third component\\nincreases preference towards retaining the headings of\\nthe larger paragraphs. This scoring function produces\\nvalues in the range 3 to minus 2. Note that it is not\\nsymmetrical.\\nTo determine if two paragraphs are to be merged,\\nwe require that the similarity between these two para-\\ngraphs, in both directions, is above a given threshold.\\nIf the threshold is exceeded, the two most similar para-\\ngraphs are merged, keeping the heading of the para-\\ngraph with the lowest score out of the two. Subsequently\\nthe similarity matrix is recalculated, and the process is\\nrepeated until no paragraph pairs can bemerged.We opti-\\nmize this threshold on a sample of nursing notes from\\nthe test data where paragraph information and head-\\nings are removed. A threshold is found that enables the\\nsystem to generate approximately the same number of\\nparagraphs as in the original versions of these nursing\\nnotes.\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 5 of 12\\nSystem evaluation\\nIn this experiment the focus is on evaluating how the\\nsystem performs at the intended task. Two versions of\\nthe system are manually evaluated, NoMerging and\\nWithMerging, where the difference is that NoMerging\\nonly performs steps 1\\u00963, while WithMerging also per-\\nforms step 4. This comparison is done to see if the para-\\ngraph merging (step 4) can be done without reducing\\nsystem performance according to the evaluators\\u0092 assess-\\nments. To perform the evaluation two domain experts\\nwith nursing background first evaluated the paragraphs\\nindividually. Then we consulted a third domain expert\\nwho provided a third opinion for the instances where the\\ntwo disagreed. Finally the three of them agreed on the final\\nconsensus version which we report here.\\nThe evaluation focuses on two aspects of the struc-\\ntured notes produced by the system: 1) The correctness of\\nthe assigned subject headings at paragraph level. Table 1\\nshows the classes used by the evaluators; 2) The quality of\\nthe formed paragraphs, i.e. sentence grouping. The classes\\nused in this assessment are shown in Table 2.\\nThe nursing notes from the training data have been\\nplanned, structured and written with subject headings in\\nmind. One could argue that by simply removing headings\\nand paragraph information, automated evaluation could\\nbe implemented. However, we found that the sentences\\nhere, which are structured under subject headings, have a\\ntendency to be biased towards the topic of their headings,\\nand sometimes their meaning can only be interpreted in\\nthe context of their headings. Also, this structuring forces\\nthe nurses to write very short and concise things, whereas\\nwhen given the freedom to write in a narrative manner,\\nmore complex sentence structures are present. Thus, to\\nobtain relevant nursing notes for evaluation of our use\\ncase \\u0096 notes written in a free narrative style without plan-\\nning for or considering the use of paragraphs and subject\\nheadings \\u0096 we asked five domain experts with nursing\\nbackground to write notes based on made up artificial\\npatients. In total, 40 nursing notes, each note representing\\none day of provided care for a patient, were generated. The\\ntop part of Fig. 1 shows an example of one such nursing\\nnote.\\nTable 1 Classes used by the evaluators when assessing the\\nheadings assigned by the system\\nClass Description\\n1 Correct: the subject heading suits the text in this paragraph.\\n2 Partly correct: the subject heading only suits some of the text,\\nnot all.\\n3 Incorrect: the subject heading does not seem to suit any of\\nthe text.\\n4 Unable to assess: unable to asses whether or not this subject\\nheading is suitable.\\nTable 2 Classes used by the evaluators when assessing the\\nparagraphs formed by the system\\nClass Description\\na Sensible grouping: it makes sense to have these sentences\\ngrouped together as a separate paragraph based on their\\ntopic(s) (even if the subject heading may not fit).\\nb Inconsistent/problematic grouping \\u0096 alt1: one or more\\nsentences in this paragraph would fit better in other para-\\ngraph(s) in this note.\\nc Inconsistent/problematic grouping \\u0096 alt2: one or more\\nsentences in this paragraph do not belong in this or any of the\\nother paragraphs in this note.\\nd Unable to assess: unable to evaluate this paragraph.\\nThese 40 nursing notes were fed to the two versions of\\nthe system, NoMerging and WithMerging. For eval-\\nuation purposes the output was stored as spreadsheets,\\none for each system, each containing both the origi-\\nnal and the generated/structured version of each nursing\\nnote.\\nStatistical analyzes were performed to investigate differ-\\nences in themanual evaluations of the two system versions\\n(Pearson\\u0092s chi-squared test), as well as to see if there is\\na possible correlation between manual evaluations and\\nthe classification model\\u0092s confidence values (Spearman\\u0092s\\nrho).\\nTo gain some qualitative feedback on the system, we also\\nasked the evaluators to answer the following open-ended\\nquestions:\\nQ1: Can you mention the main strengths that you found\\nwith the system(s)?\\nQ2: Can you mention the main weaknesses that you\\nfound with the system(s)?\\nQ3: Do you, or do you not, think that this kind of a\\nsystem would be helpful when it comes to nursing\\ndocumentation, and why?\\nData analysis\\nWe hypothesize that the large amount of subject headings\\nin the FinCC classification standard may cause confusion\\namong the nurses in terms of what headings should be\\nused in documenting the various aspects of the admin-\\nistrated care. Thus, to obtain a deeper understanding of\\nthe evaluated sentence classification model and the care\\ndocumentation conventions of the nurses, we analyze\\nthe heading representations learned by the classification\\nmodel \\u0096 reflecting how they have been used \\u0096 and how\\nthis may differ from their description and intended use\\nbased on FinCC.\\nThe weights of the fully connected output layer of the\\ntrained classifier can be seen as semantic representations\\nof the subject headings since the weights corresponding\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 6 of 12\\nFig. 1 Nursing note example. Top: Without any particular structure or assigned subject headings. Input to the system. Bottom: Grouped into\\nparagraphs with assigned headings. Output from the system. This has been translated from Finnish to English\\nto a given heading define how strongly the heading is\\nactivated for a given input sentence, compared against\\nother possible headings. Thus, two headings with similar\\nweights will have similar probabilities of being assigned\\nto a given input sentence. Inversely, under the assump-\\ntion that the model has learned the classification task well,\\nit can be hypothesized that if two headings have similar\\nweights, the sentences assigned under these headings in\\nthe training data are also similar. Note that these represen-\\ntations are not based on the names of the subject headings,\\nbut instead on the actual sentences written under the\\nheadings.\\nOur main goal in this analysis is to verify whether we are\\nable to find subject headings which are semantically sim-\\nilar according to our classification model, but far apart in\\nthe used FinCC taxonomy, or vice versa. This allows us to\\nidentify conflicts between the actual use of headings and\\ntheir intended use according to the taxonomy. To mea-\\nsure the distances of the subject heading representations\\nwe simply calculate the cosine distance across all heading\\npairs.\\nThe used FinCC classification standard is comprised of\\n3 top level categories: nursing diagnoses, nursing inter-\\nventions and nursing outcomes, however the nursing out-\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 7 of 12\\ncome headings are not present in the used data. Both\\nnursing diagnoses and interventions use a hierarchical\\nstructure with maximum depth of 3. To form a single tree,\\nwe connect the diagnoses and interventions categories\\nwith an artificial root node. This combined tree has amax-\\nimum depth of 4. To measure the distances of headings in\\nFinCC we calculate the shortest path between the head-\\ning nodes in the tree. Although simple, this approach has\\nshown strong performance in measuring concept similar-\\nities in other biomedical ontologies [35].\\nOnce we have the two distances calculated for all sub-\\nject heading pairs \\u0096 cosine distance and distance in the\\ntree \\u0096 we rank each pair based on these two, resulting\\nin two distinct rankings. The conflicting pairs that we\\nselect for further analysis are the ones being furthest apart\\naccording to these two rankings.\\nSince the nursing notes include the used subject head-\\nings as plain text, without containing the actual FinCC\\nidentifiers, we use strict string matching to map the head-\\nings to the corresponding FinCC concepts. This leaves us\\nwith 263 headings for this analysis out of the total 676\\nheadings in our data set. The excluded headings either\\noriginate from the older classification standard or contain\\nspelling variations.\\nResults\\nIn this section we first present the results from the sys-\\ntem evaluation. Next we highlight some of the observa-\\ntions from the analysis of subject heading representations\\naccording to the classification model and the underlying\\nclassification standard.\\nSystem evaluation results\\nThis experiment provided insight into how the system\\nperforms at the intended task of assigning applicable sub-\\nject headings and grouping sentences into paragraphs.\\nTable 3 shows how well the assigned subject headings fit\\nthe text in the paragraphs. Table 4 reflects what the eval-\\nuators think about the integrity of the paragraphs formed\\nby the system.\\nSee Fig. 1 for an example showing a input note to the\\nsystem (top) and the output (bottom) where the text is\\ngrouped into paragraphs with assigned subject headings.\\nTable 3 Subject headings evaluation results. See Table 1 for an\\nexplanation of the classes\\nClass NoMerging n(tot=396) WithMerging n(tot=305)\\n1 70.45% 279 71.15% 217\\n2 14.65% 58 16.72% 51\\n3 14.14% 56 11.80% 36\\n4 0.76% 3 0.33% 1\\n1+2 85.10% 337 87.87% 268\\nTable 4 Paragraph (sentence grouping) evaluation results. See\\nTable 2 for an explanation of the classes\\nClass NoMerging n(tot=396) WithMerging n(tot=305)\\na 79.55% 315 79.02% 241\\nb 15.66% 62 12.13% 37\\nc 3.79% 15 8.52% 26\\nd 1.01% 4 0.33% 1\\nOverall these results show that the system is able to\\nprovide suitable subject headings for about 71% of the\\nparagraphs (class \\u00911\\u0092). They also indicate that about 79%\\nof the paragraphs formed are sensible (class \\u0091a\\u0092). By sensi-\\nble paragraphs we mean that all the sentences within are\\nrelated to the same topic and that none of them would fit\\nbetter elsewhere in the corresponding nursing note.\\nWhen using NoMerging the number of paragraphs\\nformed is 396, with an average of 9.9 per note (min=3,\\nmax=19). When using WithMerging, which also per-\\nforms the paragraph merging step, the number of para-\\ngraphs is reduced by 23%, down to 305, with the average\\nper note being 7.6 (min=2, max=17).\\nWe also calculated how many of the formed para-\\ngraphs were consistent (class \\u0091a\\u0092) while also having a\\nsuitable subject heading (class \\u00911\\u0092). The result is seen in\\nTable 5 and shows that 66.67% (NoMerging) and 68.85%\\n(WithMerging) of the paragraphs are both sensible and\\nhave a correctly describing subject heading assigned to\\nthem. These results show that the merging step results in\\nbasically no loss in performance.\\nPearson\\u0092s chi-squared tests were performed to see\\nwhether there are statistically significant differences\\nbetween the evaluation results of NoMerging and\\nWithMerging based on 1) the subject heading correct-\\nness evaluations, and 2) the paragraph (sentence merging)\\nquality evaluation results2. The evaluation of 1) does not\\nseem to be dependent on what system version was used\\n(X2 (2, N = 697) = 1.20, p = 0.55). However, there is a\\nstatistically significant difference between the two when\\nlooking at 2) (X2 (2, N = 696) = 8.12, p = 0.02).\\nIt is possible that the confidence values of the classifier\\nmay provide some indication of paragraph correctness in\\nthat there is a correlation between the classifier\\u0092s confi-\\ndence value for an assigned heading and the paragraph\\nbeing correct according to the manual evaluation results.\\nUsing Spearman\\u0092s rho to compare the manual evaluation\\nresults of WithMerging with the classifiers confidence\\nvalues for each paragraph\\u0092s assigned heading (average\\nacross sentences), we found there to be a negative correla-\\ntion between classifier confidence values and the heading\\nassignment ratings (Spearman\\u0092s rho = -0.42, \\u0093moderate\\u0094);\\n2Here we excluded classes \\u00914\\u0092 and \\u0091d\\u0092 due to their low frequency (n&lt;5).\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 8 of 12\\nTable 5 Results showing the percentage of sensible paragraphs\\n(i.e. sentence groupings) with correct headings assigned\\nClass NoMerging n(tot=396) WithMerging n(tot=305)\\n1 and a 66.67% 264 68.85% 210\\nas well as between classifier confidence values and the rat-\\nings of th'</span></dd><dt>text11</dt><dd><span style=white-space:pre-wrap>'RESEARCH Open Access\\nDe-identifying free text of Japanese\\nelectronic health records\\nKohei Kajiyama1, Hiromasa Horiguchi2, Takashi Okumura3, Mizuki Morita4 and Yoshinobu Kano1*\\nAbstract\\nBackground: Recently, more electronic data sources are becoming available in the healthcare domain. Electronic\\nhealth records (EHRs), with their vast amounts of potentially available data, can greatly improve healthcare.\\nAlthough EHR de-identification is necessary to protect personal information, automatic de-identification of Japanese\\nlanguage EHRs has not been studied sufficiently. This study was conducted to raise de-identification performance\\nfor Japanese EHRs through classic machine learning, deep learning, and rule-based methods, depending on the\\ndataset.\\nResults: Using three datasets, we implemented de-identification systems for Japanese EHRs and compared the de-\\nidentification performances found for rule-based, Conditional Random Fields (CRF), and Long-Short Term Memory\\n(LSTM)-based methods. Gold standard tags for de-identification are annotated manually for age, hospital, person, sex,\\nand time. We used different combinations of our datasets to train and evaluate our three methods. Our best F1-\\nscores were 84.23, 68.19, and 81.67 points, respectively, for evaluations of the MedNLP dataset, a dummy EHR\\ndataset that was virtually written by a medical doctor, and a Pathology Report dataset. Our LSTM-based method\\nwas the best performing, except for the MedNLP dataset. The rule-based method was best for the MedNLP dataset.\\nThe LSTM-based method achieved a good score of 83.07 points for this MedNLP dataset, which differs by 1.16\\npoints from the best score obtained using the rule-based method. Results suggest that LSTM adapted well to\\ndifferent characteristics of our datasets. Our LSTM-based method performed better than our CRF-based method,\\nyielding a 7.41 point F1-score, when applied to our Pathology Report dataset. This report is the first of study\\napplying this LSTM-based method to any de-identification task of a Japanese EHR.\\nConclusions: Our LSTM-based machine learning method was able to extract named entities to be de-identified\\nwith better performance, in general, than that of our rule-based methods. However, machine learning methods are\\ninadequate for processing expressions with low occurrence. Our future work will specifically examine the\\ncombination of LSTM and rule-based methods to achieve better performance.\\nOur currently achieved level of performance is sufficiently higher than that of publicly available Japanese de-\\nidentification tools. Therefore, our system will be applied to actual de-identification tasks in hospitals.\\nKeywords: De-identification, Electronic health records, Japanese language\\nBackground\\nRecently, more electronic data sources are becoming\\navailable in the healthcare domain. Utilization of\\nelectronic health records (EHRs), with their vast\\namounts of potentially useful data, is an important task\\nin the healthcare domain. New legislation in Japan has\\naddressed the treatment of medical data. The \\u0093Act on\\nthe Protection of Personal Information [1]\\u0094 was revised\\nin 2017 to stipulate that developers de-identify \\u0093special\\ncare-required personal information.\\u0094 This legislation\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: kano@inf.shizuoka.ac.jp\\n1Faculty of Informatics, Shizuoka University, Johoku 3-5-1, Naka-ku,\\nHamamatsu, Shizuoka 432-8011, Japan\\nFull list of author information is available at the end of the article\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 \\nhttps://doi.org/10.1186/s13326-020-00227-9\\nfurther restricts the use of personal identification codes\\nincluding individual numbers (e.g. health insurance card\\nnumbers, driver\\u0092s license card numbers, and governmen-\\ntal personnel numbers), biometric information (e.g. fin-\\ngerprints, DNA, voice, and appearances), and\\ninformation related to disability. This legislation can be\\ncompared with the \\u0093Health Insurance Portability and\\nAccountability Act (HIPAA) [2]\\u0094 of the United States, in\\nthat the Japanese Act in 2017 includes additional codes,\\nwith abstract specifications such as \\u0093you should strive\\nnot to discriminate or impose improper burdens,\\u0094 and\\nwith exclusion of birth dates and criminal histories, as\\nstipulated by HIPAA. Another related act of Japanese le-\\ngislation, the \\u0093Act on Anonymously Processed Medical\\nInformation to Contribute to Medical Research and De-\\nvelopment [3]\\u0094 was established in 2018. This legislation\\nallows specific third-party institutes to handle EHRs,\\nthereby promoting wider utilization of medical data.\\nDe-identification of structured data in EHRs is easier\\nthan that of unstructured data because it is straightfor-\\nward to apply de-identification methods to structured\\ndata such as numerical tables. Although de-identification\\nof unstructured data in EHRs is necessary, it is virtually\\nimpossible to de-identify the huge number of documents\\nmanually.\\nSeveral earlier works have examined EHR de-\\nidentification. The Informatics for Integrating Biology &amp;\\nthe Bedside (i2b2) task [4] in 2006 was intended for\\nautomatic de-identification of clinical records to satisfy\\nHIPAA requirements [2]. An earlier study prepared 889\\nEHRs, comprising 669 EHRs for training and 220 EHRs\\nfor testing. Their annotations included 929 patient tags,\\n3751 doctor tags, 263 location tags, 2400 hospital tags,\\n7098 date tags, 4809 id tags, 232 phone_number tags,\\nand 16 age tags. The best performing method of i2b2 in-\\ncorporated diverse features such as a lexicon, part-of-\\nspeech identification, word frequencies, and dictionaries\\nfor learning using an ID3 tree learning algorithm.\\nGrouin and Zweigenbaum [5] prepared 312 cardiovas-\\ncular EHRs in French, with 3142 tags annotated by two\\nannotators (kappa = 0.87). Their tags include 238 date\\ntags, 205 last_name tags, 109 first_name tags, 43 hospital\\ntags, 22 town tags, 8 zip_code tags, 8 address tags, 8\\nphone tags, 8 med_device tags, 3 serial_number tags. Of\\nthe person tags, 75% were replaced with other French\\nperson names. The other 25% were replaced with inter-\\nnational names. They also collected 10 photopathology\\ndocuments, for which a single annotator assigned 29\\ndate tags, 68 last_name tags, 53 first_name tags, 17 hos-\\npital tags, 17 town tags, 13 zip_code tags, 14 address\\ntags, 1 phone tag, 1 med_device tag, and 7 serial_number\\ntags. They performed de-identification experiments\\nusing 250 documents as their training data and 62 docu-\\nments as their test data for the cardiology corpus. They\\nobtained better F1-scores (exact match, 0.883; overlap\\nmatch, 0.887) using conditional random fields (CRF)\\nthan they obtained using their rule-based method (exact\\nmatch, 0.843; overlap match, 0.847). However, their\\nrule-based method was better for the photopathology\\ncorpus (exact match, 0.681; overlap match, 0.693) than\\ntheir CRF-based method (exact match, 0.638; overlap\\nmatch, 0.638) because the data were fewer than those of\\nthe cardiology corpus.\\nGrouin and Névéol [6] discussed annotation guidelines\\nfor French clinical records. After collecting 170,000 doc-\\numents of 1000 patient records from five hospitals, they\\nfirst prepared a rule-based system and their CRF-based\\nsystem from their earlier study [5], which we described\\nearlier. Their rule-based system relies on 80 patterns\\nspecifically designed to process the training corpus, and\\nlists which they gathered from existing resources from\\nthe internet. They randomly selected 100 documents\\n(Set 1) from their dataset and applied both systems. For\\neach document, they randomly showed one output of\\nthe two systems to the annotators for revision. They ap-\\nplied their rule-based system to another set of 100 docu-\\nments (Set 2), which were further reviewed and revised\\nby a human annotator. They re-trained their CRF-based\\nsystem using the revised Set 2 annotations, which is fur-\\nther applied to the other set of 100 documents (Set 3).\\nAnnotators reviewed these annotations in subsets for\\ndifferent agreement analyses. The study also compared\\nhuman revision times among different annotation sets,\\nwhich was a main objective of their study. They anno-\\ntated 99 address tags, 101 zip_code tags, 462 date tags,\\n47 e-mail tags, 224 hospital tags, 59 identifier tags, 871\\nlast_name tags, 750 first_name tags, 383 telephone tags,\\n218 city tags, in Set 1. They reported their rule-based\\nmethod as better (0.813) in terms of the F1-score than\\ntheir CRF-based method (0.519) when evaluated with 50\\ndocuments in Set 1. When trained with Set 2, the corpus\\nof the same domain, their CRF-based system performed\\nbetter, yielding 0.953 for Set 3 and 0.888 for Set 1 in\\ntheir F1-scores.\\nFrom the Stockholm EPR [7], a Swedish database of\\nmore than one million patient records from two thou-\\nsand clinics, Dalianis and Velupillai [8] extracted 100 pa-\\ntient records to create gold standard for automatic de-\\nidentifications based on HIPAA. They annotated 4423\\ntags, including 56 age tags, 710 date_part tags, 500 full_\\ndate tags, 923 last_name tags, 1021 health_care_unit\\ntags, 148 location tags, and 136 phone_number tags.\\nThey pointed out that Swedish morphology is more\\ncomplex than that of English. It includes more inflec-\\ntions, making the de-identification task in Swedish more\\ndifficult.\\nJian et al. [9] compiled a dataset of 3000 documents in\\nChinese. It comprises 1500 hospitalization records, 1000\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 2 of 12\\nsummaries, 250 consulting records, and 250 death re-\\ncords. They extracted 300 documents from this dataset\\nrandomly, discussed a mode of de-identification with\\nlower annotation cost. They annotated their tags to\\nthese 300 documents (kappa = 0.76 between two annota-\\ntors for their 100 document subset). Then they applied\\ntheir pattern-matching module to these 300 documents,\\nyielding a dense set of 201 sentences that include PHI\\n(Protected Health Information). These 201 sentences in-\\ncluded 141 name tags, 51 address tags, and 22 hospital\\ntags.\\nDu et al. [10] conducted de-identification experiments\\nusing 14,719 discharge summaries in Chinese: two stu-\\ndents annotated 25,403 tags. This dataset includes 6403\\ninstitution tags, 11,301 date tags, 33 age tags, 2078 pa-\\ntient_name tags, 3912 doctor_name tags, 326 province\\ntags, 310 city tags, 774 country tags, 917 street tags, 277\\nadmission_num tags, 21 pathological_num tags, 23 x-\\nray_num tags, 263 phone tags, 420 doctor_num tags, and\\n13 ultrasonic_num tags (inter-annotator agreement was\\n96%, kappa = 0.826). Their experiments demonstrated\\nthat their method of combining rules and CRF per-\\nformed best, yielding a 98.78 F1-score. The Chinese lan-\\nguage shares some issues with the Japanese language:\\nthey both require tokenization because no spaces exist\\nbetween words. This issue makes de-identification tasks\\nmore difficult than they are in other languages.\\nThe reports described above present a range of differ-\\nent evaluation scores. However they adopted different\\nannotation criteria, which make direct comparison diffi-\\ncult. For instance, Grouin and Névéol used more de-\\ntailed annotations than those used by Jian et al., as\\nfollows. Jian et al. introduced Doctor and Patient tags,\\nbut evaluated both simply as Name. Grouin and Névéol\\nintroduced ZipCode, Identifier, Telephone, and City tags,\\nnone of which is annotated in the work of Jian et al.\\nAdditionally, they assigned Last Name and First Name\\ntags, where performance of First Name was better than\\nLast Name by around 10 points. However, both are\\nworse than the results reported by Jian et al., probably\\nbecause Jian et al. applied their pattern-matching algo-\\nrithm to filter their training data. Regarding Address\\ntags, Jian et al. obtained a 94.2 point F-score, whereas\\nthe Grouin and Névéol CRF method obtained scores of\\nfewer than 10 points. As Grouin and Névéol suggested,\\neliminating City tags in street names can greatly improve\\ntheir results: their rule-based method yielded an 86 point\\nF-score.\\nUnfortunately, automatic de-identification of EHRs\\nhas not been studied sufficiently for Japanese language.\\nDe-identification shared tasks for Japanese EHRs were\\nheld as tasks in MedNLP-1 [11]. Then named entity ex-\\ntraction was attempted in MedNLP-2 [12] tasks using\\ndatasets similar to MedNLP-1. We designate MedNLP-1\\nsimply as MedNLP hereinafter because we specifically\\nexamine de-identification tasks but not other tasks held\\nin the MedNLP shared task series.\\nRegarding machine learning methods, Support Vector\\nMachine (SVM) [13] and CRF [14] were used often in\\nearlier Named Entity Recognition (NER) tasks in\\naddition to rule-based methods. Recent deep learning\\nmethods include Long-Short Term Memory (LSTM)\\n[15] with character-embedding and word-embedding\\n[16], which performed best for the CoNLL 2002 [17]\\n(Spanish and Dutch) and CoNLL 2003 [18] (English and\\nGerman) NER shared task data: these tasks require de-\\ntection of \\u0093personal\\u0094, \\u0093location\\u0094, \\u0093organization\\u0094, and\\n\\u0093other\\u0094 tag types. Another LSTM model, which is similar\\nto earlier work [16], was also applied to a task of NER\\nfrom Japanese newspapers [19]. Although deep neural\\nnetwork models have been showing better results re-\\ncently, rule-based methods are still often better than ma-\\nchine learning methods, especially when insufficient\\nannotated data are available.\\nTo evaluate the effectiveness of such different methods\\nfor the Japanese language, we implemented two EHR de-\\nidentification systems for the Japanese language in our\\nearlier work [20]. We used the MedNLP shared task\\ndataset and our own dummy EHR dataset, which was\\nwritten as a virtual database by medical professionals\\nwho hold medical doctor certification. Based on this\\nearlier work, we added a new dataset of pathology re-\\nports to this study, for which we annotated the following\\ntags. De-identification tags of age, hospital, sex, time,\\nand person are annotated manually in all these datasets,\\nfollowing the annotation standard of the MedNLP\\nshared task to facilitate comparison with earlier studies.\\nWe assume these annotations as our gold standard for\\nour de-identification task. To these three datasets, we\\napplied a rule-based method, a CRF-based method, and\\nan LSTM-based method. Additionally, we have anno-\\ntated our own tags to these three datasets by three anno-\\ntators to calculate inter-annotator agreement. We have\\nobserved the coherency of the original annotations of\\nthe datasets. Overall, this study differs from our earlier\\nwork [20] in that we added a new pathology dataset and\\nits annotations, trained and evaluated our machine\\nlearning models using the new dataset, and evaluated\\nthe results using newly created annotations by three an-\\nnotators to observe characteristics of the original and\\nour own annotations.\\nDatasets\\nOur datasets were derived from three sources: MedNLP,\\ndummy EHRs, and pathology reports. Irrespective of the\\ndataset source, de-identification tags of five types are an-\\nnotated manually: age (numerical expressions of sub-\\nject\\u0092s ages including its numerical classifiers), hospital\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 3 of 12\\n(hospital names), sex (male or female), time (subject re-\\nlated time expressions with its numerical classifiers), and\\nperson (person names). Characteristics of these datasets\\nare presented in Table 1. It is noteworthy that texts of\\nthe MedNLP and dummy EHRs are not actual texts, but\\nthey were written by medical professionals, each of\\nwhom holds medical doctor certification. However, char-\\nacteristics of the descriptions differ between these two\\nsources, probably because of differences of the writers.\\nThe number of annotators is not described for the\\nMedNLP dataset, but a single annotator created the an-\\nnotations of the dummy EHR dataset and the Pathology\\nReport dataset, individually.\\nMedNLP shared task dataset\\nWe used the MedNLP de-identification task dataset for\\ncomparison with earlier studies that have used the same\\ndataset. This dataset includes the dummy EHRs\\n(discharge summaries) of 50 patients. Although the\\ntraining dataset and test dataset were provided from the\\nshared task organizers, the test dataset of the formal run\\nis not publicly available now. It is not possible to\\ncompare results directly with earlier works in the\\nMedNLP shared task formal run (Tables 2 and 3 show\\nthe formal run results). However, both training and test\\ndatasets were originally parts of a single dataset. There-\\nfore, we can discuss their characteristics in comparison\\nwith those found in earlier works conducted using the\\ntraining dataset only. We calculated inter-annotator\\nagreement by three annotators for the training dataset.\\nThe average F1-score of three pairs among these three\\nannotators was 86.1, in 500 sentences of this dataset.\\nDummy EHRs\\nAnother source is our original dummy EHRs. We\\nbuilt our own dummy EHRs of 32 patients, assuming\\nthat the patients were hospitalized. Documents of our\\ndummy EHRs were written by medical professionals\\n(doctors). We added manual annotations for de-\\nidentification following the guidelines of the MedNLP\\nshared task. These annotations were originally\\nassigned by a single annotator. Additionally, we added\\nTable 1 Dataset characteristics\\nDataset name MedNLP Dummy-EHRs Pathology Reports\\n# of documents 50 reports 32 pairs of records and summaries 1000 reports\\n# of sentences 2244 8183 3012\\n# of tokens 42,621 154,132 194,449\\n# of all tags 490 3017 295\\n# of age tags 56 39 0\\n# of hospital tags 75 170 31\\n# of person tags 0 135 224\\n# of sex tags 4 16 0\\n# of time tags 355 2657 40\\nExample in\\noriginal Japanese\\ntext\\n????????&lt;a &gt; 64\\n?&lt;/a &gt;? &lt; x &gt;??&lt;/\\nx &gt;?\\n???????????&lt;a &gt; 86?&lt;/a &gt; &lt;x &gt;?\\n?&lt;/x &gt;????\\n&lt;&lt;???? &lt;h &gt;?????????\\n?&lt;/h &gt;? &lt; p &gt;???&lt;/p&gt;\\nExample\\ntranslated into\\nEnglish\\nA &lt; a &gt; 64-year-old&lt;/a &gt; &lt;x &gt;\\nman&lt;/x &gt; works in a factory\\nAn &lt;a &gt; 86-year-old&lt;/a &gt; &lt;x &gt; woman&lt;/x &gt;\\nbedridden in a nursing home. Total assistance\\nrequired\\n&lt;&lt;Ex-hospital sample &lt; h &gt; Shizudai\\nDermatology Clinic&lt;/h &gt; , &lt; p &gt; Satoshi\\nKuwata&lt;/p&gt;\\nTable 2 Overall results\\nP R F A\\nC3 89.59 91.67 90.62 99.58\\nB3 91.67 86.57 89.05 99.54\\nB1 90.05 87.96 88.99 99.49\\nB2 90.82 87.04 88.89 99.52\\nC1 92.42 84.72 88.41 99.49\\nA1 91.50 84.72 87.98 99.47\\nC2 91.50 84.72 87.98 99.46\\nA2 90.15 84.72 87.35 99.41\\nD1 86.10 74.54 79.90 99.36\\nG1 82.09 76.39 79.14 99.38\\nD3 85.87 73.15 79.00 99.35\\nD2 80.81 74.07 77.29 99.24\\nH2 76.17 75.46 75.81 99.28\\nH1 75.81 75.46 75.64 99.27\\nH3 74.88 74.54 74.71 99.26\\nP, R and F were calculated at the phrase level: P, precision; R, recall; F, F1-measure;\\nand A, accuracy. A was calculated in the word level (the agreement ratio of B-*, I-*\\nand O).\\nThe first column stands for participants\\u0092 team names, where the first letter stands\\nfor a team ID and the second numerical value stands for a submission run ID\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 4 of 12\\nnew annotations by three annotators to a part of this\\ndataset and calculated inter-annotator agreement. The\\naverage F1-score of three pairs among these three\\nannotators was 76.1 for 730 sentences of the Dummy\\nEHR dataset.\\nPathology reports\\nThe other source is a dataset of 1000 short pathology\\nreports, that differ greatly from the EHRs above. Pathology\\nreports describe pathological findings by which personal\\ninformation (names of patients, doctors, hospitals, and\\ntime expressions) frequently appears, but for which tags of\\nsex and age rarely appear. Personal names, hospital names,\\nand dates were manually de-identified beforehand by the\\ndataset provider, and replaced with special characters. For\\nmachine learning methods to support realistic training\\nand evaluation, we replaced these special characters with\\nrandomly assigned real entity names as follows. For the\\nhospital names, we collected 96,167 hospital names which\\ncover most of the Japanese hospital names, published by\\nthe Japanese government. For the person names, we\\nmanually created 20 dummy-family names and 20\\ndummy-first names using one of the last names only, or\\ncombining one of the last names and one of the first\\nnames. Additionally, we calculated the inter-annotator\\nagreement by three annotators. The average F1-score of\\nthree pairs among these three annotators was 80.2 for 500\\nsentences of this dataset. This Pathology Report dataset is\\nthe only real (not dummy) dataset among our three\\ndatasets. Because we received manually de-identified\\nversion of the original real pathology reports, no ethical\\nreview was necessary.\\nMethods\\nWe used a Japanese morphological analyzer, Kuromoji,1\\nfor tokenization and part-of-speech (POS) tagging. We\\nregistered our customized dictionary, derived from\\nWikipedia entry names and entries of the Japanese\\nStandard Disease-code master [21], to this morphological\\nanalyzer in addition to the analyzer\\u0092s default dictionary.\\nWe implemented rule-based, CRF-based, and LSTM-\\nbased methods.\\nRule-based method\\nUnfortunately, the implementation of the best system for\\nthe MedNLP-1 de-identification task [22] is not publicly\\navailable. We implemented our own rule-based program\\nbased on the descriptions in their paper, to replicate the\\nsame system to the greatest extent possible. We present\\ntheir rules below for a target word x for each tag type.\\nAge\\nIf x\\u0092s detailed POS is \\u0093numeral\\u0094, then apply the rules in\\nTable 4.\\nHospital\\nIf one of following keywords appeared in x, then mark it\\nas hospital: ?? (a near clinic or hospital), ?? (this\\nclinic or hospital), or ?? (same clinic or hospital).\\nIf x\\u0092s POS is \\u0093noun\\u0094 and if detailed POS is not \\u0093non-au-\\ntonomous word\\u0094, or if x is either \\u0093?\\u0094, \\u0093?\\u0094, \\u0093?\\u0094 or \\u0093?\\u0094 (these\\nsymbols are used for manual de-identification because the\\ndatasets are dummy EHRs), and if suffix of x is one of the\\nTable 3 Detailed results for each privacy type in MedNLP-1 (De-identification task)\\n&lt;a &gt; age &lt;x &gt; sex &lt;t &gt; time &lt;h &gt; hospital name\\nP R F P R F P R F P R F\\nC3 90.32 87.5 88.89 100 100 100 87.16 91.49 89.27 97.30 94.74 96.00\\nB3 90.00 84.38 87.10 100 50.00 66.67 91.30 89.36 90.32 97.06 86.84 91.67\\nB1 93.33 87.5 90.32 100 100 100 90.65 89.36 90.00 89.47 89.47 89.47\\nB2 90.00 84.38 87.10 100 100 100 91.24 88.65 89.93 91.89 89.47 90.67\\nC1 96.67 90.62 93.55 100 50.00 66.67 91.18 87.94 89.53 93.55 76.32 84.06\\nA1 92.86 81.25 86.67 100 50.00 66.67 91.04 86.52 88.73 91.89 89.47 90.67\\nC2 96.67 90.62 93.55 100 50.00 66.67 89.13 87.23 88.17 96.77 78.95 86.96\\nA2 92.86 81.25 86.67 100 50.00 66.67 89.05 86.52 87.77 91.89 89.47 90.67\\nD1 92.31 75.00 82.76 100 50.00 66.67 82.84 78.72 80.73 96.15 65.79 78.12\\nG1 80.65 78.12 79.37 100 50.00 66.67 84.56 81.56 83.03 72.73 63.16 67.61\\nD3 88.89 75.00 81.36 100 50.00 66.67 83.08 76.60 79.70 96.15 65.79 78.12\\nD2 92.31 75.00 82.76 100 50.00 66.67 75.86 78.01 76.92 96.15 65.79 78.12\\nH2 83.87 81.25 82.54 100 100 100 73.79 75.89 74.83 77.78 73.68 75.68\\nH1 80.65 78.12 79.37 100 100 100 75.86 78.01 76.92 70.27 68.42 69.33\\nH3 83.87 81.25 82.54 100 100 100 73.79 75.89 74.83 70.27 68.42 69.33\\nP, R and F were calculated at the phrase level: P, precision; R, recall; F, F1-measure; and A, accuracy. A was calculated in the word level (the agreement ratio of B-*, I-* and O).\\nThe first column stands for participants\\u0092 team names, where the first letter stands for a team ID and the second numerical value stands for a submission run ID\\n1https://www.atilika.com/ja/kuromoji/\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 5 of 12\\nfollowing keywords, then mark it as hospital:?? (hospital\\nor clinic),????? (clinic), or?? (clinic).\\nSex\\nIf x is either ?? (man), ?? (woman), men, women,\\nman, woman (in English), then mark it as sex.\\nTime\\nIf x\\u0092s detailed POS is \\u0093numeral\\u0094 and if x consists\\nof four-digit-numbers+slash+two-or-one-digit-numbers\\n(corresponds to \\u0093yyyy/mm\\u0094) or two-or-one-digit-\\nnumbers+slash+two-or-one-digit-numbers (corresponds to\\n\\u0093mm/dd\\u0094), then mark it as time.\\nTable 4 Rules used for our rule-based method, original Japanese with English translations\\nOption 1 main rule Option 2\\n?\\n(next)\\n??? two years ago ?? (from)\\n?\\n(before)\\n?? last year ?? (until)\\n???\\n(before hospitalization)\\n?? last month ? (\\u0091s)\\n???\\n(after hospitalization)\\n?? last week ?? (early)\\n????\\n(after visit)\\n?? yesterday ?? (last)\\n??\\n(a.m.)\\n?? this year -- (from)\\n??\\n(p.m.)\\n?? this month -- (from)\\n????\\n(after onset)\\n?? this week ?? (over)\\n??????\\n(after onset)\\n?? today ?? (under)\\n??????\\n(after care)\\n?? today ?? (from)\\n?? next year ? (when)\\n?? next month ? (about)\\n?? next week ?? (about)\\n?? tomorrow ?? (about)\\n??? the week after next ?? (early)\\n??? day after tomorrow ?? (mid)\\n?? same year ?? (late)\\n?? same month ? (spring)\\n?? same day ? (summer)\\n?? following year ? (fall)\\n?? the next day ? (winter)\\n?? the next morning ? (morning)\\n?? the previous day ? (noon)\\n?? early morning ? (evening)\\n??? after that ? (night)\\nxx? xx (year) ?? (early morning)\\nxx? xx (month) ?? (early morning)\\nxx?? xx (week) ?? (before)\\nxx? xx (day) ?? (after)\\nxx? xx (o\\u0092clock) ?? (evening)\\nxx? xx (minutes) ?? (about)\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 6 of 12\\nIf x\\u0092s detailed POS is \\u0093numeral\\u0094 and followed by either\\nof ? (old), ? (old), or? (\\u0091s), then mark it as time.\\nIf x is followed further by either of \\u0093??\\u0094, \\u0093??\\u0094, \\u0093?\\n?\\u0094, \\u0093??\\u0094, \\u0093??\\u0094, \\u0093??\\u0094, \\u0093?\\u0094, \\u0093?\\u0094, \\u0093??\\u0094, \\u0093??\\u0094,\\n\\u0093??\\u0094, \\u0093????\\u0094, \\u0093????\\u0094, \\u0093???\\u0094, \\u0093????\\u0094,\\nor \\u0093????\\u0094, then include these words in the span of\\nthe marked time tag.\\nCRF-based method\\nWe implemented a CRF-based system because many\\nparticipants used CRFs in the MedNLP-1 de-identification\\ntask, including the second-best team and the baseline\\nsystem. The best participant used a rule-based system, as\\ndescribed previously. We used the MALLET2 library for\\nCRF implementation. We defined five training features for\\neach token3: part-of-speech (POS), detailed POS, character\\ntype (Hiragana, Katakana, Kanji, or Number), a binary\\nfeature whether a token is included in our user dictionary\\nor not, and another binary feature whether a token is\\nbeginning of its sentence or not.\\nLSTM-based method\\nOur LSTM-based method combines bidirectional LSTM\\n(bi-LSTM) and CRF, using character-based and word-\\nbased embeddings (Fig. 1) following earlier work that\\nhad been reported as successful for other languages [16].\\nFor word-based embedding, we used the existing\\nWord2Vec [23] model, which was trained using Japanese\\nWikipedia.4 We used bi-LSTM to embed characters;\\nthen we concatenated these two embeddings. This\\nconcatenated output was fed to another bi-LSTM and\\nthen sent to a CRF to output IOB tags.\\nOur implementation has been made publicly available\\nin GitHub.5 Table 5 presents the parameter settings.\\nResults\\nExperiment settings and evaluation metrics\\nWe followed the evaluation metrics of the MedNLP-1\\nshared task using IOB2 tagging [24]. We used four-fold\\ncross validation, whereas the rule-based method requires\\nno training data. We prepared five datasets: MedNLP\\n(MedNLP), dummy EHRs (dummy), pathology reports\\n(pathology), and MedNLP + dummy EHRs (MedNLP +\\ndummy). We also prepared a dataset that comprises\\nthese three datasets (all). For each dataset, we applied\\ncross validation. The CRF and LSTM are trained with\\nthree patterns of training data: the target dataset only,\\none of other datasets only, MedNLP + dummy, and all.\\nOur evaluation uses a strict match of named entity\\nspans, calculating F1-scores, precisions, and recalls.\\nTable 6 presents the evaluation results.\\nResults obtained using the MedNLP dataset\\nIn this MedNLP dataset, the total number of sex is very\\nsmall; that of person is zero. The rule-based system per-\\nformed best in terms of the F1-score because its rules\\nwere tuned originally to the very MedNLP dataset.\\nLSTM performed best for age and time, probably be-\\ncause these tags exhibit typical patterns of less variation.\\nLSTM is superior to Rule, except for sex and hospital.\\nRegarding sex, we observe better performance when\\nLSTM uses more training data. Therefore, the data size\\nis expected to have been the reason why LSTM was not\\ngood in sex.\\nResults obtained using the dummy EHR dataset\\nLSTM (M + d) performed best in terms of the F1-score.\\nCRF performed better when trained by M+ d dataset\\nthan with the target dataset only. This performance in-\\ncrease consists of decrease of age and increase of all\\nother tags, suggesting that these two datasets differ in\\ntheir age tag annotation scheme.\\nThe overall performance of this dummy EHR dataset\\nis worse than the MedNLP dataset, suggesting that the\\ndummy EHR dataset is more difficult to de-identify.\\nResults obtained using the pathology report dataset\\nThe LSTM-based method was better (81.67) than the\\nCRF-based method (74.26), as shown by the 7.41 point\\nF1-score when applied to our Pathology Report dataset.\\nOur rule-based system achieved very high recall, but\\nvery low precision scores for time, exhibiting a difference\\nby 38 points. The pathology reports include many clin-\\nical inspection values written in an \\u0093xx/yy\\u0094 format, which\\nmight engender confusion with dates expressed in an\\n\\u0093mm/dd\\u0094 format. We applied a workaround to limit\\n[1 &lt; = mm &lt; = 12] and [1 &lt; = dd &lt; = 31], but it was insuf-\\nficient: we need contextual information, not just rules.\\nIn addition, hospital is better than time, with less differ-\\nence (15 points) of precision and recall.\\nWhen trained with the Pathology Report dataset only,\\nits performance is better than our rule-based system.\\nWhen trained with the M+ d dataset, which does not\\ncontain the pathology dataset, neither CRF nor LSTM\\nworks fine because the pathology reports differ greatly in\\nterms of their styles of description and named entities.\\nDiscussion\\nThese results suggest that our datasets have quite differ-\\nent characteristics in what context and in what form\\ntheir named entities appear, but LSTM adapted to these\\ndifferences well. Adding the Pathological Report dataset\\n2http://mallet.cs.umass.edu/\\n3Hereinafter, \\u0093token\\u0094 means a \\u0093morpheme\\u0094 of the Japanese language,\\nwhich does not have any space between tokens. A \\u0093morpheme\\u0094 is the\\nsmallest meaningful unit in a language.\\n4http://www.cl.ecei.tohoku.ac.jp/~m~suzuki/jawiki_vector/\\n5https://github.com/johokugsk\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 7 of 12\\nto the training data seems to degrade the system per-\\nformance for other target test datasets because of the\\ndifferent dataset characteristics (examples presented in\\nTable 1). For example, when trained with the Patho-\\nlogical Report dataset, the hospital tags of the MedNLP\\ndataset show lower performance because of the different\\ndescriptions of hospital names among these two data-\\nsets. The Pathological Report dataset has full hospital\\nnames such as \\u0093Shizudai Dermatology Clinic,\\u0094 but the\\nother two datasets have more casual descriptions such as\\nFig. 1 Conceptual figure of our LSTM-based model, showing embedding and NER in separate figures. + means concatenation. The first figure\\nshows the embedding part, where Wx is an x\\nth input word, Lx,i is an i\\nth letter of the word Wx, r denotes right to left (forward) LSTM, l denotes left\\nto right (backward) LSTM, Vx is an intermediate node which corresponds to Wx. The second figure shows the NER part, where fl denotes forward\\nLSTM, bl denotes backward LSTM, c denotes concatenated vector, finally a CRF layer is shown with an example predicted named entities in the\\nBIO annotation style\\nTable 5 LSTM parameter settings\\nWord embedding size 200\\nCharacter embedding size 100\\nHidden layer of character 100\\nHidden layer of LSTM 300\\nLearning rate 0.001\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 8 of 12\\nTable 6 Evaluation results for each tag and in total, for different methods (rule, CRF, LSTM) and different evaluation datasets\\n(MedNLP, dummy EHR, and pathology reports). M, d, and P respectively denote tr'</span></dd><dt>text12</dt><dd><span style=white-space:pre-wrap>'Wolff et al. Journal of Biomedical Semantics           (2020) 11:12 \\nhttps://doi.org/10.1186/s13326-020-00226-w\\nRESEARCH Open Access\\nMethodologically grounded semantic\\nanalysis of large volume of chilean medical\\nliterature data applied to the analysis of\\nmedical research funding efficiency in Chile\\nPatricio Wolff1, Sebastián Ríos1, David Clavijo1, Manuel Graña2* and Miguel Carrasco3\\nAbstract\\nBackground: Medical knowledge is accumulated in scientific research papers along time. In order to exploit this\\nknowledge by automated systems, there is a growing interest in developing text mining methodologies to extract,\\nstructure, and analyze in the shortest time possible the knowledge encoded in the large volume of medical literature.\\nIn this paper, we use the Latent Dirichlet Allocation approach to analyze the correlation between funding efforts and\\nactually published research results in order to provide the policy makers with a systematic and rigorous tool to assess\\nthe efficiency of funding programs in the medical area.\\nResults: We have tested our methodology in the Revista Médica de Chile, years 2012-2015. 50 relevant semantic\\ntopics were identified within 643 medical scientific research papers. Relationships between the identified semantic\\ntopics were uncovered using visualization methods. We have also been able to analyze the funding patterns of\\nscientific research underlying these publications. We found that only 29% of the publications declare funding sources,\\nand we identified five topic clusters that concentrate 86% of the declared funds.\\nConclusions: Our methodology allows analyzing and interpreting the current state of medical research at a national\\nlevel. The funding source analysis may be useful at the policy making level in order to assess the impact of actual\\nfunding policies, and to design new policies.\\nKeywords: Data science, Machine learning, Latent Dirichlet allocation, Healthcare management, Strategy\\nBackground\\nDue to the speed of innovation and change of research\\ntrends in the medical community, research topic tax-\\nonomies published by governmental agencies for funding\\ncalls often diverge from the reality of the research practice.\\nOur working hypothesis is that semantic topic analysis\\nprovides an unbiased and accurate portrait of the actual\\nresearch topics that are generating published results. In\\nthis paper we exploit the information from a national\\n*Correspondence: manuel.grana@ehu.es\\n2Computational Intelligence Group, University of Basque Country, P. Manuel\\nLardizabal 1, 20018 San Sebastián, Spain\\nFull list of author information is available at the end of the article\\nmedical publication, described below, to identify the areas\\nof active research, correlating them with the acknowl-\\nedged funding sources, and non-funded personal effort\\nbacking these scientific results. This analysis provides the\\npolicymaker with a systematic, unbiased, and automated\\ntool for the evaluation of the results of funding programs,\\nallowing to assess the coherence of the national research\\nfunding policies with the actual research outcomes.\\nMethodology background'</span></dd><dt>text13</dt><dd><span style=white-space:pre-wrap>'RESEARCH Open Access\\nAn integrative knowledge graph for rare\\ndiseases, derived from the Genetic and\\nRare Diseases Information Center (GARD)\\nQian Zhu1*\\u0086 , Dac-Trung Nguyen1\\u0086, Ivan Grishagin1, Noel Southall1, Eric Sid2 and Anne Pariser2\\nAbstract\\nBackground: The Genetic and Rare Diseases (GARD) Information Center was established by the National Institutes\\nof Health (NIH) to provide freely accessible consumer health information on over 6500 genetic and rare diseases. As\\nthe cumulative scientific understanding and underlying evidence for these diseases have expanded over time,\\nexisting practices to generate knowledge from these publications and resources have not been able to keep pace.\\nThrough determining the applicability of computational approaches to enhance or replace manual curation tasks,\\nwe aim to both improve the sustainability and relevance of consumer health information, but also to develop a\\nfoundational database, from which translational science researchers may start to unravel disease characteristics that\\nare vital to the research process.\\nResults: We developed a meta-ontology based integrative knowledge graph for rare diseases in Neo4j. This\\nintegrative knowledge graph includes a total of 3,819,623 nodes and 84,223,681 relations from 34 different\\nbiomedical data resources, including curated drug and rare disease associations. Semi-automatic mappings were\\ngenerated for 2154 unique FDA orphan designations to 776 unique GARD diseases, and 3322 unique FDA\\ndesignated drugs to UNII, as well as 180,363 associations between drug and indication from Inxight Drugs, which\\nwere integrated into the knowledge graph. We conducted four case studies to demonstrate the capabilities of this\\nintegrative knowledge graph in accelerating the curation of scientific understanding on rare diseases through the\\ngeneration of disease mappings/profiles and pathogenesis associations.\\nConclusions: By integrating well-established database resources, we developed an integrative knowledge graph\\ncontaining a large volume of biomedical and research data. Demonstration of several immediate use cases and\\nlimitations of this process reveal both the potential feasibility and barriers of utilizing graph-based resources and\\napproaches to support their use by providers of consumer health information, such as GARD, that may struggle\\nwith the needs of maintaining knowledge reliant on an evolving and growing evidence-base. Finally, the successful\\nintegration of these datasets into a freely accessible knowledge graph highlights an opportunity to take a\\ntranslational science view on the field of rare diseases by enabling researchers to identify disease characteristics,\\nwhich may play a role in the translation of discover across different research domains.\\nKeywords: GARD, Rare diseases, Ontology, Data integration, Knowledge graph\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: qian.zhu@nih.gov\\n\\u0086Qian Zhu and Dac-Trung Nguyen contributed equally to this work.\\n1Division of Pre-Clinical Innovation, National Center for Advancing\\nTranslational Sciences (NCATS), National Institutes of Health (NIH), Rockville,\\nMD 20850, USA\\nFull list of author information is available at the end of the article\\nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 \\nhttps://doi.org/10.1186/s13326-020-00232-y\\nIntroduction\\nAn estimated 30 million people in the United States are\\naffected by a rare disease, which is defined as a disease\\nthat affects fewer than 200,000 individuals in the United\\nStates [1]. The majority of rare disease are thought to\\nhave a genetic etiology [2] with studies reporting them\\nresponsible for almost 10% of adult and 30% of pediatric\\nhospitalizations [3]. Despite the great heterogeneity of\\ndiseases included in this definition, many patients and\\ntheir families share in common struggles, such as with\\ndiagnostic delay leading to \\u0093an average of 7.6 years\\u0094 from\\ninitial onset of symptoms to receiving a diagnosis and\\nrequiring the involvement of 7.3 physicians on average\\n[4]. These shared challenges faced in the broader rare\\ndisease patient community are often due to a lack of\\neither up-to-date information or awareness amongst\\nproviders and the public at large. Efforts to tackle these\\nissues led to the passage of the Rare Disease Act of 2002\\nand the establishment of several programs by the\\nNational Institutes of Health (NIH) to improve research\\nactivities and public access to information on rare dis-\\neases. In particular, the Genetic and Rare Diseases\\n(GARD) information center was charged with providing\\nfreely accessible consumer health information in plain\\nlanguage, and it has been investigating the challenge of\\nshifting from an entirely manual process to leveraging\\ncomputational approaches to curate the accumulated\\nbiomedical and clinical research knowledge of over 6500\\nrare diseases, and more rapidly make information ac-\\ncessible 1) to educate patients, families, and health care\\nproviders with more accurate and real-time knowledge\\nabout a rare disease, and 2) to support novel scientific\\nresearch efforts and apply disease-agnostic translational\\nscience approaches to the field of rare diseases as a\\nwhole [5].\\nGiven the pace of ongoing scientific discovery, parsing\\nthrough the accumulated research publications and\\nconveying this knowledge in a plain language format ac-\\ncessible to low-health literacy audiences presents a sig-\\nnificant task for a single disease, let alone for over 6500\\nrare diseases. Thus, a huge amount of effort to accumu-\\nlate and curate data for rare diseases has been made glo-\\nbally. For instance, the GARD Information Center\\nprovides interpretable profiles in plain language for each\\nrare disease [5]; Orphanet focuses on expert manual cur-\\nation of a disease\\u0092s clinical presentation [6]; and Online\\nMendelian Inheritance in Man® (OMIM®) conducts a\\nsimilar expert-driven focus on defining genotype and\\nphenotype relationships [7]. The discreteness of such\\nheterogeneous data, however, impedes their direct use\\nfor consumer audiences. To overcome this barrier, in\\nthis study, we integrated these well-known resources in\\none knowledge graph to semantically interconnect all\\ndata together by means of the data points as nodes and\\ntheir relationships as edges, as a first step in bridging the\\nuse of these resources in consumer-facing health\\ninformation.\\nBiomedical data integration is an important and tech-\\nnical approach to tackling biomedical problems. Current\\nprogress in computational technology allows vast data\\nstorages and powerful computational processes to be\\nmore affordable and accessible. As a result, biomedical\\nscientists have gradually gained an awareness of the im-\\nportance of pooling diverse types of data pertaining to a\\nspecific medical entity to enhance their research under-\\nstanding [8]. Representing integrative data in the form of\\na graph has attracted many interests, particularly in the\\nbiomedical domain. Karczewski K, et al. have reviewed\\nand discussed the potential and usage and challenges of\\nintegrating diverse types of omics data for human health\\nand disease [9]. Biomedical Informatics Research Net-\\nwork (BIRN) is an integrative resource by semantically\\nintegrating data produced by multiple institutions for\\ndata analysis on Neurosciences [10]. Similar efforts have\\nalso begun to emerge with applications directed at the\\nfield in rare disease, such as the semantic Diseasecard,\\nwhich integrates rare disease data from distinct sources\\nin a semantic web environment [11]. A similar EU plat-\\nform, RD-Connect connects databases, registries, bio-\\nbanks and clinical bioinformatics to support research in\\ndiscovering new genes, biomarkers, and therapeutic\\ntargets more quickly and efficiently [12]. The Monarch\\nInitiative as another analytic platform, semantically inte-\\ngrates genotype and phenotype data across differing spe-\\ncies and sources [13], and has led to the establishment\\nof MONDO (Monarch Merged Disease Ontology) [14]\\nas a cohesive ontology for connecting many of the dis-\\nease databases and resources. The integrative knowledge\\ngraph we introduce in this study applies well-established\\nrare disease data drawn from GARD, Orphanet, OMIM\\nand MONDO as a backbone, and then expands to a\\nwide spectrum of additional biomedical data, including\\nphenotypes, genes and curated FDA approved drugs and\\nFDA orphan drug designations.\\nThere are demonstrated merits and successes in using\\ngraph database to support the management of large bio-\\nmedical datasets. While relational databases excel at man-\\naging relationships between data, graph databases provide\\nunique abilities to manage n-th degree relationships\\namong complex types of biomedical data. Furthermore,\\ngraph databases are particularly apt at representing hier-\\narchical data, such as disease categories and complex se-\\nmantic relationships among different types of data. Neo4j\\nas a graph database management system [15], has been\\nwidely applied in such use cases within the biomedical do-\\nmain. Such as, Gratzl S, et al. demonstrated the utility of\\nNeo4j in developing integrated visual analysis platform for\\nbiomedical data [16]; Himmelstein D, et al. constructed\\nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 Page 2 of 13\\nHetionet, an integrative Neo4j network that encodes\\nknowledge from millions of biomedical studies to\\nprioritize drugs for repurposing [17]. In this paper, we\\nintroduce this rare diseases integrative knowledge graph,\\nbuilt in Neo4j as a backend graph database ingesting a\\nlarge variety of biomedical datasets. We detail data prepar-\\nation and entity resolution methodologies in generating\\ninitial insights and results, and the potential benefits for\\nutilizing a knowledge graph-based approach to interpret\\nbiomedical research at a scale and pace that would be un-\\nsustainable when limited to the manual curation efforts\\nthat define current processes used in curating consumer\\nhealth information.\\nMaterials\\nAt the time of writing, the knowledge graph integrates\\n34 different biomedical datasets including GARD. We\\nbriefly describe several primary resources as below.\\nRare disease related data resources\\nBesides GARD data retrieved from our internal database,\\nall other datasets were downloaded from NCBO Biopor-\\ntal [18].\\nGARD is currently managed by the Office of Rare\\nDiseases Research (ORDR) within the National Center\\nfor Advancing Translational Sciences (NCATS), has\\nremained an important portal for patients, health-care\\nprofessionals, and researchers seeking to understand the\\ncurrent state of genetic and rare diseases. GARD in-\\ncludes curated disease information comprised of 15 dif-\\nferent sections, such as summary, diagnosis, inheritance,\\netc. Notably, not all of GARD diseases have a complete\\nlist of these 15 information sections, due to data unavail-\\nability at the time of curation and update. In this study,\\nwe extracted and applied disease specific information\\nsections, if applicable from our internal GARD database.\\nOther sections, such as Resources, Organizations will be\\nexplored in the future [5].\\nOrphanet is a unique resource, gathering and improv-\\ning knowledge on rare diseases so as to improve the\\ndiagnosis, care and treatment of patients with rare dis-\\neases [6].\\nMonarch Disease Ontology (MONDO) is a semi-\\nautomatically constructed ontology that merges multiple\\ndisease resources to yield a coherent merged ontology\\n[14].\\nOnline Mendelian Inheritance in Man® (OMIM®) is\\na comprehensive, authoritative compendium of human\\ngenes and genetic phenotypes. The full-text, referenced\\noverviews in OMIM contain information on all known\\nmendelian disorders and over 15,000 genes [7].\\nHuman Phenotype Ontology (HPO) provides a stan-\\ndardized vocabulary of phenotypic abnormalities en-\\ncountered in human disease [19].\\nFDA orphan drugs\\nFDA orphan drug designation provides orphan desig-\\nnations to drugs and biologics, which are defined as\\nthose intended for the safe and effective treatment, diag-\\nnosis or prevention of rare diseases/disorders [20]. In\\nthis study, we employed orphan drug designation data\\nfrom the FDA [21], several examples of FDA orphan\\ndrug designations retrieved from the FDA are shown in\\nTable 1. Specifically we utilized the associations between\\nFDA designated drugs (the column of \\u0093Generic Name\\u0094\\nin Table 1) and their designations (the column of \\u0093Or-\\nphan Designation\\u0094 in Table 1). Although the data is pre-\\nsented in a structured form, orphan designation is\\ncaptured in free text, such as examples shown in Table\\n1. In that manner, additional curation was conducted in\\nthis study to be able to map orphan designations to\\nGARD diseases and designated drugs to UNII (Unique\\nIngredient Identifier).\\nInxight Drugs is a drug resource developed by NCAT\\nS. Inxight Drugs [22] incorporates the most comprehen-\\nsive subset of substances and related biological mecha-\\nnisms pertaining to translational research and connects\\nthem to the appropriate disease indications. As part of\\nInxight Drugs, explicit connections between drugs and\\nconditions were manually identified from scientific arti-\\ncles, press releases, FDA labels, and large-scale databases\\n(e.g. AdisInsight [23]). For those identified associations,\\nthe curators manually matched conditions to MeSH,\\nDisease Ontology (DO), and drugs to UNII (Unique In-\\ngredient Identifier). For example, one association pre-\\nsenting in Inxight Drugs is as \\u0093CYROMAZINE\\u0094 (with\\nUNII: CA49Y29RA9) has indication of \\u0093MYIASIS,\\nCUTANEOUS MYIASIS OF SHEEP\\u0094. In this study, we\\nextracted associations between FDA approved drugs and\\ndiseases, and integrated them into our integrative know-\\nledge graph.\\nMethods\\nIn this paper, we detail the process of developing the in-\\ntegrative knowledge graph for rare diseases with inclu-\\nsion of multiple well-known biomedical datasets\\nincluding GARD. We also demonstrate the use of this\\nintegrative graph to support biomedical research for rare\\ndiseases. More details about this process is described as\\nbelow.\\nData collection\\nGARD data is curated in two folds, manual curation by\\ninformation specialists from GARD, and programmatic\\nextraction from Orphanet. The curated data is stored in\\na relational database, from where we extracted GARD\\ndata for this study. GARD provides comprehensive in-\\nformation about rare diseases from different aspects,\\nincluding summary, sign and symptoms, treatment,\\nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 Page 3 of 13'</span></dd><dt>text14</dt><dd><span style=white-space:pre-wrap>'REVIEW Open Access\\nNatural language processing algorithms\\nfor mapping clinical text fragments onto\\nontology concepts: a systematic review\\nand recommendations for future studies\\nMartijn G. Kersloot1,2* , Florentien J. P. van Putten1, Ameen Abu-Hanna1, Ronald Cornet1 and Derk L. Arts1,2\\nAbstract\\nBackground: Free-text descriptions in electronic health records (EHRs) can be of interest for clinical research\\nand care optimization. However, free text cannot be readily interpreted by a computer and, therefore, has\\nlimited value. Natural Language Processing (NLP) algorithms can make free text machine-interpretable by\\nattaching ontology concepts to it. However, implementations of NLP algorithms are not evaluated\\nconsistently. Therefore, the objective of this study was to review the current methods used for developing\\nand evaluating NLP algorithms that map clinical text fragments onto ontology concepts. To standardize the\\nevaluation of algorithms and reduce heterogeneity between studies, we propose a list of recommendations.\\nMethods: Two reviewers examined publications indexed by Scopus, IEEE, MEDLINE, EMBASE, the ACM Digital\\nLibrary, and the ACL Anthology. Publications reporting on NLP for mapping clinical text from EHRs to\\nontology concepts were included. Year, country, setting, objective, evaluation and validation methods, NLP\\nalgorithms, terminology systems, dataset size and language, performance measures, reference standard,\\ngeneralizability, operational use, and source code availability were extracted. The studies\\u0092 objectives were\\ncategorized by way of induction. These results were used to define recommendations.\\nResults: Two thousand three hundred fifty five unique studies were identified. Two hundred fifty six studies\\nreported on the development of NLP algorithms for mapping free text to ontology concepts. Seventy-seven\\ndescribed development and evaluation. Twenty-two studies did not perform a validation on unseen data and\\n68 studies did not perform external validation. Of 23 studies that claimed that their algorithm was\\ngeneralizable, 5 tested this by external validation. A list of sixteen recommendations regarding the usage of\\nNLP systems and algorithms, usage of data, evaluation and validation, presentation of results, and\\ngeneralizability of results was developed.\\n(Continued on next page)\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: m.g.kersloot@amsterdamumc.nl\\n1Amsterdam UMC, University of Amsterdam, Department of Medical\\nInformatics, Amsterdam Public Health Research Institute Castor EDC, Room\\nJ1B-109, PO Box 22700, 1100 DE Amsterdam, The Netherlands\\n2Castor EDC, Amsterdam, The Netherlands\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 \\nhttps://doi.org/10.1186/s13326-020-00231-z\\n(Continued from previous page)\\nConclusion: We found many heterogeneous approaches to the reporting on the development and evaluation of NLP\\nalgorithms that map clinical text to ontology concepts. Over one-fourth of the identified publications did not perform\\nan evaluation. In addition, over one-fourth of the included studies did not perform a validation, and 88% did not\\nperform external validation. We believe that our recommendations, alongside an existing reporting standard, will\\nincrease the reproducibility and reusability of future studies and NLP algorithms in medicine.\\nKeywords: Ontologies, Entity linking, Annotation, Concept mapping, Named-entity recognition, Natural language\\nprocessing, Evaluation studies, Recommendations for future studies\\nBackground\\nOne of the main activities of clinicians, besides providing\\ndirect patient care, is documenting care in the electronic\\nhealth record (EHR). Currently, clinicians document clin-\\nical findings and symptoms primarily as free-text descrip-\\ntions within clinical notes in the EHR since they are not\\nable to fully express complex clinical findings and nuances\\nof every patient in a structured format [1, 2]. These free-\\ntext descriptions are, amongst other purposes, of interest\\nfor clinical research [3, 4], as they cover more information\\nabout patients than structured EHR data [5]. However,\\nfree-text descriptions cannot be readily processed by a\\ncomputer and, therefore, have limited value in research\\nand care optimization.\\nOne method to make free text machine-processable is\\nentity linking, also known as annotation, i.e., mapping\\nfree-text phrases to ontology concepts that express the\\nphrases\\u0092 meaning. Ontologies are explicit formal specifica-\\ntions of the concepts in a domain and relations among\\nthem [6]. In the medical domain, SNOMED CT [7] and\\nthe Human Phenotype Ontology (HPO) [8] are examples\\nof widely used ontologies to annotate clinical data. After\\nthe data has been annotated, it can be reused by clinicians\\nto query EHRs [9, 10], to classify patients into different\\nrisk groups [11, 12], to detect a patient\\u0092s eligibility for clin-\\nical trials [13], and for clinical research [14].\\nNatural Language Processing (NLP) can be used to\\n(semi-)automatically process free text. The literature indi-\\ncates that NLP algorithms have been broadly adopted and\\nimplemented in the field of medicine [15, 16], including\\nalgorithms that map clinical text to ontology concepts\\n[17]. Unfortunately, implementations of these algorithms\\nare not being evaluated consistently or according to a pre-\\ndefined framework and limited availability of data sets and\\ntools hampers external validation [18].\\nTo improve and standardize the development and evalu-\\nation of NLP algorithms, a good practice guideline for\\nevaluating NLP implementations is desirable [19, 20].\\nSuch a guideline would enable researchers to reduce the\\nheterogeneity between the evaluation methodology and\\nreporting of their studies. Generic reporting guidelines\\nsuch as TRIPOD [21] for prediction models, STROBE\\n[22] for observational studies, RECORD [23] for studies\\nconducted using routinely-collected health data, and\\nSTARD [24] for diagnostic accuracy studies, are available,\\nbut are often not used in NLP research. This is presum-\\nably because some guideline elements do not apply to\\nNLP and some NLP-related elements are missing or un-\\nclear. We, therefore, believe that a list of recommenda-\\ntions for the evaluation methods of and reporting on\\nNLP studies, complementary to the generic reporting\\nguidelines, will help to improve the quality of future\\nstudies.\\nIn this study, we will systematically review the\\ncurrent state of the development and evaluation of\\nNLP algorithms that map clinical text onto ontology\\nconcepts, in order to quantify the heterogeneity of\\nmethodologies used. We will propose a structured list\\nof recommendations, which is harmonized from exist-\\ning standards and based on the outcomes of the re-\\nview, to support the systematic evaluation of the\\nalgorithms in future studies.\\nMethods\\nThis study consists of two phases: a systematic review of\\nthe literature and the formation of recommendations\\nbased on the findings of the review.\\nLiterature review\\nA systematic review of the literature was performed\\nusing the Preferred Reporting Items for Systematic re-\\nviews and Meta-Analyses (PRISMA) statement [25].\\nSearch strategy and study selection\\nWe searched Scopus, IEEE, MEDLINE, EMBASE, the As-\\nsociation for Computing Machinery (ACM) Digital Library,\\nand the Association for Computational Linguistics (ACL)\\nAnthology for the following keywords: Natural Language\\nProcessing, Medical Language Processing, Electronic Health\\nRecord, reports, charts, clinical notes, clinical text, medical\\nnotes, ontolog*, concept*, encod*, annotat*, code, and cod-\\ning. We excluded the words \\u0091reports\\u0092 and \\u0091charts\\u0092 in the\\nACL and ACM databases since these databases also contain\\npublications on non-medical subjects. The detailed search\\nstrategies for each database can be found in Additional file\\n2. We searched until December 19, 2019 and applied the\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 2 of 21\\nfilters \\u0093English\\u0094 and \\u0093has abstract\\u0094 for all databases. More-\\nover, we applied the filters \\u0093Medicine, Health Professions,\\nand Nursing\\u0094 for Scopus, the filters \\u0093Conferences\\u0094, \\u0093Jour-\\nnals\\u0094, and \\u0093Early Access Articles\\u0094 for IEEE, and the filter\\n\\u0093Article\\u0094 for Scopus and EMBASE. EndNote X9 [26] and\\nRayyan [27] were used to review and delete duplicates.\\nThe selection process consisted of three phases. In the\\nfirst phase, two independent reviewers with a Medical\\nInformatics background (MK, FP) individually assessed\\nthe resulting titles and abstracts and selected publica-\\ntions that fitted the criteria described below.\\nInclusion criteria were:\\n\\001 Medical language processing as the main topic of\\nthe publication\\n\\001 Use of EHR data, clinical reports, or clinical notes\\n\\001 Algorithm performs annotation\\n\\001 Publication is written in English\\nFig. 1 PRISMA flow diagram\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 3 of 21\\nSome studies do not describe the application of NLP in\\ntheir study by only listing NLP as the used method, instead\\nof describing its specific implementation. Additionally,\\nsome studies create their own ontology to perform NLP\\ntasks, instead of using an established, domain-accepted\\nontology. Both approaches limit the generalizability of the\\nstudy\\u0092s methods. Therefore, we defined the following exclu-\\nsion criteria:\\n\\001 Implementation was not described\\n\\001 Implementation does not use an existing established\\nontology for encoding\\nTable 1 Induced objective tasks with their definition and an example\\nInduced NLP task(s) Description Example\\nConcept detection 1 Assign ontology concepts to phrases in free\\ntext (i.e., entity linking or annotation)\\n\\u0093Systolic blood pressure\\u0094 can be represented as SNOMED-CT\\nconcept 271649006 | Systolic blood pressure (observable entity) |\\nEvent detection Detect events in free text \\u0093Patient visited the outpatient clinic in January 2020\\u0094 is an\\nevent of type Visit.\\nRelationship detection Detect semantic relationships between\\nconcepts in free text\\nThe concept Lung cancer in \\u0093This patient was diagnosed with\\nrecurrent lung cancer\\u0094 is related to the concept Recurrence.\\nText normalization Transform free text into a single canonical\\nform\\n\\u0093This patient was diagnosed with influenza last year.\\u0094 becomes\\n\\u0093This patient be diagnose with influenza last year.\\u0094\\nText summarization Create a short summary of free text and\\npossible restructure the text based on this\\nsummary\\n\\u0093Last year, this patient visited the clinic and was diagnosed with\\ndiabetes mellitus type 2, and in addition to his diabetes, the\\npatient was also diagnosed with hypertension\\u0094 becomes\\n\\u0093Last year, this patient was diagnosed with diabetes mellitus\\ntype 2 and hypertension\\u0094.\\nClassification Assign categories to free text A report containing the text \\u0093This patient is not diagnosed\\nyet\\u0094 will be assigned to the category Undiagnosed.\\nPrediction Create a predictive model based on free text Predict the outcome of the APACHE score based on the\\n(free-text) content in a patient chart.\\nIdentification Identify documents (e.g., reports or patient\\ncharts) that match a specific condition\\nbased on the contents of the document\\nFind all patient charts that describe patients with hypertension\\nand a BMI above 30.\\nSoftware development Develop new or build upon existing NLP\\nsoftware\\nA new algorithm was developed to map ontology concepts\\nto free text in clinical reports.\\nSoftware evaluation Evaluate the effectiveness of NLP software The mapping algorithm has an F-score of 0.874.\\n1.Also known as Medical Entity Linking and Medical Concept Normalization\\nTable 2 Induced objective categories with their definition and associated NLP task(s)\\nInduced category Induced NLP task(s) Definition\\nComputer-assisted coding Concept detection Perform semi-automated annotation (i.e., with a human in the loop)\\nInformation comparison Concept detection\\nEvent detection\\nRelationship detection\\nCompare extracted structured information to information available in free-text form\\nInformation enrichment Concept detection\\nEvent detection\\nRelationship detection\\nText normalization\\nText summarization\\nExtract structured information from free text and attach this new information to the source\\nInformation extraction Concept detection\\nEvent detection\\nRelationship detection\\nExtract structured information from free text\\nPrediction Classification\\nPrediction\\nIdentification\\nUse structured information to classify free-text reports, predict outcomes, or identify cases\\nSoftware development\\nand evaluation\\nSoftware development\\nSoftware evaluation\\nDevelop new NLP software or evaluate new or existing NLP software\\nText processing Text normalization\\nText summarization\\nTransform free text into a new, more comprehensible form\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 4 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\nA\\nfs\\nha\\nr\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nD\\nat\\na\\nW\\nar\\neh\\nou\\nse\\nD\\nat\\na\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n(C\\nPT\\n,H\\nC\\nPC\\nS,\\nIC\\nD\\n-\\n10\\n,I\\nC\\nD\\n10\\nC\\nM\\n/\\nIC\\nD\\n9C\\nM\\n,\\nLO\\nIN\\nC\\n,M\\neS\\nH\\n,S\\nN\\nO\\nM\\nED\\n-\\nC\\nT,\\nRx\\nN\\nor\\nm\\n)\\nN\\not\\nlis\\nte\\nd\\nN\\no,\\non\\nly\\nlin\\nks\\nto\\ncT\\nA\\nKE\\nS\\nso\\nur\\nce\\nco\\nde\\n[2\\n9]\\nA\\nln\\naz\\nza\\nw\\ni\\n20\\n16\\nU\\nK\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nPh\\nen\\noC\\nH\\nF\\nco\\nrp\\nus\\n1\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n0]\\nA\\ntu\\ntx\\na\\n20\\n18\\nSp\\nai\\nn\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nEH\\nR\\ndo\\ncu\\nm\\nen\\nts\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nN\\new\\nIC\\nD\\n(S\\nN\\nO\\nM\\nED\\n-C\\nT\\nfo\\nr\\nno\\nrm\\nal\\niz\\nat\\nio\\nn)\\nN\\not\\nye\\nt,\\nai\\nm\\nto\\nem\\nbe\\nd\\nit\\nin\\nhu\\nm\\nan\\n-s\\nup\\ner\\nvi\\nse\\nd\\nlo\\nop\\nN\\not\\nlis\\nte\\nd\\n[3\\n1]\\nBa\\nrr\\net\\nt\\n20\\n13\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nPa\\nlli\\nat\\niv\\ne\\nca\\nre\\nco\\nns\\nul\\nt\\nle\\ntt\\ner\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\no,\\nbu\\nt\\npl\\nan\\nne\\nd\\n[3\\n2]\\nBe\\nck\\ner\\n20\\n16\\nG\\ner\\nm\\nan\\ny\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSh\\nA\\nRe\\n/C\\nLE\\nF\\nco\\nrp\\nus\\n(2\\n01\\n3)\\n2\\nEx\\nis\\ntin\\ng\\nG\\ner\\nm\\nan\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT\\n(E\\nng\\nlis\\nh)\\n,\\nU\\nM\\nLS\\n(G\\ner\\nm\\nan\\n)\\nN\\not\\nye\\nt,\\nst\\nill\\nun\\nde\\nr\\nde\\nve\\nlo\\npm\\nen\\nt\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n3]\\nBe\\nck\\ner\\n20\\n19\\nG\\ner\\nm\\nan\\ny\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nof\\npa\\ntie\\nnt\\ns\\nw\\nith\\nkn\\now\\nn\\nco\\nlo\\nre\\nct\\nal\\nca\\nnc\\ner\\nO\\nw\\nn\\nG\\ner\\nm\\nan\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nYe\\ns,\\nle\\nd\\nto\\nim\\npr\\nov\\ned\\nqu\\nal\\nity\\nof\\nca\\nre\\nfo\\nr\\nco\\nlo\\nre\\nct\\nal\\npa\\ntie\\nnt\\ns\\nN\\not\\nlis\\nte\\nd\\n[3\\n4]\\nBe\\nja\\nn\\n20\\n15\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nan\\nd\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nda\\nta\\nse\\nt\\n(2\\n01\\n0)\\n3\\nO\\nw\\nn\\n+\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\no\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n5]\\nC\\nas\\ntr\\no\\n20\\n10\\nSp\\nai\\nn\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nw\\nith\\n\\u0091m\\nos\\nt\\nre\\nle\\nva\\nnt\\nin\\nfo\\nrm\\nat\\nio\\nn\\u0092\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n6]\\nC\\nat\\nlin\\ng\\n20\\n18\\nU\\nK\\nN\\no\\nSo\\nft\\nw\\nar\\ne\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nM\\nIM\\nIC\\n-II\\nId\\nat\\nas\\net\\n4\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nIC\\nD\\n-9\\n-C\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[3\\n7]\\nC\\nha\\npm\\nan\\n20\\n04\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nEm\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n8]\\nC\\nhe\\nn\\n20\\n16\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nan\\nd\\npr\\nog\\nre\\nss\\nno\\nte\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[3\\n9]\\nC\\nhi\\nar\\nam\\nel\\nlo\\n20\\n16\\nIta\\nly\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\n(c\\nar\\ndi\\nol\\nog\\ny,\\ndi\\nab\\net\\nol\\nog\\ny,\\nhe\\npa\\nto\\nlo\\ngy\\n,n\\nep\\nhr\\nol\\nog\\ny,\\nan\\nd\\non\\nco\\nlo\\ngy\\n)\\nO\\nw\\nn\\nIta\\nlia\\nn\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[4\\n0]\\nC\\nho\\nde\\ny\\n20\\n16\\nU\\nSA\\nSe\\nm\\nEv\\nal\\n(2\\n01\\n4)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nIC\\nU\\nD\\nat\\na:\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns,\\nEC\\nG\\n,\\nec\\nho\\n,a\\nnd\\nra\\ndi\\nol\\nog\\ny\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[4\\n1]\\nC\\nhu\\nng\\n20\\n05\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nEc\\nho\\nca\\nrd\\nio\\ngr\\nam\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nye\\nt,\\nit\\nw\\nill\\nbe\\nus\\ned\\nto\\npo\\npu\\nla\\nte\\na\\nre\\ngi\\nst\\nry\\nN\\not\\nlis\\nte\\nd\\n[4\\n2]\\nC\\nom\\nbi\\n20\\n18\\nIta\\nly\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nVi\\ngi\\nSe\\ngn\\n(a\\ndv\\ner\\nse\\ndr\\nug\\nre\\nac\\ntio\\nns\\n)\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nIta\\nlia\\nn\\n+\\nEn\\ngl\\nis\\nh\\nN\\new\\nM\\ned\\nD\\nRA\\nYe\\ns,\\nim\\npl\\nem\\nen\\nte\\nd\\nin\\nVi\\ngi\\nFa\\nrm\\nac\\no\\nPs\\neu\\ndo\\nco\\nde\\n[4\\n3]\\nD\\ne\\nBr\\nui\\njn\\n20\\n11\\nC\\nan\\nad\\na\\ni2\\nb2\\n/V\\nA\\n(2\\n01\\n0)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nH\\nos\\npi\\nta\\nld\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nan\\nd\\npr\\nog\\nre\\nss\\nre\\npo\\nrt\\ns\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[4\\n4]\\nD\\nei\\nss\\ner\\not\\nh\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSi\\nx\\nse\\nts\\nof\\nre\\nal\\npa\\ntie\\nnt\\nda\\nta\\nfro\\nm\\nfo\\nur\\ndi\\nffe\\nre\\nnt\\nm\\ned\\nic\\nal\\nce\\nnt\\ner\\ns.\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nH\\nPO\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[4\\n5]\\nD\\nem\\nne\\nr-\\nFu\\nsh\\nm\\nan\\n20\\n17\\nU\\nSA\\nN\\no\\nSo\\nft\\nw\\nar\\ne\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nBi\\noS\\nco\\npe\\n5 ,\\nN\\nC\\nBI\\ndi\\nse\\nas\\ne\\nco\\nrp\\nus\\n6 ,\\ni2\\nb2\\n/\\nVA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3 ,\\nSh\\nA\\nRe\\nco\\nrp\\nus\\n7 ,\\nLH\\nC\\nte\\nst\\nco\\nlle\\nct\\nio\\nn\\n(b\\nio\\nlo\\ngi\\nca\\nl/\\ncl\\nin\\nic\\nal\\njo\\nur\\nna\\nla\\nbs\\ntr\\nac\\nts\\n)\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nYe\\ns,\\nus\\ned\\nin\\not\\nhe\\nr\\npa\\npe\\nrs\\nid\\nen\\ntif\\nie\\nd\\nin\\nlit\\ner\\nat\\nur\\ne\\nse\\nar\\nch\\nYe\\ns\\n[4\\n6]\\nD\\niv\\nita\\n20\\n14\\nU\\nSA\\nPa\\nrt\\ns:\\ni2\\nb2\\n/V\\nA\\nSo\\nft\\nw\\nar\\ne\\nRa\\nnd\\nom\\nly\\nse\\nle\\nct\\ned\\ncl\\nin\\nic\\nal\\nre\\nco\\nrd\\ns\\nfro\\nm\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\n(le\\nve\\nl0\\n+\\n9)\\nYe\\ns,\\nus\\ned\\nby\\nVA\\nYe\\ns\\n[4\\n7]\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 5 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\n(C\\non\\ntin\\nue\\nd)\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\n(2\\n01\\n0)\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nth\\ne\\nm\\nos\\nt\\nfre\\nqu\\nen\\nt\\ndo\\ncu\\nm\\nen\\nt\\nty\\npe\\ns\\nIn\\nfo\\nrm\\nat\\nic\\ns\\nan\\nd\\nC\\nom\\npu\\ntin\\ng\\nIn\\nfra\\nst\\nru\\nct\\nur\\ne\\nD\\nua\\nrt\\ne\\n20\\n18\\nPo\\nrt\\nug\\nal\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nD\\nea\\nth\\nce\\nrt\\nifi\\nca\\nte\\ns,\\ncl\\nin\\nic\\nal\\nbu\\nlle\\ntin\\ns,\\nan\\nd\\nau\\nto\\nps\\ny\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nPo\\nrt\\nug\\nue\\nse\\nN\\new\\nIC\\nD\\n-1\\n0\\nYe\\ns,\\nus\\ned\\nby\\nPo\\nrt\\nug\\nes\\ne\\nM\\nin\\nis\\ntr\\ny\\nof\\nH\\nea\\nlth\\nfo\\nr\\nne\\nar\\nre\\nal\\n-t\\nim\\ne\\nde\\nat\\nh\\nca\\nus\\ne\\nsu\\nrv\\nei\\nlla\\nnc\\ne\\nN\\not\\nlis\\nte\\nd\\n[4\\n8]\\nFa\\nlis\\n20\\n19\\nU\\nK\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nM\\nIM\\nIC\\n-II\\nId\\nat\\nas\\net\\n4\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nIC\\nD\\n-9\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[4\\n9]\\nFe\\nrr\\não\\n20\\n13\\nPo\\nrt\\nug\\nal\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nIn\\npa\\ntie\\nnt\\nad\\nul\\nt\\nep\\nis\\nod\\nes\\nfro\\nm\\nth\\ne\\nEH\\nR\\nO\\nw\\nn\\nPo\\nrt\\nug\\nue\\nse\\nN\\new\\nIC\\nD\\n-9\\n-C\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n0]\\nG\\ner\\nbi\\ner\\n20\\n11\\nFr\\nan\\nce\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nom\\npu\\nte\\nriz\\ned\\nem\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nm\\ned\\nic\\nal\\nre\\nco\\nrd\\ns\\nO\\nw\\nn\\nFr\\nen\\nch\\nN\\new\\nIC\\nD\\n-1\\n0,\\nC\\nC\\nA\\nM\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nA\\nTC\\n,M\\neS\\nH\\n,I\\nC\\nPC\\n-2\\n,\\nD\\nC\\nR\\nN\\not\\nye\\nt,\\nw\\nill\\nbe\\nin\\nte\\ngr\\nat\\ned\\nin\\nto\\na\\nC\\nD\\nSS\\nN\\not\\nlis\\nte\\nd\\n[5\\n1]\\nG\\noi\\nco\\nec\\nhe\\na\\nSa\\nla\\nza\\nr\\n20\\n13\\nSp\\nai\\nn\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nD\\nia\\ngn\\nos\\ntic\\nte\\nxt\\nfro\\nm\\npa\\ntie\\nnt\\nre\\nco\\nrd\\ns\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nN\\new\\nIC\\nD\\n-9\\n-C\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n2]\\nH\\nam\\nid\\n20\\n13\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nN\\not\\nes\\nof\\nIra\\nq\\nan\\nd\\nA\\nfg\\nha\\nni\\nst\\nan\\nve\\nte\\nra\\nns\\nfro\\nm\\nth\\ne\\nVA\\nna\\ntio\\nna\\nlc\\nlin\\nic\\nal\\nda\\nta\\nba\\nse\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[5\\n3]\\nH\\nas\\nsa\\nnz\\nad\\neh\\n20\\n16\\nA\\nus\\ntr\\nal\\nia\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSh\\nA\\nRe\\n/C\\nLE\\nF\\nco\\nrp\\nus\\n(2\\n01\\n3)\\n2\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nap\\npl\\nic\\nab\\nle\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[5\\n4]\\nH\\nel\\nw\\ne\\n20\\n17\\nLe\\nba\\nno\\nn\\nN\\no\\nC\\nom\\npu\\nte\\nr-\\nas\\nsi\\nst\\ned\\nco\\ndi\\nng\\nM\\nIM\\nIC\\n-II\\nId\\nat\\nas\\net\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\n,I\\nC\\nD\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n5]\\nH\\ner\\nsh\\n20\\n01\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nRa\\ndi\\nol\\nog\\ny\\nim\\nag\\ne\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\no,\\nst\\nill\\nin\\nde\\nve\\nlo\\npm\\nen\\nt/\\nte\\nst\\nin\\ng\\nPs\\neu\\ndo\\nco\\nde\\n[5\\n6]\\nH\\noo\\nge\\nnd\\noo\\nrn\\n20\\n15\\nN\\net\\nhe\\nrla\\nnd\\ns\\nN\\no\\nPr\\ned\\nic\\ntio\\nn\\nC\\non\\nsu\\nlta\\ntio\\nn\\nno\\nte\\ns\\nof\\npa\\ntie\\nnt\\ns\\nin\\na\\npr\\nim\\nar\\ny\\nca\\nre\\nse\\ntt\\nin\\ng\\nO\\nw\\nn\\nD\\nut\\nch\\nN\\new\\nSN\\nO\\nM\\nED\\n-C\\nT,\\nU\\nM\\nLS\\n,I\\nC\\nPC\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n7]\\nJin\\nda\\nl\\n20\\n13\\nU\\nSA\\ni2\\nb2\\n(2\\n01\\n2)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n2)\\n8\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nM\\neS\\nH\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n8]\\nKa\\nng\\n20\\n09\\nKo\\nre\\na\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nO\\nw\\nn\\nKo\\nre\\nan\\nN\\new\\nKO\\nM\\nET\\n,U\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n9]\\nKe\\nrs\\nlo\\not\\n20\\n19\\nN\\net\\nhe\\nrla\\nnd\\ns\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\n(N\\non\\n-s\\nm\\nal\\nlc\\nel\\nl)\\nLu\\nng\\nca\\nnc\\ner\\nch\\nar\\nts\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[6\\n0]\\nKö\\nni\\ng\\n20\\n19\\nG\\ner\\nm\\nan\\ny\\nN\\no\\nSo\\nft\\nw\\nar\\ne\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nD\\nis\\nch\\nar\\nge\\nle\\ntt\\ner\\ns\\nfro\\nm\\nBA\\nSE\\n-II\\nst\\nud\\ny\\nO\\nw\\nn\\nG\\ner\\nm\\nan\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nW\\nin\\nge\\nrt\\n-N\\nom\\nen\\ncl\\nat\\nur\\ne\\nN\\no,\\nst\\nill\\nha\\ns\\nto\\npr\\nov\\ne\\nits\\nva\\nlu\\ne\\nN\\not\\nlis\\nte\\nd\\n[6\\n1]\\nLi\\n20\\n15\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nco\\nm\\npa\\nris\\non\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nan\\nd\\ndi\\nsc\\nha\\nrg\\ne\\npr\\nes\\ncr\\nip\\ntio\\nn\\nlis\\nts\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nRx\\nN\\nor\\nm\\nN\\not\\nye\\nt,\\npl\\nan\\ns\\nto\\nm\\nov\\ne\\nto\\npr\\nod\\nuc\\ntio\\nn\\nPs\\neu\\ndo\\nco\\nde\\n[6\\n2]\\nLi\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nEH\\nR\\nno\\nte\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nM\\ned\\nD\\nRA\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[6\\n3]\\nLi\\nng\\nre\\nn\\n20\\n16\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nSt\\nru\\nct\\nur\\ned\\nan\\nd\\nun\\nst\\nru\\nct\\nur\\ned\\nda\\nta\\nfro\\nm\\ntw\\no\\nEH\\nR\\nda\\nta\\nba\\nse\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,I\\nC\\nD\\n-9\\n,R\\nxN\\nor\\nm\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[1\\n2]\\nLi\\nu\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nfro\\nm\\ndi\\nffe\\nre\\nnt\\nin\\nst\\nitu\\ntio\\nns\\n+\\nPu\\nbM\\ned\\nC\\nas\\ne\\nre\\npo\\nrt\\nab\\nst\\nra\\nct\\ns\\nO\\nw\\nn\\n+\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nH\\nPO\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[6\\n4]\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 6 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\n(C\\non\\ntin\\nue\\nd)\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\nLo\\nw\\ne\\n20\\n09\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSi\\nng\\nle\\n-s\\npe\\nci\\nm\\nen\\npa\\nth\\nol\\nog\\ny\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[6\\n5]\\nLu\\no\\n20\\n14\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nPa\\nth\\nol\\nog\\ny\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nYe\\ns,\\ncu\\nrr\\nen\\ntly\\nw\\nor\\nki\\nng\\non\\npr\\noj\\nec\\nt\\nin\\nm\\nul\\ntip\\nle\\nho\\nsp\\nita\\nls\\nN\\not\\nlis\\nte\\nd\\n[6\\n6]\\nM\\ney\\nst\\nre\\n20\\n06\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nC\\nlin\\nic\\nal\\ndo\\ncu\\nm\\nen\\nts\\nfo\\nrm\\nad\\nul\\nt\\nin\\npa\\ntie\\nnt\\ns\\nin\\na\\nca\\nrd\\nio\\nva\\nsc\\nul\\nar\\nun\\nit\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n(le\\nve\\nl0\\n),\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nye\\nt,\\nte\\nst\\nin\\ng\\nin\\npr\\nac\\ntic\\ne\\nN\\not\\nlis\\nte\\nd\\n[6\\n7]\\nM\\ney\\nst\\nre\\n20\\n10\\nU\\nSA\\ni2\\nb2\\n(2\\n00\\n9)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nda\\nta\\nse\\nt\\n(2\\n00\\n9)\\n9\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\not\\nye\\nt,\\npo\\nss\\nib\\nle\\nin\\nte\\ngr\\nat\\nio\\nn\\nin\\nre\\nse\\nar\\nch\\nin\\nfra\\nst\\nru\\nct\\nur\\ne\\nN\\not\\nlis\\nte\\nd\\n[6\\n8]\\nM\\nin\\nar\\nd\\n20\\n11\\nFr\\nan\\nce\\ni2\\nb2\\n/V\\nA\\n(2\\n01\\n0)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[6\\n9]\\nM\\nis\\nhr\\na\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nfro\\nm\\nN\\nIH\\nC\\nlin\\nic\\nal\\nC\\nen\\nte\\nr\\nda\\nta\\nw\\nar\\neh\\nou\\nse\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n,H\\nPO\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[7\\n0]\\nN\\ngu\\nye\\nn\\n20\\n18\\nA\\nus\\ntr\\nal\\nia\\nN\\no\\nC\\nom\\npu\\nte\\nr-\\nas\\nsi\\nst\\ned\\nco\\ndi\\nng\\nH\\nos\\npi\\nta\\nlp\\nro\\ngr\\nes\\ns\\nno\\nte\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nSN\\nO\\nM\\nED\\nC\\nT,\\nIC\\nD\\n-1\\n0-\\nA\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n1]\\nO\\nel\\nlri\\nch\\n20\\n15\\nU\\nK\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nPu\\nbM\\ned\\nab\\nst\\nra\\nct\\ns,\\ncl\\nin\\nic\\nal\\ntr\\nia\\nl\\nin\\nfo\\nrm\\nat\\nio\\nn,\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3 ,\\nSH\\nA\\nRE\\n/C\\nLE\\nF\\n(2\\n01\\n3)\\n2\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[7\\n2]\\nPa\\ntr\\nic\\nk\\n20\\n11\\nA\\nus\\ntr\\nal\\nia\\ni2\\nb2\\n/V\\nA\\n(2\\n01\\n0)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n3]\\nPé\\nre\\nz\\n20\\n18\\nSp\\nai\\nn\\nN\\no\\nTe\\nxt\\npr\\noc\\nes\\nsi\\nng\\nSp\\non\\nta\\nne\\nou\\ns\\nD\\nTs\\nra\\nnd\\nom\\nly\\nse\\nle\\nct\\ned\\nen\\ntr\\nie\\ns\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nN\\new\\nIC\\nD\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n4]\\nRe\\nát\\neg\\nui\\n20\\n18\\nC\\nan\\nad\\na\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n00\\n8)\\n10\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nRx\\nN\\nor\\nm\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n5]\\nRo\\nbe\\nrt\\ns\\n20\\n11\\nU\\nSA\\ni2\\nb2\\n/V\\nA\\n(2\\n01\\n0)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,I\\nC\\nD\\n-9\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n6]\\nRo\\nus\\nse\\nau\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nco\\nm\\npa\\nris\\non\\nED\\nen\\nco\\nun\\nte\\nrs\\nfo\\nr\\npa\\ntie\\nnt\\ns\\nw\\nith\\nhe\\nad\\nac\\nhe\\ns\\nw\\nho\\nre\\nce\\niv\\ned\\nhe\\nad\\nC\\nT\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n:S\\nN\\nO\\nM\\nED\\nC\\nT,\\nRa\\ndL\\nex\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[7\\n7]\\nSa\\nvo\\nva\\n20\\n10\\nU\\nSA\\ni2\\nb2\\n(2\\n00\\n6,\\n20\\n08\\n)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSu\\nbs\\net\\nof\\ncl\\nin\\nic\\nal\\nno\\nte\\ns\\nfro\\nm\\nth\\ne\\nEM\\nR\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nRx\\nN\\nor\\nm\\nYe\\ns,\\nus\\ned\\nin\\not\\nhe\\nr\\npa\\npe\\nrs\\nid\\nen\\ntif\\nie\\nd\\nin\\nlit\\ner\\nat\\nur\\ne\\nse\\nar\\nch\\nYe\\ns\\n[7\\n8]\\nSh\\niv\\nad\\ne\\n20\\n15\\nU\\nSA\\ni2\\nb2\\n/U\\nTH\\nea\\nlth\\n(2\\n01\\n4)\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n4)\\n11\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[1\\n1]\\nSh\\noe\\nnb\\nill\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nEH\\nR\\nno\\nte\\ns\\nfro\\nm\\nhy\\npe\\nrt\\nen\\nsi\\non\\npa\\ntie\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[7\\n9]\\nSo\\nhn\\n20\\n14\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nw\\nith\\nm\\ned\\nic\\nat\\nio\\nn\\nm\\nen\\ntio\\nns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nRx\\nN\\nor\\nm\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[8\\n0]\\nSo\\nlti\\n20\\n08\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nC\\nar\\ndi\\nol\\nog\\ny\\nam\\nbu\\nla\\nto\\nry\\npr\\nog\\nre\\nss\\nno\\nte\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[8\\n1]\\nSo\\nria\\nno\\n20\\n19\\nSp\\nai\\nn\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ncl\\nin\\nic\\nal\\nem\\ner\\nge\\nnc\\ny\\ndi\\nsc\\nha\\nrg\\ne\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nN\\new\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nye\\nt\\nYe\\ns\\n[8\\n2]\\nSo\\nys\\nal\\n20\\n18\\nU\\nSA\\nPa\\nrt\\ns:\\ni2\\nb2\\nSo\\nft\\nw\\nar\\ne\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nfro\\nm\\nth\\ne\\ni2\\nb2\\n/V\\nA\\nO\\nw\\nn\\n+\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nYe\\ns,\\nus\\ned\\nby\\nva\\nrio\\nus\\nYe\\ns\\n[8\\n3]\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 7 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\n(C\\non\\ntin\\nue\\nd)\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\n(2\\n00\\n9\\n+\\n20\\n10\\n),\\nSh\\nA\\nRe\\n/C\\nLE\\nF\\n(2\\n01\\n3)\\n,S\\nem\\n-E\\nVA\\nL\\n(2\\n01\\n4)\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3 ,\\nou\\ntp\\nat\\nie\\nnt\\ncl\\nin\\nic\\nvi\\nsi\\nt\\nno\\nte\\ns,\\nm\\noc\\nk\\ncl\\nin\\nic\\nal\\ndo\\ncu\\nm\\nen\\nts\\nEx\\nis\\ntin\\ng\\nin\\nst\\nitu\\ntio\\nns\\nan\\nd\\nin\\ndu\\nst\\nria\\nl\\nen\\ntit\\nie\\ns\\nSp\\nas\\ni?\\n20\\n15\\nU\\nK\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nM\\nRI\\nre\\npo\\nrt\\ns\\nof\\npa\\ntie\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nTR\\nA\\nK,\\nU\\nM\\nLS\\n,M\\nED\\nC\\nIN\\n,\\nRa\\ndL\\nex\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[8\\n4]\\nSt\\nra\\nus\\ns\\n20\\n13\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nPa\\nth\\nol\\nog\\ny\\nre\\npo\\nrt\\ns\\nof\\nbr\\nea\\nst\\nan\\nd\\npr\\nos\\nta\\nte\\nca\\nnc\\ner\\npa\\ntie\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[8\\n5]\\nSu\\nng\\n20\\n18\\nTa\\niw\\nan\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nas\\nes\\nof\\nad\\nul\\nt\\npa\\ntie\\nnt\\ns\\nw\\nith\\nA\\nIS\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[8\\n6]\\nTc\\nhe\\nch\\nm\\ned\\njie\\nv\\n20\\n18\\nFr\\nan\\nce\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nQ\\nua\\ner\\no\\n(F\\nre\\nnc\\nh\\nM\\nED\\nLI\\nN\\nE\\nab\\nst\\nra\\nct\\ntit\\nle\\ns\\n+\\nEM\\nEA\\ndr\\nug\\nla\\nbe\\nls\\n)+\\nC\\nép\\niD\\nC\\n(IC\\nD\\n-1\\n0\\nco\\ndi\\nng\\nof\\nde\\nat\\nh\\nce\\nrt\\nifi\\nca\\nte\\ns)\\nEx\\nis\\ntin\\ng\\nFr\\nen\\nch\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nte\\nrm\\nin\\nol\\nog\\nie\\ns\\n(IC\\nD\\n-1\\n0)\\nYe\\ns,\\nav\\nai\\nla\\nbl\\ne\\nin\\nSI\\nFR\\nBi\\noP\\nor\\nta\\nl\\nYe\\ns\\n[8\\n7]\\nTe\\nrn\\noi\\ns\\n20\\n18\\nFr\\nan\\nce\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nEn\\ndo\\nsc\\nop\\ny\\nre\\npo\\nrt\\ns\\nw\\nrit\\nte\\nn\\nbe\\ntw\\nee\\nn\\n20\\n15\\nan\\nd\\n20\\n16\\nO\\nw\\nn\\nFr\\nen\\nch\\nN\\new\\nC\\nC\\nA\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[8\\n8]\\nTr\\nav\\ner\\ns\\n20\\n04\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nhi\\nef\\nco\\nm\\npl\\nai\\nnt\\nte\\nxt\\nen\\ntr\\nie\\ns\\nfo\\nr\\nal\\nl\\nem\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nvi\\nsi\\nts\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[8\\n9]\\nTu\\nlk\\nen\\ns\\n20\\n19\\nBe\\nlg\\niu\\nm\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[9\\n0]\\nU\\nsu\\ni\\n20\\n18\\nJa\\npa\\nn\\nN\\no\\nPr\\ned\\nic\\ntio\\nn\\nEl\\nec\\ntr\\non\\nic\\nm\\ned\\nic\\nat\\nio\\nn\\nhi\\nst\\nor\\ny\\nda\\nta\\nfro\\nm\\nph\\nar\\nm\\nac\\ny\\nO\\nw\\nn\\nJa\\npa\\nne\\nse\\nN\\new\\nIC\\nD\\n-1\\n0\\nN\\not\\nye\\nt,\\nex\\npe\\nct\\nto\\nus\\ne\\nit\\nN\\not\\nlis\\nte\\nd\\n[9\\n1]\\nVa\\nltc\\nhi\\nno\\nv\\n20\\n19\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nRa\\ndi\\nol\\nog\\ny\\nre\\npo\\nrt\\ns,\\nem\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nno\\nte\\ns\\n+\\not\\nhe\\nr\\ncl\\nin\\nic\\nal\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT,\\nRa\\ndL\\nex\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[9\\n2]\\nW\\nad\\nia\\n20\\n18\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nC\\nhe\\nst\\nC\\nT\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT,\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[9\\n3]\\nW\\nal\\nke\\nr\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nTr\\nea\\ntm\\nen\\nt\\nsi\\nte\\ns\\nfro\\nm\\nEM\\nR\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[9\\n4]\\nXi\\ne\\n20\\n19\\nC\\nhi\\nna\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nM\\nIM\\nIC\\n-II\\nId\\nat\\nas\\net\\n4\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nIC\\nD\\n-9\\n-C\\nM\\n,I\\nC\\nD\\n-1\\n0\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[9\\n5]\\nXu\\n20\\n11\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nC\\nRC\\npa\\ntie\\nnt\\nca\\nse\\ns\\nfro\\nm\\nth\\ne\\nSy\\nnt\\nhe\\ntic\\nD\\ner\\niv\\nat\\niv\\ne\\nda\\nta\\nba\\nse\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\no,\\nst\\nill\\nun\\nde\\nr\\nde\\nve\\nlo\\npm\\nen\\nt\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[9\\n6]\\nYa\\nda\\nv\\n20\\n13\\nU\\nSA\\nN\\no\\nPr\\ned\\nic\\ntio\\nn\\nEm\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nC\\nT\\nim\\nag\\nin\\ng\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nYe\\ns,\\nco\\nm\\nm\\nan\\nd\\nlin\\ne\\nco\\nm\\nm\\nan\\nd\\n[9\\n7]\\nYa\\no\\n20\\n19\\nU\\nSA\\nN\\no\\nPr\\ned\\nic\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n00\\n8)\\n10\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nPa\\nrt\\n(S\\nor\\nl)\\n[9\\n8]\\nZe\\nng\\n20\\n18\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nPr\\nog\\nre\\nss\\nno\\nte\\ns\\nan\\nd\\nbr\\nea\\nst\\nca\\nnc\\ner\\nsu\\nrg\\nic\\nal\\npa\\nth\\nol\\nog\\ny\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[9\\n9]\\nZh\\nan\\ng\\n20\\n13\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nan\\nd\\nG\\nEN\\nIA\\nco\\nrp\\nus\\n(M\\nED\\nLI\\nN\\nE\\nab\\nst\\nra\\nct\\ns)\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[1\\n00\\n]\\nZh\\nou\\n20\\n06\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nRe\\nco\\nrd\\ns\\nof\\npa\\ntie\\nnt\\ns\\nw\\nith\\nbr\\nea\\nst\\nco\\nm\\npl\\nai\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\no,\\nst\\nill\\nun\\nde\\nr\\nde\\nve\\nlo\\npm\\nen\\nt\\nN\\not\\nlis\\nte\\nd\\n[1\\n01\\n]\\nZh\\nou\\n20\\n11\\nU\\nSA\\nN\\no\\nSo\\nft\\nw\\nar\\ne\\nC\\nO\\nPD\\nan\\nd\\nC\\nA\\nD\\npa\\ntie\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nSN\\nO\\nM\\nED\\nC\\nT,\\nRx\\nN\\nor\\nm\\n,\\nYe\\ns,\\nde\\nsc\\nrib\\ned\\nin\\not\\nhe\\nr\\nN\\not\\nlis\\nte\\nd\\n[1\\n02\\n]\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 8 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\n(C\\non\\ntin\\nue\\nd)\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nU\\nM\\nLS\\n,P\\nPL\\n,M\\nD\\nD\\n,H\\nL7\\nva\\nlu\\ne\\nse\\nts\\npa\\npe\\nr\\n(1\\n03\\n])\\nZh\\nou\\n20\\n14\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nA\\ndm\\nis\\nsi\\non\\nno\\nte\\ns\\nan\\nd\\ndi\\nsc\\nha\\nrg\\ne\\nsu\\nm\\nm\\nar\\nie\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT,\\nH\\nL7\\nRo\\nle\\nC\\nod\\nes\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[1\\n03\\n]\\n1.\\nPh\\nen\\noC\\nH\\nF\\nco\\nrp\\nus\\n:n\\nar\\nra\\ntiv\\ne\\nre\\npo\\nrt\\ns\\nfr\\nom\\nel\\nec\\ntr\\non\\nic\\nhe\\nal\\nth\\nre\\nco\\nrd\\ns\\n(E\\nH\\nRs\\n)\\nan\\nd\\nlit\\ner\\nat\\nur\\ne\\nar\\ntic\\nle\\ns\\n2.\\nSh\\nA\\nRe\\n/C\\nLE\\nF\\nco\\nrp\\nus\\n(2\\n01\\n3)\\n:n\\nar\\nra\\ntiv\\ne\\ncl\\nin\\nic\\nal\\nre\\npo\\nrt\\ns\\n3.\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nda\\nta\\nse\\nt\\n(2\\n01\\n0)\\n:d\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nan\\nd\\npr\\nog\\nre\\nss\\nre\\npo\\nrt\\ns\\n4.\\nM\\nIM\\nIC\\n-II'</span></dd><dt>text15</dt><dd><span style=white-space:pre-wrap>'RESEARCH Open Access\\nOntological representation, classification\\nand data-driven computing of phenotypes\\nAlexandr Uciteli1,2* , Christoph Beger1,3, Toralf Kirsten2,4,5, Frank A. Meineke1,2 and Heinrich Herre1,2*\\nAbstract\\nBackground: The successful determination and analysis of phenotypes plays a key role in the diagnostic process,\\nthe evaluation of risk factors and the recruitment of participants for clinical and epidemiological studies. The\\ndevelopment of computable phenotype algorithms to solve these tasks is a challenging problem, caused by various\\nreasons. Firstly, the term \\u0091phenotype\\u0092 has no generally agreed definition and its meaning depends on context.\\nSecondly, the phenotypes are most commonly specified as non-computable descriptive documents. Recent\\nattempts have shown that ontologies are a suitable way to handle phenotypes and that they can support clinical\\nresearch and decision making.\\nThe SMITH Consortium is dedicated to rapidly establish an integrative medical informatics framework to provide\\nphysicians with the best available data and knowledge and enable innovative use of healthcare data for research\\nand treatment optimisation. In the context of a methodological use case \\u0091phenotype pipeline\\u0092 (PheP), a technology\\nto automatically generate phenotype classifications and annotations based on electronic health records (EHR) is\\ndeveloped. A large series of phenotype algorithms will be implemented. This implies that for each algorithm a\\nclassification scheme and its input variables have to be defined. Furthermore, a phenotype engine is required to\\nevaluate and execute developed algorithms.\\nResults: In this article, we present a Core Ontology of Phenotypes (COP) and the software Phenotype Manager\\n(PhenoMan), which implements a novel ontology-based method to model, classify and compute phenotypes from\\nalready available data. Our solution includes an enhanced iterative reasoning process combining classification tasks\\nwith mathematical calculations at runtime. The ontology as well as the reasoning method were successfully\\nevaluated with selected phenotypes including SOFA score, socio-economic status, body surface area and WHO BMI\\nclassification based on available medical data.\\nConclusions: We developed a novel ontology-based method to model phenotypes of living beings with the aim\\nof automated phenotype reasoning based on available data. This new approach can be used in clinical context,\\ne.g., for supporting the diagnostic process, evaluating risk factors, and recruiting appropriate participants for clinical\\nand epidemiological studies.\\nKeywords: Phenotype definition, Phenotype classification, Phenotype calculation, Phenotype ontology, Phenotype\\nreasoning\\n© The Author(s). 2020, corrected publication 2020. Open Access This article is licensed under a Creative Commons Attribution\\n4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as\\nlong as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\\nand indicate if changes were made. The images or other third party material in this article are included in the article\\'s Creative\\nCommons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative\\nCommons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need\\nto obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\\nlicenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.\\n0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: auciteli@imise.uni-leipzig.de; heinrich.herre@imise.uni-\\nleipzig.de\\n1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),\\nUniversity of Leipzig, Leipzig, Germany\\nFull list of author information is available at the end of the article\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 \\nhttps://doi.org/10.1186/s13326-020-00230-0\\nBackground\\nDespite its long ago introduction in 1909 by Wilhelm\\nJohannsen, the term \\u0091phenotype\\u0092 still has no generally\\nagreed definition [1]. Usually, a phenotype is consid-\\nered as an observable characteristic or trait of an or-\\nganism, such as its morphology, function, behaviour\\nor its biochemical and physiological properties [1\\u00963].\\nScheuermann et al. define a phenotype as a (combin-\\nation of) bodily feature(s) (physical components, bod-\\nily qualities or bodily processes) of an organism\\ndetermined by the interaction of its genetic make-up\\nand environment [4]. From the medical perspective,\\nclinical (clinically abnormal) and disease phenotypes\\n(clinical phenotype characterising a single disease) are\\nconsidered. According to Scheuermann et al., a dis-\\nease phenotype can exist without being observed. Ob-\\nserved bodily features that could be of clinical\\nrelevance are called \\u0091Sign\\u0092( \\u0093\\u0085 observed in a physical\\nexamination and is deemed by the clinician to be of\\nclinical significance\\u0094) or \\u0091Symptom\\u0092 ( \\u0093\\u0085 observed by\\nthe patient and is hypothesized by the patient to be a\\nrealization of a disease\\u0094) [4].\\nCorrect determination of phenotypes plays a key\\nrole for diagnosis of diseases, evaluation of risk fac-\\ntors and recruitment of patients for clinical and epi-\\ndemiological studies [5, 6]. One challenge is to\\ntranslate phenotype algorithms, which \\u0093are most com-\\nmonly represented as non-computable descriptive\\ndocuments and knowledge artifacts\\u0094 [7], into\\nmachine-readable form. This paper focuses on devel-\\noping a general phenotype representation model that\\ncan be used for data-driven phenotype computing,\\ni.e., software-supported determination of phenotypes\\nbased on the data of an organism. The model to be\\ndeveloped must support both the biological and the\\nmedical views of the phenotype notion. Recent at-\\ntempts have shown that ontologies are suitable to\\nhandle phenotypes and that they can support clinical\\nresearch and decision making [8\\u009610].\\nThere is a large ongoing initiative in Germany, the so\\ncalled German Medical Informatics Initiative (MII) [11,\\n12] that aims at making clinical data available for re-\\nsearch. Most German university hospitals participate in\\none of four funded consortia. Smart Medical Informa-\\ntion Technology for Healthcare (SMITH) is one of these\\nconsortia [13]. Within the ongoing SMITH project, a\\nphenotyping pipeline (PheP) will be established to sys-\\ntematically develop, evaluate and execute validated algo-\\nrithms and models for classifying and annotating patient\\ncare data. These annotations and derivatives will be pro-\\nvided for triggering alerts and actions, data sharing and\\ndeep analyses of patient care and outcomes. The general\\ndesign and concept of the SMITH phenotyping pipeline\\nis presented in [14].\\nIn this article, we propose a novel ontology-based\\nmethod to model and compute phenotypes. Our ap-\\nproach provides an extended reasoning combining\\nphenotypic data to derive complex phenotypes based on\\ncalculations and classifications.\\nMethods\\nPhenotypes can be derived from available data that may\\nhave been measured (quantitative data) or observed and\\nqualitatively described (categorical data). The data can,\\nfor example, come from Electronic Health Records\\n(EHR) (clinical data) or from a research database of a\\nclinical/epidemiological study (research data). In SMIT\\nH, the required EHR data will be integrated into a cen-\\ntral Health Data Storage (HDS) at each site. The inte-\\ngrated data is homogeneously represented in each HDS\\nusing HL7 FHIR [15] and can be queried utilising FHIR\\nSearch [16] (Fig. 1). Structured data from different\\nsource systems in hospitals as well as unstructured doc-\\numents will be extracted, transformed and loaded into\\nthe HDS. Natural Language Processing (NLP) techniques\\nare used to extract and transform relevant data from un-\\nstructured EHR documents into structured form. In\\nSMITH and the German Medical Informatics Initiative,\\nthe software tool ART-DECOR [17] is used to specify an\\noverarching global schema, the so-called core data set\\n[18]. The core data subsumes the minimal set of data el-\\nements that each site (i.e., University Hospital) needs to\\nprovide in a harmonised manner. In this way, data ele-\\nments are specified based on HL7 templates, their re-\\nspective value sets, referenced terminologies, exemplified\\nuse scenarios and data. These specifications are the basis\\nfor the ontology-based phenotype representation in our\\napproach.\\nFig. 1 Integration of the PhenoMan. The Metadata Manager models\\nbasic data elements using ART-DECOR. The Phenotype Designer\\nimports the ART-DECOR specification and develops phenotype\\nmodels (PheSO) utilising the PhenoMan Editor. The PhenoMan\\nrequests required input data from the FHIR Server, computes\\nphenotypes and writes the results back to the FHIR Server\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 2 of 17\\nHL7 FHIR and FHIR Search\\nHealthcare records are increasingly digitised. The\\nEHR must be discoverable and understandable. The\\npatient data must be structured and standardised to\\nsupport machine-based processing and automated\\nclinical decision making. The FHIR (Fast Healthcare\\nInteroperability Resources) specification is a HL7\\nstandard for modelling and exchanging healthcare in-\\nformation [15]. FHIR provides a base set of resource\\ntypes representing relevant clinical concepts that can\\nbe used to store and exchange data in order to solve\\na wide range of healthcare related problems. For\\neach resource type, the corresponding information\\ncontents and structure are specified. The resources\\ncan be used either by themselves or combined to\\ncomplex documents representing a coherent set of\\nhealthcare information [15]. The FHIR resource types\\ninclude, inter alia, \\u0091Patient\\u0092 (\\u0093Demographics and other\\nadministrative information about an individual or\\nanimal receiving care or other health-related ser-\\nvices.\\u0094), \\u0091Observation\\u0092 (\\u0093Measurements and simple as-\\nsertions made about a patient, device or other\\nsubject.\\u0094) and \\u0091Condition\\u0092 (\\u0093A clinical condition, prob-\\nlem, diagnosis, or other event, situation, issue, or clin-\\nical concept that has risen to a level of concern.\\u0094)\\n[15].\\nThe FHIR Search Framework [16] is part of the\\nHL7 FHIR standard and provides a range of opera-\\ntions and parameters (series of name = value pairs) to\\nsearch for existing FHIR resources in the underlying\\nrepository. In the simplest case, a search is executed\\nby performing a GET operation in the RESTful\\nframework:\\nGET [base-url]/[resource-type]?name = value&amp;...{&amp;_\\nformat = [mime-type]}}.\\ne.g., GET [base-url]/Patient?gender =male.\\nFor numeric parameter types (number, date or quan-\\ntity), a value range can be defined using a prefix to the\\nparameter value (e.g., gt = greater than, le = less or\\nequal).\\nThe \\u0091&amp;\\u0092 (AND) operator between single search criteria is\\nused to search for the intersection of resources that match\\nall criteria specified by each individual search parameter\\n(e.g., Patient?gender =male&amp;birthdate = gt1970). To search\\nfor resources with one of the specified parameter values\\n(OR), the values must be separated by a comma (e.g., Obser-\\nvation?code= http://loinc.org?3141-9, http://snomed.info/\\nsct?27113001, i.e., weight code from LOINC or SNOMED).\\nThe following query contains AND combinations of single\\ncriteria (code AND value-quantity) as well as OR linking of\\ncode values and can be used to search for weight observa-\\ntions where the weight is greater than 75 kg:\\nObservation?code= http://loinc.org?3141-9, http://snome-\\nd.info/sct?27113001&amp;value-quantity=gt75??kg.\\nART-DECOR\\nART-DECOR is an open-source tool suite that supports\\nthe creation and maintenance of HL7 templates, value\\nsets, scenarios and datasets [17]. To specify and hier-\\narchically structure required data elements (items, con-\\ncepts, variables) we use the Dataset Editor of ART-\\nDECOR. Data elements can possess several attributes,\\nsuch as name, description (in different languages) and\\nvalue domain (including data type, unit and possible\\nvalue set) (Fig. 2a).\\nOne of the most important components of a data\\nelement is its terminology associations. A termin-\\nology association defines the binding of dataset con-\\ncepts to relevant terminology [17]. To associate a\\ndata element with a terminology concept, the corre-\\nsponding code (including the URI or ID of the ter-\\nminology) must be specified. For instance, the\\nconcept \\u0091Fasting glucose [Mass/volume] in Serum or\\nPlasma\\u0092 from LOINC (URI: \\u0091http://loinc.org\\u0092) has the\\ncode \\u00911558\\u00966\\u0092 (Fig. 2a).\\nFurthermore, additional properties of data elements\\ncan be defined as key-value pairs. We use this function-\\nality to specify the mapping between the data element\\nand the corresponding FHIR resource type (e.g., for fast-\\ning glucose, key: \\u0091FHIR\\u0092, value: \\u0091Observation\\u0092) required\\nfor phenotype computing. Depending on the resource\\ntype, different FHIR Search parameters must be used to\\nquery the relevant FHIR resources. Moreover, the differ-\\nent structure of the resulting resources must be consid-\\nered to extract required data.\\nThe resulting dataset specification is available in XML\\nor JSON and can be parsed by our software.\\nOntological architecture\\nOur objective was to design the PhenoMan software\\naccording to the three-ontology method [19]. This\\nmethod is based on interactions of three different\\nkinds of ontologies: a task ontology (TO), a domain\\nontology (DO) and a top-level ontology (TLO). The\\nTO serves as the conceptual model for the software,\\nthe DO provides the domain-specific knowledge,\\nwhereas the TLO integrates the TO and the DO and\\nis used as foundation of them.\\nIn our case, the Core Ontology of Phenotypes (COP,\\nsee section \\'Core Ontology of Phenotypes (COP)\\') func-\\ntions as a TO. It describes the general structure of valid\\nphenotype specifications and thus enables the Pheno-\\nMan to create such specifications and to use them for\\nphenotype computing. Concrete phenotype specifica-\\ntions (domain-specific knowledge) are represented in\\nPhenotype Specification Ontologies (PheSO, see section\\n\\'Phenotype Specification Ontologies (PheSO)\\') playing\\nthe role of domain ontologies (DO) in our architecture.\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 3 of 17\\nFor the foundation of the TO we used the General\\nFormal Ontology (GFO) [20] as TLO. GFO has already\\nbeen successfully applied for a foundation of phenotype-\\nrelated notions. For instance, a novel approach to repre-\\nsent complex phenotypes in OWL was proposed im-\\nproving the consistency and expressiveness of formal\\nphenotype descriptions [10]. Another pillar of GFO for a\\ngrounding of phenotypes is the foundational ontology of\\nproperties, attributives and data (GFO-Data [21])\\nproviding an extensive classification of properties (and\\nattributives). In the current paper, we especially refer-\\nence the property notion of GFO (including distinction\\nbetween single and composite properties [22]) in our\\nphenotype representation model supporting data-driven\\nphenotype computing.\\nOne of the advantages of the three-ontology method is\\nthat the software only needs to implement the access to\\nentities (classes, properties) of the TO (COP), whereas\\nFig. 2 Mapping between ART-DECOR, PheSO, FHIR Subscription and FHIR Observation entities. a Specification of the data element \\u0091fasting\\nglucose\\u0092 in ART-DECOR. b Annotations of the corresponding class Fasting_Glucose after importing the ART-DECOR specification into the PheSO. c\\nSubscription generated for the class Fasting_Glucose. The criteria (FHIR Search query) is encoded. The original URL part is Observation?code=\\nhttp://loinc.org|1558-6. d Observation of fasting glucose provided by FHIR Server. The observation code, value, date and the referenced patient\\nare specified. (The same colour of the border indicates the mapping between the entities)\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 4 of 17\\nthe entities of the corresponding DO (PheSO) are proc-\\nessed dynamically. The PhenoMan uses the COP as an\\ninterface to access the PheSO entities.\\nAdditional requirements for the ontological modelling\\nwere:\\n\\u0096 Developing in OWL (using OWL API [23], HermiT\\n[24] and Openllet [25])\\n\\u0096 Modelling all attributes and relations that are\\nrelevant for reasoning as object or data properties\\n\\u0096 Modelling all attributes and relations that are not\\nrelevant for reasoning as annotations\\n\\u0096 Usage of general class axioms (based on \\u0091subclass\\nof\\u0092) instead of equivalence classes if only one\\ndirection (\\u0091is-a\\u0092 relation) is relevant for reasoning.\\nSoftware design\\nWe defined the following main requirements for the\\noverall system (Fig. 1):\\n\\u0096 The system must support a phenotype specification\\nproviding a GUI tool (see section \\'Specification of\\nphenotypes\\').\\n\\u0096 The phenotype specifications must be saved in a\\nstandardised ontology (see sections \\'Core Ontology\\nof Phenotypes (COP)\\' and \\'Phenotype Specification\\nOntologies (PheSO)\\').\\n\\u0096 The system must be able to correctly compute\\nphenotypes based on a phenotype specification\\n(ontology) and input data (see section \\'Classification\\nand calculation of phenotypes\\').\\n\\u0096 The system must support an additional\\nimplementation of mapping components for\\naccessing required data and metadata repositories.\\nExample components for metadata import from\\nART-DECOR as well as for interaction with FHIR\\nservers (e.g., SMITH HDS) must be implemented\\n(see sections \\'Data procurement\\' and \\'Transmission\\nof inferred phenotype classes to the FHIR Server\\').\\nThe PhenoMan accesses the FHIR Server, extracts\\nphenotype-specific data, computes the specified pheno-\\ntypes and writes the results back to the FHIR Server. For\\nthis purpose, the PhenoMan provides an API and acts as\\na web service (using Dropwizard [26]) (Fig. 1). The Phe-\\nnoMan is implemented in Java using OWL API [23] and\\ntwo reasoners, HermiT [24] and Openllet [25]. For cal-\\nculations we utilize the Java Expression Evaluator (Eva-\\nlEx) [27], but the integration of other libraries (e.g., for\\nexecuting R scripts) or rule systems (e.g., SWIRL or\\nDrools) is also possible. The EvalEx enables evaluating\\nmathematical and Boolean (inter alia, Boolean operators\\nand IF-THEN-ELSE structures) expressions and sup-\\nports defining custom functions and operators.\\nThe PhenoMan Editor1 is a desktop app, which is also\\ndeveloped with Java and bundled with the PhenoMan\\nAPI. It offers a graphical user interface based on Java\\nSwing to specify attributes of phenotype classes and cat-\\negories using appropriate form fields. On saving, form\\ncontent is transmitted to the PhenoMan API and written\\ninto the ontology. The editor can be executed on a local\\nmachine with a Java runtime environment 8 or higher\\nand was developed with the aim of rapidly defining\\nphenotype models.\\nEvaluation\\nAn evaluation of our approach was designed and con-\\nducted. The main objectives of the evaluation were to\\nprove:\\n1. Correct functioning of all software components\\n2. Faultless communication of the software with the\\nFHIR Server\\n3. Correctness of all provided phenotype specifications\\n4. Correct functioning of the overall system by\\ncomparison with a corresponding SPSS\\nimplementation of selected phenotypes.\\nWe evaluated the PhenoMan at different levels. Firstly,\\nwe tested all functionalities of the PhenoMan API (espe-\\ncially read/write in the ontology and computing pheno-\\ntypes) and the communication of the PhenoMan Service\\nwith the FHIR Server by a set of static JUnit tests using\\nfixtures (i.e., example PheSOs and patient data).\\nSecondly, each phenotype specification is shipped with\\na structured representation (spreadsheet) of test data (in-\\nput and output), such that the respective phenotype al-\\ngorithm can be automatically tested. The criterion for a\\nsuccessful execution of the JUnit tests was a match be-\\ntween the results calculated by PhenoMan based on pro-\\nvided input data and the corresponding output data.\\nFinally, we selected some test case algorithms/deriva-\\ntives (such as socio-economic status [28], body mass\\nindex [29], waist circumference and waist-hip ratio [30])\\nfrom the LIFE Adult study [31] running at the LIFE Re-\\nsearch Centre for Civilization Diseases, University of\\nLeipzig. There, derivatives are usually implemented by\\nepidemiologists, statisticians and other researchers using\\nthe statistics software SPSS [32] and R or are database\\n(SQL) queries and functions, which are automatically ex-\\necuted at night based on daily captured data. The result-\\ning data are directly stored within the LIFE research\\ndatabase in tabular form. More details about partici-\\npants, their invitation and consenting as well as\\n1Source code and releases of the PhenoMan Editor are available on\\nGitHub under the GPL-3.0 license: https://github.com/Onto-Med/Phe-\\nnoMan-Editor\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 5 of 17\\nexaminations, interviews, questionnaires and taken spe-\\ncimen can be found in [31]. We reproduced selected\\nSPSS derivatives using the PhenoMan and developed\\nparameterised JUnit tests to comparatively evaluate the\\naccuracy of the PhenoMan against the corresponding\\nSPSS implementation at the LIFE Research Centre. The\\ncriterion for a successful comparison was a match be-\\ntween the results calculated by PhenoMan and SPSS\\nsoftware for each dataset. The performance of our ap-\\nproach is not a critical issue in our use case (the pheno-\\ntype computing could run overnight).\\nThis evaluation included data of thousands of LIFE\\nparticipants.\\nResults\\nCore Ontology of Phenotypes (COP)\\nWe developed the Core Ontology of Phenotypes\\n(COP, Fig. 3) to model, classify and calculate pheno-\\ntypes based on instance data sets (e.g., of a patient).\\nIn this article, we consider a phenotype as a\\ndependent individual (in the sense of General Formal\\nOntology, GFO [20]), for example, the weight of a\\nspecific person. Hereinafter, abstract instantiable en-\\ntities that are instantiated by phenotypes are called\\nphenotype classes. For instance, the abstract property\\n\\u0091weight\\u0092 possesses individual weights as instances. We\\ndistinguish between single and composite properties,\\nand correspondingly, between single and composite\\nphenotypes. A composite property is defined as a\\nproperty that has single properties as parts [22].\\nBased on the definitions of single and composite\\nproperties [22], we define single phenotypes as single\\nproperties (e.g., age, weight, height) and composite\\nphenotypes as composite properties (e.g., height and\\nweight, BMI, SOFA score [33]) of an organism or of\\none of its subsystems. Properties of an organism are\\nconsidered as all documentable information about it,\\nwhereby the modeller is left to decide what is rele-\\nvant to the current situation. These can be, for ex-\\nample, observable characteristics or traits of an\\norganism [1\\u00963] or possible manifestations of clinical\\nphenotypes, such as signs, symptoms or dispositions\\n[4]. The corresponding data can be modelled using\\nthe FHIR Observation or Condition resources.\\nComposite phenotypes are divided into combined\\nand derived phenotypes. A combined phenotype is\\nonly a combination of corresponding phenotypes (e.g.,\\na combination of height and weight), whereas a de-\\nrived phenotype is an additional property (e.g., BMI)\\nderived from the corresponding phenotypes (height\\nand weight). In the framework of GFO we modelled\\nproperties using the class gfo: Property. In the present\\narticle, composite phenotype classes are modelled\\nusing a Boolean expression based on has_part relation\\n(e.g., weight and height: has_part some height and\\nhas_part some weight). Derived phenotype classes\\nadditionally define a calculation rule/mathematical\\nformula (e.g., BMI = weight [kg] / height [m]2). Fur-\\nthermore, combined phenotype classes can associate\\ncertain conditions with specific predefined values\\n(scores), which can be used, e.g., in further formulas.\\nFor example, if bilirubin value is greater than 12 mg/\\ndL, then the value 4 is used for the calculation of the\\nSOFA score [33].\\nAdditionally, we distinguish between restricted and\\nnon-restricted phenotype classes, depending on whether\\ntheir extensions (set of instances) are restricted to a cer-\\ntain range of individual phenotypes by defined condi-\\ntions or all instances are allowed. For example, the\\nphenotype class \\u0091age\\u0092 is instantiated by the ages of all liv-\\ning beings (non-restricted), whereas the phenotype class\\n\\u0091young age\\u0092 is instantiated by the ages of the young ones,\\ne.g., if the age is below 30 years (restricted).\\nPhenotype Specification Ontologies (PheSO)\\nWe consider a phenotype algorithm as a sequence of in-\\nstructions (1) to classify phenotypes (single or compos-\\nite) in phenotype classes or (2) to derive additional\\nproperties (derived phenotypes) from the phenotypes of\\nan organism. Phenotype algorithms can be implemented,\\nfor example, using a programming language or a statis-\\ntics software (e.g., SPSS or R). Our approach is to separ-\\nate the specification of phenotypes (models) from the\\nimplementation of corresponding algorithms. The COP\\nprovides a basic model to specify phenotypes in a stan-\\ndardised way, while the PhenoMan implements the gen-\\neral approach, common for all COP-based specifications.\\nIt is not our aim to completely model the EHR. Instead,\\nour approach can support the modelling and calculation\\nof selected phenotypes in a user-friendly standardised\\nmanner.\\nPhenotypes are modelled in Phenotype Specification\\nOntologies (PheSO) using the COP. The phenotype clas-\\nses and axioms (classification and calculation rules) con-\\ntained in the PheSO are used by PhenoMan to execute\\nthe corresponding phenotype algorithm. PheSOs are em-\\nbedded in the COP in such a way that the classes of the\\nPheSO are subclasses of the COP classes. Every PheSO\\nsubclass of the COP classes cop: Single_Phenotype, cop:\\nCombined_Phenotype or cop: Derived_Phenotype is a\\nphenotype class and is instantiated by phenotypes. The\\nFig. 3 Core Ontology of Phenotypes (COP)\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 6 of 17\\ndirect subclasses are non-restricted (e.g., Fasting_Glu-\\ncose, Fig. 4a), while the subclasses of the non-restricted\\nphenotype classes are restricted (e.g., Fasting_Glucose_\\nABNORMAL, i.e., fasting glucose is greater equal 125\\nmg/dL, Fig. 4a3).\\nPhenotype classes possess various common attributes\\n(e.g., labels, descriptions and codes of external concepts).\\nOther attributes vary depending on the type of the\\nphenotype class. The following are examples of such\\nattributes:\\n\\u0096 Non-restricted single phenotype (NSiP) class: unit of\\nmeasure and optional aggregate function.\\n\\u0096 Restricted single (RSiP) and derived phenotype\\n(RDeP) class: restriction.\\n\\u0096 Restricted combined phenotype (RCoP) class:\\nBoolean expression (based on RSiP, RCoP and RDeP\\nclasses) and optional score value.\\n\\u0096 Non-restricted derived phenotype (NDeP) class:\\nmathematical formula and Boolean expression\\nconsisting of AND-linked variables used in the for-\\nmula (NSiP and non-restricted combined phenotype\\n(NCoP) classes). If a NCoP class is used as a vari-\\nable, the RCoP classes (subclasses) of the NCoP class\\nmust have score values that should be used in the\\nformula.\\nSimple attributes of the phenotype classes are defined\\nas annotations. The logical relations between phenotype\\nclasses as well as range restrictions are represented in\\nOWL by anonymous equivalent classes or general class\\naxioms based on property restrictions.\\nPhenotype Manager (PhenoMan)\\nWe developed the software Phenotype Manager (Pheno-\\nMan), which implements a multistage reasoning ap-\\nproach combining standard reasoners (e.g., Pellet or\\nHermiT) and mathematical calculations. This section\\nbriefly outlines the main functionality of our solution.\\nSpecification of phenotypes\\nThe PhenoMan Editor is an interactive user interface for\\nmanaging and developing PheSOs. The user is able to\\ncreate a new PheSO or to load an existing ontology. The\\nFig. 4 Parts of the T2DM PheSO in Protégé. Middle: Phenotype classes (a Single, b Derived, c Combined). Left: Example annotations of the\\nphenotype classes. Right: Anonymous equivalent classes and general class axioms\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 7 of 17\\nPhenoMan Editor provides appropriate forms to browse,\\ncreate and edit categories and phenotype classes of the\\nontology. Value range restrictions, for example, are de-\\nfined by selecting a comparison operator and entering\\nthe corresponding values (Fig. 5). Boolean expressions\\nare built by drag-and-dropping the phenotype classes\\nfrom the left side into the expression form field and en-\\ntering relevant operators (Fig. 6). After submission, the\\nform data is transmitted to the PhenoMan API and is\\nstored in the actual PheSO.\\nFurthermore, an ART-DECOR specification (XML)\\nof relevant data elements can be imported in the\\nPheSO. For each data element, a NSiP class is gener-\\nated. All relevant attributes (name, codes, FHIR re-\\nsource type, data type, unit, etc.) specified in ART-\\nDECOR are defined as annotations of corresponding\\nclasses (Fig. 2a, b).\\nData procurement\\nAfter starting the PhenoMan Service, FHIR subscriptions\\n(rest-hooks) [34] are generated and transmitted to the\\nFHIR Server. The structure of the subscription resource\\nis very simple. The main parts of the resource are the\\ncriteria and the channel. The FHIR Server uses the cri-\\nteria (FHIR Search query) to determine resources for\\nwhich notifications have to be generated. When re-\\nsources are identified (after creating or updating) meet-\\ning the criteria, a notification is sent to the address\\n(\\u0091endpoint\\u0092) specified in the section \\u0091channel\\u0092.\\nFig. 5 Specification of the class Fasting_Glucose_ABNORMAL with the PhenoMan Editor form. We left out some of the metadata fields for\\nbetter visibility\\nFig. 6 Specification of the class T2DM_Case_3 with the PhenoMan Editor form\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 8 of 17\\nIn the configuration file of the PhenoMan, a directory\\ncontaining all available phenotype specifications (Phe-\\nSOs) as well as the address (URL) of the PhenoMan ser-\\nvice (including the PheSO name) are defined. For each\\nNSiP class of each available PheSO (in the defined direc-\\ntory) a subscription is created. To generate the subscrip-\\ntion criteria (FHIR Search query), the PhenoMan uses\\nthe resource type and codes specified in the correspond-\\ning NSiP class as annotations (Fig. 2b, c). The \\u0091endpoint\\u0092\\nattribute is automatically filled with the URL of the Phe-\\nnoMan service defined in the configuration file. The\\nremaining parts of the subscription resource (\\u0091status\\u0092,\\n\\u0091type\\u0092 and \\u0091payload\\u0092) take default values (\\u0091active\\u0092, \\u0091rest-\\nhook\\u0092 and \\u0091application/json\\u0092) (Fig. 2c).\\nAfter receiving a notification (including the\\ncomplete resource, Fig. 2d), the PhenoMan Service re-\\nquests further resources (for all other NSiP classes of\\nthe corresponding PheSO) using FHIR Search. The\\ngenerated FHIR Search queries are primarily based\\nupon the codes specified for the NSiP classes (simi-\\nlarly to subscription criteria), contain a reference to\\nthe patient and can additionally support possible ag-\\ngregate functions.\\nClassification and calculation of phenotypes\\nAfter receiving required resources, the PhenoMan starts\\ninferring phenotypes.\\nFirst, the relevant information is extracted from re-\\nceived resources and inserted into the ontology. On the\\none hand, the individual properties (single phenotypes)\\nare inserted as instances of the direct subclasses of cop:\\nSingle_Phenotype and the values are modelled as prop-\\nerty assertions based on the has_value relation. On the\\nother hand, a composite phenotype is defined as an in-\\nstance of the class cop: Composite_Phenotype, which\\ncombines all the single phenotype instances using prop-\\nerty assertions based on has_part relation. Then, our\\nmultistage reasoning algorithm is executed. The algo-\\nrithm consists of the following steps:\\n1. Classification step. A standard reasoner classifies the\\nexisting instances (assignment to classes).\\na. Single phenotype instances are classified in RSiP\\nclasses based on property restrictions.\\nb. The composite phenotype instance is classified\\nin RCoP classes based on the specified Boolean\\nexpression and inferred RSiP, RCoP and RDeP\\nclasses.\\nc. The composite phenotype instance is classified\\nin NDeP classes based on the specified Boolean\\nexpression and corresponding NSiP and NCoP\\nclasses. In this case, all variable values required\\nfor calculating formulas are present.\\nd. Available instances of NDeP classes\\n(representing calculated values) are classified in\\nRDeP classes based on proper'</span></dd></dl>\n"
            ],
            "text/markdown": "text1\n:   <span style=white-space:pre-wrap>'Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 \\nhttps://doi.org/10.1186/s13326-019-0218-0\\nRESEARCH Open Access\\nCombining lexical and context features\\nfor automatic ontology extension\\nSara Althubaiti1,2, S¸enay Kafkas1,2 , Marwa Abdelhakim1,2 and Robert Hoehndorf1,2*\\nAbstract\\nBackground: Ontologies are widely used across biology and biomedicine for the annotation of databases. Ontology\\ndevelopment is often a manual, time-consuming, and expensive process. Automatic or semi-automatic identification\\nof classes that can be added to an ontology can make ontology development more efficient.\\nResults: We developed a method that uses machine learning and word embeddings to identify words and phrases\\nthat are used to refer to an ontology class in biomedical Europe PMC full-text articles. Once labels and synonyms of a\\nclass are known, we use machine learning to identify the super-classes of a class. For this purpose, we identify lexical\\nterm variants, use word embeddings to capture context information, and rely on automated reasoning over\\nontologies to generate features, and we use an artificial neural network as classifier. We demonstrate the utility of our\\napproach in identifying terms that refer to diseases in the Human Disease Ontology and to distinguish between\\ndifferent types of diseases.\\nConclusions: Our method is capable of discovering labels that refer to a class in an ontology but are not present in an\\nontology, and it can identify whether a class should be a subclass of some high-level ontology classes. Our approach\\ncan therefore be used for the semi-automatic extension and quality control of ontologies. The algorithm, corpora and\\nevaluation datasets are available at https://github.com/bio-ontology-research-group/ontology-extension.\\nKeywords: Disease ontology, Embeddings, Neural network\\nBackground\\nThe biomedical community has spent significant\\nresources to develop biomedical ontologies which con-\\ntain and define the basic classes and relations that occur\\nwithin a domain. Biomedical ontologies are developed by\\ndomain experts and are often developed in conjunction\\nwith the needs arising in literature-based curation of\\nbiological databases.\\nManual curation of databases based on literature is a\\nvery time-consuming task due to the massive amounts of\\nliterature, and automated methods have been developed\\nearly on to aid in curation [1]. One of the key tasks in\\ncomputational support for literature curation is the auto-\\nmatic concept recognition of mentions of ontology classes\\nin text [2]. An ontology class is an intensionally defined\\n*Correspondence: robert.hoehndorf@kaust.edu.sa\\n1Computational Bioscience Research Center, King Abdullah University of\\nScience and Technology, 23955-6900 Thuwal, Saudi Arabia\\n2Computer, Electrical and Mathematical Sciences and Engineering Division,\\nKing Abdullah University of Science and Technology, 23955-6900 Thuwal,\\nSaudi Arabia\\nentity that has a formal descriptionwithin an ontology and\\naxioms that determine its relation with other classes [3]. In\\nnatural language, multiple terms and phrases can be used\\nto refer to an ontology class [4], and the formal depen-\\ndencies within an ontology further determine whether a\\nterm refers to a class or not (i.e., whether a term refers to\\na particular class may depend on background knowledge,\\nin particular subclass relations, contained in an ontol-\\nogy). For example, the Disease Ontology (DO) [5] declares\\nPrediabetes syndrome (DOID:11716) to be a subclass\\nof Diabetes mellitus (DOID:9351), and based on this\\ninformation we assume that any reference to, or mention\\nof, Prediabetes syndrome is also a reference to Diabetes\\nmellitus (with respect to DO).\\nThere are several text mining systems designed for\\nontology concept recognition in text. These methods are\\neither based on lexical methods and therefore applicable\\nto a wide range of ontologies [6, 7] or they are domain-\\nspecific and rely on machine learning [8]. Text mining\\n© The Author(s). 2020 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the\\nCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver\\n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 2 of 13\\nbased-methods can also be used to automatically or semi-\\nautomatically construct and extend ontologies [9, 10]. For\\nexample, Lee et al. [11] focus on text mining of relations\\nthat are asserted in text between mentions of ontology\\nclasses that has been used to refine ontology classes in the\\nGene Ontology (GO) [12]. Text mining can also be used to\\nsuggest new subclasses and sibling classes in ontologies,\\nfor exampleWächter and Schroeder [13] carried out a text\\nmining based-system from different text sources which is\\nused for extending OBO ontologies by semi-automatically\\ngenerating terms, definitions and parent\\u0096child relations.\\nXiang et al. [14] have developed a pattern-based system\\nfor generating and annotating a large number of ontology\\nterms, following ontology design patterns and providing\\nlogical axioms that may be added to an ontology. Recently,\\nclustering based on statistical co-occurrence measures\\nwere also used to extend ontologies [15].\\nHere, we introduce a novel method relying on machine\\nlearning to identify whether a word used in text refers\\nto a class that could be included in a particular ontol-\\nogy. Essentially, our method classifies terms to determine\\nif they are usually mentioned in the same context as the\\nlabels and synonyms of classes in an ontology (which are\\nused as seeds to train the classifier); this classifier can then\\nbe applied to unseen terms. Furthermore, our method can\\nalso be used to expand ontologies by suggesting terms that\\nare mentioned within the same context as specific classes\\nin an ontology.\\nWe demonstrate the utility of our method in identi-\\nfying words referring to diseases from DO in full text\\narticles. We select the DO because the labels and syn-\\nonyms of DO classes are relatively easy to detect in text\\nand a large number of computational methods rely on\\naccess to a comprehensive disease ontology [16\\u009619]. Our\\nmethod achieves highly accurate (F-score &gt; 90%) and\\nrobust results, is capable of recognizing multiple different\\nclasses including those defined formally through logical\\noperators, and combines dictionary-based and context-\\nbased features; therefore, our method is also capable of\\nfinding new words that refer to a class. We manually\\nevaluate the results and suggest several additions to the\\nDO.\\nMethods\\nBuilding a disease dictionary\\nWe built a dictionary from the labels and synonyms\\nof classes in the Disease Ontology (DO), downloaded\\non 5 February 2018 from http://disease-ontology.org/\\ndownloads/. The dictionary consisted of 21,788 terms\\nbelonging to 6,831 distinct disease classes from DO. We\\nutilized the dictionary with the Whatizit tool [20] and\\nannotated the ontology class mentions along with their\\nidentifiers in approximately 1.6 million open access full-\\ntext articles from the Europe PMC database [21] (http://\\neuropepmc.org/ftp/archive/v.2017.06/) and generated a\\ncorpus annotated with mentions of classes in DO. We\\npreprocessed the corpus by removing stop words such as\\n\\u0093the\\u0094, \\u0093a\\u0094, and \\u0093is\\u0094 as well as some punctuation characters.\\nGenerating context-based features\\nWe use Word2Vec [22] to generate word embedding.\\nSpecifically, we use a skip-gram model which aims to find\\nword representations that are useful for predicting the\\nsurrounding words in a given sentence or a document\\nconsisting of sequence of words; w1,w2, ...,wK . The objec-\\ntive is to maximize the average log probability using the\\nfollowing formula:\\nV (w) = 1K\\nK?\\nk=1\\nK?\\n?c?j?c;j \\003=0\\nlog p(wK+j|wK ) (1)\\nwhere word vectors V (w) are computed by averaging over\\nthe number of words K and c is the size of the training\\ncontext. We generated the word embedding by using the\\ndefault parameter settings of theWord2Vec gensim imple-\\nmentation: vector size (dimensionality) of 100, window\\nsize 5, minimum occurrence count of 5, and we use a\\nskip-gram (sg) model.\\nSupervised training\\nWe carried out a set of experiments to choose the optimal\\ntraining algorithm to design our model. In our experi-\\nments we used default parameters for the training algo-\\nrithms but different hidden layers for Artificial Neural\\nNetworks (ANNs) [23]. Our experiments show that the\\nANN model outperforms an SVM model [24] (see Addi-\\ntional file 1: Table 1 for full details), and our model\\nperforms best with 200 neurons in a single hidden layer\\n(we tested a single hidden layer with a size of 10, 50,\\n100, and 200 neurons). We report results accordingly to a\\nmodel with 200 neurons in the remainder of this work. In\\nANNs, multiple neurons are organized in layers. Typically,\\ndifferent layers perform different kinds of transforma-\\ntions on their inputs [25]. In our experiments, we used\\nan ANN with an input layer of different sizes, a single\\nhidden layer that uses a sigmoid activation function, and\\nan output layer that differs based on the experiment. We\\ntrain each classifier in a supervised manner, using 10-fold\\nstratified cross-validation. Additionally, we report testing\\nperformance on an independent 20% testing set which\\nwe generated by randomly removing data points before\\ntraining.\\nRecognizing ontology classes in text\\nWe used two approaches to recognize the mention of\\nontology classes in text. Our first approach relies solely on\\nlabels and synonyms of the classes within a given ontol-\\nogy O and can be used to determine whether a word refer\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 3 of 13\\nto a class in O. We first obtain an ontology O in the Web\\nOntology Language (OWL) [26] format and extract a list\\nof class labels and synonyms L from O; we further utilize\\na text corpus T as input to our method. Then, we gener-\\nate word embeddings (i.e., vector-space encodings of the\\ncontexts in which a word occurs) for all words in our text\\ncorpus T and train a supervised machine learning model\\nto classify whether a word refers to a class in O or not\\n(using the L\\u0092s words as positive training instances and all\\nothers as negative instances).\\nFigure 1 illustrates the workflow of our first approach.\\nOur method is generic and can, in principle, be applied\\nto any ontology as long as the ontology provides labels\\n(or synonyms), these labels can be identified in text, and\\nthe ontology from which the labels are extracted is more\\nor less limited to a single domain. For example, refer-\\nence ontologies in the OBO Foundry [27] are usually\\nsingle domain ontologies and therefore suitable for our\\nmethod. Ontologies that would not be suitable are appli-\\ncation ontologies that cover multiple domains, such as the\\nExperimental Factor Ontology (EFO) [28] (although our\\nmethods can be applied to parts of it). It is most useful to\\nextend an existing ontology with new labels, synonyms, or\\nclasses.\\nIn our second approach, we rely on annotations from\\nthe Whatizit tool [20] to identify the mention of ontology\\nclasses in text and determine their specific superclasses in\\nan ontology. Our approach takes an ontology O in OWL\\nformat, a set of ontology classes S = {C1, ...,Cn}, and a\\ncorpus of text T as inputs.\\nThis approach first uses Whatizit as a named entity\\nrecognition and normalization tool to normalize class\\nlabels and synonyms in text by replacing all mentions\\nof a class with the class identifier (i.e., the class URI).\\nWe annotate 15,183 distinct terms using Whatizit; the\\ntotal dictionary consists of 21,788 terms (derived from\\nthe labels and synonyms of classes in DO). We then train\\nWord2Vec model that captures the context of the men-\\ntion of the class and generates a vector space embedding\\nfor that class. Given such vector space embeddings for\\na set of classes in O, we use the vector space embed-\\ndings as input to a machine learningmethod that classifies\\nwhether another class appears in a similar context. We\\nuse this method to determine if a class should belong the\\nsuperclass of C in O. Figure 2 illustrates the workflow of\\nthis approach.\\nThe main difference between the two approaches is that\\nthe first approach broadly identifies terms or words that\\nrefer to classes within a domain (as defined by the sum\\nof classes within an ontology) while the second approach\\ncan determine whether a term or word refers to a class\\nthat should appear as a subclass of a more specific ontol-\\nogy class. Both methods generate \\u0093seed\\u0094 words in text and\\nthen use these seeds first to generate context-based fea-\\ntures (through Word2Vec) and use these context-based\\nfeatures in a supervised machine learning classifier.\\nManual analysis process\\nWe manually evaluate some of our findings. The manual\\nevaluation is based on the medical expert knowledge of\\nthe evaluator who is a trained clinician, and supplemented\\nby literature search to validate some findings or resolve\\nconflicts. Mainly, results were confirmed by searching\\nfor review papers that characterize a condition. Overall,\\nFig. 1 Label-based workflow. The workflow describes how words (in red) are classified as disease or \\u0093other\\u0094\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 4 of 13\\nFig. 2 Annotation-based workflow. In this workflow, we first normalize the mentions of disease classes in the corpus and then apply Word2Vec to\\ngenerate embeddings for classes, not merely words\\nmanual curation following the suggestions by our classi-\\nfier took 10-15 min per sample (which included identify-\\ning related classes in the DO and drafting an explanation\\nfor cases which disagree with the DO).\\nResults\\nBroad classification of domain-specific terms: application\\nto diseases\\nOur method is a workflow that can be used to identify\\nwhether a term or phrase commonly refers to a class\\nthat may be included in a domain-specific ontology as a\\nlabel, synonym, or a new class. To achieve this goal, we\\nuse the existing labels and synonyms within a domain-\\nontology as \\u0093seeds\\u0094 to train a machine learning classifier\\nthat determines whether a new term is sufficiently similar\\nto an existing label or synonym and may therefore also be\\nincluded in the ontology. We represent terms primarily by\\nthe context in which they occur within a large corpus of\\ntext; we useWord2Vec [22] for this purpose.We then train\\nan Artificial Neural Network classifier in a supervised\\nmanner to distinguish between the terms already included\\nwithin a domain ontology (and therefore expected to refer\\nto a particular kind of phenomena) and randomly chosen\\nterms not included in the ontology (and therefore most\\nlikely not referring to a phenomenon within the domain\\nof the ontology).\\nWe demonstrate our method using the Human Dis-\\nease Ontology (DO) [5] and applying it to the terms\\noccurring in a large corpus of full-text biomedical articles\\n(see \\u0093Methods\\u0094). First, we tested whether our approach is\\ncapable of identifying words that refer to the Disease class\\n(DOID:4), i.e., whether our method can detect terms\\nthat refer to a disease. We generated word embeddings\\nfor every disease terms and other words in our corpus of\\nfull-text articles.\\nFigure 3 illustrates the distribution of the terms refer-\\nring to a diseases in DO and other words mentioned in\\nour corpus which do not belong to DO using the t-SNE\\ndimensionality reduction [29]. We can see that the terms\\nare clearly different and should be separable through a\\nmachine learning system.\\nTherefore, we trained a machine learning model to\\nrecognize whether a word refers to the disease or not\\nusing the word embeddings as input. We split the vec-\\ntor space embeddings into a training and testing dataset\\nand consider all embeddings referring to disease as pos-\\nitive instances and all others as negatives. We do not\\napply any filtering before selecting the positive or negative\\nsamples. We randomly select negatives equal to the num-\\nber of positives (7,932 positives and 7,932 negatives). We\\nwithhold 20% of randomly chosen positive and negative\\ninstances for testing, train a model on the remaining 80%\\nthrough 10-fold cross validation, and report the perfor-\\nmance results on the 20% test set. Evaluated on the testing\\nset, we can distinguish between disease and non-disease\\nterms with an F-score of 95% and AUC of 96% (see Table 1\\nand Figure 4).\\nTo better understand the source of errors and whether\\nour approach can be used to reliably extend ontolo-\\ngies (either with additional labels and synonyms, or new\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 5 of 13\\nFig. 3 a) The visualization of the embeddings using the t-SNE for binary-classification task b) The visualization of the embeddings using the t-SNE\\nfor classifying infectious diseases. c) The visualization of the embeddings using the t-SNE for classifying anatomical diseases. d) The visualization of\\nthe embeddings using the t-SNE for classifying the combination of infectious and anatomical diseases\\nclasses), we performed a manual analysis on a set of 20\\nfalse positive samples out of 197 which are not the label or\\nsynonym of a disease class DO but are classified as disease\\nby our classifier (see Table 2). We found that the majority\\nof the 20 false positive samples refer to either diseases or\\nphenotypes (where phenotypes are the observable char-\\nacteristics of an organism that may occur manifestations,\\nor signs and symptoms, of a disease, but do not constitute\\na disease on its own). For example, Aphthosis is a pre-\\ndiction of our method which refers to a human disorder\\nthat is not currently in the DO; the majority of false pos-\\nitives are disease-related terms that do not explicitly refer\\nto a disease. For example, we predictedmal-absorption as\\na disease term which may refer to a phenotype in some\\ncontexts. Our findings indicate that an ANN classifier\\ncan identify known terms referring to diseases, and can\\nfurther suggest novel terms which may prove useful for\\nontology development and extension.\\nFine-grained classification: distinguishing between groups\\nof diseases\\nAs our method showed capability to identify terms refer-\\nring to a disease, we next tested whether our method can\\nalso distinguish between different types of diseases. For\\nthis purpose, we used the embeddings generated from a\\npre-processed corpus in which we normalize all mentions\\nTable 1 F-score and AUC for our four experiments using different hidden layer sizes\\nClassification Hidden layer sizes 10 50 100 200\\nNumber of classes F-score AUC F-score AUC F-score AUC F-score AUC\\nDiseases 2 94.65% 95.31% 94.83% 95.97% 95.32% 96.06% 94.49% 95.99%\\nInfectious disease 5 95.65% 95.01% 96.01% 95.74% 95.43% 95.22% 95.68% 96.42%\\nAnatomical disease 13 69.18% 77.22% 70.15% 80.24% 70.20% 76.98% 72.00% 85.11%\\nInfectious + anatomical diseases 17 71.07% 84.75% 73.13% 84.03% 72.61% 84.98% 72.67% 83.66%\\nThe values in bold represent the highest AUC and F-score within each experiments\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 6 of 13\\nFig. 4 ROC curves for each experiment (Diseases, Infectious disease, Anatomical disease and a combination of Infectious disease + Anatomical disease)\\nof a disease in our corpus using Whatizit tool. The dis-\\nease dictionary that we utilized with Whatizit includes a\\ntotal of 21,788 terms (labels and synonyms) from DO. We\\nfound that 15,183 of these 21,788 terms appeared in our\\ncorpus and we generate an embedding vector for each of\\nthem. We then first trained a neural network model to\\nrecognize whether a disease-term refers to the Infectious\\nDisease (DOID:0050117) class or not, and furthermore\\nwhether our method is able to distinguish between the\\nfour different types of infectious disease in DO (i.e., bac-\\nterial, fungal, parasitic, or viral infectious disease). As\\ntraining data, we used the word embeddings generated for\\nDO classes, and we used the Elk reasoner to split them\\ninto four types of infectious diseases, and an additional\\nclass for diseases that are not a subclass of Infectious Dis-\\nease in DO. We randomly select 20% of the disease in\\nDO as validation set and train the neural network classi-\\nfier using 10-fold cross-validation on the remaining 80%\\nto separate diseases into one of the five classes (non-\\ninfectious, bacterial, fungal, parasitic and viral infections).\\nTable 1 shows the performance achieved on the validation\\nset.While the performance is less than predicting whether\\na term refers to a disease, our classifier can distinguish\\nbetween specific disease classes.\\nWe manually analyzed a set of 20 false positive samples\\nout of 38 which are not a subclass of Infectious disease in\\nthe DO but are classified as infectious by our classifier (see\\nTable 3). We found that 7 of these 20 cases can be sug-\\ngested to be subclasses of the specific infectious disease\\nthey have been classified with but do not have a subclass\\nrelation asserted or inferred in DO. For example, the term\\nsyphilitic meningitis (DOID:10073) is a disease that our\\nmethod classify as a bacterial infectious disease but it is\\nnot classified as infectious in the DO.\\nMoreover, to test the strength of our method to distin-\\nguish between disease classes, we further trained a neural\\nnetwork model to distinguish between the 12 different\\nsubclasses of Disease of anatomical entity (DOID:7), as\\nwell as an additional class for diseases not classified as\\nsubclasses of Disease of anatomical entity. We used the\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 7 of 13\\nTable 2 Manually analyzed disease terms predicted as disease\\nTerm Manual analysis result Explanation for the suggested diseases\\nFACTO other -\\nleucoencephalopathy other -\\nAphthosis Disease A disease refers to a condition with repetitive mucosal ulcers\\n[30, 31].\\nDesmoid other -\\nmetapneumovirus other -\\nTracheobronchomalacia Disease A rare condition with abnormal flaccidity of both the trachea\\nand the bronchi which results in possibility of narrowing or\\ncollapse of the airway [32\\u009634].\\nRESLES Disease A rare condition characterized by transient lesions in the cen-\\ntral part of the splenium of the corpus callosum (SCC), followed\\nby complete reversibility on follow-up magnetic resonance\\nimaging (MRI) after a variable period. It coincides with different\\ndiseases [35, 36].\\nmal-absorption other -\\nacroparesthesias other -\\nlimb-shaking other -\\npineocytomas Disease A rare disease that has an Orphanet ID: ORPHA:251912. It is\\none of the pineal parenchymal tumors and is considered the\\nleast aggressive one [37, 38].\\nhypomineralisation other -\\nneurognathostomiasis Disease It is a severe form of human gnathostomiasis, DOID:11379,\\nwhich can lead to disease and death, it involves the nervous\\nsystem [39\\u009641].\\nMetastasis other -\\nmyelomatosis Disease A type of cancer that begins in plasma cells that produce anti-\\nbodies. It could be one of the synonyms of multiple myeloma\\nDOID:9538 [42, 43].\\nAMRF Disease An OMIM disease, OMIM:254900 [44].\\narthralgia other -\\nfibrodentinoma Disease Fibrodentinoma is a benign odontogenic tumor that occurs\\nin children and young adults. The disease name usually is\\nrepresented as \\u0093Ameloblastic Fibrodentinoma\\u0094 [45, 46].\\ninfantile-ataxia other -\\nknowlesi other -\\nThe terms in bold represent the correctly validated terms (by a clinician) that classified as diseases terms using our method (in Diseases classification experiment).\\nsame method to split the classes in training and test set as\\nbefore. Results are shown in Table 1 and demonstrate that\\nour method can also be useful to classify diseases in their\\nanatomical sub-systems.\\nWe manually analyzed a set of 20 false positive samples\\nout of 127 which are not a subclass of Anatomical dis-\\nease in the DO but are classified as being a subclass of a\\nparticular anatomical system disease by our classifier (see\\nTable 4). We found that 12 of the 20 false positives can be\\nsuggested to be subclasses of the specific anatomical sys-\\ntem disease they have been classified with but do not have\\nsuch a subclass relation asserted or inferred in DO. For\\nexample, we classify Narcolepsy (DOID:8986) as a Ner-\\nvous system anatomical disease, and this may be added as\\na new subclass axiom to DO.\\nAs it is often inconvenient to train separate classifiers,\\nwe also combined both tasks and trained a multi-class\\nclassifier to classify disease classes either as infectious or\\nanatomical, or as other disease. We evaluate the perfor-\\nmance of this combined model (see Table 1), and our\\nmachine learning system achieves an AUC up to 84% (see\\nFigure 4). These results demonstrate it may be possible to\\nidentify new subclasses, although the performance drops\\nwhen we increase the complexity of the classification\\nproblem by distinguishing between more subclasses.\\nDiscussion\\nWe developed a method to automatically expand ontolo-\\ngies in the biomedical domain with new classes, syn-\\nonyms, or axioms. We demonstrate the utility of our\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 8 of 13\\nTable 3 Sample of manually analyzed disease terms predicted as infectious disease\\nDisease terms Ontology class assigned\\nby ANN\\nManual analysis result Suggested additional\\nclassification\\nDOID Explanation\\nPelizaeus-Merzbacher\\ndisease\\nViral infectious disease Non-infectious (inherited\\ndisorder)\\n- - -\\nKaposi\\u0092s sarcoma Viral infectious disease Viral infectious disease herpes simplex DOID:8566 The disease is caused by\\nHuman herpesvirus 8\\nwhich is Herpesviridae\\ninfection.\\nmaxillary sinusitis Bacterial infectious disease Bacterial infectious disease\\n(usually start viral and\\nprogress to either\\nbacterial or fungal)\\n- - It is an infection in the\\nmaxillary sinuses which\\ncould be due to different\\netiology, one of them is\\nbacterial [47].\\nkeratosis follicularis Bacterial infectious disease Non-infectious (genetic\\ndisease)\\n- - -\\nchronic rheumatic\\npericarditis\\nViral infectious disease The condition is triggered\\nby autoimmune reaction\\nto infection, mainly group\\nA streptococci.\\n- - -\\ngastroparesis Viral infectious disease In most cases the nerve is\\ndamaged by diabetes or\\nsurgery, however, a viral\\ninfection might be a cause\\n- - A condition in which the\\nstomach suffers from\\nparesis that affects the\\nfood movement to the\\nsmall intestine [48, 49].\\nosmotic diarrhea Bacterial infectious disease symptom - - -\\nfamilial cold\\nautoinflammatory\\nsyndrome\\nViral infectious disease Non-infectious (inherited\\ndisease)\\n- - -\\nangular cheilitis Fungal infectious disease Etiology is controversial,\\nmost commonly fungal or\\nbacterial.\\n- - Ambiguous.\\nBinder syndrome Viral infectious disease Congenital disease - - -\\nhypohidrosis Bacterial infectious disease Multi-causal - - -\\nSjogren\\u0092s syndrome Viral infectious disease autoimmune disease - - -\\nmedian rhomboid\\nglossitis\\nFungal infectious disease Etiology is controversial,\\nhowever it is considered\\nas a variant of orallesion\\nassociated with candida\\ninfection [50].\\n- - Ambiguous.\\nGoodpasture syndrome Viral infectious disease autoimmune disease - - -\\nsyphilitic meningitis Bacterial infectious disease Bacterial infectious disease syphilis DOID:4166 Considering the same\\nconcept of etiology, both\\ndiseases are caused by\\nbacterial infection\\n(Treponema pallidum).\\nacute diarrhea Viral infectious disease symptom - - -\\nWHIM syndrome Bacterial infectious disease Congenital disease - - -\\nerythrasma Fungal infectious disease Bacterial infection disease - - -\\nchronic wasting disease Parasitic infectious disease Neurodegenerative\\ndisorder\\n- - -\\nscarlet fever Bacterial infectious disease Bacterial infectious disease rheumatic fever DOID:1586 The disease is caused by\\nGroup A bacteria of the\\ngenus Streptococcus,\\nsame causative agent for\\nRheumatic fever.\\nThe terms in bold represent the correctly validated terms (by a clinician) that classified as infectious diseases terms using our method (in Infectious disease classification\\nexperiment).\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 9 of 13\\nTable 4 Sample of manually analyzed disease terms classified as affecting particular anatomical systems\\nDisease terms Ontology\\nclass\\nOntology\\nclass\\nassigned by\\nANN\\nManual analysis\\nresult\\nSuggested\\nadditional\\nclassification\\nDOID Explanation\\nTimothy\\nsyndrome\\ngenetic\\ndisease\\ncardiovascular\\nsystem\\ndisease\\nCannot specify\\n(affect multiple\\nparts)\\n- - -\\nFamilial periodic\\nparalysis\\ndisease of\\nmetabolism\\ncardiovascular\\nsystem\\ndisease\\nmusculoskeletal\\nsystem disease\\n- - -\\nHyperprolactinemiadisease of\\nmetabolism\\nendocrine\\nsystem\\ndisease\\nendocrine system\\ndisease\\npituitary gland\\ndisease\\nDOID:53 The pituitary gland is\\nthe endocrine gland\\nresponsible for\\nsecreting prolactin.\\nAngiokeratoma\\ncircumscriptum\\ndisease of\\ncellular\\nproliferation\\ngastrointestinal\\nsystem\\ndisease\\ncardiovascular\\nsystem disease\\n- - -\\nZollinger-\\nEllison\\nsyndrome\\nsyndrome gastrointestinal\\nsystem\\ndisease\\ngastrointestinal\\nsystem disease\\npeptic ulcer disease DOID:750 It is a disease that\\naffects either\\npancreas, duodenum,\\nor both of them. Both\\norgans are pats of the\\nGIT system. The\\ndisease pathology\\nis mainly excessive\\ngastrin secretion with\\nsubsequent peptic\\nulcers.\\nPolycystic liver\\ndisease\\ngenetic\\ndisease\\ngastrointestinal\\nsystem\\ndisease\\ngastrointestinal\\nsystem disease\\nliver disease DOID:409 It is a genetic disorder\\nthat affects primarily\\nthe liver.\\nBilirubin\\nmetabolic\\ndisorder\\ndisease of\\nmetabolism\\nhematopoietic\\nsystem\\ndisease\\nhematopoietic\\nsystem disease\\nkernicterus due to\\nisoimmunization\\nDOID:12043 Bilirubin disorder\\ncould be a result of\\nblood pathology,\\nsame as for the\\nmentioned\\nclassification\\nDOID:12043.\\nAlpha\\nthalassemia\\ngenetic\\ndisease\\nhematopoietic\\nsystem\\ndisease\\nhematopoietic\\nsystem disease\\nhemoglobinopathy DOID:2860 The disease is mainly a\\nhemoglobin\\ndisorder with\\nhematological\\nphenotypes.\\nKabuki syndrome syndrome immune sys-\\ntem disease\\nNot anatomical\\n- multisystems\\n- - -\\nAmyloidosis disease of\\nmetabolism\\nimmune sys-\\ntem disease\\nNot anatomical -\\nmultisystems\\n- - -\\nFatty liver disease disease of\\nmetabolism\\nmusculoskeletal\\nsystem\\ndisease\\ngastrointestinal\\nsystem disease\\n- - -\\nRenal-hepatic-\\npancreatic\\ndysplasia\\nphysical\\ndisorder\\nmusculoskeletal\\nsystem\\ndisease\\nCannot specify\\n(affect multiple\\nparts)\\n- - -\\nRadioulnar syn-\\nostosis\\nphysical\\ndisorder\\nmusculoskeletal\\nsystem\\ndisease\\nmusculoskeletal\\nsystem disease\\nbone development\\ndisease/Synostosis\\nDOID:0080006/\\nDOID:11971\\nThere is already an\\nentity in the DO for\\nsynostosis under\\nbone development\\ndisease.\\nHypophosphatasia genetic\\ndisease\\nmusculoskeletal\\nsystem\\ndisease\\nmusculoskeletal\\nsystem disease\\nbone remodeling\\ndisease\\nDOID:0080005 We could suggest\\nan additional\\nclassification based\\non the main affected\\nsystem. Our\\nsuggestive\\nclassification is\\nmusculoskeletal since\\nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 10 of 13\\nTable 4 Sample of manually analyzed disease terms classified as affecting particular anatomical systems (Continued)\\nthe disease is mainly\\naffecting\\nmineralization of\\nthe bone with\\nphenotypes similar to\\nthose of Rickets\\nDOID:10609.\\nNarcolepsy disease of mental\\nhealth\\nnervous\\nsystem\\ndisease\\nnervous system\\ndisease\\n* * *\\nAceruloplasminemia disease of metabolism nervous\\nsystem\\ndisease\\nnervous system\\ndisease\\nneurodegeneration\\nwith brain iron\\naccumulation\\nDOID:0110734 The disease main\\npathophysiology is\\neither the absence\\nor dysfunction of\\nceruloplasmin with\\nsubsequent iron\\naccumulation in\\nvarious organ, mainly\\nthe brain.\\nGlomangiomatosis disease of cellular pro-\\nliferation\\nnervous\\nsystem\\ndisease\\ncardiovascular\\nsystem disease\\n- - -\\nDeafness-dystonia-\\noptic neuronopathy\\nsyndrome\\ndisease of metabolism nervous sys-\\ntem disease\\nnervous system\\ndisease\\nnervous system\\ndisease; since it\\ncovers many\\nsubclasses to\\nwhich we can map\\nmany aspects of\\nthis disease\\nDOID:863 The disease\\u0092s\\nphenotypes reflect\\nneurological affection\\nofmultiple parts in the\\nnervous system.\\nTrophoblastic\\nneoplasm\\ndisease of cellular\\nproliferation\\nreproductive\\nsystem\\ndisease\\nreproductive\\nsystem disease\\nFemale\\nreproductive organ\\ncancer\\nDOID:120 The term refers to the\\ngroup of\\nmalignant neoplasms\\nthat consist of\\nabnormal\\nproliferation of\\ntrophoblastic tissues\\nsimilar to\\nchoriocarcinoma\\nDOID:3596 and\\ngestational\\ntrophoblastic\\nneoplasia\\nDOID:3590.\\nCryptorchidism physical disorder reproductive\\nsystem\\ndisease\\nreproductive\\nsystem disease\\ntesticular disease DOID:2519 The term refers to\\nundescended testicle.\\n*Nacrolepsy: is classified as a sleep disorde'</span>text2\n:   'RESEARCH Open Access\\nTemporal information extraction from\\nmental health records to identify duration\\nof untreated psychosis\\nNatalia Viani1*, Joyce Kam1, Lucia Yin1, André Bittar1, Rina Dutta1,2, Rashmi Patel1,2, Robert Stewart1,2 and\\nSumithra Velupillai1\\nAbstract\\nBackground: Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health,\\nas longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about\\nwhen psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic\\nhealth records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information\\nunderlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research.\\nNatural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant\\ninformation in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are\\nlikely to include the information of interest. Next, temporal information extraction methods are needed to identify'text3\n:   <span style=white-space:pre-wrap>'RESEARCH Open Access\\nDisclosing Main authors and Organisations\\ncollaborations in bioprinting through\\nnetwork maps analysis\\nLeonardo Azael García-García1* and Marisela Rodríguez-Salvador2\\nAbstract\\nBackground: Scientific activity for 3D bioprinting has increased over the past years focusing mainly on fully functional\\nbiological constructs to overcome issues related to organ transplants. This research performs a scientometric analysis on\\nbioprinting based on a competitive technology intelligence (CTI) cycle, which assesses scientific documents to establish\\nthe publication rate of science and technology in terms of institutions, patents or journals. Although analyses of\\npublications can be observed in the literature, the identification of the most influential authors and affiliations has not\\nbeen addressed. This study involves the analysis of authors and affiliations, and their interactions in a global framework.\\nWe use network collaboration maps and Betweenness Centrality (BC) to identify of the most prominent actors in\\nbioprinting, enhancing the CTI analysis.\\nResults: 2088 documents were retrieved from Scopus database from 2007 to 2017, disclosing an exponential growth\\nwith an average publication increase of 17.5% per year. A threshold of five articles with ten or more cites was\\nestablished for authors, while the same number of articles but cited five or more times was set for affiliations. The\\nauthor with more publications was Atala A. (36 papers and a BC = 370.9), followed by Khademhosseini A. (30\\ndocuments and a BC = 2104.7), and Mironov (30 documents and BC = 2754.9). In addition, a small correlation was\\nobserved between the number of collaborations and the number of publications. Furthermore, 1760 institutions with a\\nmedian of 10 publications were found, but only 20 within the established threshold. 30% of the 20 institutions had an\\nexternal collaboration, and institutions located in and close to the life science cluster in Massachusetts showed a strong\\ncooperation. The institution with more publications was the Harvard Medical School, 61 publications, followed by the\\nBrigham and Women\\u0092s hospital, 46 papers, and the Massachusetts Institute of Technology with 37 documents.\\nConclusions: Network map analysis and BC allowed the identification of the most influential authors working on\\nbioprinting and the collaboration between institutions was found limited. This analysis of authors and affiliations and\\ntheir collaborations offer valuable information for the identification of potential associations for bioprinting researches\\nand stakeholders.\\nKeywords: Network map analysis, Betweenness centrality, Bioprinting, Text mining, Collaboration analysis,\\nscientometrics, competitive technology intelligence\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: L.A.Garcia-Garcia@sussex.ac.uk\\n1University of Sussex, School of Engineering and Informatics, Falmer,\\nBrighton, UK\\nFull list of author information is available at the end of the article\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics\\n           (2020) 11:3 \\nhttps://doi.org/10.1186/s13326-020-0219-z\\nBackground\\nResearch articles are public documents that report scien-\\ntific advancements to share knowledge and promote devel-\\nopment in science. These documents contain fundamental\\ninformation regarding not only to research but also to the\\norganizations and authors involved. This data is of interest\\nto identify leading organizations and to map scientific\\ncollaborations.\\nScientometric tools such as co-citation analysis, biblio-\\ngraphic coupling, or co-author analysis can help to achieve\\nthese goals. Co-citation analysis and bibliographic coupling\\nare mainly used to measure the flow of information based\\non the documents selected by authors, while co-author\\nanalysis is more focused on the analysis of collaboration be-\\ntween authors, taking into consideration the social aspect\\nof the research collaboration. Furthermore, co-author ana-\\nlysis has been proved to be useful to determine the multi\\nand interdisciplinary of the institutions and their collabora-\\ntions [1]. Co-author analysis requires information related to\\nauthors\\u0092 aliases, affiliations, publications, areas of research,\\nand their collaborations. This information can be obtained\\nfrom digital libraries (DL) aimed to create systems for the\\nidentification of authors such as ORCID, which was created\\nby non-profit organizations, or ResearcherID, Scopus,\\nPubMED or Web of Science, which are companies that are\\ndeveloping their unique identifiers for authors [2\\u00964]. When\\nevaluating advances in science and technology, names of\\nauthors and affiliations become major indicators, as 1) their\\nnumber of citations by peers correlates to their acknow-\\nledgment as influential on their area of research [5] and 2)\\ncontributes to determining the specific disciplines involved\\nin the research [1], both are important elements to nurture\\nthe decision-making process. In this sense, Competitive\\nIntelligence (CI) acquires a relevant role, through the defin-\\nition, collection, analysis, and presentation of relevant infor-\\nmation [6]. The CI process can be further enhanced by\\nincorporating feedback form experts to validate the infor-\\nmation obtained [7]. CI is fundamental to research and de-\\nvelopment (R&amp;D), including products or processes with\\nradical novelty, such as bioprinting.\\nBioprinting is an emerging technology, a variant of addi-\\ntive manufacturing that involves the fabrication of 3D con-\\nstructs for living tissues and organs [8, 9]. This discipline\\nis growing at an accelerated pace, involving branches of\\nknowledge such as biology and engineering. Bioprinting\\nhas been developed to assist the needs of a fast-growing\\npopulation. This technique has potential social and eco-\\nnomic impacts [10, 11], including a huge effect in organ\\ntransplants, where one of the main objectives is the print-\\ning of functional biological structures to help in the short-\\nage of organs, thus overcoming long waiting lists and\\nissues related to the transplanted organs such as rejection\\n[10\\u009612]. Although there have been significant signs of pro-\\ngress in the past years, there are some areas of research to\\nbe explored in this incipient technology [11]. Since acad-\\nemy and industry have acknowledged that bioprinting will\\nhave a significant impact on the health-care sector in the\\nfollowing years, the identification of technology trends in\\n3D bioprinting [13, 14], including potential printing tech-\\nniques [15], becomes crucial to stay competitive and to\\ndevelop new technologies in this field. With this aim,\\nRodriguez-Salvador et al. [7] performed a patentometric\\nand scientometric analysis in bioprinting to identify trends\\nand to explore the knowledge landscape of this technology.\\nIn addition, they also identified the most prolific institu-\\ntions, being the MIT (113 publications) the number one,\\nfollowed by Nanyang Technology University (103 publica-\\ntions), and Tsinghua University (93 publications); They\\nalso found that the three first countries with more publica-\\ntions were USA with 1491, followed by China with 744,\\nand Germany with 377 [7]. These analyses are mostly\\nbased on the frequency of documents by affiliation and\\ncountry, and no inclusion or exclusion terms were set. The\\ninsights obtained can be enhanced with the identification\\nof the leading scientists and their field of expertise, thus\\ndistinguishing the principal areas of current research and\\ndetermining potential opportunities for R&amp;D.\\nIn order to unveil scientific and technological trends, it is\\nimportant to face big volumes of information using text\\nmining. This activity can be applied to identify and extract\\npotentially useful information from texts. It combines tools\\nsuch as machine learning, artificial intelligence, and statis-\\ntics to analyse large amounts of both structured and un-\\nstructured data. The information obtained can contribute\\nto understanding patterns in data by making use of tools\\nsuch as text categorization, text clustering, information ex-\\ntraction, among others [16]. Information retrieval, word\\nfrequency analysis, word distribution, pattern recognition,\\nand visualisation techniques are some of the most frequent\\npractices [17]. As a conclusion, text mining adds important\\nvalue to the pattern recognition by structuring the content\\nof data from textual sources for research, data analysis,\\nbusiness or competitive intelligence (CI) [17\\u009619].\\nA fundamental topic for the CI is the determination of\\nkey players, such as the main organizations and authors in-\\nvolved in scientific advancements. Network analysis can be\\nused to identify the collaboration in a visual pattern, where\\neither the authors or affiliations are represented by nodes\\nand their collaborations can be seen as the connection\\namong them. Moreover, the nodes with common attributes\\nof interest for the analysis can be grouped using clusteriza-\\ntion. Clusterization allows to group components with simi-\\nlar characteristics, such as research topics or techniques.\\nWhen clustering collaborations, the closer the nodes in au-\\nthors or affiliations network maps, the more similarities\\nthey share [5, 20]. Furthermore, collaboration analysis can\\nbe strengthened with the assessment of the Betweenness\\nCentrality (BC) to determine the level of association of the\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 2 of 13\\nnodes according to their position in the network. A\\nstraightforward measurement of the association level can\\nbe the connectivity, but it fails to disclose the importance\\nof a node. To overcome this, BC measure can be calculated\\nto evaluate the importance of a node and its social inter-\\naction in a network as this measure counts the number of\\nregions in the map connected by each element, setting\\ntheir importance in the flow of information [21, 22].\\nScientometric and patentometric techniques have been\\nused recently to analyse the number publications per year,\\nthe main authors, and organizations to determine the main\\nadvancements in bioprinting (methods, materials, etc.)\\n[23\\u009625]. Scientometric and text mining can be used to de-\\ntect the authors and affiliations with more publications\\nand more influence in bioprinting. This information can\\nbe an input for people looking for well-known experts in\\nbioprinting or state-of-the-art developments in the field.\\nTo achieve the main goal of this paper, a customised\\nsearch query was used to gather documents from Scopus.\\nThe query included keywords highly used in the most\\ncited papers on bioprinting. Two network maps of collab-\\norations, one for authors and one for affiliations, were\\ngenerated and analysed. Further analyses were carried out\\nto estimate the BC measurement, and the relationship be-\\ntween number of publications and the number of collabo-\\nrations. These parameters were used for the identification\\nof the most prolific (those with more publications on this\\ntopic) and important authors and institutions involved in\\nthe publications of advances in bioprinting.\\nThis analysis is the first attempt to undertake a quantita-\\ntive analysis using a network analysis approach and the\\ncalculation of centrality measurements to strengthen the\\nCI methodologies. The findings enhance the perception of\\nthe importance of collaborations among institutions for\\nthe generation of high-quality scientific outcomes and for\\nthe dissemination of the knowledge generated, helping\\nboth researchers and stakeholders in the identification of\\npotential opportunities for research and collaboration.\\nMethods\\nThis paper is focused on the network analysis of authors\\nand institutions from scientific publications in bioprinting.\\nThe analysis comprises both, a network analysis on the\\ncollaboration among institutions and one that deals with\\nthe collaboration among authors. The network maps were\\ngenerated in Gephi, an open-source software for network\\nanalysis [26\\u009634]. Betweenness centrality was calculated\\nfor both, authors and institutions\\u0092 collaborations.\\nThe adequate identification of specific keywords on the\\ntopic of interest is a determining step in the search strat-\\negy, as they contribute to the appropriate establishment of\\nthe search queries. A preliminary search in Scopus using\\nonly the term bioprinting with no period of time defined\\nwas the first stage of this research. Scopus was selected for\\nthe information retrieval as this is a major scientific data-\\nbase that includes information from more than 20,000 sci-\\nentific journals [35]. The ten most cited papers identified\\nthrough this search were selected, as they have been\\nacknowledged as referents for the topic. Table 1, García-\\nGarcía[36], shows the ten articles that formed the first set\\nof documents. These papers were used to identify the key-\\nwords to form the search queries. A text mining program\\nwas specially coded to carry out the text-mining of these\\npublications, thus determining the most relevant keywords\\non the topic. With a broader range of terms and their syn-\\nonyms we guarantee the inclusion of a wide range of pub-\\nlications compared to searches performed using only the\\nterm bioprinting. Three different types of keywords were\\nsearched in the whole text of the papers, being 1) the most\\nfrequent terms, 2) terms containing the word bio, and 3)\\nthe collocations, which are the juxtapositions of two words\\nwith a greater frequency. A cleaning of terms was accom-\\nplished manually afterward to sort them out according to\\nspecialized language of the subject. The identified key-\\nwords were separated by subtopics (i. e. technology,\\nprocess, and application) to form the search queries. A set\\nof 23 searches were performed with the selected termin-\\nology prior to the development of the definite query.\\nThese searches were used to identify the correct grouping\\nof terms and the exclusion terms.\\nThe search query was formed using the keywords previ-\\nously identified in combination with Boolean and proxim-\\nity operators, and exclusion terms. For this stage, the\\ndefinite search was carried out by defining the period of\\ntime, from 1 January 2000 to 15 November 2017 (when\\nthe information collection was concluded). The main\\nquery is observed in the appendix A1. The collection activ-\\nity involved the use of the query to search in title, abstract\\nand keywords. A quick review of titles and abstracts of the\\ndocuments found was carried out to discard those not\\nrelated to bioprinting.\\nThe bibliographic information of the documents identi-\\nfied in Scopus was retrieved and exported in a CSV format\\nto be cleaned and analysed. A cleaning process and the\\ncomplete normalization of the data was carried out to\\nstandardize authors and affiliations names. We performed\\na manual name disambiguation for both authors and affili-\\nations. The two authors analysed manually all the names\\non each one of the publications gathered. Every time a\\nsimilar name was observed, name disambiguation was car-\\nried out by looking to the full name, affiliation, and e-mail.\\nThe level of agreement on the disambiguation performed\\nby the authors was measured using Cohen\\u0092s kappa [37].\\nCo-author analysis was limited exclusively to the informa-\\ntion of the publications gathered and we did not require\\nfurther information from available DLs.\\nThe analysis to identify the most influential authors\\nand affiliations was carried out by setting a threshold for\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 3 of 13\\nTa\\nb\\nle\\n1\\nC\\nom\\npa\\nris\\non\\nof\\nth\\ne\\nto\\np\\nte\\nn\\nci\\nte\\nd\\npa\\npe\\nrs\\nfro\\nm\\nSc\\nop\\nus\\nob\\nta\\nin\\ned\\nfro\\nm\\nth\\ne\\nse\\nar\\nch\\nof\\n`b\\nio\\npr\\nin\\ntin\\ng\\u0092\\nan\\nd\\nth\\ne\\nde\\nve\\nlo\\npe\\nd\\nse\\nar\\nch\\nqu\\ner\\ny\\nin\\ntit\\nle\\ns,\\nab\\nst\\nra\\nct\\ns,\\nor\\nke\\nyw\\nor\\nds\\nTo\\np\\nte\\nn\\nre\\nsu\\nlts\\nus\\nin\\ng\\nth\\ne\\nke\\nyw\\nor\\nd\\nbi\\nop\\nrin\\ntin\\ng\\n[3\\n6]\\nTo\\np\\nte\\nn\\nar\\ntic\\nle\\ns\\nus\\nin\\ng\\nth\\ne\\nde\\nve\\nlo\\npe\\nd\\nse\\nar\\nch\\nst\\nrin\\ng\\nTi\\ntle\\nA\\nut\\nho\\nrs\\nYe\\nar\\nSo\\nur\\nce\\nC\\nite\\ns\\nTi\\ntle\\nA\\nut\\nho\\nr\\nYe\\nar\\nSo\\nur\\nce\\nC\\nite\\ns\\n1\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nof\\ntis\\nsu\\ne\\nan\\nd\\nor\\nga\\nns\\n[3\\n8]\\n.\\nM\\nur\\nph\\ny,\\nS.\\nV.\\n,\\nA\\nta\\nla\\n,A\\n.\\n20\\n14\\nN\\nat\\nur\\ne\\nBi\\not\\nec\\nhn\\nol\\nog\\ny\\n32\\n(8\\n),\\npp\\n.7\\n73\\n\\u00967\\n85\\n14\\n98\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nof\\ntis\\nsu\\nes\\nan\\nd\\nor\\nga\\nns\\n[3\\n8]\\n.\\nM\\nur\\nph\\ny\\nS.\\nV.\\n,A\\nta\\nla\\nA\\n.\\n20\\n14\\nN\\nat\\nur\\ne\\nBi\\not\\nec\\nhn\\nol\\nog\\ny\\n32\\n(8\\n),\\npp\\n.\\n77\\n3\\u0096\\n78\\n5.\\n14\\n98\\n2\\nSc\\naf\\nfo\\nld\\n-fr\\nee\\nva\\nsc\\nul\\nar\\ntis\\nsu\\ne\\nen\\ngi\\nne\\ner\\nin\\ng\\nus\\nin\\ng\\nbi\\nop\\nrin\\ntin\\ng\\n[3\\n9]\\n.\\nN\\nor\\not\\nte\\n,C\\n.,\\nM\\nar\\nga\\n,\\nF.\\nS.\\n,N\\nik\\nla\\nso\\nn,\\nL.\\nE.\\n,\\nFo\\nrg\\nac\\ns,\\nG\\n.\\n20\\n09\\nBi\\nom\\nat\\ner\\nia\\nls\\n30\\n(3\\n0)\\n,p\\np.\\n59\\n10\\n\\u00965\\n91\\n7\\n60\\n0\\nM\\nic\\nro\\nsc\\nal\\ne\\nte\\nch\\nno\\nlo\\ngi\\nes\\nfo\\nr\\ntis\\nsu\\ne\\nen\\ngi\\nne\\ner\\nin\\ng\\nan\\nd\\nbi\\nol\\nog\\ny\\n[4\\n0]\\n.\\nKh\\nad\\nem\\nho\\nss\\nei\\nni\\nA\\n.,\\nLa\\nng\\ner\\nR.\\n,B\\nor\\nen\\nst\\nei\\nn\\nJ.,\\nVa\\nca\\nnt\\niJ\\n.P\\n.\\n20\\n06\\nPr\\noc\\n.N\\nat\\nl.\\nAc\\nad\\n.\\nSc\\ni.\\nU\\n.S\\n.A\\n.,\\n10\\n3\\n(8\\n),\\npp\\n.2\\n48\\n0\\u0096\\n24\\n87\\n.\\n11\\n63\\n3\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nof\\nva\\nsc\\nul\\nar\\niz\\ned\\n,\\nhe\\nte\\nro\\nge\\nne\\nou\\ns\\nce\\nll-\\nla\\nde\\nn\\ntis\\nsu\\ne\\nco\\nns\\ntr\\nuc\\nts\\n[2\\n4]\\n.\\nKo\\nle\\nsk\\ny,\\nD\\n.B\\n.,\\nTr\\nub\\ny,\\nR.\\nL.\\n,G\\nla\\ndm\\nan\\n,A\\n.S\\n.,\\nH\\nom\\nan\\n,K\\n.A\\n.,\\nLe\\nw\\nis\\n,J\\n.A\\n.\\n20\\n14\\nAd\\nva\\nnc\\ned\\nM\\nat\\ner\\nia\\nls\\n26\\n(1\\n9)\\n,p\\np.\\n31\\n24\\n\\u00963\\n13\\n0\\n58\\n8\\nC\\nlin\\nic\\nal\\ntr\\nan\\nsp\\nla\\nnt\\nat\\nio\\nn\\nof\\na\\ntis\\nsu\\ne-\\nen\\ngi\\nne\\ner\\ned\\nai\\nrw\\nay\\n[4\\n1]\\n.\\nM\\nac\\nch\\nia\\nrin\\niP\\n.,\\nJu\\nng\\neb\\nlu\\nth\\nP.\\n,\\nG\\no\\nT.\\n,A\\nsn\\nag\\nhi\\nM\\n.A\\n.,\\nRe\\nes\\nL.\\nE.\\n,\\nC\\nog\\nan\\nT.\\nA\\n.,\\nD\\nod\\nso\\nn\\nA\\n.,\\nM\\nar\\nto\\nre\\nll\\nJ.,\\nBe\\nlli\\nni\\nS.\\n,P\\nar\\nni\\ngo\\ntt\\no\\nP.\\nP.\\n,\\nD\\nic\\nki\\nns\\non\\nS.\\nC\\n.,\\nH\\nol\\nla\\nnd\\ner\\nA\\n.P\\n.,\\nM\\nan\\nte\\nro\\nS.\\n,C\\non\\nco\\nni\\nM\\n.T\\n.,\\nBi\\nrc\\nha\\nll\\nM\\n.A\\n.\\n20\\n08\\nTh\\ne\\nLa\\nnc\\net\\n37\\n2\\n(9\\n65\\n5)\\n,p\\np.\\n20\\n23\\n\\u00962\\n03\\n0.\\n10\\n14\\n4\\nPr\\nin\\ntin\\ng\\nan\\nd\\npr\\not\\not\\nyp\\nin\\ng\\nof\\ntis\\nsu\\nes\\nan\\nd\\nsc\\naf\\nfo\\nld\\ns\\n[2\\n3]\\n.\\nD\\ner\\nby\\n,B\\n.\\n20\\n12\\nSc\\nie\\nnc\\ne\\n33\\n8\\n(6\\n10\\n9)\\n,\\npp\\n.9\\n21\\n\\u00969\\n26\\n51\\n0\\nM\\nec\\nha\\nni\\nca\\nlp\\nro\\npe\\nrt\\nie\\ns\\nan\\nd\\nce\\nll\\ncu\\nltu\\nra\\nlr\\nes\\npo\\nns\\ne\\nof\\npo\\nly\\nca\\npr\\nol\\nac\\nto\\nne\\nsc\\naf\\nfo\\nld\\ns\\nde\\nsi\\ngn\\ned\\nan\\nd\\nfa\\nbr\\nic\\nat\\ned\\nvi\\na\\nfu\\nse\\nd\\nde\\npo\\nsi\\ntio\\nn\\nm\\nod\\nel\\nlin\\ng\\n[4\\n2]\\n.\\nH\\nut\\nm\\nac\\nhe\\nr\\nD\\n.W\\n.,\\nSc\\nha\\nnt\\nz\\nT.\\n,\\nZe\\nin\\nI.,\\nN\\ng\\nK.\\nW\\n.,\\nTe\\noh\\nS.\\nH\\n.,\\nTa\\nn\\nK.\\nC\\n.\\n20\\n01\\nJo\\nur\\nna\\nlo\\nf\\nBi\\nom\\ned\\nic\\nal\\nM\\nat\\ner\\nia\\nls\\nRe\\nse\\nar\\nch\\n55\\n(2\\n),\\npp\\n.2\\n03\\n\\u00962\\n16\\n.\\n93\\n9\\n5\\nA\\ndd\\niti\\nve\\nm\\nan\\nuf\\nac\\ntu\\nrin\\ng\\nof\\ntis\\nsu\\nes\\nan\\nd\\nor\\nga\\nns\\n[4\\n3]\\n.\\nM\\nel\\nch\\nel\\ns,\\nF.\\nP.\\nW\\n.,\\nD\\nom\\nin\\ngo\\ns,\\nM\\n.A\\n.N\\n.,\\nKl\\nei\\nn,\\nT.\\nJ.,\\nBa\\nrt\\nol\\no,\\nP.\\nJ.,\\nH\\nut\\nm\\nac\\nhe\\nr,\\nD\\n.W\\n.\\n20\\n12\\nPr\\nog\\nre\\nss\\nin\\nPo\\nly\\nm\\ner\\nSc\\nie\\nnc\\ne\\n37\\n(8\\n),\\npp\\n.\\n10\\n79\\n\\u00961\\n10\\n4\\n49\\n5\\nSo\\nlid\\nfre\\nef\\nor\\nm\\nfa\\nbr\\nic\\nat\\nio\\nn\\nof\\nth\\nre\\ne-\\ndi\\nm\\nen\\nsi\\non\\nal\\nsc\\naf\\nfo\\nld\\ns\\nfo\\nr\\nen\\ngi\\nne\\ner\\nin\\ng\\nre\\npl\\nac\\nem\\nen\\nt\\ntis\\nsu\\nes\\nan\\nd\\nor\\nga\\nns\\n[4\\n4]\\n.\\nLe\\non\\ng\\nK.\\nF.\\n,C\\nhe\\nah\\nC\\n.M\\n.,\\nC\\nhu\\na\\nC\\n.K\\n.\\n20\\n03\\nBi\\nom\\nat\\ner\\nia\\nls\\n24\\n(1\\n3)\\n,p\\np.\\n23\\n63\\n\\u00962\\n37\\n8.\\n73\\n9\\n6\\n25\\nth\\nan\\nni\\nve\\nrs\\nar\\ny\\nar\\ntic\\nle\\n:E\\nng\\nin\\nee\\nrin\\ng\\nhy\\ndr\\nog\\nel\\ns\\nfo\\nr\\nbi\\nof\\nab\\nric\\nat\\nio\\nn\\n[4\\n5]\\n.\\nM\\nal\\nda\\n,J\\n.,\\nVi\\nss\\ner\\n,J\\n.,\\nM\\nel\\nch\\nel\\ns,\\nF.\\nP.\\n,G\\nro\\nll,\\nJ.,\\nH\\nut\\nm\\nac\\nhe\\nr,\\nD\\n.W\\n.\\n20\\n13\\nAd\\nva\\nnc\\ned\\nM\\nat\\ner\\nia\\nls\\n25\\n(3\\n6)\\n,p\\np.\\n50\\n11\\n\\u00965\\n02\\n8\\n46\\n5\\nSt\\nem\\nce\\nll-\\nba\\nse\\nd\\ntis\\nsu\\ne\\nen\\ngi\\nne\\ner\\nin\\ng\\nw\\nith\\nsi\\nlk\\nbi\\nom\\nat\\ner\\nia\\nls\\n[4\\n6]\\n.\\nW\\nan\\ng\\nY.\\n,K\\nim\\nH\\n.-J\\n.,\\nVu\\nnj\\nak\\n-\\nN\\nov\\nak\\nov\\nic\\nG\\n.,\\nKa\\npl\\nan\\nD\\n.L\\n.\\n20\\n06\\nBi\\nom\\nat\\ner\\nia\\nls\\n27\\n(3\\n6)\\n,p\\np.\\n60\\n64\\n\\u00966\\n08\\n2.\\n65\\n7\\n7\\nA\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nsy\\nst\\nem\\nto\\npr\\nod\\nuc\\ne\\nhu\\nm\\nan\\n-s\\nca\\nle\\ntis\\nsu\\ne\\nco\\nns\\ntr\\nuc\\nts\\nw\\nith\\nst\\nru\\nct\\nur\\nal\\nin\\nte\\ngr\\nity\\n[4\\n7]\\n.\\nKa\\nng\\n,H\\n.-W\\n.,\\nLe\\ne,\\nS.\\nJ.,\\nKo\\n,I\\n.K\\n.,\\nYo\\no,\\nJ.J\\n.,\\nA\\nta\\nla\\n,A\\n.\\n20\\n16\\nN\\nat\\nur\\ne\\nBi\\not\\nec\\nhn\\nol\\nog\\ny\\n34\\n(3\\n),\\npp\\n.3\\n12\\n\\u00963\\n19\\n46\\n6\\nSc\\naf\\nfo\\nld\\n-fr\\nee\\nva\\nsc\\nul\\nar\\ntis\\nsu\\ne\\nen\\ngi\\nne\\ner\\nin\\ng\\nus\\nin\\ng\\nbi\\nop\\nrin\\ntin\\ng\\n[3\\n8]\\n.\\nN\\nor\\not\\nte\\nC\\n.,\\nM\\nar\\nga\\nF.\\nS.\\n,\\nN\\nik\\nla\\nso\\nn\\nL.\\nE.\\n,F\\nor\\nga\\ncs\\nG\\n.\\n20\\n09\\nBi\\nom\\nat\\ner\\nia\\nls\\n30\\n(3\\n0)\\n,p\\np.\\n59\\n10\\n\\u00965\\n91\\n7\\n60\\n0\\n8\\nPr\\nin\\ntin\\ng\\nth\\nre\\ne-\\ndi\\nm\\nen\\nsi\\non\\nal\\ntis\\nsu\\ne\\nan\\nal\\nog\\nue\\ns\\nw\\nith\\nde\\nce\\nllu\\nla\\nriz\\ned\\nex\\ntr\\nac\\nel\\nlu\\nla\\nr\\nm\\nat\\nrix\\nbi\\noi\\nnk\\n[4\\n8]\\n.\\nPa\\nti,\\nF.\\n,J\\nan\\ng,\\nJ.,\\nH\\na,\\nD\\n.-H\\n.,\\nKi\\nm\\n,D\\n.-H\\n.,\\nC\\nho\\n,D\\n.-W\\n.\\n20\\n14\\nN\\nat\\nur\\ne\\nCo\\nm\\nm\\nun\\nic\\nat\\nio\\nns\\n53\\n,9\\n35\\n41\\n2\\nO\\nrg\\nan\\npr\\nin\\ntin\\ng:\\nTi\\nss\\nue\\nsp\\nhe\\nro\\nid\\ns\\nas\\nbu\\nild\\nin\\ng\\nbl\\noc\\nks\\n[4\\n9]\\n.\\nM\\niro\\nno\\nv\\nV.\\n,V\\nis\\nco\\nnt\\niR\\n.P\\n.,\\nKa\\nsy\\nan\\nov\\nV.\\n,F\\nor\\nga\\ncs\\nG\\n.,\\nD\\nra\\nke\\nC\\n.J.\\n,M\\nar\\nkw\\nal\\nd\\nR.\\nR.\\n20\\n09\\nBi\\nom\\nat\\ner\\nia\\nls\\n30\\n(1\\n2)\\n,p\\np.\\n21\\n64\\n\\u00962\\n17\\n4.\\n59\\n4\\n9\\nTi\\nss\\nue\\nen\\ngi\\nne\\ner\\nin\\ng\\nby\\nse\\nlf-\\nas\\nse\\nm\\nbl\\ny\\nan\\nd\\nbi\\no-\\npr\\nin\\ntin\\ng\\nof\\nliv\\nin\\ng\\nce\\nlls\\n[5\\n0]\\n.\\nJa\\nka\\nb,\\nK.\\n,N\\nor\\not\\nte\\n,C\\n.,\\nM\\nar\\nga\\n,F\\n.,\\nVu\\nnj\\nak\\n-\\nN\\nov\\nak\\nov\\nic\\n,G\\n.,\\nFo\\nrg\\nac\\ns,\\nG\\n.\\n20\\n10\\nBi\\nof\\nab\\nric\\nat\\nio\\nn\\n2\\n(2\\n),0\\n22\\n00\\n1\\n29\\n0\\n3D\\nbi\\nop\\nrin\\ntin\\ng\\nof\\nva\\nsc\\nul\\nar\\niz\\ned\\n,\\nhe\\nte\\nro\\nge\\nne\\nou\\ns\\nce\\nll-\\nla\\nde\\nn\\ntis\\nsu\\ne\\nco\\nns\\ntr\\nuc\\nts\\n[2\\n4]\\n.\\nKo\\nle\\nsk\\ny\\nD\\n.B\\n.,\\nTr\\nub\\ny\\nR.\\nL.\\n,\\nG\\nla\\ndm\\nan\\nA\\n.S\\n.,\\nBu\\nsb\\nee\\nT.\\nA\\n.,\\nH\\nom\\nan\\nK.\\nA\\n.,\\nLe\\nw\\nis\\nJ.A\\n.\\n20\\n14\\nAd\\nva\\nnc\\ned\\nM\\nat\\ner\\nia\\nls\\n26\\n(1\\n9)\\n,p\\np.\\n31\\n24\\n\\u00963\\n13\\n0\\n58\\n8\\n10\\n3D\\nBi\\nop\\nrin\\ntin\\ng\\nof\\nhe\\nte\\nro\\nge\\nne\\nou\\ns\\nao\\nrt\\nic\\nva\\nlv\\ne\\nco\\nnd\\nui\\nts\\nw\\nith\\nal\\ngi\\nna\\nte\\n/g\\nel\\nat\\nin\\nhy\\ndr\\nog\\nel\\n[5\\n1]\\n.\\nD\\nua\\nn,\\nB.\\n,H\\noc\\nka\\nda\\ny,\\nL.\\nA\\n.,\\nKa\\nng\\n,K\\n.H\\n.,\\nBu\\ntc\\nhe\\nr,\\nJ.T\\n.\\n20\\n13\\nJo\\nur\\nna\\nlo\\nfB\\nio\\nm\\ned\\nic\\nal\\nM\\nat\\ner\\nia\\nls\\nRe\\nse\\nar\\nch\\n-\\nPa\\nrt\\nA\\n10\\n1\\nA\\n(5\\n),\\npp\\n.\\n12\\n55\\n\\u00961\\n26\\n4\\n24\\n4\\nBi\\nnd\\nin\\ng\\nan\\nd\\nco\\nnd\\nen\\nsa\\ntio\\nn\\nof\\npl\\nas\\nm\\nid\\nD\\nN\\nA\\non\\nto\\nfu\\nnc\\ntio\\nna\\nliz\\ned\\nca\\nrb\\non\\nna\\nno\\ntu\\nbe\\ns:\\nTo\\nw\\nar\\nd\\nth\\ne\\nco\\nns\\ntr\\nuc\\ntio\\nn\\nof\\nna\\nno\\ntu\\nbe\\n-b\\nas\\ned\\nge\\nne\\nde\\nliv\\ner\\ny\\nve\\nct\\nor\\ns\\n[5\\n2]\\n.\\nSi\\nng\\nh\\nR.\\n,P\\nan\\nta\\nro\\ntt\\no\\nD\\n.,\\nM\\ncC\\nar\\nth\\ny\\nD\\n.,\\nC\\nha\\nlo\\nin\\nO\\n.,\\nH\\noe\\nbe\\nke\\nJ.,\\nPa\\nrt\\nid\\nos\\nC\\n.D\\n.,\\nBr\\nia\\nnd\\nJ.-\\nP.\\n,P\\nra\\nto\\nM\\n.,\\nBi\\nan\\nco\\nA\\n.,\\nKo\\nst\\nar\\nel\\nos\\nK.\\n20\\n05\\nJo\\nur\\nna\\nlo\\nft\\nhe\\nAm\\ner\\nic\\nan\\nCh\\nem\\nic\\nal\\nSo\\nci\\net\\ny\\n12\\n7\\n(1\\n2)\\n,p\\np.\\n43\\n88\\n\\u00964\\n39\\n6.\\n57\\n4\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 4 of 13\\neach analysis. A threshold of five documents cited at\\nleast ten times was set for authors, while for the institu-\\ntions we selected those with five documents cited at least\\nfive times. These inclusion parameters were based on\\nthe median number of cites for the whole set of docu-\\nments, which was 10.33, and the median number of pub-\\nlications per author was 5.76. For institutions, the mean\\nnumber of publications was 10 with the same median of\\ncites for the documents, 10.33; however, only three affili-\\nations were within the threshold, hence the median for\\ncitations and documents was reduced to 5 to include\\nmore affiliations.\\nTo identify the most prolific authors, the top ten au-\\nthors with more frequency within the threshold defined\\nwere selected and a Pearson correlation was computed to\\ndetermine the relationship existing between the number\\nof publications and the number of co-authors. The au-\\nthors were clustered by the similarity of areas of research\\nin the network maps and those with higher networking\\nwere identified by BC calculation. The number of times a\\nnode is taken as a connection for the shortest paths\\nbetween two other nodes can be estimated through BC,\\nwhich measures the node\\u0092s connection to different groups\\non a network map, being of a higher value the one who\\nconnects more groups [53]. The BC is obtained using the\\nequation [53]:\\nCB vð Þ ¼\\nX\\nv?s?t\\n?st vð Þ\\n?st\\nWhere ?st is the total of shortest paths from node s to\\nnode t, and ?st (v) is the number of those paths that go\\nthrough v.\\nThe information within the threshold was imported\\ninto VOSviewer, a software for data analysis and visual-\\nisation [54, 55], to perform the network map analysis.\\nThe authors or institutions are represented by nodes or\\nvertex in the network maps, and their connections are\\nrepresented by links or edges; in this document, the\\nterms are used indistinctly to refer to authors or affilia-\\ntions and their connections. Two undirected network\\nmaps were constructed from two matrices, representing\\nonly the correlation and not causality. A matrix of au-\\nthors and a matrix of affiliations were generated using\\nthe visualisation of singularities (VOS) of the VOSviewer\\nsoftware [55]. The clustering was performed in VOS-\\nviewer, computed using the default Field Independent\\nClustering Model (FICM) [55]. The statistical analysis to\\ndetermine the BC of the nodes forming both maps was\\nperformed in Gephi. The final step of the analysis was\\nthe consultation with experts in 3D bioprinting to valid-\\nate the results. Experts from UK and Asia were selected\\nbased on their international presence and impact in the\\nfield considering elements such as their number of cites,\\npublications, projects, and their availability. Instead of\\nproviding the experts with a list of authors found on the\\nresults of this research, we asked them to provide a list\\nof authors working on bioprinting according to their\\nown criteria. This method was used to reduce bias in\\ntheir selection, as they provided a list acknowledging\\ntheir peers based on their own experience. Is it worth\\nmentioning that the experts requested anonymity, there-\\nfore, we can only provide professional details of three of\\nthem at the time they were consulted. One of the ex-\\nperts was affiliated to the School of materials at the Uni-\\nversity of Manchester and had more than 10,000 Scopus\\ncitations. A second expert was affiliated to the Faculty of\\nEngineering at the University of Nottingham and had\\nmore than 760 citations. A third one was affiliated to the\\nSingapore Centre for 3D printing at Nanyang Technol-\\nogy University with more than 14,000 citations.\\nResults\\nFrom the initial search, where the ten most cited articles in\\nbioprinting from Scopus were considered, the top-cited\\narticle is 3D bioprinting of tissue and organs [37]. This is a\\nreview of different techniques used in bioprinting cited\\n1498 times, as seen in Table 1; the second most cited art-\\nicle is Scaffold-free vascular tissue engineering using bio-\\nprinting [38]. This article describes a fully biological\\nmethod to fabricate tubular vascular grafts and has been\\ncited less than 50% of the first author, 600 times; the third\\npaper, entitled 3D bioprinting of vascularized heterogeneous\\ncell-laden tissue constructs [24] was cited 446 times and\\ndescribes methods to generate vascularized tissue con-\\nstructs. The second and third papers are focused on one of\\nthe biggest challenges faced to print fully functional organs,\\nthe fabrication of scaffold-free blood vessels with mechan-\\nical properties close to the naturally grown vessels. Five of\\nthe ten articles were published in journals related to mate-\\nrials, four of them in general science journals (Nature Bio-\\ntechnology, Nature Communications, and Science), and\\none in the journal of Biofabrication, as can be observed in\\nTable 1. The results of the searches in Scopus using only\\nthe term bioprinting and those obtained using the search\\nquery developed are listed and compared in Table 1. It can\\nbe observed that the paper entitled 3D bioprinting of tissue\\nand organs still in first place in both results. The second\\npaper listed in the results from the search string is Micro-\\nscale technologies for tissue engineering biology by Khadem-\\nhosseini et al. [39] with 77% of the cites of the most first\\npublication, 1163, followed by the paper Clinical trans-\\nplantation of a tissue-engineered airway by Macchiarini\\net al. [40], published in the Lancet. Interestingly, the first\\nthree papers are published in three major journals covering\\nbiology and medical-related science, and six out of the 10\\npapers on this search were published in journals related to\\nmaterials and one in chemical engineering.\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 5 of 13\\nUsing the previously defined search query a total of\\n2088 publications were found from 2007 to November\\n15 of 2017 (when information collection activity ended).\\nFigure 1 shows the number of publications per year,\\nthere is a remarkable growth, where the highest number\\nof publications is 339 for 2017.\\nAfter the data selection and cleaning, a total of 228\\nauthors were found within the threshold of at least 5 docu-\\nments with 10 or more citations. 89 of the authors were\\nfound with repeated surnames. A Cohen\\u0092s kappa (?) of\\n0.62 was obtained for the agreement on the author name\\ndisambiguation. Values from 0.61 to 0.8 are ranked as\\nGood [36]. A collaboration was observed in 93% of the au-\\nthors, being 792 the total number of connections in the\\nmap. Regarding affiliations, a total of 20 organizations fall\\ninto the inclusion threshold, from which only 30% had an\\nexternal collaboration.\\nFrom the analysis, only ten authors were found to have\\nmore than 18 documents, as seen in Fig. 2, where the\\nnumber of documents and the number of co-authors for\\neach of them are shown. The author with more documents\\nfalling in the threshold defined is Atala A. with 36 docu-\\nments and 13 co-authors. The following author, Khadem-\\nhosseini A., had a total of 30 documents and more than\\ndouble of collaborations for the first author, 27 co-authors,\\nbeing the one with more connections. Mironov V. was in\\nthird place with 30 documents, and 20 co-authors. A Pear-\\nson correlation analysis was performed to determine the\\nrelationship between documents and co-authors, and a\\nweak positive correlation was observed, as the Pearson cor-\\nrelation coefficient had a value r = 0.29 for the top ten au-\\nthors, stating a lack of relationship between the number of\\nco-authors and the number of publications.\\nFigure 3 shows the network map of the author\\u0092s col-\\nlaboration, where the nodes\\u0092 size is proportional to their\\nBC value. The nodes representing the authors were\\ngrouped in a total of 17 clusters. From the BC calcula-\\ntion, the most prolific author, Atala A., was at the Wake\\nForest Institute for Regenerative Medicine from the\\nWake Forest University School of Medicine, Winston\\nSalem, United States when the information was gathered\\n(15 November 2017). According to Scopus altmetrics,\\nthis author had an h-index of 89, 850 documents pub-\\nlished, and a total of 17,376 citations working with 150\\nco-authors at the time of the analysis (see Table 2). On\\nthe other hand, under the inclusion terms, this author\\npublished a total of 36 documents on the topic analysed,\\nhaving 13 connections, 2851 citations, and a between-\\nness centrality value of 370.9.\\nThe second most prolific author found is Khademhos-\\nseini Ali L.I., affiliated with the Brigham and Women\\u0092s\\nHospital, Department of Medicine, Boston, United States,\\nwhen the data was collected. This author had an h-index\\nof 88, a total of 645 papers, with a total of 16,704 citations\\nand 150 co-authors, as stated in the Scopus altmetrics.\\nConsidering the inclusion terms, this author accounted\\nfor 30 documents, 27 connections, 3047 citations, and a\\nbetweenness centrality of 2104.9 (see Table 2).\\nFig. 1 Publications per year in bioprinting\\nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 6 of 13\\nThe third author was Mironov V., from the Laboratory\\nfor Biotechnological Research \\u00913D bioprinting solutions\\u0092,\\nMoscow, Russian federation. The Scopus altmetrics showed\\nthat this author had 105 papers, 3231 cites, and an h-index\\nof 31, co-authoring with 150 people. In this analysis, the au-\\nthor accounted for 30 documents, 20 links, 1009 citations,\\nand a betweenness centrality of 2754.9 (see Table 2).\\nAccording to the network map and the BC calculations,\\nMironov V. was stated as the author with a higher influ-\\nence in the knowledge flow of the collaboration network, as\\nit had the higher BC, followed by Khademhosseini A. While\\nMironov was affiliated to a biotechnological research\\nlaboratory, Atala and Khademhosseini were associated to\\ntwo of the top ten research departments in bioprinting\\nfound on this analysis.\\nThe authors ranked by the experts were compared\\nwith the most influential authors disclosed in this study,\\nas it can be seen from Table 3.\\nThree of the ten top authors in this scientometric study\\nwere considered as influential by the experts consulted,\\nAtala A., Mironov V., and Wei Sun; who were listed among\\nthe top five authors in both cases. The top three authors\\nfrom this study, who are listed in Table 3, are also the main\\ninfluential authors with a higher BC (see Table 2).\\nInstitutions\\u0092 research efforts can be better estimated by\\nthe number and the quality of their publications, therefore\\nthe affiliations wit'</span>text4\n:   <span style=white-space:pre-wrap>'Keet Journal of Biomedical Semantics            (2020) 11:4 \\nhttps://doi.org/10.1186/s13326-020-00224-y\\nDATABASE Open Access\\nThe African wildlife ontology tutorial\\nontologies\\nC. Maria Keet\\nAbstract\\nBackground: Most tutorial ontologies focus on illustrating one aspect of ontology development, notably language\\nfeatures and automated reasoners, but ignore ontology development factors, such as emergent modelling guidelines\\nand ontological principles. Yet, novices replicate examples from the exercise they carry out. Not providing good\\nexamples holistically causes the propagation of sub-optimal ontology development, which may negatively affect the\\nquality of a real domain ontology.\\nResults: We identified 22 requirements that a good tutorial ontology should satisfy regarding subject domain, logics\\nand reasoning, and engineering aspects. We developed a set of ontologies about African Wildlife to serve as tutorial\\nontologies. A majority of the requirements have been met with the set of African Wildlife Ontology tutorial ontologies,\\nwhich are introduced in this paper. The African Wildlife Ontology is mature and has been used yearly in an ontology\\nengineering course or tutorial since 2010 and is included in a recent ontology engineering textbook with relevant\\nexamples and exercises.\\nConclusion: The African Wildlife Ontology provides a wide range of options concerning examples and exercises for\\nontology engineering well beyond illustrating just language features and automated reasoning. It assists in\\ndemonstrating tasks concerning ontology quality, such as alignment to a foundational ontology and satisfying\\ncompetency questions, versioning, and multilingual ontologies.\\nKeywords: Ontology engineering, Tutorial ontology, African wildlife\\nBackground\\nThe amount of educational material to learn about ontolo-\\ngies is increasing gradually, and there is material for dif-\\nferent target audiences, including domain experts, applied\\nphilosophers, computer scientists and software develop-\\ners, and practitioners. These materials may include a tuto-\\nrial ontology to illustrate concepts and principles and may\\nbe used for exercises. There are no guidelines as to what\\nsuch a tutorial ontology should be about and should look\\nlike. The two most popular tutorial ontologies are about\\nwine and pizza, which are not ideal introductory subject\\ndomains on closer inspection (discussed below), they are\\nlimited to OWLDL only, and are over 15 years old by now,\\nCorrespondence: mkeet@cs.uct.ac.za\\nDepartment of Computer Science, University of Cape Town, 18 University\\nAvenue, Rondebosch, Cape Town, South Africa\\nhence, neither taking into consideration the more recent\\ninsights in ontology engineering nor the OWL 2 standard\\nwith its additional features.\\nConsidering subject domains in the most closely related\\narea, conceptual modelling for relational databases, there\\nis a small set of universes of discourse that are used in\\nteaching throughout the plethora of teaching materials\\navailable: the video/DVD/book rentals, employees at a\\ncompany, a university, and, to a lesser extent, flights and\\nairplanes. Neither of these topics for databases lend them-\\nselves well for ontologies, for the simple reason that the\\ntwo have different purposes. It does raise the question as\\nto what would be suitable and, more fundamentally, what\\nit is that makes some subject domain suitable but not\\nanother, and, underlying that, what the requirements are\\nfor an ontology to be a good tutorial ontology.\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 2 of 11\\nTable 1 Summary of main extant tutorial ontologies\\nOntology Year Stated aim Content Language Modelling issues Automated\\nreasoning\\nCurrent OE (e.g.,\\nODP, FO)\\nwine 2001 novice \\u0091all aspects\\u0092\\n(methodology,\\nmodelling, reasoning)\\nfor OE\\nsomewhat generic,\\nrepetitive, limited\\nextensibility\\nframes; the\\nwine.owl in\\nOWL DL is\\nbased on it\\nmultiple (e.g., class vs\\ninstance, hasX)\\nyes no\\npizza 2004 Protégé user guide,\\nalso illustrate OWL\\nand reasoning\\nsomewhat generic,\\nrepetitive, limited\\nextensibility\\nOWL DL multiple (e.g., hasX,\\nFO commitment, lack\\nof domain &amp; range)\\nyes no\\nuniversity 2005 illustrate OWL and\\nreasoning\\ngeneric, CDM (cf.\\nontology) scope,\\nvery small\\n&lt;OWL DL\\n(ALCIN)\\nmultiple (e.g., XorY,\\nnaming of\\nindividuals)\\nyes no\\nzooAnimals 2011 illustrate DL&amp;OWL\\nand some GoodOD\\nmodelling guidelines\\ngeneric, a lot of\\ndetail, easily\\nextensible\\n&lt;OWL 2 DL\\n(SHO)\\nfew yes partially\\n(BioTopLite)\\nfamily\\nhistory\\n2013 illustrate OWL 2 DL\\nand reasoning\\nspecific to author\\u0092s\\nfamily, not extensible\\nOWL 2 DL multiple (e.g., hasX,\\nFO commitment, lack\\nof domain &amp; range)\\nerror no\\nshirt 2015 illustrate the design of\\nthe FMA\\ngeneric, structure\\nspecific to FMA,\\nrepetitive, not\\nextensible\\n&lt;OWL 2 DL\\n(ALCIQ)\\nfew (lack of domain\\n&amp; range)\\nnone very limited\\n(reference\\nontology)\\nAbbreviations: OE: ontology engineering; ODP: ontology design pattern; FO: foundational ontology; CDM: conceptual data model; FMA: foundational model of anatomy;\\nOWL DL is SHOIN(D) and OWL 2 DL is SROIQ(D) in DL notation\\nIn this paper, we will first analyse existing tutorial\\nontologies and highlight some issues. We then proceed\\nto formulate a preliminary, first, list of requirements that\\ntutorial ontologies should meet. The African Wildlife\\nOntology (AWO) tutorial ontologies are then introduced\\nbriefly and held against the requirements. The scope of\\nthis paper is thus to introduce the AWO tutorial ontolo-\\ngies and to frame it in that context. Finally, we discuss and\\nconclude.\\nTutorial ontologies: issues and comparison\\nThere are several tutorial ontologies, which are sum-\\nmarised in Table 1 and discussed in this subsection; the\\nnext subsection that summarises the problems.\\nOf the six tutorial ontologies considered in detail, two\\nare popular, being the Wine Ontology and the Pizza\\nontology, since they are part of the W3C OWL guide\\nand designed for the most popular ontology development\\nenvironment (Protégé), respectively. They have various\\nshortcomings as tutorial ontologies, however, especially\\nconcerning modelling practices or styles (see also [1]).\\nThe Wine ontology in its current form emanates from\\nthe \\u0093Ontology development 101\\u0094 tutorial [2] with its\\nframes and slots that was subsequently transferred into\\nOWL1 and used in the \\u0093OWL guide\\u0094 [3], which is a W3C\\nRecommendation. While the guide contains some good\\nsuggestions, such as that \\u0093Synonyms for the same con-\\ncept do not represent different classes\\u0094 [2], there are also\\n1http://www.w3.org/TR/2003/PR-owl-guide-20031209/wine\\nmodelling issues, notably that the ontology is replete with\\nthe class-as-instance error that is promoted by the incor-\\nrect statement in the tutorial \\u0093Individual instances are the\\nmost specific concepts represented in a knowledge base.\\u0094\\n[2] (e.g., TaylorPort as instance of Port and MalbecGrape\\nas instance of Grape instead of as subclass of ), and the\\nsub-optimal object property naming scheme of \\u0091hasX\\u0092 ,\\nsuch as adjacentRegion between two Regions rather than\\nthe reusable and generic adjacent. Further, it uses differ-\\nent desiderata in the direct subclassing of wine such as the\\nlikes of Bordeaux and Loire (region-based) and Chardon-\\nnay and Cabernet Sauvignon (grape-based), and then\\nthere are other criteria, like DessertWine (food pairing-\\nbased grouping) and \\u0091wine descriptor\\u0092 ones (DryWine,\\nRedWine, TableWine), This does make it interesting for\\nshowing classification reasoning (except the undesirable\\ndeduction that DryWine ? TableWine), but is not ideal\\nfrom amodelling viewpoint. Further, from a tutorial view-\\npoint: there are many repetitions, such as very many\\nwineries, which distract from the principles, and it lacks\\nannotations.\\nThe Pizza ontology tutorial was created for the Pro-\\ntégé user manual and OWL DL ontology language [4].\\nIt reflects the state of the art at that time, yet much has\\nhappened over the past 15 years. For instance, there are\\nnew OWL 2 features and there are foundational ontolo-\\ngies that provide guidance for representing attributes (cf.\\nPizza\\u0092s ValuePartition). Pizza\\u0092s DomainConcept throws a\\nlearner straight into philosophical debates, which may not\\nbe useful to start with, and, for all practical purposes,\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 3 of 11\\nduplicates owl:Thing. Like the Wine ontology, it has\\nthe \\u0091hasX\\u0092 naming scheme for object properties, such as\\nhasTopping, including the name of the class it is sup-\\nposed to relate to, which is a quirk that is a combination\\nof a workaround for not having qualified number restric-\\ntions (anOWL 1 artefact) and of a sub-optimal ontological\\nanalysis of the relation (in casu, of how the toppings really\\nrelate to the rest of the pizza) that reduces chance of\\nontology reuse and alignment. Also, this propagates into\\nstudents\\u0092 modelling approaches: students\\u0092 ontologies in\\nearlier instances of the author\\u0092s course on ontology engi-\\nneering included, among others, a sandwich ontology with\\nhasFilling, an electrical circuit board ontology with hasIso-\\nlator, furniture with hasHeadboard. Modelling issues\\nare compounded by the statement \\u0093we generally advise\\nagainst doing [domain and range declarations]\\u0094 in the\\ntutorial documentation. When one aims to get novices\\nto use Protégé and OWL so as not get too many error\\nwith the automated reasoners, that might make sense,\\nbut ontologically, fewer constraints make an ontology less\\ngood because it admits more unintended models. Finally,\\nit has repetitive content to show features, which may be\\ndistracting, and, as with Wine, there is only one \\u0091final\\u0092\\nontology, despite that multiple releases are common in\\npractice.\\nOther tutorial ontologies include Family History,\\nzooAnimals, University, and Shirt. Family History [5] is\\ndeveloped by the same group as Pizza and aims to teach\\nabout advanced OWL 2 features and maximise the use\\nof inferencing. Loading it in Protégé 5.2 results in three\\npunning errors, since it mixes three object properties with\\nannotation properties (affecting 32 axioms), which is dis-\\nallowed, and trying to classify it without the three anno-\\ntation properties returned an OutOfMemoryError (on a\\nMacBookPro, 2.6 GHz and 8GB of memory), which is not\\nideal to start a tutorial with. Concerning modelling issues,\\nParentOfRobert illustrates one can use individuals in class\\nexpressions, but just that the language allows it, does not\\nmean it is ontologically a good idea that must be taught.\\nIt also has the \\u0091hasX\\u0092 semantic weakness, very few anno-\\ntations, DomainEntity being subsumed by owl:Thing,\\nand multiple data properties. In contrast to Pizza and\\nWine, all the declared instances are instances and the\\nontology has different versions as one goes along in the\\nchapters. It has some subject domain aspects descending\\ninto politics, which would render it unsuitable for teach-\\ning in several countries, such as stating that Sex? Female\\nunionsq Male (enforcing a gender binary) and that Person \\004\\n? 2 hasParent.Person (multiple constructions are possible\\nbiologically, societally, and legally).\\nThe remaining tutorial ontologies have been developed\\nby different \\u0091schools\\u0092 of views on ontology engineering\\n(OE), which is readily apparent in their scope and con-\\ntent. The zooAnimals tutorial ontology [6] comes closest\\nto our aims for a versatile tutorial ontology, demonstrat-\\ning multiple OWL features, avoiding modelling issues\\nsuch as class vs instance, and it is informed by a top-\\ndomain ontology (BioTop) as well as deep philosophical\\nnotions such as dispositions. It puts them all together\\ninto one ontology instead of gradual extensions, how-\\never, which is off-putting at a novice stage. One may\\nquibble about some of the content, such as simplifica-\\ntions that Plant ? ?hasProperPart.Chloroplast (notably,\\nsome parasitic plants and all myco-heterotrophic plants\\ndo not have chloroplasts) and there are unintended unde-\\nsirable deductions\\u0097i.e., logically implied, but incorrect\\nontologically\\u0097such as marineAnimal \\004 Omnivore since\\nnot all such animals are omnivores. Any simplified \\u0091com-\\nmon generic subject domain\\u0092 is likely to have some short-\\ncuts that are not 100% scientifically accurate, and it may be\\na fine line between tutorial approximation and modelling\\nmistake.\\nThe University ontology focuses on illustrating OWL\\nfeatures and automated reasoning, rather than modelling.\\nFor instance, it has AcademicStaff with sibling NonAca-\\ndemicStaff where a \\u0093non-X\\u0094 complement class is sub-\\noptimal, especially when there is a term for it. The repre-\\nsentation of Student \\004 Person is an advanced modelling\\naspect that can be improved upon with a separate branch\\nfor roles played by an object. The Computer Science\\nOntology was based on the University Ontology tuto-\\nrial and contains artificial classes, like unions of classes\\n(ProfessorinHCIorAI) and underspecified or incorrect indi-\\nviduals like AI and HCI (e.g., some course instance would\\nbe CS_AI-sem1-2018 instead).\\nThe Shirt ontology is a tutorial ontology to explain the\\nstructure and organisation of the Foundational Model of\\nAnatomy in a simpler way2 and therefore does not have\\nthe hasX naming scheme for object properties, it has no\\ndata properties and no instances. It has many annotations\\nwith explanations of the entities. There are no inferences.\\nRegarding suitability of the subject domains of the\\nontologies assessed, they are mixed. Wine misses many\\nwine-producing regions in the Americas (e.g., Chile), in\\nEurope (e.g. Spain, Bulgaria), and elsewhere (e.g., South\\nAfrica) and Pizza lacks varieties beyond Italian and Amer-\\nican ones, and both are served regularly in a relatively\\nsmall part of the world, therewith reducing their appeal\\ninternationally. Family history and a university as subject\\ndomains veer too easily into the area of database design for\\na single application, rather than application-independent\\ngeneric knowledge for an ontology. Shirts and zoo animals\\ndo not have these shortcomings.\\nFinally, more or less related textbooks were consid-\\nered [7\\u009611]. Only the \\u0093Semantic Web for the working\\n2http://xiphoid.biostr.washington.edu/fma/shirt_ontology/shirt_ontology_1.\\nphp\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 4 of 11\\nOntologist\\u0094 (2nd ed.) has sample files for the book\\u0092s many\\nsmall examples3 with two reoccurring subject domains,\\nbeing English literature and products.\\nProblems to address\\nThe previous section described several problems with\\nexisting tutorial ontologies. Notably, the recurring short-\\ncomings are that\\ni) good modelling practices are mostly ignored in\\nfavour of demonstrating language features,\\nautomated reasoning, and tools\\nii) when good modelling practices and at least some\\nrecent ontology engineering advances are included, it\\nfalls short on language features and gradual\\nextensions.\\nThis has a negative effect on learning about ontology\\ndevelopment, for tutorial ontology practices are nonethe-\\nless seen by students as so-called \\u0091model answers\\u0092 even if\\nit were not intended to have that function.\\nThe ontology survey does not reveal what may be the\\ncharacteristics of a good tutorial ontology and, to the best\\nof our knowledge, there is no such list of comprehen-\\nsive criteria for tutorial ontologies specifically. Schober\\net al. [6] propose a partial list with seven high-level\\ncontent requirements indeed, such as a common sense\\nknowledge subject domain that lends itself well to demon-\\nstrate the \\u0093classic\\u0094 modelling challenges, but it omits the\\nessential components of logic, reasoning, and engineering\\nrequirements. Scoping it more broadly, one could con-\\nsider modelling guidelines and automated checkers for\\nproduction level ontologies, such as [12\\u009615]. They can\\ninform the development of tutorial ontologies, in partic-\\nular to avoid such issues as the class vs. instance error in\\nthe provided sample ontology, but that is different from\\neducating students about the foundations and reasons for\\nsuch guidelines starting from a basic level of modelling\\nto more advanced issues. For instance, disjointness and\\ncovering constraints among subclasses of a parent class is\\nindeed desirable together with coherent criteria to declare\\na taxonomy [15], but that does not let students observe or\\nexperience mistakes, i.e., learn what is suboptimal or does\\nnot work and why. A tutorial ontology also would have\\nto be able to accommodate common pitfalls and grad-\\nual quality improvements, among other things, which are\\nnot covered by the general guidelines. Also, general guide-\\nlines tend to follow one commitment over another\\u0097e.g.,\\nthe GoodOD guidelines favour a realist approach with\\nthe BFO foundational ontology\\u0097but for teaching OE in\\ngeneral, students need learn to be cognisant of multiple\\npossible commitments, their consequences when choos-\\ning one or the other, and have at least one practical\\n3http://www.workingontologist.org/Examples.zip; Last accessed: 26-11-2018.\\nexample of such a difference to illustrate it, which general\\nguidelines do not provide.\\nPotential benefits of the African wildlife ontology tutorial\\nontologies\\nIn order to address these problems, we introduce the\\nAfrican Wildlife Ontology (AWO). The AWO has been\\ndeveloped and extended over 8 years. It meets a range of\\ndifferent tutorial ontology requirements, notably regard-\\ning subject domain, use of language features and auto-\\nmated reasoning, and its link with foundational ontologies\\non the one hand and engineering on the other. It aims to\\ntake a principled approach to tutorial ontology develop-\\nment, which thereby not only may assist a learner, but,\\nmoreover from a scientific viewpoint, it might serve as a\\nstarting point for tutorial ontology creation or improve-\\nment more broadly, and therewith in the future contribute\\nto an experimental analysis of tutorial ontology qual-\\nity. This could benefit educational material for ontology\\ndevelopment.\\nAlso, educationally, there is some benefit to \\u0091reusing\\u0092\\nthe same ontology to illustrate a range of aspects, rather\\nthan introducing many small ad hoc examples, for then\\nlater in a course, it makes it easier for the learners to see\\nthe advances they have made. This is also illustrated with\\noffering multiple versions of the ontology, which clearly\\nindicate different types of increments.\\nFinally, the AWO can be used on its own or together\\nwith the textbook \\u0093An Introduction to Ontology Engineer-\\ning\\u0094 [16], which contains examples, tasks and exercises\\nwith the AWO.\\nConstruction and content\\nThe construction of the AWO tutorial ontologies has gone\\nthrough an iterative development process since 2010. This\\ninvolved various extensions and improvements by design,\\nmainly to address the increasing amount of requirements\\nto meet, and maintenance issues, such as resolving link\\nrot of an imported ontology. Rather than describing the\\nprocess of the iterative development cycles, we present\\nhere a \\u0091digest\\u0092 version of it. First, a set of tutorial ontol-\\nogy requirements are presented together, then a brief\\noverview of the AWO content is described, and subse-\\nquently we turn to which of these requirements are met\\nby the AWO.\\nOE tutorial ontology requirements\\nTutorials on ontologies may have different foci and it\\nis unlikely that an ontology used for a specific tutorial\\nwill meet all requirements. The ontology should meet the\\nneeds for that tutorial or course, and that should be stated\\nclearly. As such, this list is intended to serve as a set of con-\\nsiderations when developing a tutorial ontology. Each item\\neasily can take up a paragraph of explanation. We refrain\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 5 of 11\\nfrom this by assuming the reader of this paper is suffi-\\nciently well-versed in ontology engineering and seeking\\ninformation on tutorial ontologies. For indicative purpose,\\nthe requirements are categorised under three dimensions:\\nthe subject domain of the ontology, logics &amp; reasoning,\\nand engineering factors.\\nSubject domain\\nThe tutorial ontology\\u0092s subject domain, also called uni-\\nverse of discourse, should be versatile to be able to cater\\nplausibly for a range of modelling aspects. We specify\\nseven requirements for it, as follows.\\n1. It should be general and commonsensical domain\\nknowledge, so as to be sufficiently intuitive for\\nnon-experts to be able to understand content and\\nadd knowledge. Optionally, it may be an enjoyable\\nsubject domain to make it look more interesting and,\\nperhaps, also uncontroversial4 to increase chance of\\nuse across different settings and cultures.\\n2. The content should be not wrong ontologically,\\nneither regarding how things are represented (e.g.,\\nno classes as instances) nor the subject domain\\nsemantics (e.g., whales are mammals, not fish).\\n3. It needs to be sufficiently international or\\ncross-cultural so that experimentation with a\\nscenario with multiple natural languages for\\nmultilingual ontologies is plausible.\\n4. Its contents should demonstrate diverse aspects\\nsuccinctly when illustrating a point (in contrast to\\nbeing repetitive in content).\\n5. It needs to be sufficiently versatile to illustrate the\\nmultiple aspects in ontology development (see\\nbelow), including the use of core relations (e.g.,\\nmereology).\\n6. It should permit extension to knowledge that\\nrequires features beyond Description Logics-based\\nOWL species, so as to demonstrate representation\\nlimitations and pointers to possible directions of\\nsolutions (e.g., temporal aspects, non-monotonicity,\\nfull first-order logic).\\n7. The subject domain should be able to possibly be\\nused in a range of use case scenarios (database\\nintegration, science, NLP, and so on).\\nLogics &amp; reasoning\\nSince a core feature of ontologies is their logic under-\\npinning, a tutorial ontology thus also will need to meet\\ncriteria for the representation language and automated\\nreasoning over it. They are as follows.\\n4Recent political issues include complaints with subject domains of exercises\\nthat perpetuate stereotypes and simplifications, such as, but not limited to, the\\ngender binary, who can marry whom, and gendered subject domains\\nperceived to reside at the edges of the spectrum.\\nI. The ontology should be represented in a logic that has\\ntool support for modelling and automated reasoning.\\nII. The ontology should be represented in a logic that\\nhas tool support for \\u0091debugging\\u0092 features that\\n\\u0091explain\\u0092 the deductions, meaning at least showing\\nthe subset of axioms involved in a deduction.\\nIII. It should permit simple classification examples and\\neasy examples for showing unsatisfiability and\\ninconsistency, such that it does not involve more\\nthan 2-3 axioms in the explanation, and also longer\\nones for an intermediate level.\\nIV. The standard reasoning tasks should terminate fairly\\nfast (&lt; 5 s) for most basic exercises with the\\nontology, with the \\u0091standard\\u0092 reasoning tasks being\\nsubsumption/classification, satisfiability, consistency,\\nquerying and instance retrieval.\\nV. The representation language should offer some way\\nof importing or linking ontologies into a network of\\nontologies.\\nVI. The language should be expressive enough to\\ndemonstrate advanced modelling features (e.g.,\\nirreflexivity and role composition).\\nVII. The logic should be intuitive for the modelling\\nexamples at least at the start; e.g., if there is a need for\\nternary relations, then the logic should permit\\nn-aries with n ? 3 so that it can be represented as\\nsuch, rather than as an approximation with a\\nreification and a workaround pattern.\\nEngineering and development tasks\\nAn ontology is an artefact, which has to be built and\\nmaintained. To this end, there are multiple approaches,\\nmethodologies, methods, and software tools of which at\\nleast a subset will have to become part of an ontologist\\u0092s\\ntoolbox. We identified eight broad requirements:\\nA. At least some ontology development methods and\\ntools should be able to use the ontology, be used for\\nimprovement of the ontology, etc.\\nB. The ontology needs to permit short/simple\\ncompetency questions (CQs) and may permit long\\nand complicated CQs, which are formulated for the\\nontology\\u0092s content and where some can be answered\\non the ontology and others cannot.\\nC. At least some of the top-level classes in the hierarchy\\nshould be straight-forward enough to be easily linked\\nto a leaf category from a foundational ontology (e.g.,\\nAnimal is clearly a physical object, but the ontological\\nstatus of Algorithm is not immediately obvious).\\nD. It should be relatable to, or usable with, or else at\\nleast amenable to the use of, ontology design\\npatterns, be they content patterns or other types.\\nE. It is beneficial if there is at least one ontology\\nsufficiently related to its contents, so that it can be\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 6 of 11\\nused for tasks such as comparison, alignment, and\\nontology imports.\\nF. It is beneficial if there are relevant related\\nnon-ontological resources that could be used for\\nbottom-up ontology development.\\nG. It should be able to show ontology quality\\nimprovements gradually, stored in different files.\\nH. It should not violate basic ontology design principles\\n(e.g., classes and relations vs. implementation\\ndecisions with data properties and data types when\\nrepresenting qualities of entities, such as an animal\\u0092s\\nweight).\\nWhile this list may turn out not to be exhaustive in the\\nnear future, it is expected to be sufficient for introduc-\\ntory levels of ontology development tutorials and courses.\\nEither way, this list of requirements are already hard\\nto meet in one single ontology. For instance, simplicity\\n(Items 3, III, and B) vs. complicated extensions and onto-\\nlogical precision (Items 6 and C) cannot be fully met\\nat once. On the flip side, some requirements are closely\\nrelated or overlap, such as design principles (Item H) and\\nnot being wrong ontologically (Item 2) since some of the\\nformer are informed by the latter.\\nContent of the AWO \\u0096 at a glance\\nThe principal content of the AWO is, in the first stage\\nat least, \\u0091intuitive\\u0092 knowledge about African wildlife.\\nThis subject domain originated from an early Semantic\\nWeb book ([8], Section 4.3.1) that was restructured and\\nextended slightly for its first, basic version; see Table 2\\nand Fig. 1. It has descriptions of typical wildlife animals,\\nsuch as Lion and Elephant, and what they eat, including\\nImpala (a type of antelope), and Twig or Leaf, respectively.\\nBasic extensions in the simple version of the ontology\\n(v1) include plant parts, so as to demonstrate parthood\\nand its transitivity, and carnivore vs. herbivore, which\\nmake it easy to illustrate disjointness, subsumption rea-\\nsoning, and unsatisfiable classes, and carnivorous plants\\nto demonstrate logical consequences of declaring domain\\nand range axioms 5. Most elements have been annotated\\nwith informal descriptions, and several annotations link to\\ndescriptions on Wikipedia.\\nLike the aforementioned Family History ontology, there\\nare several versions of the AWO that reflect different\\nstages of learning. In the case of the AWO, this is not\\nspecifically with respect to OWL language features, but\\none of notions of ontology quality and where one is in\\nthe learning process. For instance, version 1a contains\\nanswers to several competency questions\\u0097i.e., quality\\n5in casu, declaring eats too restrictively with as domain only Animal: then\\neither it will result in an unsatisfiable CarnivorousPlant (if Animal and Plant are\\ndeclared disjoint) or it will result in the undesirable deduction that\\nCarnivorousPlant \\004 Animal\\nrequirements that an ontology ought to meet [17]\\u0097that\\nwere formulated for Exercise 5.1 in the \\u0093Methods and\\nmethodologies\\u0094 chapter of [16]. Versions 2 and 3, on\\nthe other hand, have the AWO aligned to the DOLCE\\nand BFO foundational ontologies, respectively, whose dif-\\nferences and merits are discussed in Chapter 6 of the\\ntextbook, ensuring discussion of refinements in ontologi-\\ncal precision with, e.g., processes and dispositions (e.g., an\\nEating class with participating objects cf. an eats object\\nproperty). Their respective versions with the answers to\\nthe related exercises have the name appended with an \\u0091a\\u0092\\nas well. Version 4 has some contents \\u0091cleaned up\\u0092, par-\\ntially based on what the OOPS! tool [14] detected; it uses\\nmore advanced language features; and takes steps in the\\ndirection of adhering to science more precisely with finer\\ngranularity, such as type of carnivores and distinguishing\\nbetween types of roots.\\nThere are also four versions in different natural lan-\\nguages, being in isiZulu, Afrikaans, Dutch, and Spanish,\\nwhich mainly serve the purpose of illustrating issues with\\nmultilingual settings of ontology use, which relates to\\ncontent in Chapter 9 of the textbook.\\nAWO against the requirements\\nThe AWO meets most of the requirements (see Table 3).\\nConcerning the subject domain, the content is general,\\nversatile, not wrong, sufficiently international, and not\\nrepetitive (Items 1-4). The AWO includes the core rela-\\ntion of parthood for, especially, plants and their parts, with\\noptional straightforward extensions with the participation\\nrelation (e.g., animals participating in a Chasing event)\\nand membership (animal collectives, such as Herd; see v4\\nof the AWO), therewithmeeting Item 5. Representation of\\nrelevant domain knowledge beyond Description Logics-\\nbased OWL species (Item 6) could include information\\nabout temporal segregation of foraging or commensal-\\nism, inclusion of species with distinct successive phases\\nwith substantial morphological changes (e.g., Caterpil-\\nlar/Butterfly), and the notion of rigidity between what an\\nobject is and the role it plays (e.g., Lion playing the role of\\nPredator; see v4 of the AWO). The subject domain is also\\nfertile ground for exceptions that may be represented with\\nnon-monotonic logics; typical examples are that, gener-\\nally, birds fly and plants have chlorophyl, but not all of\\nthem (e.g., the penguin and the dodder, respectively). Use\\ncase scenarios (Item 7) may be, among others, science\\nof African wildlife, activism on endangered species, and\\napplications such as a database integration and manage-\\nment system for zoos and for tourism websites.\\nRegarding the logics and reasoning, the AWO is rep-\\nresented in OWL [19], and thus has ample tooling sup-\\nport for knowledge representation, reasoning, and basic\\ndebugging/explanation, with ontology development envi-\\nronment tools such as Protégé (Items I-III). The AWO\\nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 7 of 11\\nTable 2 AWO ontologies, with their main differences\\nFile name Difference\\nAfricanWildlifeOntology.xml This is the file from http://www.iro.umontreal.ca/~lapalme/ift6281/OWL/AfricanWildlifeOntology.xml,\\nthat was based on the description in [8]\\nAfricanWildlifeOntologyWeb.owl AfricanWildlifeOntology.xml + changed the extension to .owl and appended the name\\nwith Web. This ontology gave at the time (in 2010) a load error in the then current version of Protégé\\ndue to the use of Collection in the definition of Herbivore\\nAfricanWildlifeOntology0.owl AfricanWildlifeOntologyWeb.owl + that section on Collection removed\\nAfricanWildlifeOntology1.owl AfricanWildlifeOntology0.owl + several classes and object properties were added (up to\\nSRI DL expressiveness), more annotations, URI updated (described in Example 4.1 in [16])\\nAfricanWildlifeOntology1a.owl AfricanWildlifeOntology1.owl + new content for a selection of the CQs in Exercise 5.1 in\\n[16] (its CQ5, CQ8) and awo_12 of the CQ dataset [18])\\nAfricanWildlifeOntology2.owl AfricanWildlifeOntology1.owl + OWL-ised DOLCE (Dolce-Lite.owl) was imported\\nand aligned\\nAfricanWildlifeOntology2a.owl AfricanWildlifeOntology2.owl + answers to the questions in Example 6.2 in [16] on\\nfoundational ontology alignment\\nAfricanWildlifeOntology3.owl AfricanWildlife'</span>text5\n:   <span style=white-space:pre-wrap>'Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 \\nhttps://doi.org/10.1186/s13326-020-00221-1\\nRESEARCH Open Access\\nNeural side effect discovery from user\\ncredibility and experience-assessed online\\nhealth discussions\\nVan-Hoang Nguyen* , Kazunari Sugiyama, Min-Yen Kan and Kishaloy Halder\\nAbstract\\nBackground: Health 2.0 allows patients and caregivers to conveniently seek medical information and advice via\\ne-portals and online discussion forums, especially regarding potential drug side effects. Although online health\\ncommunities are helpful platforms for obtaining non-professional opinions, they pose risks in communicating\\nunreliable and insufficient information in terms of quality and quantity. Existing methods in extracting user-reported\\nadverse drug reactions (ADRs) in online health forums are not only insufficiently accurate as they disregard user\\ncredibility and drug experience, but are also expensive as they rely on supervised ground truth annotation of\\nindividual statement. We propose a NEural ArchiTecture for Drug side effect prediction (NEAT), which is optimized on\\nthe task of drug side effect discovery based on a complete discussion while being attentive to user credibility and\\nexperience, thus, addressing the mentioned shortcomings. We train our neural model in a self-supervised fashion\\nusing ground truth drug side effects from mayoclinic.org. NEAT learns to assign each user a score that is\\ndescriptive of their credibility and highlights the critical textual segments of their post.\\nResults: Experiments show that NEAT improves drug side effect discovery from online health discussion by 3.04%\\nfrom user-credibility agnostic baselines, and by 9.94% from non-neural baselines in term of F1. Additionally, the latent\\ncredibility scores learned by the model correlate well with trustworthiness signals, such as the number of \\u0093thanks\\u0094\\nreceived by other forum members, and improve credibility heuristics such as number of posts by 0.113 in term of\\nSpearman\\u0092s rank correlation coefficient. Experience-based self-supervised attention highlights critical phrases such as\\nmentioned side effects, and enhances fully supervised ADR extraction models based on sequence labelling by 5.502%\\nin terms of precision.\\nConclusions: NEAT considers both user credibility and experience in online health forums, making feasible a\\nself-supervised approach to side effect prediction for mentioned drugs. The derived user credibility and attention\\nmechanism are transferable and improve downstream ADR extraction models. Our approach enhances automatic\\ndrug side effect discovery and fosters research in several domains including pharmacovigilance and clinical studies.\\nKeywords: Online health communities, Drug side effect discovery, Credibility analysis, Deep learning, Natural\\nlanguage processing\\n*Correspondence: vhnguyen@u.nus.edu\\nSchool of Computing, National University of Singapore, 13 Computing Drive,\\n117417 Singapore, Singapore\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 2 of 16\\nBackground\\nSeeking medical opinions from online health commu-\\nnities has become popular: 71% of adults aged 18\\u009629\\n(equivalent to 59% of all U.S. adults) reported consulting\\nonline health websites for opinions [1]. These opinions\\ncome from an estimated twenty to one hundred thousand\\nhealth-related websites [2], inclusive of online health com-\\nmunities that network patients with each other to pro-\\nvide information and social support [3]. Platforms such\\nas HealthBoards1 and MedHelp2 feature users report-\\ning their own health experiences, inclusive of their self-\\nreviewed drugs and medical treatments. Hence, they are\\nvaluable sources for researchers [4, 5].\\nAlthough patients use these platforms to access valuable\\ninformation about drug reactions, there are challenges\\nto their effective, large-scale use. There is lexical varia-\\ntion where users describe the same side effect differently.\\nFor example, dizziness can be expressed as giddiness or\\nmy head is spinning, posing difficulty to most feature-\\nbased or keyword matching approaches. Separately, there\\nare valid concerns regarding credibility of user-generated\\ncontents to be harvested at large in which research has\\nshown to be of variable quality and should be approached\\nwith caution [6\\u00969]. One proxy indicator for information\\nquality is the author\\u0092s trustworthiness [10]. In the con-\\ntext of social media or online forums, user trustworthiness\\nis often approximated via ratings from other users, i.e.,\\nnumber of thanks or upvotes [11], or via their consistency\\nof reporting credible information [12, 13]. In addition to\\ncredibility, forum members also offer expertise thanks to\\ntheir own experience \\u0096 with prescriptions in particular \\u0096\\nand facilitate responses to drug queries [14]. For instance,\\nwhile reporting expected side effects for a specific treat-\\nment, patients with long-term use of certain drugs can be\\na complementary source of information:\\nWhile my experience of 10 years is with Paxil, I expect that Zoloft will be\\nthe same. You should definitely feel better within 2 weeks. One way I found to\\nmake it easier to sleep was to get lots of exercize [sic]. Walk or run or whatever\\nto burn off that anxiety. \\u0096 User 3690.\\nThe above is an answer to a thread asking for expected\\nside effects for depression treatment with Zoloft.\\nUser 3690\\u0092s history of active discussion on other anti-\\ndepressants such as Lexapro and Xanax lends credibil-\\nity to them being an authority on depression treatments.\\nWe noticed that Zoloft (mentioned in the thread)\\nshares many common side effects with the other two\\nanti-depressants: \\u0093changed behavior,\\u0094 \\u0093dry mouth,\\u0094 and\\n\\u0093sleepiness or unusual drowsiness.\\u0094 as illustrated in Table 1.\\nMany such examples suggest that drugs which are often\\nprescribed together for the same treatment, such as anti-\\ndepressants, are likely to be discussed within a same\\n1https://www.healthboards.com/\\n2https://medhelp.org/\\nTable 1 Side effects of anti-depressants\\nDrugs Side effects\\nLexapro chills, constipation, cough, decreased appetite, decreased\\nsexual desire, diarrhea, drymouth, joint pain, muscle\\nache, tingling feeling, sleepiness or unusual\\ndrowsiness, unusual dream, sweating, ...\\nXanax abdominal or stomach pain, muscle weakness , changed\\nbehavior, chills, cough, decreased appetite, decreased\\nurine, diarrhea, difficult bowel movement, cough, dry\\nmouth, tingling feeling, sleepiness or unusual\\ndrowsiness, slurred speech, sweating, yellow eye,..\\nZoloft changed behavior, decreased sexual desire, diarrhea,\\ndrymouth, heartburn, sleepiness or unusual\\ndrowsiness, sweating,..\\nThe Drugs and Side effects columns respectively list the anti-depressants and their\\nside effects extracted from a drug\\u0096side effect database. Side effects in common\\namong those listed are bold\\nthread and share common side effects. In addition, users\\nwho have experienced certain drug reactions are more\\noutspoken and active on those discussions involving drugs\\nof similar side effects. These signals arise from the rich\\ncontext of online health information; hence, we expect\\nsystems to explore beyond individual statements. Specif-\\nically, they should consider the complete discussion con-\\ntent as well as the global experience of each involved users,\\nin order to discover drug side effects or extract adverse\\ndrug reactions (ADRs).\\nWe argue that modeling user expertise from experi-\\nenced side effects is more robust compared against gen-\\neral user profile and engagement features [13, 14], as user\\nexpertise provides more meaningful signals for side effect\\ndiscovery. To the best of our knowledge, there is no pre-\\nvious work that incorporates user expertise in side effect\\ndiscovery in discussion forums at either the thread or\\npost level. In this work, given online health discussions,\\nwe propose a novel end-to-end neural architecture that\\njointly models each author\\u0092s credibility, their global expe-\\nrience and their post\\u0092s textual content to discover the\\nside effect of unseen drugs. We optimize the model on\\na self-supervised task of predicting side effect of men-\\ntioned drugs for complete threads, where ground truth\\nis accessible. Our key observation is that users can be\\ngrouped into clusters that share the same expertise or\\ninterest in certain drugs, possibly due to their common\\ntreatment or medical history. We incorporate this crit-\\nical observation into our user model in representing a\\npost\\u0092s content via a cluster-sensitive attention mechanism\\n[15]. We also follow general definition of truth discov-\\nery and let the model learn a credibility score that is\\nunique to every user and descriptive of their trustworthi-\\nness. Our experiments include an overall ablation study\\nto validate the significance of each model component.\\nThis paper extends our former work [16] by conducting\\na correlation study that analyzes the representativeness of\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 3 of 16\\nlearned credibility scores and a comparison between our\\nself-supervised attention-based approach and traditional\\nsupervised sequence labeling approaches on side effect\\nmention extraction.\\nWe summarize our contributions as follows:\\n\\u0095 We propose a NEural ArchiTecture, NEAT, that\\ncaptures 1) user expertise and 2) credibility, 3) the\\nsemantic content of individual posts and 4) the\\ncomplete discussion thread, to improve side effect\\ndiscovery from online health discussions. NEAT\\u0092s\\nmain means of user credibility and experience\\nassessment can be easily adopted by various neural\\nattentional encoders [17, 18].\\n\\u0095 We formulate a self-supervised task of side effect\\nprediction of mentioned drugs for the proposed\\nnetwork to jointly optimize its components.\\n\\u0095 We conduct experiments to verify the validity of our\\nlearned credibility and the robustness of\\nself-supervised attention-based extraction,\\ncomparing against traditional supervised sequence\\nlabeling baselines.\\nRelated work\\nWe first review existing approaches to drug side effect\\ndiscovery from health forums and social media. Next, we\\nexamine how these works incorporate user credibility and\\nexpertise in their learning objective. Finally, we justify\\nour choice of neural architecture by discussing its mod-\\neling capability of context-rich structures such as online\\ndiscussion.\\nDrug Side Effect Discovery. Existing methods for drug\\ndiscovery from online content extract drugs at post and\\nstatement level. ADR mining systems typically include a\\nnamed entity recognition (NER) model and a relationship\\nor semantic role labeling model [19, 20]. Recent neu-\\nral approaches address lexical variation in user-generated\\ncontent \\u0096 the difficulty faced by traditional keyword\\nmatching and rule-based approaches \\u0096 to improve recog-\\nnition and labeling components [21, 22]. Distributed word\\nrepresentations [23, 24] constructed from context can\\ncapture semantics based on the hypothesis that syn-\\nonyms often share similar contextual words. For example,\\n\\u0093headache\\u0094 and \\u0093cephalea\\u0094 will have close representations\\nif they share contextual words such as \\u0093head\\u0094 or \\u0093pain\\u0094.\\nApproaches to sub-word embedding [25, 26] model the\\nmorphology of words by leveraging sub-word or charac-\\nter information. These representations are naturally inte-\\ngrated into neural sequential models [17, 18, 27] that are\\nsensitive to syntactic order. However, supervised sequence\\nlabeling or mention extraction approaches require labo-\\nrious annotations at the word (token) level, and are\\nonly capable of discovering side effects that are explic-\\nitly present in the text. Expert supervision or additional\\nsemantic matching models are also required to map such\\nrecognized text segments to standardized vocabularies or\\nthesaurii [28]. In contrast, our proposed self-supervised\\ntask formulation discovers the aggregated side effects of\\nmentioned drugs for each community discussion by con-\\nsidering the whole thread\\u0092s content. The list of discussed\\ndrugs are tagged by forummoderators or obtained by pat-\\ntern matching. This learning design not only effectively\\nalleviates the need for expensive, finer-grained annota-\\ntions but also allows for the prediction of side effects not\\nexplicitly mentioned in the discussion.\\nUser Credibility and Expertise Integration. Credi-\\nbility is of the utmost concern in large-scale knowl-\\nedge harvesting [8, 29, 30]. Previous work on side effect\\ndiscovery from individual statements or posts derive\\ninformation credibility by verifying a statement\\u0092s men-\\ntioned side effects against ground truth drug side effect\\ndatabases, and assess associated user credibility by mea-\\nsuring the percentage of a user\\u0092s credible statements\\n[13, 31]. In contrast, our approach to side effect discov-\\nery from discussions by jointly modeling multiple posts\\nand authors eschews the assessment of statement cred-\\nibility and derives user credibility differently. We assign\\neach user a positive score that is used to weight their\\npost content in representing the discussion\\u0092s holistic con-\\ntent. Suchweighted summation is detailedmathematically\\nin Appendix 1 to conform to the general principle of\\ntruth discovery, where sources providing credible infor-\\nmation should be assigned higher credibility scores, and\\nthe information that is supported by credible sources will\\nbe regarded as true [10]. Although our dataset does not\\nprovide any ground truth for user trustworthiness, we fol-\\nlowed the previous usage of ratings or upvotes in online\\nforums and adopted the number of \\u0093thanks\\u0094 received from\\nother forum members [11] as our proxy for user trust-\\nworthiness. Previous works have modeled user expertise\\nbased on user profiles such as demographics; activity\\nfeatures such as posting frequency and posting pattern\\nthrough time series and network analysis [13, 14]. As\\nshown in an earlier example in Section 1, modeling user\\nexpertise from previously experienced side effects better\\ncaptures author authoritativeness for certain side effects.\\nIt is also universally applicable to any online platform.\\nModeling Online Discussion Content and Structure\\nAs our work makes use of the rich topographical proper-\\nties of online communities, we briefly review approaches\\nfor modeling textual content and post-thread discus-\\nsion structure. Previous works use probabilistic graphical\\nmodels implicitly to represent textual content (especially,\\ntopicmodeling) as bags-of-words [28, 32] or inventories of\\nstylistic and linguistic features [13]. Such lightweight rep-\\nresentation are well-suited in moderately short contexts,\\ni.e., sentences or posts. However, in terms of modeling\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 4 of 16\\nlong discussions consisting of multiple posts, state-of-the-\\nart models for Community Question Answering (CQA)\\nfeature hierarchical neural architectures [33\\u009635]. In term\\nof encoding text, sequential encoders such as Long Short-\\nTermMemory (LSTM) [36] or Convolutional Neural Net-\\nworks (CNN) [18] are capable of encoding long-term\\ndependencies and semantic expressiveness by leverag-\\ning word embeddings. In terms of encoding hierarchical\\nstructures such as community discussions consisting of\\npost- and thread-level features, neural architectures allow\\nfor straightforward and efficient integration of multiple\\nlearning objectives. In addition, our neural architecture,\\nNEAT, incorporates attention mechanism that focuses on\\nessential phrases while encoding post content, and joint\\nuser credibility learning while optimizing for the side\\neffect discovery objective.\\nMethods\\nBasic Terminology. To ensure a consistent representa-\\ntion, we define some terms and formalize them as follows:\\n\\u0095 A drug d has a set of side effects,\\nSd = {s1, s2, . . . , s|Sd|}\\u0095 A post p is a message in online forums and contains a\\nsequence of words. Each post p belongs to the set of\\nall online forum posts P and is written by a user u\\nand belongs to a thread t.\\n\\u0095 A user u is a member of an online forum and\\nparticipates in a list of threads, i.e.,\\nTu = {t1, t2, . . . , t|Tu|} by writing at least one post in\\neach thread. We use the terms user and author, as\\nwell as user experience and user expertise\\ninterchangeably. Each user belongs to the set of all\\nonline forum users U and is characterized by their\\ncredibility and expertise. Credibility wu of user u\\nreflects the probability of user u provide trustworthy\\nor helpful information, and is approximated from the\\nnumber of \\u0093thanks\\u0094 given from other forummembers.\\n\\u0095 A thread t (see Table 2) is an ordered collection of\\npost\\u0096user pairs,\\nQt = {(p1,u1) , (p2,u2) , . . . ,\\n(\\np|Qt |,u|Qt |\\n)}.\\nEach thread discusses the treatment for a particular\\ncondition and entails a list of prescribed drugs\\nDt = {d1, d2, . . . , d|Dt |}. Hence, every thread has a list\\nof aggregated potential side effects defined as\\nSt = Sd1 ? Sd2 · · · ? Sd|Dt | .\\nTask Definition. Drug side effect discovery from discus-\\nsions is the task of assigning the most relevant subset of\\npotential side effects to threads discussing certain drugs,\\nfrom a large collection of side effects. We view the drug\\nside effect discovery problem as a multi-label classifica-\\ntion task. In our setting, an instance of item\\u0096label is a\\ntuple\\n(xt , y\\n)\\nwhere xt is the feature vector of thread t\\nderived from its list of post\\u0096user pairs Qt and y is the side\\neffect label vector i.e., y ? {0, 1}|S|, where |S| is the number\\nof possible side effect labels. Given training instances, we\\ntrain our classifier to predict the list of drug side effects in\\nunseen threads discussing unseen drugs.\\nFormal Hypothesis. Given a thread t with Qt , we\\nhypothesize that considering the credibility and experi-\\nence of user u ? (p,u) ? Qt improves the quality of feature\\nrepresentation in thread t, resulting in better drug side\\neffect discovery performance.\\nSelf-supervised Drug Side Effect Discovery. We pro-\\npose a self-supervised learning objective. Instead of\\nrelying on the identical and independently distributed\\nassumption of fully supervised learning, we construct\\nthe dataset from threads that can discuss a set of com-\\nmon drugs. We look up the side effects of these men-\\ntioned drugs via a drug\\u0096side effect medical database\\nobtained from Mayo Clinic portal. Our self-supervised\\ntask explores discussion-based side effect discovery which\\nalleviates the need for finer-grain annotation compared\\nagainst existing approach of statement-based side effect\\ndiscovery. We also propose our neural architecture,\\nTable 2 A sample discussion thread from an online health community\\nUser IDs Posts Mentioned drugs Aggregated side effects\\n3690 While my experience of 10 years is with Paxil,\\nI expect that Zoloft will be the same. You\\nshould definitely feel better within 2 weeks.\\nOne way I found to make it easier to sleep\\nwas to get lots of exercize. Walk or run or\\nwhatever to burn off that anxiety.\\nZoloft, Paxil changed behavior, decreased sexual desire,\\ndiarrhea, dry mouth, heart-burn, sleepiness\\nor unusual drowsiness,...\\n26521 I\\u0092ve heard of people going \\u0093cold turkey\\u0094 and\\nhaving withdrawal at 6 months! Please, get\\nin contact with a doctor ASAP! \\u0093common\\nsymptoms include dizziness, electric shock-\\nlike sensations, sweating, nausea, insomnia,\\ntremor, confusion, nightmares and vertigo\\u0094\\nThe User IDs and Posts columns respectively list the IDs of users involved in the discussions and their messages. The Mentioned drugs and Aggregated side effects columns\\nrespectively list the explicitly discussed drugs and their combined side effects\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 5 of 16\\nNEAT, that jointly models user credibility, expertise and\\ntext content with attention while optimizing for the self-\\nsupervised objective. The network has three major com-\\nponents: 1) user expertise representation with rich multi-\\ndimensional vectors; 2) cluster-sensitive attention being\\ncapable of focusing on relevant phases for post con-\\ntent encoding improvement; and 3) credibility weighting\\nmechanism which effectively learns to assign credibility\\nscore to each user, based on their content. We discuss its\\nimplementation in the following sections. Figure 1 shows\\nthe detailed network architecture of our model.\\nUser Expertise Representation (UE). We embed each\\nuser u ? U as a vector vu so that the vector captures user\\nu\\u0092s experience with certain side effects. As each user u par-\\nticipates in the threads Tu, entailing a list of experienced\\nside effects, we derive user side effect experience vector\\nv?u ? R|S| where S is the set of all possible side effects\\nand v?ui = nui where user u has discussed ith side effect\\nin nui threads. We obtain a user drug experience matrix\\nM? ? R|U|×|S| where jth row of M? denotes user side\\neffect experience vector of jth user. To avoid learning from\\nsparse multi-hot encoded representations and to improve\\nFig. 1 The neural architecture of our proposed NEAT. The wu and vu boxes denote Credibility Weight (CW) component and User Expertise (UE)\\ncomponent. The yellow boxes and blue boxes denote Cluster Attention (CA) component and neural text encoders with attention. The highlighted\\nwords in red denoted the text segments that are being attended by the encoder. The ×, ?, and ? symbols denote the multiplication, summation,\\nand sigmoid, respectively\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 6 of 16\\nthe model\\u0092s scalability with the number of side effects, we\\nperform dimensionality reduction, specifically principal\\ncomponent analysis (PCA) [37], to our experience matrix\\nM? obtained from training set. Figure 2 shows percentage\\nof variance explained versus number of included principal\\ncomponents. Since our PCA plots do not show signifi-\\ncant improved percentage of variance explained beyond\\n100 components, we use g = 100 components, reduc-\\ning our original M? ? R|U|×|S| to user expertise matrix\\nM ? R|U|×g .\\nUser Cluster Attention (CA).We make an assumption\\nvia observations that users in online health communities\\ncan be effectively grouped into clusters based on their\\nprevious side effect experience. The advantages of clus-\\ntering users is twofold: First, since users in the same\\nclusters share certain parameters, they are jointly mod-\\neled and more active forum members leverage less active\\nones. Second, clustering efficiently reduces the number\\nof parameters to learn and improves optimization and\\ngeneralization. We apply K-means \\u0096 a distance-based\\nunsupervised clustering algorithm [38] \\u0096 to binary-valued\\nuser experience vectors v?u after normalization. By using\\ncosine similarity, the algorithm effectively groups users\\nwith a high number of co-occurred side effects in the same\\ncluster. To determine the number of clusters c, we plot\\nthe silhouette scores against the number of clusters and\\nobserve the sharp drop after c = 7 (Fig. 3). The average\\nsilhouette score is 0.57 for our choice of c = 7, indi-\\ncating that users are moderately matched to their own\\ngroups and separated from other groups. The top 5 most\\ncommon side effects in each clusters are shown in Table 3.\\nIn the larger domain of natural language processing,\\nattention has become an integral part for modeling text\\nsequences [39, 40]. By learning to focus on essential text\\nsegments, attention allows text encoders to capture long\\nterm semantic dependencies with regard to auxiliary con-\\ntextual information [41, 42]. In our related task of ADR\\nmentions extraction, attention has been adopted recently\\nin neural sequence labelling models [21, 43], resulting\\nin promising improvement. Inspired by the concept, we\\nenhance text encoding with user expertise attention. Even\\nthough the attention is adjusted to the non-extractive self-\\nsupervised task of thread-level drug side effect discovery,\\nwe hypothesize that our model learns to highlight the\\nmentioned accurate side effects, and can be used as a\\nself-supervised baseline for side effect extraction. Based\\non the previously obtained clustering results, we assign a\\nlearnable cluster attention vector for each user group and\\nincorporate their expertise into the text encoding process.\\nPost Content Encoding. NEAT takes the content of a\\nthread t as input, which is a list of post\\u0096user pairs Qt .\\nPost pi of pair (pi,ui) ? Qt consists of a sequence of\\nwords xpi = {w1, . . . ,wn} with length n. We seek to rep-\\nresent a post pi as a vector vp that effectively captures\\nits semantics through an encoding function f (xpi) mod-\\neled by a neural text encoding module (the blue boxes\\nin Fig. 1). We embed each word into a low dimensional\\nvector and transform the post into a sequence of word\\nvectors {vw1 , vw2 , . . . , vwn}. Each word vector is initialized\\nusing pre-trained GloVe [24] embeddings, and each out-\\nof-vocabulary word vector is initialized randomly. We\\nmake use of modularity \\u0096 a major advantage of neural\\nFig. 2 Principal component analysis on user experience vectors. The horizontal axis denotes the number of principal components chosen for PCA,\\nwhile the vertical axis denotes their percentage of variance explained. We notice that the percentage of variance explained does not increase\\nsignificantly after 100 principal components\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 7 of 16\\nFig. 3 Silhouette scores for User Clustering. The horizontal axis denotes the number of clusters chosen for K-means clustering, while the vertical axis\\ndenotes their the silhouette scores. We notice that the silhouette scores drop sharply after 7 clusters.\\narchitectures \\u0096 and design the post content encoder as\\na standalone component that can be easily updated with\\nany state-of-the-art text encoder. In this work, we pro-\\nvide two neural text encoders: long-short term memory\\n(LSTM, see Fig. 4) [36] and convolutional neural net-\\nworks (CNN, see Fig. 5) [18], both of which incorporates\\nattention mechanism.\\nA bi-directional LSTM encodes the word vector\\nsequence and outputs two sequences of hidden states: a\\nforward sequence, Hf = hf1,hf2, . . . ,hfn that starts from\\nthe beginning of the text; and a backward sequence,Hb =\\nhb1,hb2, . . . ,hbn that starts from the end of the text. Formany\\nsequence encoding tasks, knowing both past (left) and\\nfuture (right) contexts has proven to be effective [44]. The\\nstates hfi ,hbj ? Re of the forward and backward sequences\\nare computed as follows:\\nhfi = LSTM(hfi?1, vwi), hbj = LSTM(hbj+1, vwj),\\nwhere e is the number of encoder units, and hfi ,hbj are the\\nith and jth hidden state vector of the forward (f ) and back-\\nward (b) sequence. We derive the cluster attention vector\\nas vai ? Re for each user ci, from which the weights of\\neach hidden state hfj and hbj based on their similarity with\\nthe attention vector are:\\nwaj =\\nexp(vaihj)?n\\nl=1 exp(vaihl)\\n. (1)\\nThe intuition behind Eq. (1), inspired by Luong et al.\\n[39], is that hidden states which are similar to the atten-\\ntion vector vai should be paid more attention to; hence\\nare weighted higher during document encoding. vai is\\nadjusted during training to capture hidden states that are\\nsignificant in forming the final post representation. waj\\nis then used to compute forward and backward weighted\\nfeature vectors:\\nhf =\\nn?\\nj\\nwajh\\nf\\nj , hb =\\nn?\\nj\\nwajhbj . (2)\\nWe concatenate the forward and backward vectors to\\nobtain a single vector, following previous bi-directional\\nLSTM practice [45].\\nTable 3 Most common experienced side effects for each user\\ncluster ci (i = 1 to 7)\\nCluster Most common experienced side effects\\nc1 vision blurred, yellow skin, vision double, yellow eye,\\nnose stuffy\\nc2 headache, itch, stomach pain, weak, nausea\\nc3 itch, irritate, headache, pain abdominal, stomach\\ncramp\\nc4 bad taste, nausea, tiredness, irritate, mouth ulcer\\nc5 skin red, itch, rash skin, skin peeling, burning skin\\nc6 sneezing, nose runny, nose stuffy, decrease sexual\\ndesire, pain breast\\nc7 nausea, stomach pain, vomit, diarrhea, pain\\nabdominal\\nThe left column lists the names of 7 clusters, and the right column describes the\\nmost common experienced side effects of users in each cluster\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 8 of 16\\nFig. 4 LSTM-based encoder with cluster attention. The × and + cells denote the attention-weighted summation described in Eq. (2). The C cell\\ndenotes the concatenation of the forward, hf , and backward, hb , hidden states\\nOur choice of CNN-based encoder is based on prior\\nwork [18, 46]. A convolution block k consists of two sub-\\ncomponents: a convolution layer and a cluster attention\\nlayer. In the convolution layer, a kernel of window s\\n(0 &lt; s &lt; n) of weight W is used to generate\\nthe hidden representation hkj for the word embeddings\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 9 of 16\\nFig. 5 CNN-based Encoder with Cluster Attention. The × and + cells denote the attention-weighted summation described in Eq. 2. The C cell\\ndenotes the concatenation of the final hidden states of K convolution blocks\\nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 10 of 16\\n{vwi?s+1 , · · · , vwi} as:\\nhkj = CONV (W , {vwi?s+1 , · · · , vwi}) (3)\\nwhere CONV (·) is the convolution operation described\\nin [18]. In the cluster attention layer, we first derive the\\nattention weight waj for each hidden representation hkj\\nsimilarly to the LSTM-based encoder. Attention weighted\\npooling is used to obtain the convolution block output as\\nfollows:\\nhk =\\nn?\\nj\\nwajhkj (4)\\nSince we use multiple convolution blocks of different\\nkernel sizes, the final post representation is the concate-\\nnation of K block outputs hk .\\nThread Content Encoding with Credibility Weights\\n(CW). For every post\\u0096user pair (pi,ui) at thread t, we\\nfirst compute feature vector vpi for post pi. NEAT then\\nconcatenates this post\\u0096user representation with user ui\\u0092s\\nexpertise vector vui to form post\\u0096user complex vector vpui .\\nThis post\\u0096user complex is weighted by a user credibil-\\nity ewui , where wui initially set to 0 per user and updated\\nwhile training for the self-supervised side effect discovery\\nobjective. We implement credibility learning according to\\nthe general intuition from the truth discovery literature:\\nusers who give quality posts, on which the model can\\nsolely base to make correct predictions, are given a higher\\ncredibility. We also exploit this credibility score to encode\\nthe thread representation by placing emphasis on the con-\\ntent of credible users. A representation of a thread that\\nmeets the above description is the weighted sum of each\\npost\\u0096user complex vector:\\nvt =\\nn?\\ni=1\\nvp?ui =\\nn?\\ni=1\\newui vpui (5)\\nMulti-label Prediction:NEAT feeds the thread content\\nrepresentation vt through a fully connected layer whose\\noutputs can be computed as follows:\\nst = W tanh(vt) + b, (6)\\nwhere W and b are weights and biases of the layer. The\\noutput vector st ? R|S| is finally passed through a sigmoid\\nactivation function ?(·), and trained using cross-entropy\\nloss L defined as follows:\\nL = 1|T |\\n|T |?\\nt=1\\n{yt · log(? (st)) + (1 ? yt) · log(1 ? ?(st))}\\n+ ?1\\n??\\nu\\nv2u + ?2\\n?\\ni\\n|wui |\\n(7)\\nWe adopt regularization that penalizes the training loss\\nwith the user experience matrix\\u0092s L2 norm by a fac-\\ntor of ?1 and the user credibility vector wu\\u0092s L1 norm\\nby a factor of ?2. The loss function is differentiable,\\nthus trainable with the Adam optimizer [47]. During\\nour gradient-based learning, user ui\\u0092s credibility score\\nwui is updated by calculating ?L?wui by back-propagation\\n(see Appendix 1).\\nResults\\nWe conduct experiments to validate the effectiveness of\\nour proposed model. We design an ablation study to high-\\nlight the effectiveness of each component of NEAT in\\nour self-supervised side effect prediction. In addition, we\\nexpand our previous work [16]. More specifically,\\n1. We verify the representativeness of the learned\\ncredi'</span>text6\n:   <span style=white-space:pre-wrap>'Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 \\nhttps://doi.org/10.1186/s13326-020-00223-z\\nRESEARCH Open Access\\nEnabling ad-hoc reuse of private data\\nrepositories through schema extraction\\nLars Christoph Gleim1* , Md Rezaul Karim1,2, Lukas Zimmermann3, Oliver Kohlbacher3,4,5,6,7,\\nHolger Stenzhorn3,8, Stefan Decker1,2 and Oya Beyan1,2\\nAbstract\\nBackground: Sharing sensitive data across organizational boundaries is often significantly limited by legal and ethical\\nrestrictions. Regulations such as the EU General Data Protection Rules (GDPR) impose strict requirements concerning\\nthe protection of personal and privacy sensitive data. Therefore new approaches, such as the Personal Health Train\\ninitiative, are emerging to utilize data right in their original repositories, circumventing the need to transfer data.\\nResults: Circumventing limitations of previous systems, this paper proposes a configurable and automated schema\\nextraction and publishing approach, which enables ad-hoc SPARQL query formulation against RDF triple stores\\nwithout requiring direct access to the private data. The approach is compatible with existing Semantic Web-based\\ntechnologies and allows for the subsequent execution of such queries in a safe setting under the data provider\\u0092s\\ncontrol. Evaluation with four distinct datasets shows that a configurable amount of concise and task-relevant schema,\\nclosely describing the structure of the underlying data, was derived, enabling the schema introspection-assisted\\nauthoring of SPARQL queries.\\nConclusions: Automatically extracting and publishing data schema can enable the introspection-assisted creation of\\ndata selection and integration queries. In conjunction with the presented system architecture, this approach can\\nenable reuse of data from private repositories and in settings where agreeing upon a shared schema and encoding a\\npriori is infeasible. As such, it could provide an important step towards reuse of data from previously inaccessible\\nsources and thus towards the proliferation of data-driven methods in the biomedical domain.\\nKeywords: Semantic web, Linked data, RDF, SPARQL, Schema extraction, Privacy, Data access, Distributed systems,\\nQuery design, Personal health train, FAIR data\\nBackground\\nData-driven methods play an increasingly important role\\nfor cost-efficient and timely research results and effec-\\ntive decision support [2] throughout numerous domain\\nsuch as economics [3], education [4], manufacturing [5],\\nhealthcare and life sciences [6\\u00968].\\nAt the same time, the data that build the founda-\\ntion of these models oftentimes underlies strict sharing\\n*Correspondence: gleim@cs.rwth-aachen.de\\nThis work is an extended version of a paper previously published at the\\nSeWeBMeDA-2018 workshop [1].\\n1Informatik 5, RWTH Aachen University, Ahornstr. 55, 52062 Aachen, Germany\\nFull list of author information is available at the end of the article\\nrequirements. For example, in the sensitive healthcare\\ndomain, although first responders, hospitals, and many\\nother stakeholders already collect valuable data for data-\\ndriven research and treatment today, large portions of this\\ndata remain inaccessible to the majority of stakeholders\\n\\u0096 largely due to ethical, administrative, legal and political\\nhurdles that render data sharing infeasible [9]. In prac-\\ntice, this leads to an inability to access large amounts of\\ndata crucial for a variety of tasks such as the optimiza-\\ntion of decision support systems, first response systems\\nand data-driven research. At the core of this issue lies the\\nlack of an effective mechanism to allow for data access\\nin a legally certain, sustainable and cost-efficient manner\\nwithout extensive delays.\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 2 of 15\\nFor example, learning health systems, allowing for data-\\ndriven research on sensitive data such as electronic health\\nrecords (EHRs), have long been said to bear the poten-\\ntial to \\u0093fill major knowledge gaps about health care costs,\\nthe benefits and risks of drugs and procedures, geo-\\ngraphic variations, environmental health influences, the\\nhealth of special populations, and personalized medicine.\\u0094\\n[10]. While a variety of such systems have been proposed\\n[10\\u009613], practical implementation has so far not become\\na reality, likely due to the aforementioned hurdles.\\nIn order to enable data economy in privacy-sensitive\\ndomains and effective reuse of existing data and research,\\nnovel approaches are emerging to overcome these limi-\\ntations. One of those approaches is the Personal Health\\nTrain (PHT) framework [14], which aims to bring algo-\\nrithms and statistical models to data sources, rather than\\nsharing data with the third parties such as researchers.\\nThe main benefit of this approach is its ability of utilizing\\nall the data, including the sensitive and private informa-\\ntion, without data having to leave the original data source.\\nA key challenge of this approach is that data users (such as\\nresearchers) are required to develop their models without\\nhaving a grasp of the actual data. Unless there are univer-\\nsally agreed information models and data set descriptions,\\nthere is a need to create and communicate a schema \\u0096 that\\nis information about the structure of the data \\u0096 to enable\\nwriting queries for heterogeneous data resources.\\nThis work is embedded in our ongoing efforts support-\\ning data reuse in healthcare environments and conducted\\nas part of the SMITH [15] and DIFUTURE [16] projects.\\nThe key contributions of this paper consist of an auto-\\nmated approach for extracting task-relevant schema from\\nRDF data sources for the efficient formulation of data\\nselection and integration queries without direct access to\\nthe data and a corresponding integration with an infor-\\nmation system architecture that allows for the subsequent\\nevaluation of that query in a secure enclave.\\nIn the following, we describes some related work and\\nthe basic foundations of our approach. Subsequently, we\\noutline the motivation of our research, as well as the key\\nchallenges of schema extraction from sensitive data with-\\nout sacrificing privacy, followed by the description of our\\nproposed schema extraction approach from existing data\\nin the methods section. We then present a number of\\nevaluation results of the proposed data selection and inte-\\ngrationmethodology, based on the schema extracted from\\na sample use case. After a discussion of our results, we fin-\\nish with a conclusion of our results and a short outlook of\\ndirections for future work.\\nRelated work\\nIn order to facilitate knowledge discovery for both\\nhumans and machines, the FAIR data principles [17]\\nhave been proposed: A set of guiding principles to make\\nresearch and scientific data Findable, Accessible, Interop-\\nerable, and Re-usable. These guidance principles promise\\nto help in the discovery, access, integration and analysis of\\ntask-appropriate scientific data and associated algorithms\\nandworkflows. Thus, FAIR is gaining a lot of attention and\\nincreasing adoption.\\nCore to realizing these principles are Semantic Web\\nTechnologies [18], which provide a framework for data\\nsharing and reuse by making the semantics of data\\nmachine interpretable. Particularly the directed, graph-\\nbased data model RDF [19\\u009621] (built entirely upon the\\nnotion of statements, i.e. data in the form of subject\\npredicate object triples) in conjunction with formal\\nconceptualizations of information models, semantics and\\nencoding conventions in RDF vocabularies and ontologies\\ntakes an important role.\\nAs such, RDF Schema (RDFS) [22] and the Web Ontol-\\nogy Language (OWL) [23] provide a proven framework in\\norder to describe (but not necessarily enforce) the struc-\\nture and semantics of data. Substantially, RDFS introduces\\nthe concepts of classes and properties as well as basic rela-\\ntions between them. OWL \\u0096 a computational logic-based\\nlanguage \\u0096 extends upon these concepts in order to repre-\\nsent rich and complex knowledge about things, groups of\\nthings, and relations between them.\\nIn the context of this work, we use the term \\u0091schema\\u0092\\nto refer to the semantic and structural annotation of data\\nusing especially these two vocabularies.\\nOn the other hand, the classical notion of schema as\\nthe formal definition of the shape that data needs to com-\\nply with in order to be valid (i.e. schema validation and\\nenforcement) also exists in the Semantic Web with the\\nShape Expression Language (ShEx) [24] and the Shapes\\nConstraint Language (SHACL) [25]. At this time, there\\nare however no established ways of sharing data shapes\\nthrough public repositories and as such, in practice, they\\nare only adapted in isolated deployments.\\nNevertheless, using RDFS and OWL, it is possible to\\ncreate domain-specific, optionally interoperable vocabu-\\nlaries and ontologies, which may declare e.g. term or con-\\ncept equivalences and dependencies between each other\\nand subsequently enable interoperability across individual\\nencodings.\\nKey to realizing the semantics described in RDFS\\nand OWL vocabularies is the inference or entailment\\nof implicit knowledge (inferred triples) that follow from\\nexplicit knowledge (dataset triples) via the semantics\\ndescribed in the corresponding vocabularies. Figure 1\\nillustrates some of the inferred triples that follow from\\nthe formal RDFS and OWL entailment semantics [26].\\nHere we assume the namespaces ex and snomed to be\\ndefined1.\\n1Likely the definitions would be http://example.org/ and http://purl.\\nbioontology.org/ontology/SNOMEDCT/\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 3 of 15\\nFig. 1 Illustration of several effects of entailment support of a SPARQL endpoint\\nIn this example, only a small number of triples is con-\\ntained in the actual dataset, while the majority of knowl-\\nedge is inferred using RDFS and OWL semantics. Notably\\neach resource is either a class, a property or an individual,\\ni.e. either schema or data.\\nPopular examples of RDFS and OWL vocabularies\\ninclude the Ontology for Biomedical Investigation (OBI)\\n[27] in the biology and healthcare domain, the GoodRela-\\ntions ontology [28] in eBusiness and theDCAT vocabulary\\n[29], which is used for the general purpose metadata\\nannotation of datasets and data catalogs.\\nIn the context of eHealth systems, support for the\\nSemantic Web is becoming more and more promi-\\nnent with candidates such as the multilingual thesaurus\\nSNOMEDCT [30], ongoing research efforts into an RDF\\nspecification of HL7 FHIR [31], as well as the establish-\\nment of clear guidelines for dataset descriptions such as\\nthe HCLS Community Profile [32].\\nVarious high-quality catalogs of freely reusable vocabu-\\nlaries exist, allowing for the easy discovery of suitable ter-\\nminology to semantically annotate data. Examples include\\nthe Linked Open Vocabulary (LOV) [33\\u009635] and the Bio-\\nPortal [36\\u009638] project.\\nThe related idea of using schema export and import for\\nfederated data access date back to as early as 1985 [39] but\\nit is only recently that the idea has receivedmore attention\\nin the context of the Semantic Web.\\nKellou-Menouer et al. [40] propose a schema discov-\\nery approach based on hierarchical clustering instead of\\ndata annotations thus leading to an approximate schema.\\nFlorenzano et al. [41], Lohmann et al. [42, 43] and\\nDudá\\u009a et al. [44] introduce approaches focused on schema\\nextraction for visualization of the data structure but do\\nnot consider publishing or reuse of the extracted schema.\\nBenedetti et al. [45, 46] propose an interesting related\\napproach for schema extraction, visualization and query\\ngeneration but do not consider interoperability issues and\\nrely on custom mechanisms for schema storage.\\nMotivation\\nRecently, Jochems et al. [47] and Deist et al. [48]\\nintroduced two related promising Semantic Web-based\\napproaches in the context of the PHT initiative, founded\\non the key concept of bringing research to the data rather\\nthan bringing data to the research. As such the underly-\\ning information system architecture enables learning from\\nprivacy sensitive data without the data ever crossing orga-\\nnizational boundaries, maintaining control over the data,\\npreserving data privacy and thereby overcoming legal and\\nethical issues common to other forms of data exchanges.\\nThe general approach of this underlying system can be\\noutlined as follows:\\n1 Initially, both the client and data provider agree upon\\na set of attributes or features, such that all\\nparticipating data providers have corresponding\\nsources of (privacy sensitive) data.\\n2 Then each data provider encodes their data using an\\n(also agreed upon) ontology or vocabulary,\\nconverting it into RDF representation. This process\\nyields proper Linked Data [49] and thus enables\\nsemantic interoperability [50].\\n3 The resulting RDF data is deployed to a private triple\\nstore at each location, providing a private SPARQL\\n[51] query endpoint, which is not directly accessible\\nby the client.\\n4 A SPARQL data query is then formulated based on\\nthe previously agreed upon encoding and a\\ncorresponding distributable processing algorithm\\ndefined.\\n5 The shared query is then executed locally at each\\ndata provider against their respective triple stores\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 4 of 15\\nand the returned data processed using the\\ncorresponding algorithm.\\n6 The local results are then combined into a global one.\\n7 Depending on the approach, steps 5 and 6 may be\\nfurther iterated.\\nWhile these approaches \\u0096 introduced in the context\\nof the PHT initiative \\u0096 work well when multiple parties\\nagree on jointly collecting, encoding and evaluating data\\nin advance \\u0096 such as is the case for conducting individual\\ncoordinated studies \\u0096 they solve the issue of interoperabil-\\nity by agreeing on a single shared knowledge representa-\\ntion and encoding methodology a priori (steps 1-3 in the\\nabove process). In an optimal setting where agreeing on a\\nsingle shared and global information model and encoding,\\nreuse of diverse and existing data could always be directly\\naccomplished with this approach.\\nHowever, to our knowledge, so far all corresponding\\nefforts have been unsuccessful. At the time of writing\\nthe popular https://fairsharing.org/ portal indexes 1084\\ndatabases using 1183 standards, suggesting that in prac-\\ntice, each collected dataset and domain much rather tends\\nto introduce its own encoding methodology.\\nAdditionally, RDF datasets de facto often combine terms\\nfrom multiple vocabularies and ontologies, sometimes\\ndeviating from the originally intended information mod-\\nels and encodings.\\nThus when trying to reuse diverse existing data, a proper\\nunderstanding of the real structure of the available data \\u0096\\ni.e. the schema of the data \\u0096 is indispensable. For a client\\nwithout direct access to the data, this information is how-\\never typically not available, since its acquisition inherently\\nrelies upon inspection of the structure of the data.\\nApproaches, such as the PHT, depend upon ad-hoc\\ndata selection and integration facilities (step 4 of the\\nPHT approach, corresponding to the first two steps of\\nthe classical Knowledge Discovery in Databases (KDD)\\nprocess [52]) for the efficient and effective extraction of\\nknowledge from private data sources. In order to enable\\nthe usage of such an approach with diverse existing\\ndata, suitable methods for the extraction and distribution\\ntask-specific schema, tailored specifically for the purpose\\nof enabling ad-hoc data selection and integration, are\\nneeded.\\nMethods\\nIn this section, we propose an automated approach for\\nextracting task-specific schema from RDF data sources in\\norder to enable the efficient formulation of SPARQL data\\nselection and integration queries without direct access to\\nthe data. First, we describe basic requirements for the\\nextracted schema, as well as the fundamental idea of the\\nschema extraction technique before subsequently intro-\\nducing a number of extensions, in order to support for\\nmore generally applicable schema extraction methodol-\\nogy. We discuss the trade-offs to be made between differ-\\nent versions of the schema extraction approach and finally\\nshow how the extracted schema can be used further for\\nthe data selection and integration.\\nIn the context of RDF data, the fundamental knowl-\\nedge required for the creation of SPARQL queries for\\ndata selection and integration consists of the various\\nrdf:type objects, the rdf:Property predicates and the struc-\\ntural relations between them. This information can itself\\nbe represented using Semantic Web Standards, such as\\nRDFS, OWL, ShEx or SHACL.\\nWhile shape languages such as ShEx and SHACL\\nare natural candidates for representing prescriptive data\\nschema, they are designed specifically for the validation of\\nclearly structured individual data shapes and to commu-\\nnicate explicit graph patterns. As such they are however\\nnot equally well suited for the formalization of the flexible\\nschema of entire semi-structured datasets.\\nRDFS on the other hand provides a simple and descrip-\\ntive structural annotation of the relationships between\\nproperties and classes and as such serves as a promising\\ncandidate for the task at hand.\\nWhile OWL further extends RDFS with a pow-\\nerful set of description logic-based modeling prim-\\nitives, the corresponding semantic complexity adds\\nsignificant overhead to the schema extraction pro-\\ncess. Especially since the extracted schema is only\\nmeant to be used for query authoring and explic-\\nitly not for reasoning, in the context of this work\\nwe generally restrict our effort to extracting schema\\nusing RDFS and the OWL owl:equivalentClass,\\nowl:equivalentProperty and owl:sameAs pred-\\nicates, which we deem most relevant in order to enable\\ninteroperability and the effective formulation of selection\\nand integration queries.\\nEspecially in order to ensure interoperability with\\nexisting Semantic Web technologies and compatibility\\nwith standard Semantic Web tools, such as schema-\\nintrospection-assisted SPARQL query builders, the\\nextracted schema should thus be available as a simple\\nRDFS and OWL vocabulary via a SPARQL endpoint.\\nSchema-introspection refers to the process of examin-\\ning the schema definition to determine which types of\\nentities exist, which properties are defined upon them and\\nsubsequently, what can be queried for. Since the schema\\nneeded to create data queries (e.g. using SPARQL) only\\ncontains basic structural information about the original\\ndata, it also conveys far less privacy critical information\\nthan exposing the actual data. As such it can be published\\npublicly without privacy concerns in many scenarios.\\nIn the following, we describe an automated approach for\\nschema extraction fromRDF data which allows for the for-\\nmulation of data selection and integration queries without\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 5 of 15\\ndirect access to the data and the subsequent evaluation of\\nthat query in a secure enclave.\\nSchema extraction\\nWe propose an approach for schema extraction based\\non exploiting key characteristics of RDF, RDFS, and\\nOWL. RDF data encoded in compliance with correspond-\\ning vocabularies inherently include metadata about their\\nsemantics and structural relationships.\\nFor the schema extraction, the rdf:type relation plays\\nthe key role, as it declares data points to be instances of\\nspecific data types or, according to RDFS terminology and\\nsemantics [53], classes. Anything that is a type in the sense\\nof occurring as the target of this relation should thus auto-\\nmatically becomes part of the schema as an entity of type\\nrdfs:Class. Additionally, any property relation (that\\nis any identifier occurring in the predicate position of a\\nsubject-predicate-object triple) which occurs in the data\\nshould be included as an entity of type rdf:Property.\\nFinally all directly describing properties of these classes\\nand properties should be included as well. For the scope\\nof this work, we assume that all data in the private data\\nrepository is sensitive and should remain private.\\nEntailment supported schema extraction Assuming\\nperfect conditions, namely proper inclusion of all used\\nvocabularies into the triple store, correct usage of those\\nvocabularies, as well as OWL entailment [26] support of\\nthe SPARQL endpoint providing access to the data, the\\nentire schema of a given RDF data set can be extracted\\nusing a single simple SPARQL CONSTRUCT query as\\ndepicted in Listing 1.\\n\\002\\nCONSTRUCT {?s ?p ?o}\\nWHERE {\\n{[] ?s []}\\nUNION {[] a ?s} .\\n?s ?p ?o .\\n}\\n\\003 \\004\\nListing 1 SPARQL schema extraction query relying on proper\\nentailment support of the endpoint.\\nNote that we explicitly define the relevant subset of all\\navailable schema information to be that which is actually\\nused in the data, i.e. the instantiated schema, and thus only\\nextract that.\\nThe preceding query constructs an RDF graph (line 1)\\ncontaining all the directly describing triples ?s ?p ?o\\nthat occur in the tripe store but having only the following\\nsubjects:\\n1 Instantiated RDF properties ?s (line 3) which\\naccording to RDF 1.1 Semantics [53] are any IRI used\\nin predicate position (c.f. rdfD2).\\n2 Instantiated RDFS classes ?s (line 4) via their\\noccurrence as the object of a triple with rdf:type\\nas the predicate. The fact that these are RDFS classes\\nfollows directly from the RDFS axiomatic triple\\nrdf:type rdfs:range rdfs:Class . in\\nconjunction with RDFS entailment pattern rdfs3\\n[53].\\nAccording to the SPARQL entailment regime, all the\\nsubclass relationships, transitive properties, equivalences\\netc. used in the data are automatically materialized (i.e.\\nincluded in the dataset as inferred knowledge as illus-\\ntrated in Fig. 1) and thus resolved and included too (c.f.\\n[53, 54]).\\nIt should be noted that the query only extracts direct\\nproperties (i.e. triples ?s ?p ?o directly related to the\\nsubject ?s) and as such, some complex constraints such\\nas OWL disjointness axioms are not included in the\\nextracted schema. However, as stated before, for the task\\nof query formulation we consider this to be sufficient.\\nDirectly instantiated schema Since in practice few\\nSPARQL endpoints actually support any kind of entail-\\nment and usually do not materialize implicit triples, the\\napplicability of this basic approach is limited. While the\\noriginal query can theoretically also be executed with-\\nout entailment support, it does not guarantee that all\\nused properties and classes are annotated accordingly\\nas rdf:Property and rdf:Class and completely\\nignores any resource ?s that lacks further describing\\ntriples ?s ?p ?o.\\nThus, in the following we introduce several revisions\\nof the initial extraction query 1 that allow us to reintro-\\nduce the missing triples without relying upon entailment\\nsupport. Additionally, many datasets de facto employ\\nterms from a number of different vocabularies and ontolo-\\ngies and deviate from the originally intended informa-\\ntion model. Since the availability of information about\\ndomain and range of the different properties employed\\nin the dataset is especially relevant in order to assist the\\nquery creation process, we further explicitly construct\\nrdfs:domain and rdfs:range statements according\\nto the property\\u0092s respective usage in the dataset.\\nIn scenarios where it is sufficient to consider only those\\ntypes and properties that are directly used in the dataset\\nor where no information whatsoever about the employed\\nvocabularies is available, it can be reasonable to disregard\\nthe inference generalizations and equivalences entirely.\\nListing 2 proposes a SPARQL query for the extraction\\nof a corresponding schema, which closely reflects the\\nstructure of the underlying data and works even if the\\ndefinitions of the employed ontologies are unavailable.\\nFor this and all further queries, we assume standard\\nSPARQL namespace and prefix definitions as specified by\\nthe World Wide Web Consortium\\u0092s OWL and SPARQL\\nspecifications [53, 55].\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 6 of 15\\n\\002\\nCONSTRUCT {\\n?predicate ?a ?b; a rdf:Property;\\nrdfs:domain ?pDomain; rdfs:range ?\\npRange.\\n?concept ?c ?d; a rdfs:Class.\\n} WHERE {\\n?s ?predicate ?o.\\nOPTIONAL {?s a ?pDomain}\\nOPTIONAL {?o a ?pRange}\\nOPTIONAL {?predicate ?a ?b}\\n[] a ?concept\\nFILTER(!isBlank(?concept))\\nOPTIONAL {?concept ?c ?d}\\n}\\n\\003 \\004\\nListing 2 Basic SPARQL schema extraction query which only\\ndiscovers RDFS classes and properties directly instantiated in the\\nqueried dataset.\\nAnalogously to query 1, we detect predicates as any\\nInternationalized Resource Identifier (IRI) used in predi-\\ncate position (line 5) and classes as IRIs used as objects of\\nRDF type triples (line 9). We also include any additional\\ninformation directly relating to those subjects that might\\nbe available in the dataset (lines 8 and 11). To explicitly\\nconstruct rdfs:domain and rdfs:range information\\nof the predicates, we further determine the rdf:type\\nof each subject (line 6) and object (line 7), if available.\\nAdditionally we filter out any class declarations without\\nan own identifier (line 10) to avoid potential referenc-\\ning issues with the extracted schema. Lastly we construct\\nthe schema graph as all discovered predicates (explicitly\\ntyped as rdf:Property) and their related informa-\\ntion (line 2) and all discovered classes (explicitly typed as\\nrdfs:Class) and their related information (line 3).\\nWhen applying this extraction approach to the dataset\\ndepicted in Fig. 1, we end up with the schema depicted in\\nFig. 2 where classes are highlighted in blue and properties\\nin green (i.e. with implicit rdf:type triples).\\nSubsequently, in this exemplary use case, following the\\nextracted schema closely one could query for instances\\nof the ex:Patient class and their corresponding prop-\\nerty ex:treatedAt, which however perfectly reflects\\nthe available dataset without inferred knowledge.\\nIt should be noted, that this extracted schema is explic-\\nitly not suited for triple entailment according to RDFS\\nsemantics, due to the conjunctive nature of multiple\\nrdfs:domain and rdfs:range definitions on prop-\\nerties (c.f. RDFS entailment patterns rdfs2 and rdfs3\\n[53]). A semantically correct alternative would be the\\nusage of Schema.org\\u0092s schema:domainIncludes and\\nschema:rangeIncludes properties in line 2, instead\\nof their RDFS equivalents. However, since RDFS domain\\nand range semantics are implemented in a variety of tools\\nfor schema exploration, visualization and assisted query\\nauthoring [56\\u009658], while schema.org semantics are not\\nequally well supported, we deliberately defer semantic\\ncorrectness to a closer representation of the underlying\\ndata\\u0092s structure.\\nLocally inferred schema In order to re-include previ-\\nously inferred information such as additional types and\\nclasses due to sub-property, subclass, domain, range or\\nequivalence relationships, we can extract the relevant\\nschema directly from the data and the full definitions of\\nthe employed ontologies using the SPARQL 1.1 Property\\nPaths [59] feature, independent of entailment support or\\nstatement materialization on the endpoint.\\nA corresponding SPARQL query is depicted in Listing 3.\\n\\002\\nWHERE {\\n?s ?x ?o. OPTIONAL {?s a ?pDomain}\\nOPTIONAL {?o a ?pRange}\\n?x (rdfs:subPropertyOf|owl:\\nequivalentProperty|^owl:\\nequivalentProperty\\n|owl:sameAs|^owl:sameAs)* ?predicate\\nOPTIONAL {?predicate ?a ?b}\\n{?predicate (rdfs:range|rdfs:domain) ?y}\\nUNION {[] a ?y}\\n?y (rdfs:subClassOf|owl:equivalentClass|^\\nowl:equivalentClass|owl:\\nsameAs|^owl:sameAs)* ?concept\\nFILTER(!isBlank(?concept))\\nOPTIONAL {?concept ?c ?d}}\\n\\003 \\004\\nListing 3 Extended WHERE clause of schema extraction query 2\\nemploying SPARQL 1.1 Property Paths to emulate RDFS\\nspecialization, domain and range semantics, as well as OWL\\nequivalence entailment.\\nThe query constructs a graph, which in addition to all\\ninstantiated RDFS classes and RDF properties (and their\\ndirect properties) includes generalizations and equivalent\\nresources of those via RDFS and OWL semantics.\\nFor both properties and classes, we resolve corre-\\nsponding generalizations directly using the relevant\\nRDFS entailment patterns (rdfs5, rdfs7, rdfs9, rdfs11)\\n[53] and concept equivalences using OWL\\u0092s owl:\\nequivalentClass, owl:equivalentProperty\\nand owl:sameAs predicates [54] in lines 5 and 9. While\\nowl:sameAs is only supposed to be used for the decla-\\nration of equivalence between individuals, it is commonly\\nmisused in practice and as such deliberately included in\\nthis query.\\nrdfs:Class annotations are further inferred follow-\\ning RDFS entailment rules rdfs2 and rdfs3 [53] from\\nrdfs:domain and rdfs:range properties declared\\non instantiated rdf:Property resources (line 7).\\nWhen applying this extraction approach to the dataset\\ndepicted in Fig. 1, we end up with the relevant schema\\ndepicted in Fig. 3. As before, classes are highlighted in blue\\nand properties in green.\\nFollowing the extracted schema, it is now also possible\\nto query for instances of the hospital and person classes, as\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 7 of 15\\nFig. 2 Directly instantiated schema extracted from Example 1\\nwell as a number of equivalent SNOMEDCT vocabulary\\nterms.\\nEmploying terminology services\\nIn practice, individual SPARQL endpoints providing\\naccess to individual datasets cannot be (and are not) bur-\\ndened with serving all vocabularies and terminologies\\nused in the dataset and related to those. That is the pur-\\npose of specialized terminology services and vocabulary\\ncatalogs, such as the aforementioned LOV and BioPortal\\nprojects.\\nIn order to resolve equivalences and generalizations\\nacross vocabularies, it is thus possible to make use of the\\nSPARQL 1.1 Federated Query protocol [60, 61] in order\\nto entail additional schema triples using external termi-\\nnology services. The query depicted in Listing 4 employs\\nfederated queries to the SPARQL endpoint http://\\nexample.org/terminology in order to accomplish\\nthis. The query further explicitly filters out all subject that\\nare blank nodes in order to avoid renaming and resolu-\\ntion issues between blank nodes from different sources\\n(c.f. [60]).\\nWhile the approach follows the same principles as the\\npreviously introduced local inference (c.f. Listing 3), here\\neach inference step also includes results from the exter-\\nnal terminology service. As such, following the exam-\\nple from before, the extracted schema would now also\\ninclude all inferred knowledge from the SNOMEDCT\\nvocabulary as well as any vocabulary known to the\\nterminology service that declares equivalences with\\nSNOMEDCT.\\nIn some cases, such as with rare diseases, even the\\nlimited communication with remote terminology services\\nmight affect data privacy, since the instantiation of cer-\\ntain very rare classes or predicates might in itself reveal\\nprivate data. In such cases a local terminology service can\\nbe employed, i.e. by creating a local deployment of the\\nLOV service or by providing local copies of the relevant\\nfull vocabularies. Nevertheless, sharing of the extracted\\nschema in such cases may still require additional consid-\\nerations.\\nUnfortunately, current implementations of federated\\nSPARQL queries still typically incur large performance\\npenalties by using suboptimal resolution strategies. As\\nsuch, in practice, it is often helpful tomanually decompose\\nthe single query into multiple query steps. An exem-\\nplary four-step approach using the SPARQL 1.1 UPDATE\\nconstruct [62, 63] can be found in the supplementary\\nFig. 3 Locally inferred relevant schema extracted from example 1\\nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 8 of 15\\nmaterials2, which also includes performance optimized\\nreformulations of the other queries.\\n\\002\\nWHERE {\\n?s ?x ?o.\\nOPTIONAL {?s a ?pDomain}\\nOPTIONAL {?o a ?pRange}\\n{?x (rdfs:subPropertyOf|owl:'</span>text7\n:   <span style=white-space:pre-wrap>'Grabar et al. Journal of Biomedical Semantics            (2020) 11:7 \\nhttps://doi.org/10.1186/s13326-020-00225-x\\nRESEARCH Open Access\\nCAS: corpus of clinical cases in French\\nNatalia Grabar1,2*\\u0086, Clément Dalloux3\\u0086 and Vincent Claveau3\\u0086\\nAbstract\\nBackground: Textual corpora are extremely important for various NLP applications as they provide information\\nnecessary for creating, setting and testing those applications and the corresponding tools. They are also crucial for\\ndesigning reliablemethods and reproducible results. Yet, in some areas, such as themedical area, due to confidentiality\\nor to ethical reasons, it is complicated or even impossible to access representative textual data. We propose the CAS\\ncorpus built with clinical cases, such as they are reported in the published scientific literature in French.\\nResults: Currently, the corpus contains 4,900 clinical cases in French, totaling nearly 1.7M word occurrences. Some\\nclinical cases are associated with discussions. A subset of the whole set of cases is enriched with morpho-syntactic\\n(PoS-tagging, lemmatization) and semantic (the UMLS concepts, negation, uncertainty) annotations. The corpus is\\nbeing continuously enriched with new clinical cases and annotations. The CAS corpus has been compared with\\nsimilar clinical narratives. When computed on tokenized and lowercase words, the Jaccard index indicates that the\\nsimilarity between clinical cases and narratives reaches up to 0.9727.\\nConclusion: We assume that the CAS corpus can be effectively exploited for the development and testing of NLP\\ntools and methods. Besides, the corpus will be used in NLP challenges and distributed to the research community.\\nKeywords: Medical area, Natural language processing, Corpus with clinical cases, Morpho-syntactic and semantic\\nannotation, Sustainability, Reproducibility\\nBackground\\nTextual corpora are central for various NLP applications\\nas they provide information necessary for creating, set-\\nting, testing and validating these applications, the cor-\\nresponding tools, and the results. Yet, in some areas,\\ndue to confidentiality or to ethical reasons, it is compli-\\ncated or even impossible to access representative textual\\ndata typically created and used by the actors of these\\nareas. For instance, medical and legal areas are concerned\\nwith these issues: in the legal area, information on law-\\nsuits and trials remains confidential, while in the medical\\narea, medical confidentiality must be respected by the\\nmedical staff. In both situations, personal data cannot\\nbe made publicly available, which prevents corpora from\\n*Correspondence: natalia.grabar@univ-lille.fr\\n\\u0086Natalia Grabar, Clé Dalloux and Vincent Claveau contributed equally to this\\nwork.\\n1CNRS, UMR 8163, F-59000 Lille, France\\n2Univ. Lille, UMR 8163 - STL - Savoirs Textes Langage, F-59000 Lille, France\\nFull list of author information is available at the end of the article\\nbeing released and makes experiments non-reproducible\\nby other researchers and with other methods. To face such\\nsituations, Natural Language Processing (NLP) proposes\\nspecific methods and tools. Hence, for several years now,\\nanonymization and de-identification methods and tools\\nhave been made available and provide competitive and\\nreliable results [1\\u00964] reaching up to 90% precision and\\nrecall. But it may still be difficult to access de-identified\\ndocuments and use them for research. One reason is that\\nthere is a risk of re-identification of people, and more\\nparticularly of patients [5, 6] because medical histories\\ncan be unique. In consequence, the application of de-\\nidentification tools on personal data often does not permit\\nto make the data freely available and usable within the\\nresearch context.\\nYet, there is a real need for the development of methods\\nand tools for several applications suited for such restricted\\nareas. For instance, in the medical area, it is impor-\\ntant to design suitable tools for information retrieval and\\nextraction, for recruiting patients for clinical trials, for\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nGrabar et al. Journal of Biomedical Semantics            (2020) 11:7 Page 2 of 10\\nperforming several other important tasks such as index-\\ning, study of temporality, negation, etc. [7\\u009613]. Another\\nimportant issue is related to the reliability of tools and\\nto the reproducibility of study results across similar data\\nfrom different sources. The scientific research and clin-\\nical communities are indeed increasingly coming under\\ncriticism for the lack of reproducibility in the biomedical\\narea [14\\u009616], but notice that, for instance, psychology is\\nconcerned with this issue as well [17\\u009619]. The first step\\ntowards the reproducibility of results is the availability of\\nfreely usable tools and corpora. In the current contribu-\\ntion, we are mainly concerned with the construction of\\nfreely available corpora for the medical domain. Yet, we\\nare aware that sharing tools and methods is also impor-\\ntant. We assume that availability of corpora may boost the\\ndesign and dissemination of other resources, methods and\\ntools for biomedical tasks and applications.\\nThe purpose of our work is to introduce the CAS cor-\\npus, that contains clinical cases in French such as those\\npublished in scientific literature or used in the education\\nand training of medical students. In what follows, we first\\npresent some existing studies onmedical corpora creation\\n(\\u0093Existing work: freely available clinical corpora\\u0094), high-\\nlighting corpora which are freely available for research.\\nWe then present the methods used for building, annota-\\ntion and analysis of the CAS corpus with clinical cases in\\nFrench (\\u0093Methods\\u0094). The results are presented in \\u0093Results\\u0094\\nand discussed in \\u0093Discussion\\u0094. We conclude with some\\ndirections for future work (\\u0093Conclusion\\u0094 sections). The\\nwork presented in this article is an extended and updated\\nversion of our previous publication [20].\\nExisting work: freely available clinical corpora\\nWithin the medical area, we can distinguish two main\\ntypes of medical corpora: scientific and clinical.\\n\\u0095 Scientific corpora are issued from scientific\\npublications and reporting. Such corpora are\\nbecoming increasingly available to researchers thanks\\nto recent and less recent initiatives dedicated to open\\npublication, such as those promoted by the NLM\\n(National Library of Medicine) through the PUBMED\\nportal1 and specifically dedicated to the biomedical\\narea, and by the HAL2 and ISTEX3 initiatives, which\\nprovide generic portals for accessing scientific\\npublications from various areas, including medicine.\\nSuch corpora contain scientific publications that\\ndescribe research studies: motivation, methods,\\nresults and issues on precise research questions.\\nOther portals may also provide access to scientific\\nliterature aimed at specific purposes, namely indexing\\n1https://www.ncbi.nlm.nih.gov/pubmed\\n2https://hal.archives-ouvertes.fr/\\n3https://www.istex.fr/\\nreliable literature, such as proposed by HON [21],\\nCISMEF [22], and other similar initiatives [23]. Some\\nexisting scientific corpora also provide annotations\\nand categorizations, such as PoS-tagging [24] and\\nnegation [25]. These are often built for the purposes\\nof shared tasks [26, 27].\\n\\u0095 Clinical corpora are related to hospital and clinical\\nevents of patients. Such corpora typically contain\\ndocuments that describe medical history of patients\\nand the medical care they are undergoing. This kind\\nof corpora is typically created and used in clinical\\ncontext as part of the healthcare process. Even after\\nde-identification, it is complicated to obtain free\\naccess to this kind of medical data and, for this\\nreason, there are very few clinical corpora freely\\navailable for research.\\nIn our work, we are mainly interested in clinical cor-\\npora: the proposed literature review of the existing work\\nis aimed at clinical corpora that are freely available for\\nresearch. We present here the main existing clinical cor-\\npora:\\n\\u0095 MIMIC (Medical Information Mart for Intensive\\nCare), now available in its third version, provides the\\nlargest available set of structured and unstructured\\nclinical data in English. MIMIC III is a single-center\\ndatabase comprising information pertaining to\\npatients admitted in critical care units at a large\\ntertiary care hospital. Those data include vital signs,\\nmedications, laboratory measurements, observations\\nand notes charted by care providers, fluid balance,\\nprocedure codes, diagnostic codes, imaging reports,\\nhospital length of stay, survival data, and more. The\\ndatabase supports applications including academic\\nand industrial research, quality improvement\\ninitiatives, and higher education coursework [28].\\nThose data are widely used by researchers, for\\ninstance for predicting mortality [29, 30], for\\ndiagnosis identification and encoding [31, 32], for\\nstudies on temporality [33] or for identifying similar\\nclinical notes [34], to cite just a few existing studies.\\nData from these corpora are also used in challenges,\\nsuch as i2b2, n2c2 and CLEF-eHEALTH.\\n\\u0095 i2b2 (Informatics for Integrating Biology and the\\nBedside)4 is an NIH-funded initiative promoting the\\ndevelopment and test of NLP tools for\\nEnglish-language documents with the purpose of\\nhealthcare improvement. In order to enhance the\\nability of NLP tools to process fine-grained\\ninformation from clinical records, i2b2 challenges\\nprovide sets of fully de-identified clinical notes\\nenriched with specific annotations [9, 11, 35], such as:\\n4https://www.i2b2.org/NLP/DataSets/Main.php\\nGrabar et al. Journal of Biomedical Semantics            (2020) 11:7 Page 3 of 10\\nde-identification, smoking status, medication-related\\ninformation, semantic relations between entities, or\\ntemporality. The clinical corpora and their\\nannotations built for the i2b2 NLP challenges are\\navailable now for general research purposes.\\n\\u0095 n2c2 (National NLP Clinical Challenges),5 held in\\n2018 and 2019, also address the processing of\\nEnglish-language clinical documents. These\\nchallenges are dedicated to other typical tasks when\\nhandling clinical documents: inclusion of patients in\\nclinical trials, detection of adverse-drug events,\\ncomputing of textual semantic similarity, concept\\nnormalization, and extraction of family history.\\n\\u0095 CLEF-eHEALTH challenges6 held in 2013 and 2014\\nprovide annotations for disorder detection and\\nabbreviation normalization. In 2016 the focus was on\\nstructuring Australian free-text nurse notes. Finally,\\nin 2016 and 2017 death reports in French, provided\\nby the CépiDc,7 have been processed for death cause\\nextraction.\\n\\u0095 eHealth-KD 2019 challenge8 targets human language\\nmodelling in a scenario in which electronic health\\ndocuments in Spanish could be machine readable\\nfrom a semantic point of view. The two proposed\\ntasks are: identification and classification of key\\nphrases, and detection of semantic relations between\\nthese key phrases.\\nFinally, medical data, close to those handled in the clini-\\ncal context, can be found in clinical trials protocols. One\\nexample is the corpus of clinical trials annotated with\\ninformation on numerical values in English [36], and on\\nnegation in French and Brazilian Portuguese [37, 38].\\nMethods\\nWe first describe the specificity of the sources and clinical\\ncases from which the CAS corpus was created (\\u0093Building\\nthe corpus\\u0094), then the annotation rationale (\\u0093Annotation\\nof the corpus\\u0094), and the principles of its comparison with\\nsimilar clinical narratives from Rennes University Hospi-\\ntal (\\u0093Comparison with clinical narratives\\u0094 sections).\\nBuilding the corpus\\nThe CAS corpus in French contains clinical cases as\\npublished in scientific literature, legal or training mate-\\nrial. Hence, it is built using material freely available\\nin online sources. The collected clinical cases are pub-\\nlished in different journals and websites from French-\\nspeaking countries in various continents. Those clinical\\n5https://n2c2.dbmi.hms.harvard.edu/\\n6https://sites.google.com/site/shareclefehealth/\\n7http://www.cepidc.inserm.fr/\\n8https://knowledge-learning.github.io/ehealthkd-2019\\ncases are related to various medical specialties (e.g. cardi-\\nology, urology, oncology, obstetrics, pulmonology, gastro-\\nenterology...).\\nThe purpose of clinical cases is to describe clinical sit-\\nuations for real de-identified or fake patients. Common\\nclinical cases are typically part of education programs\\nused for training medical students, while rare cases are\\nusually shared through scientific publications to illustrate\\nless common clinical situations. As for clinical cases which\\ncan be found in legal sources, they usually report on situ-\\nations which became complicated due to various reasons\\nemanating from different healthcare levels: medical doc-\\ntor, healthcare team, institution, health system and their\\ninteractions.\\nSimilarly to clinical documents, the content of clinical\\ncases depends on the clinical situations that are illustrated,\\nand on the disorders, but also on the purpose of the pre-\\nsented cases: description of diagnoses, treatments or pro-\\ncedures, evolution, family history, adverse-drug reactions,\\nexpected audience, etc.\\nData in published clinical cases are de-identified by the\\nauthors prior to their publication. Besides, publication\\nis usually done with the written permission of patients.\\nThe case reports can be related to any medical situa-\\ntion (diagnosis, treatment, procedure, follow-up...), to any\\nspecialty and to any disorder. The typical structure of\\nscientific publications with clinical cases starts by intro-\\nducing the clinical situation, then one or more clinical\\ncases are presented to support the situation. Schemes,\\nimaging, examination results, patient history, lab results,\\nclinical evolution, treatment, etc. can also be provided for\\nthe illustration of clinical cases. Finally, those clinical cases\\nare discussed. Hence, such cases may present an exten-\\nsive description of medical problems. Such publications\\ngather medical information related to clinical discourse\\n(clinical cases) and to scientific discourse (introduction'</span>text8\n:   <span style=white-space:pre-wrap>'RESEARCH Open Access\\nStructuring, reuse and analysis of electronic\\ndental data using the Oral Health and\\nDisease Ontology\\nWilliam D. Duncan1,2* , Thankam Thyvalikakath2,3, Melissa Haendel4, Carlo Torniai5, Pedro Hernandez6, Mei Song7,\\nAmit Acharya8, Daniel J. Caplan9, Titus Schleyer2,10\\u0086 and Alan Ruttenberg11\\u0086\\nAbstract\\nBackground: A key challenge for improving the quality of health care is to be able to use a common framework to\\nwork with patient information acquired in any of the health and life science disciplines. Patient information\\ncollected during dental care exposes many of the challenges that confront a wider scale approach. For example, to\\nimprove the quality of dental care, we must be able to collect and analyze data about dental procedures from\\nmultiple practices. However, a number of challenges make doing so difficult. First, dental electronic health record\\n(EHR) information is often stored in complex relational databases that are poorly documented. Second, there is not\\na commonly accepted and implemented database schema for dental EHR systems. Third, integrative work that\\nattempts to bridge dentistry and other settings in healthcare is made difficult by the disconnect between\\nrepresentations of medical information within dental and other disciplines\\u0092 EHR systems. As dentistry increasingly\\nconcerns itself with the general health of a patient, for example in increased efforts to monitor heart health and\\nsystemic disease, the impact of this disconnect becomes more and more severe.\\nTo demonstrate how to address these problems, we have developed the open-source Oral Health and Disease\\nOntology (OHD) and our instance-based representation as a framework for dental and medical health care\\ninformation. We envision a time when medical record systems use a common data back end that would make\\ninteroperating trivial and obviate the need for a dedicated messaging framework to move data between systems.\\nThe OHD is not yet complete. It includes enough to be useful and to demonstrate how it is constructed. We\\ndemonstrate its utility in an analysis of longevity of dental restorations. Our first narrow use case provides a\\nprototype, and is intended demonstrate a prospective design for a principled data backend that can be used\\nconsistently and encompass both dental and medical information in a single framework.\\n(Continued on next page)\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: wdduncan@gmail.com\\n\\u0086Titus Schleyer and Alan Ruttenberg are joint senior authors\\n1National Center for Ontological Research, Buffalo, NY, USA\\n2Center for Biomedical Informatics, Regenstrief institute, Inc., Indianapolis, IN,\\nUSA\\nFull list of author information is available at the end of the article\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 \\nhttps://doi.org/10.1186/s13326-020-00222-0\\n(Continued from previous page)\\nResults: The OHD contains over 1900 classes and 59 relationships. Most of the classes and relationships were\\nimported from existing OBO Foundry ontologies. Using the LSW2 (LISP Semantic Web) software library, we translated\\ndata from a dental practice\\u0092s EHR system into a corresponding Web Ontology Language (OWL) representation based\\non the OHD framework. The OWL representation was then loaded into a triple store, and as a proof of concept, we\\naddressed a question of clinical relevance \\u0096 a survival analysis of the longevity of resin filling restorations. We\\nprovide queries using SPARQL and statistical analysis code in R to demonstrate how to perform clinical research\\nusing a framework such as the OHD, and we compare our results with previous studies.\\nConclusions: This proof-of-concept project translated data from a single practice. By using dental practice data, we\\ndemonstrate that the OHD and the instance-based approach are sufficient to represent data generated in real-\\nworld, routine clinical settings. While the OHD is applicable to integration of data from multiple practices with\\ndifferent dental EHR systems, we intend our work to be understood as a prospective design for EHR data storage\\nthat would simplify medical informatics. The system has well-understood semantics because of our use of BFO-\\nbased realist ontology and its representation in OWL. The data model is a well-defined web standard.\\nKeywords: Ontology, Dental health, Informatics, Electronic heath record, OWL, SPARQL\\nBackground\\nA key challenge for improving the quality of healthcare\\nis to be able to use a common framework to work with\\npatient information acquired in any of the health and life\\nscience disciplines. The patient information collected\\nduring dental care exposes many of the challenges that\\nconfront a wider scale approach. Within dentistry, a key\\naspect for improving the quality of care is the ability to\\ncollect and analyze data about oral health conditions\\nand procedures, such as the longevity of fillings, the fre-\\nquency of patient checkups, and incidence of tooth loss.\\nRecent reports estimate that 73.8% of solo practitioners\\nand 78.7% of group practitioners in the U.S. use a com-\\nputer to manage some, and 14.3 and 15.9%, respectively,\\nall patient information on a computer [1] . In conse-\\nquence, we now have the opportunity to study dental\\nhealth services and perform outcomes research using\\nlarge amounts of secondary data obtained from geo-\\ngraphically dispersed dental practices [2].\\nLarge secondary datasets could help us more easily\\nstudy diseases in a sizable samples with increased statis-\\ntical power, track patients for an extended period of\\ntime, provide valid and representative samples, supply\\ncorrelates not commonly collected in an oral health set-\\nting, collect data in real time and ascertain potential\\nconfounders [2].\\nAnalyzing data from electronic health records (EHR),\\nhowever, presents a number of challenges. First, dental\\nEHR information is often stored in relational databases\\nthat are poorly documented and have complex relations\\nbetween tables. This makes extracting and analyzing\\ndata from even a single practice\\u0092s system difficult. Sec-\\nond, dental EHR database schemas vary depending on\\nthe vendor who developed the system. This adds diffi-\\nculty when integrating data from multiple practices.\\nThird, information is not always encoded in the same\\nway. For example, a tooth encoded as number (e.g.,\\ntooth \\u00916\\u0092) or as a character array in which the index pos-\\nition of a character represents the tooth (e.g., the \\u0091Y\\u0092 in\\nthe character array \\u0091NNNNNYNNNNNNNNNNNN\\nNNNNNNNNNNNNNN\\u0092 represents a right upper sec-\\nondary canine tooth, i.e., tooth \\u00916\\u0092). Last, dental EHR sys-\\ntems are typically only loosely specified. So, outside of a\\ncommon core of structures for the oral cavity and its\\nparts, there is wide variation in how information such as\\nspecific types of materials, details of methods, instru-\\nments, and general patient health is represented. Much\\nof this information is either semi-or unstructured text.\\nWhile we focus here on dental EHRs, these same prob-\\nlems are endemic in other EHR systems.\\nTo demonstrate how to address some of these problems,\\nwe have developed the Oral Health and Disease Ontology\\n(OHD) as a common framework for representing dental\\nhealth information embedded in a larger framework ad-\\nequate to accommodate structured representation that\\ngoes beyond that in current dental EHR systems and ex-\\ntends into general medicine. The OHD contains terms for\\nrepresenting anatomical structures (e.g., distal surface of\\ntooth), dental procedures (e.g., tooth extraction), and oral\\nconditions (e.g., caries), as well as relations between terms\\n(e.g., distal surface is part of tooth). The OHD\\u0092s structure\\nprovides a common representation of the entities that\\nEHR data is about, without being designed in a way that\\nunintentionally limits it to only dental health data. This\\nmakes it possible to use the OHD as framework for inte-\\ngrating inhomogeneous data from disparate database sys-\\ntems and support representations for future systems.\\nUsing the OHD\\u0092s terms and relations, information from\\nmultiple dental EHRs can now be translated into OWL 2\\n[3] statements, stored in a semantic database or triple\\nstore, and queried using SPARQL [4] to extract informa-\\ntion for analysis.\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 2 of 19\\nAs a proof of concept, we have translated dental EHR\\ndata from a single dental practice, and performed a sur-\\nvival analysis of the longevity of resin filling restorations.\\nThe proof of concept demonstrates both aspects of the\\nOHD that are specific to dentistry (e.g. teeth, restora-\\ntions and other procedures) as well as aspects that would\\nremain unchanged in a general medical context, such as\\ndemographic correlates. The output of this analysis is\\ndiscussed in the Results section. In the Discussion sec-\\ntion we describe the potential for wider application.\\nRelated work\\nThe work in this paper expands upon previous work de-\\nveloping the OHD [5, 6], and provides a more detailed\\nexplication of the OHD\\u0092s structure. It differs from previ-\\nous ontology work, such as PeriO [7] and BigMouth [8],\\nin two respects.\\nFirst, it focuses on the domain of dental anatomy and\\nprocedures rather than genomic information. Second,\\nthe OHD\\u0092s use of the Basic Formal Ontology and Ontol-\\nogy of Biomedical Investigations as an upper-level\\nframework sets the stage for seamlessly extending it to\\ngeneral medical information. Moreover, the OHD is not\\na data repository, such as BigMouth [8], but a semantic\\nframework for representing data that may be used in the\\ndesign of repositories \\u0096 such as our semantic\\ntechnology-based repository of information translated\\nfrom (for now) a single dental practice.\\nWe considered using SNOMED and its dental subset\\nSNODENT, but there are problems that make these\\nstandards, at the moment, unusable for our purposes.\\nFirst, their licenses restrict modification of substantial\\nparts of the standard. This prevents us from reorganiz-\\ning content according to realist principles, adding defini-\\ntions, or adding or correcting axioms. Not all countries\\nlicense to use SNOMED, and this would prevent our\\nwork from being replicable worldwide.\\nSecond, there are serious quality issues with\\nSNOMED, and SNODENT in particular [9]. A major\\nissue is the question of ontological commitment \\u0096 what\\nterms mean. The vast majority of terms in SNODENT\\nand SNOMED come without textual definitions [10],\\nand the question of what SNOMED terms actually rep-\\nresent is still up for debate.\\nThird, use of these terminologies typically is within a\\nlayered framework that brings unnecessarily complica-\\ntion [11]. In common usage these resources are bound\\nto data models of medical records [12]. That means that\\none needs to separately understand the data models and\\nthe ontology. By contrast, in our approach the ingredi-\\nents for a representation are simple \\u0096 an OWL ontology\\nand high-quality SPARQL, OWL and RDF W3C specifi-\\ncations. Those logic-based specifications are substan-\\ntially clearer than HL7 specifications.\\nThe Open Biological and Biomedical Ontology (OBO)\\nFoundry approach is to have, for any given class, a single\\nidentifier, if necessary coordinating with developers of\\nother ontologies. The realism-based approach empha-\\nsizes that classes are collections of instances, that the in-\\nstances are things in the world, and that documentation\\nshould make clear what those instances are. The OHD\\nand the semantic technologies used to implement the\\nontology make it relatively easy to merge data. The data\\nis just added together, untransformed. It is possible to\\ndo this, in theory, because all parts of the representation\\nare clearly understood, the types of entities are shared,\\nand the choice to represent particulars using the stand-\\nard methods provided by semantic web standards allow\\nfor little creativity in how concrete representations are\\nconstructed. Because our focus is on showing how a uni-\\nfied representation system works, we consider out of\\nscope general methods for harmonizing or interchanging\\ndata with different representations, as is the focus of\\nHL7.\\nRecently, authors AR and WD have started participat-\\ning in the review and development of SNODENT. It is\\nentirely possible that in the future that SNOMED and\\nSNODENT might be used in the same manner that we\\nuse OHD here. The OHD and the source code used for\\ntranslation and analysis are available in full at https://\\ngithub.com/oral-health-and-disease-ontologies/ohd-\\nontology and in part in the Additional file 1.\\nMethods\\nOntology development\\nThe OHD was developed in a collaborative effort be-\\ntween dental researchers, practicing dentists, statisti-\\ncians, informatics experts, and ontologists. Our first task\\nwas to identify which dental entities would be repre-\\nsented. To guide this process, we developed a set of re-\\nsearch questions. For example, for the research question,\\n\\u0093What is the time from one restoration to its replace-\\nment on the same tooth?\\u0094, we determined that we would\\nneed to represent restoration procedures, the dates of\\nthe procedures, patients, patients\\u0092 teeth, surfaces of\\nteeth, and the restoration materials used to restore teeth.\\nWe provide the list of driving research questions in\\nAdditional file 1.\\nOnce our domain of focus was identified, our next\\nstep was to catalog the terms1 we would need in the\\nontology. We imported the Basic Formal Ontology\\n(BFO) and the Ontology for General Medical Science\\n(OGMS) as a whole and otherwise extracted terms from\\nexisting OBO Foundry ontologies that represented en-\\ntities relevant to our dental health domain using custom\\n1In this paper, we use the word \\u0091term\\u0092 as a unique natural language\\nexpression for a class, instance, or relation in our ontology.\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 3 of 19\\nprograms as well as the OntoFox2 web tool [13]. The\\nOntoFox tool implements the Minimum Information to\\nReference an External Ontology Term (MIREOT)\\nprinciple [14]. MIREOT is a practice by which one im-\\nports a selected set of terms from another ontology ra-\\nther than including the whole ontology, as importing in\\nOWL would do. Where relevant terms were not present\\nin an existing ontology, we created new terms. Each new\\nterm was assigned an Internationalized Resource Identi-\\nfier (IRI) [15], a human-readable label, a definition or\\ndocumentation, and the name of the term\\u0092s editor(s).\\nWhen appropriate, other metadata was included, such as\\nthe reference source for a definition and comments\\nabout a term\\u0092s definition such as its rationale, scope, and\\nusage. Throughout the ontology development process,\\nthe definitions were reviewed multiple times by team\\nmembers. In the following sections, we discuss the\\nmethods for acquiring the necessary terms.\\nOntology architecture\\nThe OHD is constructed in line with a number of OBO\\nFoundry principles. The OBO Foundry [16] is a collect-\\nive of ontology developers who are committed to collab-\\noration and adherence to shared principles. The mission\\nof the OBO Foundry is to develop a family of interoper-\\nable ontologies that are both logically well-formed and\\nscientifically accurate. OBO Foundry principles include\\nuse of the Basic Formal Ontology (BFO) [17], an upper-\\nlevel ontology, use of a standard IRI identifier space, re-\\nuse, where possible, of other Foundry ontologies, and\\nthe inclusion of a textual and, where feasible, logical def-\\ninition for each class and relation.\\nOntology reuse\\nThe OHD uses BFO as its upper-level ontology. BFO is de-\\nsigned as a domain-independent ontology based on princi-\\nples of ontological realism [18] As an upper-level ontology,\\nBFO establishes categories such as material entities, pro-\\ncesses, time, space, and realizable entities (properties), as well\\nas relations among them, such as the relation between a par-\\nticipant and a process they participate in.\\nWe reuse a number of classes and relations from exist-\\ning OBO Foundry ontologies, such as the Foundational\\nModel of Anatomy (FMA) [19] and the Ontology for\\nBiomedical Investigations (OBI) [20].\\nThis construction methodology serves two purposes.\\nFirst, it allows us to leverage the experience of the devel-\\nopers of OBO ontologies. Second, adhering to OBO\\nstandards and precedents makes the OHD more easily\\ninteroperable with other OBO ontologies [16], and this\\nallows developers to reuse our classes and provide\\nfeedback on how to improve the OHD. A summary of\\nthe reused ontologies is provided in Table 1.\\nClasses from the ontologies listed in Table 1 are then\\nextended to encompass entities in the oral health do-\\nmain. At present, this includes classes for representing\\nteeth and tooth surfaces, dental procedures, patients,\\nproviders, restoration materials, dental findings, and bill-\\ning codes. Each of these classes is discussed in the fol-\\nlowing sections.\\nAnatomical structures\\nWe use the FMA\\u0092s classes to represent anatomical struc-\\ntures, such as jaws, teeth, and tooth roots. However, in\\nour initial construction of the OHD, we found that the\\nFMA was not adequate for representing surfaces of\\nteeth. The FMA\\u0092s class surface of tooth is used to repre-\\nsent the two-dimensional curved plane that forms the\\nouter boundary of a tooth. This is not suitable for repre-\\nsenting the portions of enamel into which restoration\\nmaterial is placed. Thus, we added the class surface en-\\namel of tooth to represent the portions of enamel that\\nconstitute a tooth\\u0092s anatomical crown. The need for this\\nclass was reported to the FMA\\u0092s curators, and the FMA\\nnow includes the class surface layer of tooth3 to address\\nthis.\\nUntil recently, the FMA was authored in a representa-\\ntion system called Protégé Frames. In order to use it\\nwithin the OBO framework we needed to translate from\\nthe native frames version to a version that integrates\\nwith OBO ontologies. As part of that translation, classes\\nin FMA were placed as children of the appropriate BFO\\nor OBO classes. Second, we needed to translate the\\nframes expressions [25] to OWL before we could use it\\nwith the other classes OBO classes.\\n2http://ontofox.hegroup.org 3http://purl.org/sig/ont/fma/fma290055 (accessed August 2018)\\nTable 1 Summary of ontology reuse in OHD\\nOntology Classes/relations reused or specialized\\nBasic Formal Ontology (BFO) upper-level ontology used to\\ncoordinate other OBO ontologies\\nOntology for General Medical\\nScience (OGMS) [21]\\nhealth care entities; e.g., patient role,\\nvisit, disorder\\nFoundational Model of\\nAnatomy (FMA)\\nanatomical entities; e.g., jaw, tooth,\\ntooth surface\\nOntology for Biomedical\\nInvestigations (OBI)\\nrelations between processes to\\nentities; e.g., restoration procedure has\\nspecified input some tooth\\nInformation Artifact Ontology\\n(IAO) [22]\\ninformation entities in the dental\\nhealth care domain; e.g., billing codes,\\ngoals of dental procedures\\nOntology of Medically Related\\nSocial Entities (OMRSE) [23]\\ngender of patient\\nCommon Anatomy Reference\\nOntology (CARO) [24]\\nmale and female organism\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 4 of 19\\nPatients and health care providers\\nA given dental procedure (such as an oral evaluation or\\nrestoration procedure) will minimally involve a patient\\nand the dental health care provider. We define for the\\npatient and health care provider roles that characterize\\nthe way in which patients and providers participate in\\ndental procedures.\\nIn BFO, roles are realizable entities which are in\\nturn dependent entities. A dependent entity is one\\nthat cannot exist unless the entity bearing the role\\nexists. For example, a particular patient role cannot\\nexist unless the organism that bears the role (i.e., the\\npatient) exists. A role is optional in the sense that an\\nentity may gain or lose a role without its physical\\nmakeup being changed. For instance, a person may\\ncease to be patient at some practice due to the prac-\\ntice going out of business. The practice\\u0092s going out of\\nbusiness is an event that is external to the person,\\nand, thus, does not necessitate that the person is\\nsomehow physically changed. Roles are realizable in\\nthe sense that their existence can be manifested in a\\ncorrelated process. For instance a dental hygienist role\\nis realized when the hygienist engages in processes\\nrelated to their profession, such as plaque removal\\nand application of fluoride treatment. Roles and other\\ndependent continuants inhere, or are borne by, mater-\\nial entities.\\nEmploying this distinction between roles and their\\nbearers, we define the types dental health care provider\\nand human dental patient by first defining the appropri-\\nate roles for each kind of entity, and then defining pro-\\nviders and patients as being bearers of the roles4:\\nA dental health care provider role is a role that\\ninheres in a person who is licensed to provide den-\\ntal health care and is realized in a health care\\nprocess.\\nA dental health care provider is a human being who\\nbears a dental health care provider role.\\nA patient role is a role that inheres in a person and\\nis realized by the process of being under the care of\\na physician or health care provider. (OGMS)\\nA dental patient role is a patient role that is realized\\nby the process of being under the care of a dental\\nhealth care provider.\\nA human dental patient is a human being who\\nbears a dental patient role.\\nIn order to define the patient\\u0092s gender, we use the gen-\\nder role types from the Ontology of Medically Related\\nSocial Entities (OMRSE). The OMRSE is a realist repre-\\nsentation of medically related social entities developed\\nto cover demographics data and common roles of people\\nin healthcare encounters for reuse in the context of the\\nOBO Foundry. The gender role types are defined as\\nfollows:\\nA gender role is a human social role borne by a hu-\\nman being that is realized in behavior which is con-\\nsidered socially appropriate for individuals of a\\nspecific sex in the context of a specific culture.\\n(OMRSE)\\nA female gender role is a gender role borne by a hu-\\nman being that is realized in behavior which is con-\\nsidered socially appropriate for individuals of the\\nfemale sex in the context of the culture in question.\\n(OMRSE)\\nA male gender role is a gender role borne by a hu-\\nman being that is realized in behavior which is con-\\nsidered socially appropriate for individuals of the\\nmale sex in the context of the culture in question.\\n(OMRSE)\\nFemale and male dental patients are then simply de-\\nfined by relating the patient to the female and male gen-\\nder roles:\\nA female dental patient is a human dental patient\\nwho bears a female gender role.\\nA male dental patient is a human dental patient\\nwho bears a male gender role.\\nUsing roles to define patients and dental health care\\nproviders has two advantages. First, because roles are\\nformally defined, they represent the semantics for how\\nan entity participates in a procedure. That is, for a given\\ndental procedure, the patient participant is the entity\\nwhose participation realizes the dental patient role, and\\nthe provider participant is the entity whose participation\\nrealizes the dental health care provider role. In contrast,\\nfield names and values in relational databases are purely\\nsyntactic.\\nSecond, by using gender roles instead of anatomical\\nsex to represent male and female dental patients, we\\nallow for the possibility that the gender a patient assigns\\nto himself or herself may differ from the patient\\u0092s ana-\\ntomical sex (at birth), matching the common practice of\\nrecording patient-reported gender in clinical systems. In\\nthose cases in which biological sex needs to be4Classes/relations are defined in the OHD unless indicated otherwise.\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 5 of 19\\nrepresented, the OHD includes CARO\\u0092s types female or-\\nganism and male organism:\\nA female organism is a gonochoristic organism that\\ncan produce female gametes. (CARO)\\nA male organism is a gonochoristic organism that\\ncan produce male gametes. (CARO)\\nUsing these classes, a patient\\u0092s biological sex can then\\nbe defined according to biological criteria rather than\\ngender selection.\\nDental procedures\\nWe define the class dental procedure as a subclass of\\nOGMS\\u0092 health care encounter class:\\nA health care encounter is a temporally-connected\\nhealth care process that has as participants an\\norganization or person realizing the health care pro-\\nvider role and a person realizing the patient role.\\nThe health care provider role and patient role are\\nrealized during the health care encounter. (OGMS)\\nA dental procedure is a health care encounter that\\nrealizes a dental patient role in which the patient\\nundergoes a diagnostic or therapeutic process.\\nAs illustrated in Fig. 1, specific dental procedures are\\nthen defined by specializing the dental procedure class.\\nFor instance, endodontic procedure, surgical dental pro-\\ncedure, and tooth restoration procedure are defined as\\nfollows:\\nAn endodontic procedure is a dental procedure that\\nis performed on the pulp chamber and/or root canal\\nof a tooth, or a part thereof.\\nA surgical dental procedure is a dental procedure in\\nwhich there is structural alteration of soft tissue or\\nbone in or around the oral cavity by incision or\\ndestruction of tissues or by manipulation with in-\\nstruments causing localized alteration or transporta-\\ntion of tissue, including lasers, ultrasound, ionizing\\nradiation, scalpels, probes, and needles.\\nA tooth restoration procedure is dental procedure in\\nwhich either a whole tooth or a part of a tooth is re-\\nplaced by dental restoration material in order to re-\\nestablish the tooth\\'s anatomical and functional form\\nand function.\\nMore specific surgical and restoration procedures are\\nthen defined as subclasses of these terms. For example, a\\nnon-exhaustive set of surgical and restorative procedures\\ndefined in the OHD include:\\nA tooth extraction procedure is a surgical dental\\nprocedure that removes a tooth from the oral cavity.\\nA crown restoration procedure is a tooth restoration\\nprocedure whereby an artificial crown replaces all or\\npart of the natural dental crown.\\nA direct restoration procedure is a tooth restoration\\nprocedure in which the dental restoration material is\\nplaced in the tooth via some direct dental material\\ninsertion process.\\nAn indirect restoration procedure is a tooth restor-\\nation procedure in which the dental restoration ma-\\nterial is placed in the tooth via some dental material\\ntooth attachment process.\\nAn intracoronal restoration procedure is a tooth res-\\ntoration procedure in which a dental restoration ma-\\nterial is placed into a site that is located in the\\ncrown of the tooth.\\nA veneer restoration procedure is a tooth restoration\\nprocedure in which a thin layer of material (i.e., a\\nveneer) is placed over one or more surfaces of the\\nFig. 1 A portion of the hierarchy of health care encounters in OHD. Numbers represent the number of direct subclasses for a class, some not\\nshown for reasons of space\\nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 6 of 19\\ntooth for purposes such as improving the aesthetics\\nof the tooth or protecting the tooth\\'s surface from\\ndamage.\\nPatients and providers are related to dental procedures\\nusing BFO\\u0092s has participant and realizes relations. The\\nhas participant relation is a general way of relating pro-\\ncesses to the entities involved in them. For example, an\\noral evaluation (minimally) has as participants the pa-\\ntient undergoing the evaluation and the provider doing\\nthe evaluation. The realizes relation holds between a\\nprocess and a realizable entity such as a role. A role is\\ndefined in terms of what bears it, what process realizes\\nit, and the manner in which the bearer participates in\\nthe process. As an example, consider the aforementioned\\ndental patient role and dental health care provider role.\\nWhen a dental procedure is performed, the procedure\\nrealizes the roles of the patient and provider. The person\\nupon whom the procedure is performed acts (or be-\\nhaves) as the dental patient and the person doing the\\nprocedure acts (or behaves) as the provider. In this way,\\nthe dental procedure realizes the dental patient role of\\nthe patient and the dental health care provider role of\\nthe provider.\\nBFO defines temporal regions and a relation occupies\\ntemporal region that defines the temporal span of a\\nprocess. However, we don\\u0092t use this representation for\\ntwo reasons. First, there isn\\u0092t yet an established OBO\\npractice for specifying concrete dates. Second, the time\\nof a procedure is recorded only to the granularity of a\\nday. Pending development of representations that ac-\\ncommodate these issues, we defined a date property, oc-\\ncurrence date, that relates a process to an xsd:dateTime\\nsome time during which the process occurred. Another\\ndata property birth date was defined to relate a patient\\nto their date of birth.\\nTo characterize the way in which a tooth participates\\nin a specific dental procedure, we define roles that are\\nborne by the tooth and realized in the appropriate corre-\\nsponding procedure. For example, in order to represent\\nthat a tooth undergoes a root canal treatment, we specify\\nthat a tooth bears a particular tooth to undergo endodon-\\ntic procedure role and this role is then realized in a par-\\nticular endodontic procedure.\\nFor procedures that involve restorative materials, we de-\\nfine a dental restoration material role that is borne by (i.e.\\npossessed by) the restoration material. The role helps de-\\nfine the material in a domain-neutral way. All gold is\\nmetal, but not all gold is used in dental restorations, just\\nthose that bear the dental restoration material role.\\nThis role is then realized by the corresponding restor-\\nation procedure. For instance, an intracoronal restor-\\nation procedure (see above) realizes the dental\\nrestoration material role of the material that is placed\\ninside the crown of the tooth. In procedures that involve\\na specific kind of material, we use OBI\\u0092s has_specified_\\ninput relation to express that a procedure uses that ma-\\nterial. For example, an amalgam filling restoration is de-\\nfined as follows:\\nAn amalgam filling restoration is an intracoronal\\nrestoration procedure that uses amalgam to restore\\nthe tooth.\\nAs part of the logical framework of the OHD, we then\\ninclude the axiom that an amalgam filling restoration\\nhas_specified_input some portion of amalgam restor-\\nation material.\\nRestoration materials, restored teeth, and prosthetics\\nFor dental procedures that involve the use of restoration\\nmaterials (e.g., amalgam), we define the restoration ma-\\nterials in terms of the role the material has in replacing\\nportions of the tooth. In general, dental restoration ma-\\nterial has the role of serving as a prosthetic, that is, the\\nmaterial has the role of replacing a missing body part.\\nHowever, not all prosthetics replace the function of the\\nmissing body part, for example, a prosthetic eye cannot\\nsee, although it still functions to maintain the shape of\\nthe skull near the eyes. To address this, we define the\\nterm functional prosthetic role to represent a prosthetic\\nthat performs the function of the replaced body part.\\nSince dental restoration materials perform the function\\nof parts of the tooth they replace, we define dental res-\\ntoration material role as a subtype of functional pros-\\nthetic role:\\nA functional prosthetic role is a prosthetic role that\\nis realized by activities in which the material entity\\n(bearing the role) is used a manner that is similar to\\nhow the body part that the prosthesis replaces\\nwould be used.\\nA dental restoration material role is a functional\\nprosthetic role that is borne by a portion of dental\\nrestoration material and is realized in a tooth restor-\\nation procedure in which the restoration material\\nbecomes part of a restored tooth.\\nFunctional prosthetic role is not a term that is sp'</span>text9\n:   <span style=white-space:pre-wrap>'RESEARCH Open Access\\nIdentifying disease trajectories with\\npredicate information from a knowledge\\ngraph\\nWytze J. Vlietstra1* , Rein Vos1,2, Marjan van den Akker3,4, Erik M. van Mulligen1 and Jan A. Kors1\\nAbstract\\nBackground: Knowledge graphs can represent the contents of biomedical literature and databases as subject-\\npredicate-object triples, thereby enabling comprehensive analyses that identify e.g. relationships between diseases.\\nSome diseases are often diagnosed in patients in specific temporal sequences, which are referred to as disease\\ntrajectories. Here, we determine whether a sequence of two diseases forms a trajectory by leveraging the predicate\\ninformation from paths between (disease) proteins in a knowledge graph. Furthermore, we determine the added\\nvalue of directional information of predicates for this task. To do so, we create four feature sets, based on two\\nmethods for representing indirect paths, and both with and without directional information of predicates (i.e.,\\nwhich protein is considered subject and which object). The added value of the directional information of predicates\\nis quantified by comparing the classification performance of the feature sets that include or exclude it.\\nResults: Our method achieved a maximum area under the ROC curve of 89.8% and 74.5% when evaluated with\\ntwo different reference sets. Use of directional information of predicates significantly improved performance by 6.5\\nand 2.0 percentage points respectively.\\nConclusions: Our work demonstrates that predicates between proteins can be used to identify disease trajectories.\\nUsing the directional information of predicates significantly improved performance over not using this information.\\nKeywords: Knowledge graph, Disease trajectories, Predicates, Temporal relationships, Directionality of predicates,\\nProtein-protein interactions\\nBackground\\nKnowledge graphs can be used to represent the biomed-\\nical knowledge published in literature and databases [1].\\nKnowledge is formalized as subject-predicate-object tri-\\nples, where pairs of entities are related to each other by\\npredicates [2]. By integrating triples from a variety of\\nsources, knowledge graphs can be used to perform com-\\nputational analyses on the comprehensive body of bio-\\nmedical knowledge [3]. Previous work has used such\\nanalyses to identify new relationships between pairs of\\nentities, e.g., between drugs and diseases [4, 5], genes\\nand phenotypes [6, 7], or between diseases [8, 9].\\nMuch research has been performed with knowledge\\ngraphs that only consist of proteins, commonly referred\\nto as protein-protein interaction networks. Through the\\ninvolvement of proteins in metabolic, signaling, immune,\\nand gene-regulatory networks, protein-protein inter-\\naction networks can help to mechanistically explain dis-\\nease and physiological processes [10\\u009612]. Even though\\npredicates further specify the types of interactions be-\\ntween proteins, thereby providing additional information\\nthat can be analyzed, protein-protein interaction net-\\nworks usually do not use them. Instead, most methods\\nanalyze the network topology of proteins [12]. However,\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: w.vlietstra@erasmusmc.nl\\n1Department of Medical Informatics, Erasmus University Medical Center, Dr.\\nMolewaterplein 50, 3015 GE Rotterdam, the Netherlands\\nFull list of author information is available at the end of the article\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 \\nhttps://doi.org/10.1186/s13326-020-00228-8\\nwe have recently shown that analyses that are performed\\non protein knowledge graphs benefit from predicate in-\\nformation [13].\\nBy using the predicates that specify the mechanisms\\nby which proteins interact, temporal pathobiological re-\\nlationships may also be identified, although this has not\\nbeen demonstrated yet. A key application for such tem-\\nporal analyses is the identification of disease trajectories,\\nwhich are commonly occurring temporal sequences of\\ndiseases diagnosed in patients [14, 15]. An example of a\\ndisease trajectory found in a study by Jensen et al. [14] is\\nrheumatoid arthritis-precedes-heart failure, where pre-\\ncedes is defined as \\u0093occurs earlier in time. [\\u0085]\\u0094 [16]. The\\noccurrence of the reverse, heart failure-precedes-\\nrheumatoid arthritis, was found to occur significantly\\nless frequently in the same study, and therefore was not\\nclassified as a trajectory.\\nIdentifying relationships between diseases is an im-\\nportant and popular research topic for protein-protein\\ninteraction networks (see Related work section). In such\\nanalyses diseases are represented by so-called disease\\nproteins, which are proteins encoded by genes that are\\nassociated with a disease [17, 18]. Often cited benefits\\ninclude an improved understanding of the biological\\nmechanisms underlying disease interactions [8, 19, 20],\\nand the ability to anticipate the next disease, thereby\\nproviding the knowledge necessary to improve treatment\\nplans and interventions [14, 21]. However, the temporal\\naspects of relationships between diseases still require\\nfurther investigation. We therefore aim to automatically\\ndetermine whether a given sequence of two diseases\\nforms a trajectory. We do so by leveraging the predicate\\ninformation from paths between (disease) proteins in a\\nknowledge graph. We also determine whether there is\\nadded value in using directional information of predi-\\ncates for this task.\\nRelated work\\nPrevious authors have mostly focused on identifying un-\\ndirected relationships between diseases with protein net-\\nworks [19\\u009623]. For example, Kontou et al. created a\\ndisease-disease graph, where an edge between diseases\\nindicated that they shared at least one disease gene [23].\\nSun et al. calculated the similarity between diseases\\nbased on their shared disease proteins, shared physio-\\nlogical processes associated with these proteins, or the\\ngraph structures between the proteins [20]. Li and Agar-\\nwal identified which biological pathways were associated\\nwith diseases via their disease proteins, and identified re-\\nlationships between diseases based on the number of\\nshared pathways [19]. Menche et al. identified so-called\\ndisease modules, which are clusters of closely interre-\\nlated disease proteins [22]. They found that short dis-\\ntances between the modules of diseases were predictive\\nfor pathobiological relationships. Contrary to Kontou\\net al., they demonstrated that sharing disease proteins is\\nnot a requirement for diseases to be related to each\\nother.\\nTo our knowledge, Bang et al. were the only ones to\\nuse a directed protein-protein interaction network to\\nidentify disease trajectories [21]. The disease proteins of\\npairs of diseases were used to identify shared biomolecu-\\nlar pathways, after which the locations of the disease\\nproteins within these pathways were determined. The\\ndisease with most upstream disease proteins was classi-\\nfied as the first within the sequence of diseases. Add-\\nitionally, 13 million Medicare records were used to\\ncalculate two relative risk scores for each pair of dis-\\neases, corresponding with the two possible temporal se-\\nquences of the disease pair. If the sequence determined\\nwith the protein pathways concurred with the sequence\\nthat generated the largest relative risk, that sequence\\nwas identified as a trajectory. Between a total of 2604\\ndiseases, their method suggested 61 trajectories. These\\nwere evaluated with the biomedical literature, where fur-\\nther leads were found for 16 of them. Because the au-\\nthors only evaluated the trajectories that were suggested\\nby their method, it is unclear how many trajectories the\\nmethod failed to identify.\\nMaterials &amp; methods\\nReference sets\\nThe ability of our method to identify disease trajectories\\nwas evaluated with two reference sets, which have iden-\\ntified disease trajectories by different means. The first\\nreference set consisted of statistically-derived disease tra-\\njectories from a large retrospective study of Danish hos-\\npital data, while the second set consisted of literature-\\nvalidated disease trajectories that were based on a small\\nprospective study of Dutch general-practitioner data.\\nJensen reference set\\nThe first reference set was based on a study of Jensen\\net al. [14]. They retrospectively identified 4014 disease\\ntrajectories from 6.2 million electronic patient records of\\nDanish hospitals based on diagnoses assigned over 14.9\\nyears. All diagnoses in these patient records were repre-\\nsented as International Statistical Classification of Dis-\\neases and Related Health Problems 10th Revision (ICD-\\n10) codes. Jensen used the hierarchy within the ICD-10\\nto aggregate all diagnoses to a high abstraction level,\\nresulting in 681 two-digit codes, such as \\u0093Malignant neo-\\nplasm of breast\\u0094 (C50) or \\u0093Type 2 diabetes mellitus\\u0094\\n(E11).\\nJensen derived the disease trajectories from the Danish\\nhospital data in a two-step process. First, they identified\\nsequences of two diseases that were diagnosed within 5\\nyears from each other in at least 10 patients, and which\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 2 of 11\\nhad a relative risk higher than 1. Subsequently, the direc-\\ntion of each sequence had to be corroborated by a bino-\\nmial test that compared the frequency of the sequence to\\nthe frequency of its reversed sequence. Sequences that ful-\\nfilled both criteria were classified as disease trajectories.\\nTo represent the diseases in the Jensen set on the pro-\\ntein level, we used the expert-annotated associations be-\\ntween proteins and diseases from the manually curated\\nsubset of DisGeNet [18]. The Unified Medical Language\\nSystem (UMLS) MRCONSO table was used to map the\\nICD-10 codes of the Jensen trajectories to the UMLS\\nidentifiers that are used in DisGeNet. Two diseases, \\u0093Ac-\\ncidental poisoning by and exposure to other gases and\\nvapours\\u0094 (E47) and \\u0093Influenza due to identified zoonotic\\nor pandemic influenza virus\\u0094 (J09), were lost because\\ntheir ICD-10 codes could not be mapped to a UMLS\\nidentifier. Because only 25% of the high-level diseases in\\nthe Jensen set were represented within DisGeNet, we\\nused the \\u0093narrower\\u0094 and \\u0093child\\u0094 relationships from the\\nUMLS MRREL table to identify subclasses of all diseases.\\nBy expanding the diseases with their subclasses, the per-\\ncentage of diseases to which disease proteins could be\\nassigned was increased to 68% (465 of 679 diseases).\\nFrom the 4014 disease trajectories in the Jensen set,\\nthere were 2530 trajectories where disease proteins\\ncould be assigned to both diseases (63%). These 2530\\ntrajectories, which were used as positive cases in this ref-\\nerence set, contained 453 of the 465 diseases to which\\ndisease proteins could be assigned (97%). On average,\\ndiseases had 90 disease proteins assigned to them (me-\\ndian: 29, interquartile range: 7\\u009694). Disease proteins\\nwere on average assigned to 6.2 diseases (median: 3,\\ninterquartile range: 2\\u00968).\\nA set of 168,870 non-trajectories was constructed by\\ncreating all possible sequences of the 453 included dis-\\neases, minus the trajectories that were described by Jen-\\nsen. The set of non-trajectories thereby included\\nrandom pairs of diseases, the reversed temporal se-\\nquences of these random pairs, as well as the reversed\\ntemporal sequences of the trajectories. In the following,\\nwe will refer to the trajectories and non-trajectories as\\npositive and negative cases to align with common ter-\\nminology in the machine learning field.\\nVan den Akker reference set\\nThe second reference set was based on a prospective co-\\nhort study on disease susceptibility by Van den Akker\\net al. [24]. They followed a Dutch cohort of 3460\\npatients over 2 years, during which their general practi-\\ntioner notes were examined for sequences of Inter-\\nnational Classification of Primary Care (ICPC) codes\\nthat represent chronic, permanent, and recurrent dis-\\neases. In the Netherlands, each citizen is registered with\\na general practitioner, who acts like a gatekeeper for\\nsecondary and tertiary medical care, and is responsible\\nfor maintaining a complete medical history of the\\npatient.\\nA total of 473 unique sequences of diseases were\\nfound in this cohort, containing 122 distinct diseases.\\nEach sequence was manually evaluated using the pub-\\nlished biomedical literature and medical handbooks.\\nThere were 65 sequences of diseases where the literature\\nstated that the first disease increased the susceptibility of\\nacquiring the second disease, and 408 sequences where\\nno evidence of increased susceptibility was found. To\\nmaintain consistent terminology, we will refer to se-\\nquences with increased susceptibility as trajectories or\\npositives and to sequences without increased susceptibil-\\nity as non-trajectories or negatives.\\nTo assign disease proteins to these 122 diseases we\\nfollowed the same procedure as for the Jensen set by\\nusing the MRCONSO table to map the ICPC codes to\\nUMLS identifiers, after which the MRREL table was used\\nto group them with their subclasses. Disease proteins\\ncould be assigned to 97 diseases, which formed 55 tra-\\njectories and 316 non-trajectories. On average, diseases\\nhad 137 disease proteins assigned to them (median: 49,\\ninterquartile range: 17\\u0096167). Disease proteins were on\\naverage assigned to 3 diseases (median: 2, interquartile\\nrange: 1\\u00964).\\nTo determine whether our method could also identify\\nthe correct temporal sequence of the trajectories, 54\\nadditional non-trajectories were created by reversing the\\nsequence of the diseases in the literature-supported tra-\\njectories (the reverse sequence of one trajectory was\\nalready included as a non-trajectory in the data from the\\ngeneral practitioners).\\nKnowledge graph\\nThe predicates between proteins were extracted from\\nthe Euretos Knowledge Platform (EKP), a commercially\\navailable knowledge graph (http://www.euretos.com). In\\nthe EKP, information from more than 200 knowledge\\nsources from a wide variety of domains in the life sci-\\nences is represented as triples. The biomedical entities\\nsuch as proteins, drugs, or diseases that form the sub-\\njects and objects of these triples are represented in the\\nknowledge graph as vertices, each of which has one or\\nmore identifiers associated with it from external data-\\nbases. Mappings between the entities described in the\\ndifferent knowledge sources underlying the knowledge\\ngraph were made by matching their identifiers. The\\npredicate and provenance of each triple are specified as\\npart of an edge between the two vertices that represent\\nthe subject and object. The direction of the predicate\\ngoes from subject to object. The predicates in the under-\\nlying knowledge sources were matched to a standardized\\nset of 203 predicates. If an exact match was not\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 3 of 11\\navailable, a predicate was manually mapped. If there\\nwere no explicit predicates in a database that was used\\nas a knowledge source, the predicates were manually de-\\nrived from the database schema. A path between two\\nvertices is defined as a sequence of triples, or possibly a\\nsingle triple, connecting the vertices.\\nThe contents of the EKP are represented as documents\\nin a NoSQL database, which allows them to be flexibly\\nmodelled and indexed. The EKP can be run on a\\nreasonably-powered server, requiring an 8-core proces-\\nsor and 60GB of memory as a minimum. It has previ-\\nously been used in pre-clinical research for drug efficacy\\nscreening [13], prioritizing existing drugs as repurposing\\ncandidates for autosomal dominant polycystic kidney\\ndisease [25], and pathway enrichment [26].\\nFeature sets &amp; machine learning\\nThe paths between the disease proteins were extracted\\nfrom the EKP. To keep our graph comprehensible, we\\nonly extracted paths that consisted of one or two triples,\\ni.e., paths where two disease proteins are connected by\\nat most one intermediate protein. Within this range,\\nthree scenarios for the paths between the disease proteins\\nof two diseases A and B were distinguished (Fig. 1.):\\n1) Overlap, where A and B share a disease protein,\\noptionally with a path to itself, e.g. a disease protein\\nof which two copies bind with each other\\n(homodimerization).\\n2) Direct path, where a disease protein of A and a\\ndisease protein of B are part of one triple.\\n3) Indirect path, where one intermediate protein\\nconnects the disease proteins of A and B, requiring\\na sequence of two triples.\\nTwo different methods to represent indirect paths be-\\ntween disease proteins were compared. The first method\\nconstructed so-called metapaths [5], where the sequence\\nof predicates in an indirect path was used as single feature.\\nThe second method, which we refer to as split paths, con-\\nsidered each predicate in the indirect paths as a separate\\nfeature [13]. Each method was tested both with and with-\\nout directional information of predicates. Predicates were\\neither considered to all be undirected, or predicates were\\ncategorized as being directed or undirected based on ex-\\npert assessment (described in the Assessment of predicate\\ndirectionality section below), which we refer to as the\\nMixed variation. In the overlap scenario, where the subject\\nand the object were the same protein, predicates were al-\\nways considered to be undirected.\\nAll features were binary. Figure 2 shows the four\\nfeature sets that are derived from the example\\nshown in Fig. 1. We also experimented with feature\\nsets where all predicates were directed as indicated\\nby the subject and object of the triple in the EKP.\\nHowever, because some predicates are explicitly de-\\nfined as being undirected, using any directional in-\\nformation from triples with these predicates would\\ncontradict these definitions. Nonetheless, for the\\nsake of completeness we have chosen to present\\nthese results in Additional file 1.\\nRandom forests were trained to classify the sequences of\\ndiseases as positive or negative. Classification performance\\nFig. 1 Schematic overview of the overlap, direct, and indirect scenarios that were extracted from the knowledge graph. Both diseases A and\\ndisease B have three disease proteins (DP) associated with them according to the manually curated subset of DisGeNet. DisGeNet describes that\\nDP1 is known to be associated with both diseases, while the knowledge graph describes that it has a \\u0093binds with\\u0094 relationship to itself. DP2 and\\nDP4 have a direct \\u0093inhibits\\u0094 relationship, and DP3 and DP5 are connected through an indirect path, by an intermediate protein (IP). The arrows\\nbetween the proteins indicate which protein is the subject of the \\u0093inhibits\\u0094 predicate, and which one its object. The \\u0093binds with\\u0094 predicate was\\nconsidered to be undirected by the experts, and therefore does not have a direction. Based on the paths in the knowledge graph, four feature\\nsets are created, based on two methods to represent indirect paths, and both with and without the directional information of predicates\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 4 of 11\\nwas measured with the area under the receiver operator\\ncharacteristic curve (AUC) of a 10-fold cross-validation\\nexperiment [27, 28]. We report the mean and standard de-\\nviation of the AUCs of 10 repeated cross-validation exper-\\niments. The same folds that were used in the experiments\\nwith undirected predicates were also used in the experi-\\nments with directed predicates, after which the differences\\nbetween the test folds were tested for significance with a\\ntwo-sided, paired t-test.\\nTo control for the differences in prevalence and num-\\nber of cases between the two reference sets, we also re-\\nport the classification performance after undersampling\\nthe number of positive and negative cases in the Jensen\\nset to match those in the Van den Akker set.\\nFor the best performing classifiers we also report sensi-\\ntivity and specificity at the probability cutoff for which the\\nYouden index (sensitivity + specificity \\u0096 1) is largest [29].\\nMachine learning and evaluation of results were per-\\nformed in R [30] with the packages caret [31], ranger\\n[32], and pROC [33].\\nAssessment of predicate directionality\\nThree experts with a strong biomedical background and\\nfamiliarity with knowledge graphs assessed the direction-\\nality of 47 distinct predicates that were found in the ex-\\ntracted paths. They were provided with definitions of\\nthese predicates which were obtained from the Pathway\\nCommons resource [34]. If not available, definitions\\nwere sought through the National Library of Medicine\\n[35], or the OBO foundry [36]. The assessors could\\ncategorize each predicate as \\u0093directed\\u0094, \\u0093undirected\\u0094, or\\n\\u0093don\\u0092t know\\u0094. To establish directionality, a predicate had\\nto be categorized as directed or undirected by a majority\\n(i.e., two or three) of the assessors. Predicates that con-\\ntain a negation (e.g., \\u0093does not interact with\\u0094) were auto-\\nmatically categorized the same as the corresponding\\npredicate without negation (\\u0093interacts with\\u0094), and there-\\nfore not presented to the assessors. For some predicates\\nthe categorization was straightforward. For example,\\nPathway Commons defines the predicate \\u0093interacts with\\u0094\\nas \\u0093This is an undirected relation between participant\\nFig. 2 The four feature sets that were derived from the paths between the disease proteins in Fig. 1. All features are binary: Black fields indicate a\\n\\u0093True\\u0094 value, while empty fields indicate a \\u0093False\\u0094 value. For the \\u0093Mixed\\u0094 feature sets, the \\u0093Binds with\\u0094 predicate is assessed to be undirected by\\nexperts, while the \\u0093Inhibits\\u0094 predicate is assessed to be directed\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 5 of 11\\nproteins of a molecular interaction. [\\u0085]\\u0094 , and the predi-\\ncate \\u0093catalysis precedes\\u0094 as \\u0093This relation defines di-\\nrected interactions between proteins. [\\u0085]\\u0094 [34]. Six\\npredicates did not reach a majority in the first round\\nand were anonymously commented upon by the asses-\\nsors to motivate their categorization. These comments\\nwere shared between the assessors, after which they\\ncould update their initial choice. Each predicate was\\nthen categorized with a majority.\\nTable 1 shows the 12 predicates that were categorized\\nas undirected by the three experts. The other 35 predi-\\ncates were categorized as directed. A complete overview\\nof the predicates can be found in Additional file 2.\\nResults\\nExtracted paths\\nIn total, 6859 distinct disease proteins were assigned to\\nthe diseases in both reference sets, three of which could\\nnot be mapped to the EKP. Another 430 (6.3%) of the\\ndisease proteins were not found in any of the extracted\\npaths. From these disease proteins, 314 had no relation-\\nship to any other protein in the EKP.\\nThe remaining 6426 disease proteins were involved in\\n1,338,310 direct paths and 833,546,575 indirect paths,\\nwhile 2581 disease proteins had 7354 paths to them-\\nselves. All paths were based on 2,015,738 distinct triples,\\nwhich contained 17,132 different proteins and 47 differ-\\nent predicates.\\nThe overlap scenario, where the two diseases in the\\ntrajectory share at least one disease protein (scenario 1,\\nFeature sets &amp; Machine learning section), occurred in\\n58% of the positive cases of the Jensen set, and 95% of\\nthe positive cases of the Van den Akker set. No indirect\\npaths (scenario 3, Feature sets &amp; Machine learning sec-\\ntion) were found between the disease proteins of 119\\npositive cases (4.7%), and 18,217 negative cases of the\\nJensen set (10.8%), and one positive case (1.8%) and 15\\nnegative cases (4.1%) of the Van den Akker set.\\nClassification results\\nThe classification performance for both reference sets is\\nshown in Table 2. Mixed metapaths performed best,\\nachieving mean AUCs of 89.8% for the Jensen set and\\n74.5% for the Van den Akker set. Overall, classification\\nperformance on the Van den Akker set was 9.9 to 15.3\\npercentage points lower than on the Jensen set, while\\nstandard deviations were 9.6 to 11.3 percentage points\\nhigher. Metapaths performed 4.1 to 7.0 percentage\\npoints better than split paths. The performance of the\\nmixed feature sets was 1.9 to 6.5 percentage points\\nhigher than the undirected feature sets. All differences\\nbetween mixed and undirected feature sets were signifi-\\ncant (p-values for Jensen metapaths and split paths: &lt;\\n0.001; Van den Akker metapaths: 0.02, split paths 0.001).\\nTo quantify how much of the difference in AUC be-\\ntween the two reference sets could be attributed to their\\ndifference in size, the Jensen set was undersampled to\\nthe same number of positive and negative cases as the\\nVan den Akker set. With the exception of the mixed\\nmetapaths, performance dropped below the performance\\nthat was achieved with the Van den Akker set. The\\nstandard deviations also increased from 0.9\\u00961.7% to 8.4\\u0096\\n12.3%. The latter values are comparable to the standard\\ndeviations on the Van den Akker set.\\nFigure 3 shows the receiver operating characteristic\\n(ROC) curves of the mixed metapath classifiers that per-\\nformed best. For the Jensen set, sensitivity and specificity\\nat the maximum Youden index were 79.2% and 82.4%,\\nrespectively, while for the Van den Akker set these were\\n73.6% and 64.3%.\\nError analysis\\nFor our best classifier (mixed metapath features, trained\\non the Jensen set), we analyzed the top-15 false-positive\\nand the top-15 false-negative cases, searching the litera-\\nture for information that might explain the errors. The\\nresults of our analysis of the false positives are shown in\\nTable 3. Overall, the first 10 out of the top 15 false posi-\\ntives appear to be omissions from the Jensen set rather\\nthan misclassifications. For two false-positive cases, po-\\ntential mechanisms have been suggested, but the current\\nevidence is inconclusive on whether those mechanisms\\nare valid. For the remaining three false-positive cases no\\nliterature could be found, which may therefore be inter-\\nesting leads for further investigation.\\nTable 4 shows the results for the top-15 false nega-\\ntives. For six false negatives, the second disease was\\nlikely to be caused by the treatment of the first disease.\\nFor example, the radiation that is used to treat the ma-\\nlignant neoplasm of the larynx may compromise the\\nTable 1 Predicates categorized as undirected as a result of the\\nassessment process\\nUndirected Predicates\\nbinds with\\ncoexists with\\ndoes not coexist with\\nforms protein complex with\\ninteracts with\\ndoes not interact with\\nis associated with\\nis compared with\\nis functionally related to\\nis spatially related to\\nis the same as\\northolog is associated with\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 6 of 11\\nimmune system around the throat and mouth, thereby\\nincreasing susceptibility to oropharyngeal candidiasis\\n[54]. Two false-negative trajectories are likely to have\\nmechanical causes, rather than molecular pathways. The\\ntrajectory from malignant neoplasms of the ovary to nu-\\ntrient deficiency can be explained by the blocking of the\\nintestines by the ovarian tumor, thereby blocking the en-\\ntire digestive system [53]. For four of the false-negative\\ntrajectories, no description could be found in the litera-\\nture, making their assessment impossible. Assessment of\\nthe three remaining false negatives is speculative. For ex-\\nample, the trajectory from transient ischemic attacks\\n(TIA) to vitamin B12 deficiencies may be an artifact of\\nthe medical record keeping. Vitamin B12 is known to\\nprotect against TIAs [52], so what may often happen is\\nthat a vitamin B12 deficiency is only diagnosed after the\\nmore acute TIA has been treated in an emergency room.\\nDiscussion\\nOur work demonstrates that disease trajectories can be\\nidentified with the predicates between proteins in a know-\\nledge graph. To do so, our machine-learning based meth-\\nodology needed to successfully identify both the correct\\npairs of diseases, as well as their correct temporal se-\\nquences. Overall, representing indirect paths as metapaths\\nperformed superior as compared to representing them as\\nsplit paths. Using the directional information of predicates\\nsignificantly improved performance over not using this\\ninformation. Undersampling the Jensen set to the same\\nnumber of positive and negative cases as the Van den\\nAkker set showed that its lower performance and higher\\nstandard deviations could partially be explained by its small\\nsize.\\nIn previous work, Bang et al. [21] identified disease trajec-\\ntories by calculating the relative risk between two diseases\\nand combining this with the relative position of disease pro-\\nteins in biomolecular pathways. Their method is fully\\ndependent on shared disease proteins between the two dis-\\neases, whereas our method also works when there are only\\n(in) direct paths between disease proteins. In the Jensen set,\\nthis holds for 42% of the trajectories. Performance compari-\\nson of the methods is difficult because Bang et al. only vali-\\ndated the disease trajectories that were suggested by their\\nmethod, but did not validate the non-trajectories. Thus,\\nonly the precision of their method can be calculated but no\\ninsight is provided in the number of false-negative trajec-\\ntories. A final complication for the comparison between the\\ntwo methods is the claim of Bang et al. to discover causal\\nrelationships between diseases, rather than only temporal\\nones. Unfortunately, they refer to an example to define\\ncausal relationships between diseases, making it difficult to\\npinpoint how these differ from disease trajectories.\\nAlthough we do not foresee direct clinical application\\nof our work, our high performance may persuade ex-\\nperts to further examine the protein paths underlying\\nsome positively classified trajectories, either known or\\nTable 2 Classification results for the four feature sets for both reference sets\\nJensen set Jensen set - undersampled Van den Akker set\\nMetapaths Split paths Metapaths Split paths Metapaths Split paths\\nUndirected 83.3 (1.7) 78.3 (1.7) 64.2 (12.1) 61.9 (12.3) 72.5 (11.8) 68.4 (13.0)\\nMixed 89.8 (0.9) 82.8 (1.2) 82.3 (8.4) 69.6 (13.1) 74.5 (10.5) 70.3 (11.4)\\nThe values in the columns indicate the mean AUC and its standard deviation in % of 10 cross-validation experiments\\nFig. 3 ROC curves of the mixed metapaths classifiers for the Jensen set and the Van den Akker set\\nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 7 of 11\\nnewly suggested. Interpreting these protein paths might\\nprovide additional clues about the mechanism through\\nwhich the first disease leads to the second. Identifying\\nand understanding these mechanisms is likely to im-\\nprove prevention, prediction of disease progression,\\nintervention, and drug development, thereby indirectly\\nsupporting clinical practice and health-care policy. For\\nnow, such detailed examinations of the protein paths\\nwere beyond the scope of this project.\\nA downside of working on the protein level was that not\\nall disease trajectories could be studied. More than a third\\nof the trajectories of the Jensen set, and a fifth of the Van\\nden Akker set was lost because disease proteins could not\\nbe assigned to one or both of the diseases in a trajectory.\\nEven when disease proteins could be assigned to both dis-\\neases, alternative explanations were sometimes more\\nplausible. For example, our analysis of the false-negative\\ncases suggested that some trajectories could be explained\\nmechanically, or were likely due to a side effect of the\\ntreatment for the first disease. To determine the true per-\\nformance of our method, a validated set of trajectories that\\nare caused by biomolecular mechanisms would be needed.\\nAlternatively, the range of trajectories that can be analyzed\\nmay be broadened by linking diseases to other types of\\ndisease information available in the EKP, e.g., information\\nabout drugs or physiological processes.\\nThe two reference sets that were used in this research\\nwere both based on patient'</span>text10\n:   <span style=white-space:pre-wrap>'Moen et al. Journal of Biomedical Semantics           (2020) 11:10 \\nhttps://doi.org/10.1186/s13326-020-00229-7\\nRESEARCH Open Access\\nAssisting nurses in care documentation:\\nfrom automated sentence classification to\\ncoherent document structures with subject\\nheadings\\nHans Moen1*\\u0086 , Kai Hakala1,2\\u0086, Laura-Maria Peltonen3, Hanna-Maria Matinolli3, Henry Suhonen3,4,\\nKirsi Terho3,4, Riitta Danielsson-Ojala3,4, Maija Valta4, Filip Ginter1, Tapio Salakoski1 and Sanna Salanterä3,4\\nAbstract\\nBackground: Up to 35% of nurses\\u0092 working time is spent on care documentation. We describe the evaluation of a\\nsystem aimed at assisting nurses in documenting patient care and potentially reducing the documentation workload.\\nOur goal is to enable nurses to write or dictate nursing notes in a narrative manner without having to manually\\nstructure their text under subject headings. In the current care classification standard used in the targeted hospital,\\nthere aremore than 500 subject headings to choose from, making it challenging and time consuming for nurses to use.\\nMethods: The task of the presented system is to automatically group sentences into paragraphs and assign subject\\nheadings. For classification the system relies on a neural network-based text classification model. The nursing notes\\nare initially classified on sentence level. Subsequently coherent paragraphs are constructed from related sentences.\\nResults: Based on a manual evaluation conducted by a group of three domain experts, we find that in about 69% of\\nthe paragraphs formed by the system the topics of the sentences are coherent and the assigned paragraph headings\\ncorrectly describe the topics. We also show that the use of a paragraph merging step reduces the number of\\nparagraphs produced by 23% without affecting the performance of the system.\\nConclusions: The study shows that the presented system produces a coherent and logical structure for freely written\\nnursing narratives and has the potential to reduce the time and effort nurses are currently spending on documenting\\ncare in hospitals.\\nKeywords: Patient care documentation, Nursing documentation, Electronic health records, Text classification,\\nNatural language processing, Neural networks, Model interpretation\\n*Correspondence: hnsmoen@gmail.com\\n\\u0086Hans Moen and Kai Hakala contributed equally to this work.\\n1Department of Future Technologies, University of Turku, Vesilinnantie 5,\\n20500 Turku, Finland\\nFull list of author information is available at the end of the article\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\nmade. The images or other third party material in this article are included in the article\\u0092s Creative Commons licence, unless\\nindicated otherwise in a credit line to the material. If material is not included in the article\\u0092s Creative Commons licence and your\\nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\navailable in this article, unless otherwise stated in a credit line to the data.\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 2 of 12\\nBackground\\nCare documentation is important for supporting the con-\\ntinuity of care in hospitals. According to literature, nurses\\nspend up to 35% (with an average of 19%) of their working\\ntime on documentation [1]. Naturally, if we can reduce the\\ntime that nurses spend on documentation, more time will\\nbe available for direct patient care.\\nTo support tasks such as navigation, planning and sta-\\ntistical analysis, nurses in many countries are required\\nto perform structuring of the information they write [2].\\nSuch structuring approaches include the use of documen-\\ntation standards, classifications and standardized termi-\\nnologies [3]. However, this usually adds certain restric-\\ntions and requirements to the documentation process\\ncompared to writing the information in an unstruc-\\ntured narrative manner. In Finland, nurses are nowa-\\ndays expected to structure the information they write\\nby using subject headings from the Finnish Care Clas-\\nsification (FinCC) standard [4]. This includes selecting\\nthe correct subject headings and writing the associated\\ninformation underneath. In this way, each subject head-\\ning forms a paragraph in the nursing note. As an example,\\nif a nurse wants to write something about administrated\\nwound care, he/she will first have to select an appropri-\\nate heading, e.g. \\u0093Wound\\u0094. FinCC consists primarily of two\\ntaxonomy resources, the Finnish Classification of Nurs-\\ning Diagnoses (FiCND) and the Finnish Classification of\\nNursing Interventions (FiCNI), and both of these have a\\nthree-level hierarchy. For example, one branch in FiCND\\nis: \\u0093Tissue integrity\\u0094 (level 1), \\u0093Chronic wound\\u0094 (level 2)\\nand \\u0093Infected wound\\u0094 (level 3). Another example, a branch\\nfrom FiCNI is: \\u0093Medication\\u0094 (level 1), \\u0093Pharmacotherapy\\u0094\\n(level 2) and \\u0093Pharmaceutical treatment, oral instructions\\u0094\\n(level 3). However, FinCC consists of more than 500 sub-\\nject headings, covering both interventions and diagnoses.\\nThis makes it potentially challenging and time consuming\\nfor nurses to use since they are required to memorize, use\\nand structure the information they write according to a\\nlarge number of subject headings [5].\\nWhat we are aiming for is to develop a system that can\\nassist nurses in selecting suitable subject headings and in\\nstructuring the text accordingly.We hypothesize that such\\na system has the potential to save time and effort required\\nfor documentation and ultimately free up more time for\\nother tasks. We see two use-cases for such a system: One\\nis where the system assists nurses in selecting appropri-\\nate headings when they write, in a suggestive manner, e.g.,\\nper sentence or paragraph; A second use-case is where\\nnurses are allowed to write or dictate (by voice to text)\\nin a fully unstructured narrative manner, without having\\nto take into consideration the structure or the use of sub-\\nject headings. Instead the system assigns subject headings\\nafterwards and restructures the text into paragraphs. In\\nthis study we focus on the second use-case.\\nThis is the continuation of a previously reported study\\nthat focused on assessing how an earlier version of the\\nsystem performs on the level of sentences [6]. The main\\nconclusion of that study is that a sentence classification\\nmodel trained on semi-structured nursing notes can be\\napplied on unstructured free nursing narratives without a\\nsubstantial decline in accuracy.\\nThis time we focus on paragraph-level assessment,\\nwhere we also explore a post-processing step aimed at\\nreducing the number of paragraphs initially generated\\nby the system. To evaluate our system, a team of three\\ndomain experts (aka evaluators) conduct a manual evalu-\\nation to assess both the grouping of sentences into para-\\ngraphs and the correctness of the assigned headings. In\\naddition we analyze the classification model in an attempt\\nto identify conflicts between the actual use of the sub-\\nject headings and the intended use according to the FinCC\\ntaxonomy.\\nAt the core of our system is a text classification model\\nbased on a bidirectional long short-termmemory (LSTM)\\nrecurrent neural network architecture [7, 8]. As train-\\ning data we use a large collection of nursing notes from\\na Finnish hospital which contain subject headings and\\nwhich are structured accordingly. Further, to acquire the\\ntype of narrative text that we would like to use as input\\nto the system, without a bias towards a particular struc-\\nture and subject headings, we made a set of nursing notes\\nbased on artificial patients that we use for testing.\\nRelated work\\nAs we focus on classifying individual sentences, the work\\nis closely related to other short text classification studies.\\nHowever, most of the prior work focuses on texts col-\\nlected from social media or other online sources [9\\u009611].\\nInterestingly, Zhang et al. [12] conclude that the optimal\\ntext classification method is strongly dependent on the\\nselected task, warranting domain specific research on this\\ntopic.\\nIn the clinical domain, a common objective for text\\nclassification has been the automated assignment of ICD\\ncodes to the target documents [13\\u009615]. For instance Xie et\\nal. [16] use a neural model for mapping diagnosis descrip-\\ntions extracted from discharge notes to the corresponding\\nICD codes. Similarly Koopman et al. [17] assign ICD-10\\ncodes to death certificates, but limit the scope to various\\ncancer types.\\nFor cases where available training data is scarce, Wang\\net al. [18] propose a system for producing weakly labeled\\ntraining data, where simple rules are initially used to\\nlabel a large set of unlabeled clinical documents and\\nthese labels are subsequently used as training targets for\\nmachine learning based classifiers. The approach is eval-\\nuated on smoking status and hip fracture classification,\\nbut shows mixed results, with a rule-based baseline being\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 3 of 12\\nthe strongest model in some cases. As our training dataset\\ninherently contains the used classification labels, we have\\nnot considered such weak supervision in our research.\\nTo our knowledge the most recent systematic review\\non clinical text classification was conducted by Mujtaba\\net al. [19]. In addition to comparing the classification\\napproaches utilized in different studies, the review focuses\\non the differences in the selected datasets. Their study\\nindicates that along with the medical literature, clinical\\ntext classification research mostly focuses on pathology,\\nradiology and autopsy reports, whereas other clinical doc-\\numents such as nursing care records are far less stud-\\nied. Moreover, the vast majority of the reviewed studies\\nonly evaluate their methods on English data, leading to\\nMujtaba et al. suggesting wider range of languages to be\\nincluded in these studies.\\nAs an additional note, Mujtaba et al. also conclude that\\ndeep learning methods are still relatively poorly studied in\\nthis domain. However, lately neural approaches have been\\nsuggested for a wide range of medical text classification\\npurposes [20\\u009622].\\nMore related to our research are prior studies on clinical\\nnote segmentation. Denny et al. [23] present an approach\\nfor detecting section headers in clinical notes based on the\\nfree text. More precisely, they focus on history and phys-\\nical examination documents where the goal is to identify\\nand normalize section headers as well as to detect section\\nboundaries. Li et al. [24] present a system that catego-\\nrizes sections in clinical notes into one of 15 pre-defined\\nsection labels for notes already split into sections. Their\\napproach relies on modelling the dependencies of consec-\\nutive section labels with Hidden Markov Models. In [25]\\ncoarse topics are assigned to the sections found in clin-\\nical notes. These topics are here seen as separate from\\nthe section headings used by the clinicians when writing,\\nthus the section headings are considered as input to the\\nclassifier along with the free text.\\nA distinction between our study and the prior work\\nis that we operate with an order of magnitude larger\\nset of section labels. Additionally, we rely on semi-\\nstructured nursing notes as training data with the devel-\\noped method subsequently being applied on unstruc-\\ntured notes. Thus, we do not utilize any prior knowledge\\nabout paragraphs/sections. Grouping the text into sensi-\\nble paragraphs is instead a task for the presented system \\u0096\\ntogether with assigning subject headings.\\nMethods\\nOur ultimate goal is to develop a system that is able\\nto automatically identify and classify, on sentence level,\\ninterventions and diagnoses mentioned in nursing narra-\\ntives, and further capable of grouping the text into sensible\\nparagraphs with subject headings reflecting their topics.\\nIn other words, we are aiming for a system that can let\\nnurses simply write or dictate in a narrative manner with-\\nout having to plan and structure the text with respect to\\nparagraphs and subject headings. In pursuing this goal\\nwe have implemented a prototype system with a neural\\nnetwork-based text classification model at its core. In this\\nsection we describe the data and methods used in the\\nimplementation and evaluation.\\nData\\nThe data set used for training is a collection of approxi-\\nmately 0.5 million patients\\u0092 nursing notes extracted from a\\nuniversity hospital in Finland. The selection criteria were\\npatients with any type of heart-related problem in the\\nperiod 2005 to 2009 and nursing notes from all units vis-\\nited during their hospital stay are included. The data is\\ncollected during a transition period between two classi-\\nfication standards, the latter being the mentioned FinCC\\nstandard. This means that our training data contains a\\nmixture of headings from these two. We only use sen-\\ntences occurring in paragraphs with subject headings,\\nwhich amounts to approximately 5.5 million sentences,\\n133,890 unique tokens and approximately 38.5 million\\ntokens in total. We exclude all subject headings used less\\nthan 100 times, resulting in 676 unique subject headings,\\nwhere their frequency count range from 100 to 222,984,\\nwith an average of 4,896. The individual sentences are\\nused as a training example with the corresponding subject\\nheading as the target class to be predicted. The average\\nsentence length is 7 tokens1 and the average number of\\nsentences per paragraph is 2.1. The data set is split into\\ntraining (60%), development (20%) and test (20%) sets and\\nfurther used to train and optimize the text classification\\nmodel.\\nText classification model\\nThe classification task is approached as a sentence-level\\nmulticlass classification task, where each sentence is\\nassumed to have one correct subject heading (label). Our\\ntext classification model is based on a bidirectional short-\\nterm memory (LSTM) recurrent neural network archi-\\ntecture [7, 8]. The model receives a sequence of words\\nas its input and encodes them into latent feature vectors\\n(dimensionality 300). These vectors are subsequently used\\nas the input for a bidirectional LSTM layer (dimensional-\\nity 600 per direction). As the final layer a fully connected\\nlayer with the dimensionality corresponding to the num-\\nber of target subject headings is used. The word embed-\\ndings are pretrained with Word2vec [26]. The model is\\noptimized for categorical cross-entropy with Adam opti-\\nmizer [27], stopping early based on the development set\\nperformance. As machine learning tools we primarily use\\n1Space separated units.\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 4 of 12\\nthe Keras deep learning library [28], with TensorFlow\\nlibrary [29] as backend.\\nWe want to emphasize that the focus of this paper is not\\nto find the optimal text classificationmethod and parame-\\nter settings for this task. This has instead been the focus of\\nanother study [30], where a range of different state-of-the-\\nart and baseline text classification methods are tested and\\ncompared. Results from the mentioned study indicate that\\na bidirectional version of LSTM networks performs best\\nwhen compared to other classification methods/models,\\nincluding convolutional neural networks, support vector\\nmachines and random forests [31\\u009633].\\nOn the test set, when the classifier is allowed to suggest\\none subject heading per sentence, it suggests the correct\\nheading for 54.35% of the sentences according to auto-\\nmated evaluation. When allowed to suggest 10 headings\\nper sentence, the correct one is among these 89.54% of the\\ntime (see [30] for more details).\\nSubject heading prediction and grouping into paragraphs\\nSince our prototype system relies primarily on a sentence-\\nlevel classification model, it starts by classifying each sen-\\ntence individually before grouping them into paragraphs.\\nHowever, this might arguably be the opposite order of how\\na human would approach this task. The system\\u0092s opera-\\ntion can be described as a four-step process. Step 1: First\\nthe text is split into sentences. For this we rely on a com-\\nbination of the NLTK tokenizers for Finnish [34] and a set\\nof regular expressions tailored for the clinical text. Step 2:\\nNext the classification model is used to classify each sen-\\ntence individually and assign the top predicted heading\\n(the one with the highest confidence value). Step 3: As\\na third step the sentences with the same assigned sub-\\nject heading are grouped into paragraphs. Step 4: The\\nfourth step focuses on merging paragraphs whose content\\nand assigned headings are close to each other in terms of\\nmeaning. This fourth step is included to potentially reduce\\nthe number of paragraphs to more closely simulate how\\nnurses document. Below we explain in more detail how\\nthis fourth step is done.\\nParagraphmerging explained: In the previous study [6],\\nthe evaluators reported that the system showed a ten-\\ndency to assign subject headings with a high level of\\nspecificity, and sometimes even too specific to be prac-\\ntical. For example, for two or more sentences describing\\ndifferent aspects of pain management in the same nurs-\\ning note, such as treatment and medication, the system\\nwould in some cases assign these to different subject\\nheadings, possibly headings of different level of speci-\\nficity/abstraction. This meant that, in some cases, unnec-\\nessarily many unique headings, thus paragraphs, were\\nassigned to each nursing note.\\nIn an attempt to reduce the number of paragraphs cre-\\nated, to more closely simulate how nurses document, we\\nhave implemented an experimental post-processing step\\nthat enables the system to merge paragraphs (within a\\nnursing note) that it finds to have similar subject head-\\nings. For this we primarily rely on the confidence values\\nof the classification model, as well as extracted vec-\\ntor representations of each subject heading. The LSTM\\nlayer outputs 600 dimensional sentence encodings for\\nboth directions of the input sequence, resulting in 1200\\ndimensional vectors representations for the subject head-\\nings. These we use to calculate heading similarity by\\napplying the cosine distance metric. See the \\u0093Data anal-\\nysis\\u0094 section for further description of these heading\\nvectors.\\nFirst a paragraph-to-paragraph similarity matrix is\\nformed reflecting how each paragraph would consider the\\nsubject headings from the other paragraphs (from step\\n3) as a likely candidate heading. To this end we define\\na simple asymmetric similarity function which measures\\nhow inclined a paragraph (source) is towards the head-\\ning of another paragraph (target) in the same nursing\\nnote. For each sentence in a given source paragraph we\\ntake the classifier\\u0092s confidence of the sentence belong-\\ning to the target heading and subtract the difference in\\nthe confidence between predicting the source heading\\nand the target heading. The individual sentence scores\\nare averaged and further summed with the cosine dis-\\ntance between the source and target headings and the\\nrelative size of the target paragraph (compared against\\nthe whole nursing note). The first component, relying\\non the confidence values of the classifier, describes how\\nwell the sentences fit in the target paragraph. The sec-\\nond component measures how semantically similar the\\ncompared paragraph headings are, more similar headings\\nbeing more likely to be merged. The third component\\nincreases preference towards retaining the headings of\\nthe larger paragraphs. This scoring function produces\\nvalues in the range 3 to minus 2. Note that it is not\\nsymmetrical.\\nTo determine if two paragraphs are to be merged,\\nwe require that the similarity between these two para-\\ngraphs, in both directions, is above a given threshold.\\nIf the threshold is exceeded, the two most similar para-\\ngraphs are merged, keeping the heading of the para-\\ngraph with the lowest score out of the two. Subsequently\\nthe similarity matrix is recalculated, and the process is\\nrepeated until no paragraph pairs can bemerged.We opti-\\nmize this threshold on a sample of nursing notes from\\nthe test data where paragraph information and head-\\nings are removed. A threshold is found that enables the\\nsystem to generate approximately the same number of\\nparagraphs as in the original versions of these nursing\\nnotes.\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 5 of 12\\nSystem evaluation\\nIn this experiment the focus is on evaluating how the\\nsystem performs at the intended task. Two versions of\\nthe system are manually evaluated, NoMerging and\\nWithMerging, where the difference is that NoMerging\\nonly performs steps 1\\u00963, while WithMerging also per-\\nforms step 4. This comparison is done to see if the para-\\ngraph merging (step 4) can be done without reducing\\nsystem performance according to the evaluators\\u0092 assess-\\nments. To perform the evaluation two domain experts\\nwith nursing background first evaluated the paragraphs\\nindividually. Then we consulted a third domain expert\\nwho provided a third opinion for the instances where the\\ntwo disagreed. Finally the three of them agreed on the final\\nconsensus version which we report here.\\nThe evaluation focuses on two aspects of the struc-\\ntured notes produced by the system: 1) The correctness of\\nthe assigned subject headings at paragraph level. Table 1\\nshows the classes used by the evaluators; 2) The quality of\\nthe formed paragraphs, i.e. sentence grouping. The classes\\nused in this assessment are shown in Table 2.\\nThe nursing notes from the training data have been\\nplanned, structured and written with subject headings in\\nmind. One could argue that by simply removing headings\\nand paragraph information, automated evaluation could\\nbe implemented. However, we found that the sentences\\nhere, which are structured under subject headings, have a\\ntendency to be biased towards the topic of their headings,\\nand sometimes their meaning can only be interpreted in\\nthe context of their headings. Also, this structuring forces\\nthe nurses to write very short and concise things, whereas\\nwhen given the freedom to write in a narrative manner,\\nmore complex sentence structures are present. Thus, to\\nobtain relevant nursing notes for evaluation of our use\\ncase \\u0096 notes written in a free narrative style without plan-\\nning for or considering the use of paragraphs and subject\\nheadings \\u0096 we asked five domain experts with nursing\\nbackground to write notes based on made up artificial\\npatients. In total, 40 nursing notes, each note representing\\none day of provided care for a patient, were generated. The\\ntop part of Fig. 1 shows an example of one such nursing\\nnote.\\nTable 1 Classes used by the evaluators when assessing the\\nheadings assigned by the system\\nClass Description\\n1 Correct: the subject heading suits the text in this paragraph.\\n2 Partly correct: the subject heading only suits some of the text,\\nnot all.\\n3 Incorrect: the subject heading does not seem to suit any of\\nthe text.\\n4 Unable to assess: unable to asses whether or not this subject\\nheading is suitable.\\nTable 2 Classes used by the evaluators when assessing the\\nparagraphs formed by the system\\nClass Description\\na Sensible grouping: it makes sense to have these sentences\\ngrouped together as a separate paragraph based on their\\ntopic(s) (even if the subject heading may not fit).\\nb Inconsistent/problematic grouping \\u0096 alt1: one or more\\nsentences in this paragraph would fit better in other para-\\ngraph(s) in this note.\\nc Inconsistent/problematic grouping \\u0096 alt2: one or more\\nsentences in this paragraph do not belong in this or any of the\\nother paragraphs in this note.\\nd Unable to assess: unable to evaluate this paragraph.\\nThese 40 nursing notes were fed to the two versions of\\nthe system, NoMerging and WithMerging. For eval-\\nuation purposes the output was stored as spreadsheets,\\none for each system, each containing both the origi-\\nnal and the generated/structured version of each nursing\\nnote.\\nStatistical analyzes were performed to investigate differ-\\nences in themanual evaluations of the two system versions\\n(Pearson\\u0092s chi-squared test), as well as to see if there is\\na possible correlation between manual evaluations and\\nthe classification model\\u0092s confidence values (Spearman\\u0092s\\nrho).\\nTo gain some qualitative feedback on the system, we also\\nasked the evaluators to answer the following open-ended\\nquestions:\\nQ1: Can you mention the main strengths that you found\\nwith the system(s)?\\nQ2: Can you mention the main weaknesses that you\\nfound with the system(s)?\\nQ3: Do you, or do you not, think that this kind of a\\nsystem would be helpful when it comes to nursing\\ndocumentation, and why?\\nData analysis\\nWe hypothesize that the large amount of subject headings\\nin the FinCC classification standard may cause confusion\\namong the nurses in terms of what headings should be\\nused in documenting the various aspects of the admin-\\nistrated care. Thus, to obtain a deeper understanding of\\nthe evaluated sentence classification model and the care\\ndocumentation conventions of the nurses, we analyze\\nthe heading representations learned by the classification\\nmodel \\u0096 reflecting how they have been used \\u0096 and how\\nthis may differ from their description and intended use\\nbased on FinCC.\\nThe weights of the fully connected output layer of the\\ntrained classifier can be seen as semantic representations\\nof the subject headings since the weights corresponding\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 6 of 12\\nFig. 1 Nursing note example. Top: Without any particular structure or assigned subject headings. Input to the system. Bottom: Grouped into\\nparagraphs with assigned headings. Output from the system. This has been translated from Finnish to English\\nto a given heading define how strongly the heading is\\nactivated for a given input sentence, compared against\\nother possible headings. Thus, two headings with similar\\nweights will have similar probabilities of being assigned\\nto a given input sentence. Inversely, under the assump-\\ntion that the model has learned the classification task well,\\nit can be hypothesized that if two headings have similar\\nweights, the sentences assigned under these headings in\\nthe training data are also similar. Note that these represen-\\ntations are not based on the names of the subject headings,\\nbut instead on the actual sentences written under the\\nheadings.\\nOur main goal in this analysis is to verify whether we are\\nable to find subject headings which are semantically sim-\\nilar according to our classification model, but far apart in\\nthe used FinCC taxonomy, or vice versa. This allows us to\\nidentify conflicts between the actual use of headings and\\ntheir intended use according to the taxonomy. To mea-\\nsure the distances of the subject heading representations\\nwe simply calculate the cosine distance across all heading\\npairs.\\nThe used FinCC classification standard is comprised of\\n3 top level categories: nursing diagnoses, nursing inter-\\nventions and nursing outcomes, however the nursing out-\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 7 of 12\\ncome headings are not present in the used data. Both\\nnursing diagnoses and interventions use a hierarchical\\nstructure with maximum depth of 3. To form a single tree,\\nwe connect the diagnoses and interventions categories\\nwith an artificial root node. This combined tree has amax-\\nimum depth of 4. To measure the distances of headings in\\nFinCC we calculate the shortest path between the head-\\ning nodes in the tree. Although simple, this approach has\\nshown strong performance in measuring concept similar-\\nities in other biomedical ontologies [35].\\nOnce we have the two distances calculated for all sub-\\nject heading pairs \\u0096 cosine distance and distance in the\\ntree \\u0096 we rank each pair based on these two, resulting\\nin two distinct rankings. The conflicting pairs that we\\nselect for further analysis are the ones being furthest apart\\naccording to these two rankings.\\nSince the nursing notes include the used subject head-\\nings as plain text, without containing the actual FinCC\\nidentifiers, we use strict string matching to map the head-\\nings to the corresponding FinCC concepts. This leaves us\\nwith 263 headings for this analysis out of the total 676\\nheadings in our data set. The excluded headings either\\noriginate from the older classification standard or contain\\nspelling variations.\\nResults\\nIn this section we first present the results from the sys-\\ntem evaluation. Next we highlight some of the observa-\\ntions from the analysis of subject heading representations\\naccording to the classification model and the underlying\\nclassification standard.\\nSystem evaluation results\\nThis experiment provided insight into how the system\\nperforms at the intended task of assigning applicable sub-\\nject headings and grouping sentences into paragraphs.\\nTable 3 shows how well the assigned subject headings fit\\nthe text in the paragraphs. Table 4 reflects what the eval-\\nuators think about the integrity of the paragraphs formed\\nby the system.\\nSee Fig. 1 for an example showing a input note to the\\nsystem (top) and the output (bottom) where the text is\\ngrouped into paragraphs with assigned subject headings.\\nTable 3 Subject headings evaluation results. See Table 1 for an\\nexplanation of the classes\\nClass NoMerging n(tot=396) WithMerging n(tot=305)\\n1 70.45% 279 71.15% 217\\n2 14.65% 58 16.72% 51\\n3 14.14% 56 11.80% 36\\n4 0.76% 3 0.33% 1\\n1+2 85.10% 337 87.87% 268\\nTable 4 Paragraph (sentence grouping) evaluation results. See\\nTable 2 for an explanation of the classes\\nClass NoMerging n(tot=396) WithMerging n(tot=305)\\na 79.55% 315 79.02% 241\\nb 15.66% 62 12.13% 37\\nc 3.79% 15 8.52% 26\\nd 1.01% 4 0.33% 1\\nOverall these results show that the system is able to\\nprovide suitable subject headings for about 71% of the\\nparagraphs (class \\u00911\\u0092). They also indicate that about 79%\\nof the paragraphs formed are sensible (class \\u0091a\\u0092). By sensi-\\nble paragraphs we mean that all the sentences within are\\nrelated to the same topic and that none of them would fit\\nbetter elsewhere in the corresponding nursing note.\\nWhen using NoMerging the number of paragraphs\\nformed is 396, with an average of 9.9 per note (min=3,\\nmax=19). When using WithMerging, which also per-\\nforms the paragraph merging step, the number of para-\\ngraphs is reduced by 23%, down to 305, with the average\\nper note being 7.6 (min=2, max=17).\\nWe also calculated how many of the formed para-\\ngraphs were consistent (class \\u0091a\\u0092) while also having a\\nsuitable subject heading (class \\u00911\\u0092). The result is seen in\\nTable 5 and shows that 66.67% (NoMerging) and 68.85%\\n(WithMerging) of the paragraphs are both sensible and\\nhave a correctly describing subject heading assigned to\\nthem. These results show that the merging step results in\\nbasically no loss in performance.\\nPearson\\u0092s chi-squared tests were performed to see\\nwhether there are statistically significant differences\\nbetween the evaluation results of NoMerging and\\nWithMerging based on 1) the subject heading correct-\\nness evaluations, and 2) the paragraph (sentence merging)\\nquality evaluation results2. The evaluation of 1) does not\\nseem to be dependent on what system version was used\\n(X2 (2, N = 697) = 1.20, p = 0.55). However, there is a\\nstatistically significant difference between the two when\\nlooking at 2) (X2 (2, N = 696) = 8.12, p = 0.02).\\nIt is possible that the confidence values of the classifier\\nmay provide some indication of paragraph correctness in\\nthat there is a correlation between the classifier\\u0092s confi-\\ndence value for an assigned heading and the paragraph\\nbeing correct according to the manual evaluation results.\\nUsing Spearman\\u0092s rho to compare the manual evaluation\\nresults of WithMerging with the classifiers confidence\\nvalues for each paragraph\\u0092s assigned heading (average\\nacross sentences), we found there to be a negative correla-\\ntion between classifier confidence values and the heading\\nassignment ratings (Spearman\\u0092s rho = -0.42, \\u0093moderate\\u0094);\\n2Here we excluded classes \\u00914\\u0092 and \\u0091d\\u0092 due to their low frequency (n&lt;5).\\nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 8 of 12\\nTable 5 Results showing the percentage of sensible paragraphs\\n(i.e. sentence groupings) with correct headings assigned\\nClass NoMerging n(tot=396) WithMerging n(tot=305)\\n1 and a 66.67% 264 68.85% 210\\nas well as between classifier confidence values and the rat-\\nings of th'</span>text11\n:   <span style=white-space:pre-wrap>'RESEARCH Open Access\\nDe-identifying free text of Japanese\\nelectronic health records\\nKohei Kajiyama1, Hiromasa Horiguchi2, Takashi Okumura3, Mizuki Morita4 and Yoshinobu Kano1*\\nAbstract\\nBackground: Recently, more electronic data sources are becoming available in the healthcare domain. Electronic\\nhealth records (EHRs), with their vast amounts of potentially available data, can greatly improve healthcare.\\nAlthough EHR de-identification is necessary to protect personal information, automatic de-identification of Japanese\\nlanguage EHRs has not been studied sufficiently. This study was conducted to raise de-identification performance\\nfor Japanese EHRs through classic machine learning, deep learning, and rule-based methods, depending on the\\ndataset.\\nResults: Using three datasets, we implemented de-identification systems for Japanese EHRs and compared the de-\\nidentification performances found for rule-based, Conditional Random Fields (CRF), and Long-Short Term Memory\\n(LSTM)-based methods. Gold standard tags for de-identification are annotated manually for age, hospital, person, sex,\\nand time. We used different combinations of our datasets to train and evaluate our three methods. Our best F1-\\nscores were 84.23, 68.19, and 81.67 points, respectively, for evaluations of the MedNLP dataset, a dummy EHR\\ndataset that was virtually written by a medical doctor, and a Pathology Report dataset. Our LSTM-based method\\nwas the best performing, except for the MedNLP dataset. The rule-based method was best for the MedNLP dataset.\\nThe LSTM-based method achieved a good score of 83.07 points for this MedNLP dataset, which differs by 1.16\\npoints from the best score obtained using the rule-based method. Results suggest that LSTM adapted well to\\ndifferent characteristics of our datasets. Our LSTM-based method performed better than our CRF-based method,\\nyielding a 7.41 point F1-score, when applied to our Pathology Report dataset. This report is the first of study\\napplying this LSTM-based method to any de-identification task of a Japanese EHR.\\nConclusions: Our LSTM-based machine learning method was able to extract named entities to be de-identified\\nwith better performance, in general, than that of our rule-based methods. However, machine learning methods are\\ninadequate for processing expressions with low occurrence. Our future work will specifically examine the\\ncombination of LSTM and rule-based methods to achieve better performance.\\nOur currently achieved level of performance is sufficiently higher than that of publicly available Japanese de-\\nidentification tools. Therefore, our system will be applied to actual de-identification tasks in hospitals.\\nKeywords: De-identification, Electronic health records, Japanese language\\nBackground\\nRecently, more electronic data sources are becoming\\navailable in the healthcare domain. Utilization of\\nelectronic health records (EHRs), with their vast\\namounts of potentially useful data, is an important task\\nin the healthcare domain. New legislation in Japan has\\naddressed the treatment of medical data. The \\u0093Act on\\nthe Protection of Personal Information [1]\\u0094 was revised\\nin 2017 to stipulate that developers de-identify \\u0093special\\ncare-required personal information.\\u0094 This legislation\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: kano@inf.shizuoka.ac.jp\\n1Faculty of Informatics, Shizuoka University, Johoku 3-5-1, Naka-ku,\\nHamamatsu, Shizuoka 432-8011, Japan\\nFull list of author information is available at the end of the article\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 \\nhttps://doi.org/10.1186/s13326-020-00227-9\\nfurther restricts the use of personal identification codes\\nincluding individual numbers (e.g. health insurance card\\nnumbers, driver\\u0092s license card numbers, and governmen-\\ntal personnel numbers), biometric information (e.g. fin-\\ngerprints, DNA, voice, and appearances), and\\ninformation related to disability. This legislation can be\\ncompared with the \\u0093Health Insurance Portability and\\nAccountability Act (HIPAA) [2]\\u0094 of the United States, in\\nthat the Japanese Act in 2017 includes additional codes,\\nwith abstract specifications such as \\u0093you should strive\\nnot to discriminate or impose improper burdens,\\u0094 and\\nwith exclusion of birth dates and criminal histories, as\\nstipulated by HIPAA. Another related act of Japanese le-\\ngislation, the \\u0093Act on Anonymously Processed Medical\\nInformation to Contribute to Medical Research and De-\\nvelopment [3]\\u0094 was established in 2018. This legislation\\nallows specific third-party institutes to handle EHRs,\\nthereby promoting wider utilization of medical data.\\nDe-identification of structured data in EHRs is easier\\nthan that of unstructured data because it is straightfor-\\nward to apply de-identification methods to structured\\ndata such as numerical tables. Although de-identification\\nof unstructured data in EHRs is necessary, it is virtually\\nimpossible to de-identify the huge number of documents\\nmanually.\\nSeveral earlier works have examined EHR de-\\nidentification. The Informatics for Integrating Biology &amp;\\nthe Bedside (i2b2) task [4] in 2006 was intended for\\nautomatic de-identification of clinical records to satisfy\\nHIPAA requirements [2]. An earlier study prepared 889\\nEHRs, comprising 669 EHRs for training and 220 EHRs\\nfor testing. Their annotations included 929 patient tags,\\n3751 doctor tags, 263 location tags, 2400 hospital tags,\\n7098 date tags, 4809 id tags, 232 phone_number tags,\\nand 16 age tags. The best performing method of i2b2 in-\\ncorporated diverse features such as a lexicon, part-of-\\nspeech identification, word frequencies, and dictionaries\\nfor learning using an ID3 tree learning algorithm.\\nGrouin and Zweigenbaum [5] prepared 312 cardiovas-\\ncular EHRs in French, with 3142 tags annotated by two\\nannotators (kappa = 0.87). Their tags include 238 date\\ntags, 205 last_name tags, 109 first_name tags, 43 hospital\\ntags, 22 town tags, 8 zip_code tags, 8 address tags, 8\\nphone tags, 8 med_device tags, 3 serial_number tags. Of\\nthe person tags, 75% were replaced with other French\\nperson names. The other 25% were replaced with inter-\\nnational names. They also collected 10 photopathology\\ndocuments, for which a single annotator assigned 29\\ndate tags, 68 last_name tags, 53 first_name tags, 17 hos-\\npital tags, 17 town tags, 13 zip_code tags, 14 address\\ntags, 1 phone tag, 1 med_device tag, and 7 serial_number\\ntags. They performed de-identification experiments\\nusing 250 documents as their training data and 62 docu-\\nments as their test data for the cardiology corpus. They\\nobtained better F1-scores (exact match, 0.883; overlap\\nmatch, 0.887) using conditional random fields (CRF)\\nthan they obtained using their rule-based method (exact\\nmatch, 0.843; overlap match, 0.847). However, their\\nrule-based method was better for the photopathology\\ncorpus (exact match, 0.681; overlap match, 0.693) than\\ntheir CRF-based method (exact match, 0.638; overlap\\nmatch, 0.638) because the data were fewer than those of\\nthe cardiology corpus.\\nGrouin and Névéol [6] discussed annotation guidelines\\nfor French clinical records. After collecting 170,000 doc-\\numents of 1000 patient records from five hospitals, they\\nfirst prepared a rule-based system and their CRF-based\\nsystem from their earlier study [5], which we described\\nearlier. Their rule-based system relies on 80 patterns\\nspecifically designed to process the training corpus, and\\nlists which they gathered from existing resources from\\nthe internet. They randomly selected 100 documents\\n(Set 1) from their dataset and applied both systems. For\\neach document, they randomly showed one output of\\nthe two systems to the annotators for revision. They ap-\\nplied their rule-based system to another set of 100 docu-\\nments (Set 2), which were further reviewed and revised\\nby a human annotator. They re-trained their CRF-based\\nsystem using the revised Set 2 annotations, which is fur-\\nther applied to the other set of 100 documents (Set 3).\\nAnnotators reviewed these annotations in subsets for\\ndifferent agreement analyses. The study also compared\\nhuman revision times among different annotation sets,\\nwhich was a main objective of their study. They anno-\\ntated 99 address tags, 101 zip_code tags, 462 date tags,\\n47 e-mail tags, 224 hospital tags, 59 identifier tags, 871\\nlast_name tags, 750 first_name tags, 383 telephone tags,\\n218 city tags, in Set 1. They reported their rule-based\\nmethod as better (0.813) in terms of the F1-score than\\ntheir CRF-based method (0.519) when evaluated with 50\\ndocuments in Set 1. When trained with Set 2, the corpus\\nof the same domain, their CRF-based system performed\\nbetter, yielding 0.953 for Set 3 and 0.888 for Set 1 in\\ntheir F1-scores.\\nFrom the Stockholm EPR [7], a Swedish database of\\nmore than one million patient records from two thou-\\nsand clinics, Dalianis and Velupillai [8] extracted 100 pa-\\ntient records to create gold standard for automatic de-\\nidentifications based on HIPAA. They annotated 4423\\ntags, including 56 age tags, 710 date_part tags, 500 full_\\ndate tags, 923 last_name tags, 1021 health_care_unit\\ntags, 148 location tags, and 136 phone_number tags.\\nThey pointed out that Swedish morphology is more\\ncomplex than that of English. It includes more inflec-\\ntions, making the de-identification task in Swedish more\\ndifficult.\\nJian et al. [9] compiled a dataset of 3000 documents in\\nChinese. It comprises 1500 hospitalization records, 1000\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 2 of 12\\nsummaries, 250 consulting records, and 250 death re-\\ncords. They extracted 300 documents from this dataset\\nrandomly, discussed a mode of de-identification with\\nlower annotation cost. They annotated their tags to\\nthese 300 documents (kappa = 0.76 between two annota-\\ntors for their 100 document subset). Then they applied\\ntheir pattern-matching module to these 300 documents,\\nyielding a dense set of 201 sentences that include PHI\\n(Protected Health Information). These 201 sentences in-\\ncluded 141 name tags, 51 address tags, and 22 hospital\\ntags.\\nDu et al. [10] conducted de-identification experiments\\nusing 14,719 discharge summaries in Chinese: two stu-\\ndents annotated 25,403 tags. This dataset includes 6403\\ninstitution tags, 11,301 date tags, 33 age tags, 2078 pa-\\ntient_name tags, 3912 doctor_name tags, 326 province\\ntags, 310 city tags, 774 country tags, 917 street tags, 277\\nadmission_num tags, 21 pathological_num tags, 23 x-\\nray_num tags, 263 phone tags, 420 doctor_num tags, and\\n13 ultrasonic_num tags (inter-annotator agreement was\\n96%, kappa = 0.826). Their experiments demonstrated\\nthat their method of combining rules and CRF per-\\nformed best, yielding a 98.78 F1-score. The Chinese lan-\\nguage shares some issues with the Japanese language:\\nthey both require tokenization because no spaces exist\\nbetween words. This issue makes de-identification tasks\\nmore difficult than they are in other languages.\\nThe reports described above present a range of differ-\\nent evaluation scores. However they adopted different\\nannotation criteria, which make direct comparison diffi-\\ncult. For instance, Grouin and Névéol used more de-\\ntailed annotations than those used by Jian et al., as\\nfollows. Jian et al. introduced Doctor and Patient tags,\\nbut evaluated both simply as Name. Grouin and Névéol\\nintroduced ZipCode, Identifier, Telephone, and City tags,\\nnone of which is annotated in the work of Jian et al.\\nAdditionally, they assigned Last Name and First Name\\ntags, where performance of First Name was better than\\nLast Name by around 10 points. However, both are\\nworse than the results reported by Jian et al., probably\\nbecause Jian et al. applied their pattern-matching algo-\\nrithm to filter their training data. Regarding Address\\ntags, Jian et al. obtained a 94.2 point F-score, whereas\\nthe Grouin and Névéol CRF method obtained scores of\\nfewer than 10 points. As Grouin and Névéol suggested,\\neliminating City tags in street names can greatly improve\\ntheir results: their rule-based method yielded an 86 point\\nF-score.\\nUnfortunately, automatic de-identification of EHRs\\nhas not been studied sufficiently for Japanese language.\\nDe-identification shared tasks for Japanese EHRs were\\nheld as tasks in MedNLP-1 [11]. Then named entity ex-\\ntraction was attempted in MedNLP-2 [12] tasks using\\ndatasets similar to MedNLP-1. We designate MedNLP-1\\nsimply as MedNLP hereinafter because we specifically\\nexamine de-identification tasks but not other tasks held\\nin the MedNLP shared task series.\\nRegarding machine learning methods, Support Vector\\nMachine (SVM) [13] and CRF [14] were used often in\\nearlier Named Entity Recognition (NER) tasks in\\naddition to rule-based methods. Recent deep learning\\nmethods include Long-Short Term Memory (LSTM)\\n[15] with character-embedding and word-embedding\\n[16], which performed best for the CoNLL 2002 [17]\\n(Spanish and Dutch) and CoNLL 2003 [18] (English and\\nGerman) NER shared task data: these tasks require de-\\ntection of \\u0093personal\\u0094, \\u0093location\\u0094, \\u0093organization\\u0094, and\\n\\u0093other\\u0094 tag types. Another LSTM model, which is similar\\nto earlier work [16], was also applied to a task of NER\\nfrom Japanese newspapers [19]. Although deep neural\\nnetwork models have been showing better results re-\\ncently, rule-based methods are still often better than ma-\\nchine learning methods, especially when insufficient\\nannotated data are available.\\nTo evaluate the effectiveness of such different methods\\nfor the Japanese language, we implemented two EHR de-\\nidentification systems for the Japanese language in our\\nearlier work [20]. We used the MedNLP shared task\\ndataset and our own dummy EHR dataset, which was\\nwritten as a virtual database by medical professionals\\nwho hold medical doctor certification. Based on this\\nearlier work, we added a new dataset of pathology re-\\nports to this study, for which we annotated the following\\ntags. De-identification tags of age, hospital, sex, time,\\nand person are annotated manually in all these datasets,\\nfollowing the annotation standard of the MedNLP\\nshared task to facilitate comparison with earlier studies.\\nWe assume these annotations as our gold standard for\\nour de-identification task. To these three datasets, we\\napplied a rule-based method, a CRF-based method, and\\nan LSTM-based method. Additionally, we have anno-\\ntated our own tags to these three datasets by three anno-\\ntators to calculate inter-annotator agreement. We have\\nobserved the coherency of the original annotations of\\nthe datasets. Overall, this study differs from our earlier\\nwork [20] in that we added a new pathology dataset and\\nits annotations, trained and evaluated our machine\\nlearning models using the new dataset, and evaluated\\nthe results using newly created annotations by three an-\\nnotators to observe characteristics of the original and\\nour own annotations.\\nDatasets\\nOur datasets were derived from three sources: MedNLP,\\ndummy EHRs, and pathology reports. Irrespective of the\\ndataset source, de-identification tags of five types are an-\\nnotated manually: age (numerical expressions of sub-\\nject\\u0092s ages including its numerical classifiers), hospital\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 3 of 12\\n(hospital names), sex (male or female), time (subject re-\\nlated time expressions with its numerical classifiers), and\\nperson (person names). Characteristics of these datasets\\nare presented in Table 1. It is noteworthy that texts of\\nthe MedNLP and dummy EHRs are not actual texts, but\\nthey were written by medical professionals, each of\\nwhom holds medical doctor certification. However, char-\\nacteristics of the descriptions differ between these two\\nsources, probably because of differences of the writers.\\nThe number of annotators is not described for the\\nMedNLP dataset, but a single annotator created the an-\\nnotations of the dummy EHR dataset and the Pathology\\nReport dataset, individually.\\nMedNLP shared task dataset\\nWe used the MedNLP de-identification task dataset for\\ncomparison with earlier studies that have used the same\\ndataset. This dataset includes the dummy EHRs\\n(discharge summaries) of 50 patients. Although the\\ntraining dataset and test dataset were provided from the\\nshared task organizers, the test dataset of the formal run\\nis not publicly available now. It is not possible to\\ncompare results directly with earlier works in the\\nMedNLP shared task formal run (Tables 2 and 3 show\\nthe formal run results). However, both training and test\\ndatasets were originally parts of a single dataset. There-\\nfore, we can discuss their characteristics in comparison\\nwith those found in earlier works conducted using the\\ntraining dataset only. We calculated inter-annotator\\nagreement by three annotators for the training dataset.\\nThe average F1-score of three pairs among these three\\nannotators was 86.1, in 500 sentences of this dataset.\\nDummy EHRs\\nAnother source is our original dummy EHRs. We\\nbuilt our own dummy EHRs of 32 patients, assuming\\nthat the patients were hospitalized. Documents of our\\ndummy EHRs were written by medical professionals\\n(doctors). We added manual annotations for de-\\nidentification following the guidelines of the MedNLP\\nshared task. These annotations were originally\\nassigned by a single annotator. Additionally, we added\\nTable 1 Dataset characteristics\\nDataset name MedNLP Dummy-EHRs Pathology Reports\\n# of documents 50 reports 32 pairs of records and summaries 1000 reports\\n# of sentences 2244 8183 3012\\n# of tokens 42,621 154,132 194,449\\n# of all tags 490 3017 295\\n# of age tags 56 39 0\\n# of hospital tags 75 170 31\\n# of person tags 0 135 224\\n# of sex tags 4 16 0\\n# of time tags 355 2657 40\\nExample in\\noriginal Japanese\\ntext\\n????????&lt;a &gt; 64\\n?&lt;/a &gt;? &lt; x &gt;??&lt;/\\nx &gt;?\\n???????????&lt;a &gt; 86?&lt;/a &gt; &lt;x &gt;?\\n?&lt;/x &gt;????\\n&lt;&lt;???? &lt;h &gt;?????????\\n?&lt;/h &gt;? &lt; p &gt;???&lt;/p&gt;\\nExample\\ntranslated into\\nEnglish\\nA &lt; a &gt; 64-year-old&lt;/a &gt; &lt;x &gt;\\nman&lt;/x &gt; works in a factory\\nAn &lt;a &gt; 86-year-old&lt;/a &gt; &lt;x &gt; woman&lt;/x &gt;\\nbedridden in a nursing home. Total assistance\\nrequired\\n&lt;&lt;Ex-hospital sample &lt; h &gt; Shizudai\\nDermatology Clinic&lt;/h &gt; , &lt; p &gt; Satoshi\\nKuwata&lt;/p&gt;\\nTable 2 Overall results\\nP R F A\\nC3 89.59 91.67 90.62 99.58\\nB3 91.67 86.57 89.05 99.54\\nB1 90.05 87.96 88.99 99.49\\nB2 90.82 87.04 88.89 99.52\\nC1 92.42 84.72 88.41 99.49\\nA1 91.50 84.72 87.98 99.47\\nC2 91.50 84.72 87.98 99.46\\nA2 90.15 84.72 87.35 99.41\\nD1 86.10 74.54 79.90 99.36\\nG1 82.09 76.39 79.14 99.38\\nD3 85.87 73.15 79.00 99.35\\nD2 80.81 74.07 77.29 99.24\\nH2 76.17 75.46 75.81 99.28\\nH1 75.81 75.46 75.64 99.27\\nH3 74.88 74.54 74.71 99.26\\nP, R and F were calculated at the phrase level: P, precision; R, recall; F, F1-measure;\\nand A, accuracy. A was calculated in the word level (the agreement ratio of B-*, I-*\\nand O).\\nThe first column stands for participants\\u0092 team names, where the first letter stands\\nfor a team ID and the second numerical value stands for a submission run ID\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 4 of 12\\nnew annotations by three annotators to a part of this\\ndataset and calculated inter-annotator agreement. The\\naverage F1-score of three pairs among these three\\nannotators was 76.1 for 730 sentences of the Dummy\\nEHR dataset.\\nPathology reports\\nThe other source is a dataset of 1000 short pathology\\nreports, that differ greatly from the EHRs above. Pathology\\nreports describe pathological findings by which personal\\ninformation (names of patients, doctors, hospitals, and\\ntime expressions) frequently appears, but for which tags of\\nsex and age rarely appear. Personal names, hospital names,\\nand dates were manually de-identified beforehand by the\\ndataset provider, and replaced with special characters. For\\nmachine learning methods to support realistic training\\nand evaluation, we replaced these special characters with\\nrandomly assigned real entity names as follows. For the\\nhospital names, we collected 96,167 hospital names which\\ncover most of the Japanese hospital names, published by\\nthe Japanese government. For the person names, we\\nmanually created 20 dummy-family names and 20\\ndummy-first names using one of the last names only, or\\ncombining one of the last names and one of the first\\nnames. Additionally, we calculated the inter-annotator\\nagreement by three annotators. The average F1-score of\\nthree pairs among these three annotators was 80.2 for 500\\nsentences of this dataset. This Pathology Report dataset is\\nthe only real (not dummy) dataset among our three\\ndatasets. Because we received manually de-identified\\nversion of the original real pathology reports, no ethical\\nreview was necessary.\\nMethods\\nWe used a Japanese morphological analyzer, Kuromoji,1\\nfor tokenization and part-of-speech (POS) tagging. We\\nregistered our customized dictionary, derived from\\nWikipedia entry names and entries of the Japanese\\nStandard Disease-code master [21], to this morphological\\nanalyzer in addition to the analyzer\\u0092s default dictionary.\\nWe implemented rule-based, CRF-based, and LSTM-\\nbased methods.\\nRule-based method\\nUnfortunately, the implementation of the best system for\\nthe MedNLP-1 de-identification task [22] is not publicly\\navailable. We implemented our own rule-based program\\nbased on the descriptions in their paper, to replicate the\\nsame system to the greatest extent possible. We present\\ntheir rules below for a target word x for each tag type.\\nAge\\nIf x\\u0092s detailed POS is \\u0093numeral\\u0094, then apply the rules in\\nTable 4.\\nHospital\\nIf one of following keywords appeared in x, then mark it\\nas hospital: ?? (a near clinic or hospital), ?? (this\\nclinic or hospital), or ?? (same clinic or hospital).\\nIf x\\u0092s POS is \\u0093noun\\u0094 and if detailed POS is not \\u0093non-au-\\ntonomous word\\u0094, or if x is either \\u0093?\\u0094, \\u0093?\\u0094, \\u0093?\\u0094 or \\u0093?\\u0094 (these\\nsymbols are used for manual de-identification because the\\ndatasets are dummy EHRs), and if suffix of x is one of the\\nTable 3 Detailed results for each privacy type in MedNLP-1 (De-identification task)\\n&lt;a &gt; age &lt;x &gt; sex &lt;t &gt; time &lt;h &gt; hospital name\\nP R F P R F P R F P R F\\nC3 90.32 87.5 88.89 100 100 100 87.16 91.49 89.27 97.30 94.74 96.00\\nB3 90.00 84.38 87.10 100 50.00 66.67 91.30 89.36 90.32 97.06 86.84 91.67\\nB1 93.33 87.5 90.32 100 100 100 90.65 89.36 90.00 89.47 89.47 89.47\\nB2 90.00 84.38 87.10 100 100 100 91.24 88.65 89.93 91.89 89.47 90.67\\nC1 96.67 90.62 93.55 100 50.00 66.67 91.18 87.94 89.53 93.55 76.32 84.06\\nA1 92.86 81.25 86.67 100 50.00 66.67 91.04 86.52 88.73 91.89 89.47 90.67\\nC2 96.67 90.62 93.55 100 50.00 66.67 89.13 87.23 88.17 96.77 78.95 86.96\\nA2 92.86 81.25 86.67 100 50.00 66.67 89.05 86.52 87.77 91.89 89.47 90.67\\nD1 92.31 75.00 82.76 100 50.00 66.67 82.84 78.72 80.73 96.15 65.79 78.12\\nG1 80.65 78.12 79.37 100 50.00 66.67 84.56 81.56 83.03 72.73 63.16 67.61\\nD3 88.89 75.00 81.36 100 50.00 66.67 83.08 76.60 79.70 96.15 65.79 78.12\\nD2 92.31 75.00 82.76 100 50.00 66.67 75.86 78.01 76.92 96.15 65.79 78.12\\nH2 83.87 81.25 82.54 100 100 100 73.79 75.89 74.83 77.78 73.68 75.68\\nH1 80.65 78.12 79.37 100 100 100 75.86 78.01 76.92 70.27 68.42 69.33\\nH3 83.87 81.25 82.54 100 100 100 73.79 75.89 74.83 70.27 68.42 69.33\\nP, R and F were calculated at the phrase level: P, precision; R, recall; F, F1-measure; and A, accuracy. A was calculated in the word level (the agreement ratio of B-*, I-* and O).\\nThe first column stands for participants\\u0092 team names, where the first letter stands for a team ID and the second numerical value stands for a submission run ID\\n1https://www.atilika.com/ja/kuromoji/\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 5 of 12\\nfollowing keywords, then mark it as hospital:?? (hospital\\nor clinic),????? (clinic), or?? (clinic).\\nSex\\nIf x is either ?? (man), ?? (woman), men, women,\\nman, woman (in English), then mark it as sex.\\nTime\\nIf x\\u0092s detailed POS is \\u0093numeral\\u0094 and if x consists\\nof four-digit-numbers+slash+two-or-one-digit-numbers\\n(corresponds to \\u0093yyyy/mm\\u0094) or two-or-one-digit-\\nnumbers+slash+two-or-one-digit-numbers (corresponds to\\n\\u0093mm/dd\\u0094), then mark it as time.\\nTable 4 Rules used for our rule-based method, original Japanese with English translations\\nOption 1 main rule Option 2\\n?\\n(next)\\n??? two years ago ?? (from)\\n?\\n(before)\\n?? last year ?? (until)\\n???\\n(before hospitalization)\\n?? last month ? (\\u0091s)\\n???\\n(after hospitalization)\\n?? last week ?? (early)\\n????\\n(after visit)\\n?? yesterday ?? (last)\\n??\\n(a.m.)\\n?? this year -- (from)\\n??\\n(p.m.)\\n?? this month -- (from)\\n????\\n(after onset)\\n?? this week ?? (over)\\n??????\\n(after onset)\\n?? today ?? (under)\\n??????\\n(after care)\\n?? today ?? (from)\\n?? next year ? (when)\\n?? next month ? (about)\\n?? next week ?? (about)\\n?? tomorrow ?? (about)\\n??? the week after next ?? (early)\\n??? day after tomorrow ?? (mid)\\n?? same year ?? (late)\\n?? same month ? (spring)\\n?? same day ? (summer)\\n?? following year ? (fall)\\n?? the next day ? (winter)\\n?? the next morning ? (morning)\\n?? the previous day ? (noon)\\n?? early morning ? (evening)\\n??? after that ? (night)\\nxx? xx (year) ?? (early morning)\\nxx? xx (month) ?? (early morning)\\nxx?? xx (week) ?? (before)\\nxx? xx (day) ?? (after)\\nxx? xx (o\\u0092clock) ?? (evening)\\nxx? xx (minutes) ?? (about)\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 6 of 12\\nIf x\\u0092s detailed POS is \\u0093numeral\\u0094 and followed by either\\nof ? (old), ? (old), or? (\\u0091s), then mark it as time.\\nIf x is followed further by either of \\u0093??\\u0094, \\u0093??\\u0094, \\u0093?\\n?\\u0094, \\u0093??\\u0094, \\u0093??\\u0094, \\u0093??\\u0094, \\u0093?\\u0094, \\u0093?\\u0094, \\u0093??\\u0094, \\u0093??\\u0094,\\n\\u0093??\\u0094, \\u0093????\\u0094, \\u0093????\\u0094, \\u0093???\\u0094, \\u0093????\\u0094,\\nor \\u0093????\\u0094, then include these words in the span of\\nthe marked time tag.\\nCRF-based method\\nWe implemented a CRF-based system because many\\nparticipants used CRFs in the MedNLP-1 de-identification\\ntask, including the second-best team and the baseline\\nsystem. The best participant used a rule-based system, as\\ndescribed previously. We used the MALLET2 library for\\nCRF implementation. We defined five training features for\\neach token3: part-of-speech (POS), detailed POS, character\\ntype (Hiragana, Katakana, Kanji, or Number), a binary\\nfeature whether a token is included in our user dictionary\\nor not, and another binary feature whether a token is\\nbeginning of its sentence or not.\\nLSTM-based method\\nOur LSTM-based method combines bidirectional LSTM\\n(bi-LSTM) and CRF, using character-based and word-\\nbased embeddings (Fig. 1) following earlier work that\\nhad been reported as successful for other languages [16].\\nFor word-based embedding, we used the existing\\nWord2Vec [23] model, which was trained using Japanese\\nWikipedia.4 We used bi-LSTM to embed characters;\\nthen we concatenated these two embeddings. This\\nconcatenated output was fed to another bi-LSTM and\\nthen sent to a CRF to output IOB tags.\\nOur implementation has been made publicly available\\nin GitHub.5 Table 5 presents the parameter settings.\\nResults\\nExperiment settings and evaluation metrics\\nWe followed the evaluation metrics of the MedNLP-1\\nshared task using IOB2 tagging [24]. We used four-fold\\ncross validation, whereas the rule-based method requires\\nno training data. We prepared five datasets: MedNLP\\n(MedNLP), dummy EHRs (dummy), pathology reports\\n(pathology), and MedNLP + dummy EHRs (MedNLP +\\ndummy). We also prepared a dataset that comprises\\nthese three datasets (all). For each dataset, we applied\\ncross validation. The CRF and LSTM are trained with\\nthree patterns of training data: the target dataset only,\\none of other datasets only, MedNLP + dummy, and all.\\nOur evaluation uses a strict match of named entity\\nspans, calculating F1-scores, precisions, and recalls.\\nTable 6 presents the evaluation results.\\nResults obtained using the MedNLP dataset\\nIn this MedNLP dataset, the total number of sex is very\\nsmall; that of person is zero. The rule-based system per-\\nformed best in terms of the F1-score because its rules\\nwere tuned originally to the very MedNLP dataset.\\nLSTM performed best for age and time, probably be-\\ncause these tags exhibit typical patterns of less variation.\\nLSTM is superior to Rule, except for sex and hospital.\\nRegarding sex, we observe better performance when\\nLSTM uses more training data. Therefore, the data size\\nis expected to have been the reason why LSTM was not\\ngood in sex.\\nResults obtained using the dummy EHR dataset\\nLSTM (M + d) performed best in terms of the F1-score.\\nCRF performed better when trained by M+ d dataset\\nthan with the target dataset only. This performance in-\\ncrease consists of decrease of age and increase of all\\nother tags, suggesting that these two datasets differ in\\ntheir age tag annotation scheme.\\nThe overall performance of this dummy EHR dataset\\nis worse than the MedNLP dataset, suggesting that the\\ndummy EHR dataset is more difficult to de-identify.\\nResults obtained using the pathology report dataset\\nThe LSTM-based method was better (81.67) than the\\nCRF-based method (74.26), as shown by the 7.41 point\\nF1-score when applied to our Pathology Report dataset.\\nOur rule-based system achieved very high recall, but\\nvery low precision scores for time, exhibiting a difference\\nby 38 points. The pathology reports include many clin-\\nical inspection values written in an \\u0093xx/yy\\u0094 format, which\\nmight engender confusion with dates expressed in an\\n\\u0093mm/dd\\u0094 format. We applied a workaround to limit\\n[1 &lt; = mm &lt; = 12] and [1 &lt; = dd &lt; = 31], but it was insuf-\\nficient: we need contextual information, not just rules.\\nIn addition, hospital is better than time, with less differ-\\nence (15 points) of precision and recall.\\nWhen trained with the Pathology Report dataset only,\\nits performance is better than our rule-based system.\\nWhen trained with the M+ d dataset, which does not\\ncontain the pathology dataset, neither CRF nor LSTM\\nworks fine because the pathology reports differ greatly in\\nterms of their styles of description and named entities.\\nDiscussion\\nThese results suggest that our datasets have quite differ-\\nent characteristics in what context and in what form\\ntheir named entities appear, but LSTM adapted to these\\ndifferences well. Adding the Pathological Report dataset\\n2http://mallet.cs.umass.edu/\\n3Hereinafter, \\u0093token\\u0094 means a \\u0093morpheme\\u0094 of the Japanese language,\\nwhich does not have any space between tokens. A \\u0093morpheme\\u0094 is the\\nsmallest meaningful unit in a language.\\n4http://www.cl.ecei.tohoku.ac.jp/~m~suzuki/jawiki_vector/\\n5https://github.com/johokugsk\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 7 of 12\\nto the training data seems to degrade the system per-\\nformance for other target test datasets because of the\\ndifferent dataset characteristics (examples presented in\\nTable 1). For example, when trained with the Patho-\\nlogical Report dataset, the hospital tags of the MedNLP\\ndataset show lower performance because of the different\\ndescriptions of hospital names among these two data-\\nsets. The Pathological Report dataset has full hospital\\nnames such as \\u0093Shizudai Dermatology Clinic,\\u0094 but the\\nother two datasets have more casual descriptions such as\\nFig. 1 Conceptual figure of our LSTM-based model, showing embedding and NER in separate figures. + means concatenation. The first figure\\nshows the embedding part, where Wx is an x\\nth input word, Lx,i is an i\\nth letter of the word Wx, r denotes right to left (forward) LSTM, l denotes left\\nto right (backward) LSTM, Vx is an intermediate node which corresponds to Wx. The second figure shows the NER part, where fl denotes forward\\nLSTM, bl denotes backward LSTM, c denotes concatenated vector, finally a CRF layer is shown with an example predicted named entities in the\\nBIO annotation style\\nTable 5 LSTM parameter settings\\nWord embedding size 200\\nCharacter embedding size 100\\nHidden layer of character 100\\nHidden layer of LSTM 300\\nLearning rate 0.001\\nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 8 of 12\\nTable 6 Evaluation results for each tag and in total, for different methods (rule, CRF, LSTM) and different evaluation datasets\\n(MedNLP, dummy EHR, and pathology reports). M, d, and P respectively denote tr'</span>text12\n:   <span style=white-space:pre-wrap>'Wolff et al. Journal of Biomedical Semantics           (2020) 11:12 \\nhttps://doi.org/10.1186/s13326-020-00226-w\\nRESEARCH Open Access\\nMethodologically grounded semantic\\nanalysis of large volume of chilean medical\\nliterature data applied to the analysis of\\nmedical research funding efficiency in Chile\\nPatricio Wolff1, Sebastián Ríos1, David Clavijo1, Manuel Graña2* and Miguel Carrasco3\\nAbstract\\nBackground: Medical knowledge is accumulated in scientific research papers along time. In order to exploit this\\nknowledge by automated systems, there is a growing interest in developing text mining methodologies to extract,\\nstructure, and analyze in the shortest time possible the knowledge encoded in the large volume of medical literature.\\nIn this paper, we use the Latent Dirichlet Allocation approach to analyze the correlation between funding efforts and\\nactually published research results in order to provide the policy makers with a systematic and rigorous tool to assess\\nthe efficiency of funding programs in the medical area.\\nResults: We have tested our methodology in the Revista Médica de Chile, years 2012-2015. 50 relevant semantic\\ntopics were identified within 643 medical scientific research papers. Relationships between the identified semantic\\ntopics were uncovered using visualization methods. We have also been able to analyze the funding patterns of\\nscientific research underlying these publications. We found that only 29% of the publications declare funding sources,\\nand we identified five topic clusters that concentrate 86% of the declared funds.\\nConclusions: Our methodology allows analyzing and interpreting the current state of medical research at a national\\nlevel. The funding source analysis may be useful at the policy making level in order to assess the impact of actual\\nfunding policies, and to design new policies.\\nKeywords: Data science, Machine learning, Latent Dirichlet allocation, Healthcare management, Strategy\\nBackground\\nDue to the speed of innovation and change of research\\ntrends in the medical community, research topic tax-\\nonomies published by governmental agencies for funding\\ncalls often diverge from the reality of the research practice.\\nOur working hypothesis is that semantic topic analysis\\nprovides an unbiased and accurate portrait of the actual\\nresearch topics that are generating published results. In\\nthis paper we exploit the information from a national\\n*Correspondence: manuel.grana@ehu.es\\n2Computational Intelligence Group, University of Basque Country, P. Manuel\\nLardizabal 1, 20018 San Sebastián, Spain\\nFull list of author information is available at the end of the article\\nmedical publication, described below, to identify the areas\\nof active research, correlating them with the acknowl-\\nedged funding sources, and non-funded personal effort\\nbacking these scientific results. This analysis provides the\\npolicymaker with a systematic, unbiased, and automated\\ntool for the evaluation of the results of funding programs,\\nallowing to assess the coherence of the national research\\nfunding policies with the actual research outcomes.\\nMethodology background'</span>text13\n:   <span style=white-space:pre-wrap>'RESEARCH Open Access\\nAn integrative knowledge graph for rare\\ndiseases, derived from the Genetic and\\nRare Diseases Information Center (GARD)\\nQian Zhu1*\\u0086 , Dac-Trung Nguyen1\\u0086, Ivan Grishagin1, Noel Southall1, Eric Sid2 and Anne Pariser2\\nAbstract\\nBackground: The Genetic and Rare Diseases (GARD) Information Center was established by the National Institutes\\nof Health (NIH) to provide freely accessible consumer health information on over 6500 genetic and rare diseases. As\\nthe cumulative scientific understanding and underlying evidence for these diseases have expanded over time,\\nexisting practices to generate knowledge from these publications and resources have not been able to keep pace.\\nThrough determining the applicability of computational approaches to enhance or replace manual curation tasks,\\nwe aim to both improve the sustainability and relevance of consumer health information, but also to develop a\\nfoundational database, from which translational science researchers may start to unravel disease characteristics that\\nare vital to the research process.\\nResults: We developed a meta-ontology based integrative knowledge graph for rare diseases in Neo4j. This\\nintegrative knowledge graph includes a total of 3,819,623 nodes and 84,223,681 relations from 34 different\\nbiomedical data resources, including curated drug and rare disease associations. Semi-automatic mappings were\\ngenerated for 2154 unique FDA orphan designations to 776 unique GARD diseases, and 3322 unique FDA\\ndesignated drugs to UNII, as well as 180,363 associations between drug and indication from Inxight Drugs, which\\nwere integrated into the knowledge graph. We conducted four case studies to demonstrate the capabilities of this\\nintegrative knowledge graph in accelerating the curation of scientific understanding on rare diseases through the\\ngeneration of disease mappings/profiles and pathogenesis associations.\\nConclusions: By integrating well-established database resources, we developed an integrative knowledge graph\\ncontaining a large volume of biomedical and research data. Demonstration of several immediate use cases and\\nlimitations of this process reveal both the potential feasibility and barriers of utilizing graph-based resources and\\napproaches to support their use by providers of consumer health information, such as GARD, that may struggle\\nwith the needs of maintaining knowledge reliant on an evolving and growing evidence-base. Finally, the successful\\nintegration of these datasets into a freely accessible knowledge graph highlights an opportunity to take a\\ntranslational science view on the field of rare diseases by enabling researchers to identify disease characteristics,\\nwhich may play a role in the translation of discover across different research domains.\\nKeywords: GARD, Rare diseases, Ontology, Data integration, Knowledge graph\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: qian.zhu@nih.gov\\n\\u0086Qian Zhu and Dac-Trung Nguyen contributed equally to this work.\\n1Division of Pre-Clinical Innovation, National Center for Advancing\\nTranslational Sciences (NCATS), National Institutes of Health (NIH), Rockville,\\nMD 20850, USA\\nFull list of author information is available at the end of the article\\nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 \\nhttps://doi.org/10.1186/s13326-020-00232-y\\nIntroduction\\nAn estimated 30 million people in the United States are\\naffected by a rare disease, which is defined as a disease\\nthat affects fewer than 200,000 individuals in the United\\nStates [1]. The majority of rare disease are thought to\\nhave a genetic etiology [2] with studies reporting them\\nresponsible for almost 10% of adult and 30% of pediatric\\nhospitalizations [3]. Despite the great heterogeneity of\\ndiseases included in this definition, many patients and\\ntheir families share in common struggles, such as with\\ndiagnostic delay leading to \\u0093an average of 7.6 years\\u0094 from\\ninitial onset of symptoms to receiving a diagnosis and\\nrequiring the involvement of 7.3 physicians on average\\n[4]. These shared challenges faced in the broader rare\\ndisease patient community are often due to a lack of\\neither up-to-date information or awareness amongst\\nproviders and the public at large. Efforts to tackle these\\nissues led to the passage of the Rare Disease Act of 2002\\nand the establishment of several programs by the\\nNational Institutes of Health (NIH) to improve research\\nactivities and public access to information on rare dis-\\neases. In particular, the Genetic and Rare Diseases\\n(GARD) information center was charged with providing\\nfreely accessible consumer health information in plain\\nlanguage, and it has been investigating the challenge of\\nshifting from an entirely manual process to leveraging\\ncomputational approaches to curate the accumulated\\nbiomedical and clinical research knowledge of over 6500\\nrare diseases, and more rapidly make information ac-\\ncessible 1) to educate patients, families, and health care\\nproviders with more accurate and real-time knowledge\\nabout a rare disease, and 2) to support novel scientific\\nresearch efforts and apply disease-agnostic translational\\nscience approaches to the field of rare diseases as a\\nwhole [5].\\nGiven the pace of ongoing scientific discovery, parsing\\nthrough the accumulated research publications and\\nconveying this knowledge in a plain language format ac-\\ncessible to low-health literacy audiences presents a sig-\\nnificant task for a single disease, let alone for over 6500\\nrare diseases. Thus, a huge amount of effort to accumu-\\nlate and curate data for rare diseases has been made glo-\\nbally. For instance, the GARD Information Center\\nprovides interpretable profiles in plain language for each\\nrare disease [5]; Orphanet focuses on expert manual cur-\\nation of a disease\\u0092s clinical presentation [6]; and Online\\nMendelian Inheritance in Man® (OMIM®) conducts a\\nsimilar expert-driven focus on defining genotype and\\nphenotype relationships [7]. The discreteness of such\\nheterogeneous data, however, impedes their direct use\\nfor consumer audiences. To overcome this barrier, in\\nthis study, we integrated these well-known resources in\\none knowledge graph to semantically interconnect all\\ndata together by means of the data points as nodes and\\ntheir relationships as edges, as a first step in bridging the\\nuse of these resources in consumer-facing health\\ninformation.\\nBiomedical data integration is an important and tech-\\nnical approach to tackling biomedical problems. Current\\nprogress in computational technology allows vast data\\nstorages and powerful computational processes to be\\nmore affordable and accessible. As a result, biomedical\\nscientists have gradually gained an awareness of the im-\\nportance of pooling diverse types of data pertaining to a\\nspecific medical entity to enhance their research under-\\nstanding [8]. Representing integrative data in the form of\\na graph has attracted many interests, particularly in the\\nbiomedical domain. Karczewski K, et al. have reviewed\\nand discussed the potential and usage and challenges of\\nintegrating diverse types of omics data for human health\\nand disease [9]. Biomedical Informatics Research Net-\\nwork (BIRN) is an integrative resource by semantically\\nintegrating data produced by multiple institutions for\\ndata analysis on Neurosciences [10]. Similar efforts have\\nalso begun to emerge with applications directed at the\\nfield in rare disease, such as the semantic Diseasecard,\\nwhich integrates rare disease data from distinct sources\\nin a semantic web environment [11]. A similar EU plat-\\nform, RD-Connect connects databases, registries, bio-\\nbanks and clinical bioinformatics to support research in\\ndiscovering new genes, biomarkers, and therapeutic\\ntargets more quickly and efficiently [12]. The Monarch\\nInitiative as another analytic platform, semantically inte-\\ngrates genotype and phenotype data across differing spe-\\ncies and sources [13], and has led to the establishment\\nof MONDO (Monarch Merged Disease Ontology) [14]\\nas a cohesive ontology for connecting many of the dis-\\nease databases and resources. The integrative knowledge\\ngraph we introduce in this study applies well-established\\nrare disease data drawn from GARD, Orphanet, OMIM\\nand MONDO as a backbone, and then expands to a\\nwide spectrum of additional biomedical data, including\\nphenotypes, genes and curated FDA approved drugs and\\nFDA orphan drug designations.\\nThere are demonstrated merits and successes in using\\ngraph database to support the management of large bio-\\nmedical datasets. While relational databases excel at man-\\naging relationships between data, graph databases provide\\nunique abilities to manage n-th degree relationships\\namong complex types of biomedical data. Furthermore,\\ngraph databases are particularly apt at representing hier-\\narchical data, such as disease categories and complex se-\\nmantic relationships among different types of data. Neo4j\\nas a graph database management system [15], has been\\nwidely applied in such use cases within the biomedical do-\\nmain. Such as, Gratzl S, et al. demonstrated the utility of\\nNeo4j in developing integrated visual analysis platform for\\nbiomedical data [16]; Himmelstein D, et al. constructed\\nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 Page 2 of 13\\nHetionet, an integrative Neo4j network that encodes\\nknowledge from millions of biomedical studies to\\nprioritize drugs for repurposing [17]. In this paper, we\\nintroduce this rare diseases integrative knowledge graph,\\nbuilt in Neo4j as a backend graph database ingesting a\\nlarge variety of biomedical datasets. We detail data prepar-\\nation and entity resolution methodologies in generating\\ninitial insights and results, and the potential benefits for\\nutilizing a knowledge graph-based approach to interpret\\nbiomedical research at a scale and pace that would be un-\\nsustainable when limited to the manual curation efforts\\nthat define current processes used in curating consumer\\nhealth information.\\nMaterials\\nAt the time of writing, the knowledge graph integrates\\n34 different biomedical datasets including GARD. We\\nbriefly describe several primary resources as below.\\nRare disease related data resources\\nBesides GARD data retrieved from our internal database,\\nall other datasets were downloaded from NCBO Biopor-\\ntal [18].\\nGARD is currently managed by the Office of Rare\\nDiseases Research (ORDR) within the National Center\\nfor Advancing Translational Sciences (NCATS), has\\nremained an important portal for patients, health-care\\nprofessionals, and researchers seeking to understand the\\ncurrent state of genetic and rare diseases. GARD in-\\ncludes curated disease information comprised of 15 dif-\\nferent sections, such as summary, diagnosis, inheritance,\\netc. Notably, not all of GARD diseases have a complete\\nlist of these 15 information sections, due to data unavail-\\nability at the time of curation and update. In this study,\\nwe extracted and applied disease specific information\\nsections, if applicable from our internal GARD database.\\nOther sections, such as Resources, Organizations will be\\nexplored in the future [5].\\nOrphanet is a unique resource, gathering and improv-\\ning knowledge on rare diseases so as to improve the\\ndiagnosis, care and treatment of patients with rare dis-\\neases [6].\\nMonarch Disease Ontology (MONDO) is a semi-\\nautomatically constructed ontology that merges multiple\\ndisease resources to yield a coherent merged ontology\\n[14].\\nOnline Mendelian Inheritance in Man® (OMIM®) is\\na comprehensive, authoritative compendium of human\\ngenes and genetic phenotypes. The full-text, referenced\\noverviews in OMIM contain information on all known\\nmendelian disorders and over 15,000 genes [7].\\nHuman Phenotype Ontology (HPO) provides a stan-\\ndardized vocabulary of phenotypic abnormalities en-\\ncountered in human disease [19].\\nFDA orphan drugs\\nFDA orphan drug designation provides orphan desig-\\nnations to drugs and biologics, which are defined as\\nthose intended for the safe and effective treatment, diag-\\nnosis or prevention of rare diseases/disorders [20]. In\\nthis study, we employed orphan drug designation data\\nfrom the FDA [21], several examples of FDA orphan\\ndrug designations retrieved from the FDA are shown in\\nTable 1. Specifically we utilized the associations between\\nFDA designated drugs (the column of \\u0093Generic Name\\u0094\\nin Table 1) and their designations (the column of \\u0093Or-\\nphan Designation\\u0094 in Table 1). Although the data is pre-\\nsented in a structured form, orphan designation is\\ncaptured in free text, such as examples shown in Table\\n1. In that manner, additional curation was conducted in\\nthis study to be able to map orphan designations to\\nGARD diseases and designated drugs to UNII (Unique\\nIngredient Identifier).\\nInxight Drugs is a drug resource developed by NCAT\\nS. Inxight Drugs [22] incorporates the most comprehen-\\nsive subset of substances and related biological mecha-\\nnisms pertaining to translational research and connects\\nthem to the appropriate disease indications. As part of\\nInxight Drugs, explicit connections between drugs and\\nconditions were manually identified from scientific arti-\\ncles, press releases, FDA labels, and large-scale databases\\n(e.g. AdisInsight [23]). For those identified associations,\\nthe curators manually matched conditions to MeSH,\\nDisease Ontology (DO), and drugs to UNII (Unique In-\\ngredient Identifier). For example, one association pre-\\nsenting in Inxight Drugs is as \\u0093CYROMAZINE\\u0094 (with\\nUNII: CA49Y29RA9) has indication of \\u0093MYIASIS,\\nCUTANEOUS MYIASIS OF SHEEP\\u0094. In this study, we\\nextracted associations between FDA approved drugs and\\ndiseases, and integrated them into our integrative know-\\nledge graph.\\nMethods\\nIn this paper, we detail the process of developing the in-\\ntegrative knowledge graph for rare diseases with inclu-\\nsion of multiple well-known biomedical datasets\\nincluding GARD. We also demonstrate the use of this\\nintegrative graph to support biomedical research for rare\\ndiseases. More details about this process is described as\\nbelow.\\nData collection\\nGARD data is curated in two folds, manual curation by\\ninformation specialists from GARD, and programmatic\\nextraction from Orphanet. The curated data is stored in\\na relational database, from where we extracted GARD\\ndata for this study. GARD provides comprehensive in-\\nformation about rare diseases from different aspects,\\nincluding summary, sign and symptoms, treatment,\\nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 Page 3 of 13'</span>text14\n:   <span style=white-space:pre-wrap>'REVIEW Open Access\\nNatural language processing algorithms\\nfor mapping clinical text fragments onto\\nontology concepts: a systematic review\\nand recommendations for future studies\\nMartijn G. Kersloot1,2* , Florentien J. P. van Putten1, Ameen Abu-Hanna1, Ronald Cornet1 and Derk L. Arts1,2\\nAbstract\\nBackground: Free-text descriptions in electronic health records (EHRs) can be of interest for clinical research\\nand care optimization. However, free text cannot be readily interpreted by a computer and, therefore, has\\nlimited value. Natural Language Processing (NLP) algorithms can make free text machine-interpretable by\\nattaching ontology concepts to it. However, implementations of NLP algorithms are not evaluated\\nconsistently. Therefore, the objective of this study was to review the current methods used for developing\\nand evaluating NLP algorithms that map clinical text fragments onto ontology concepts. To standardize the\\nevaluation of algorithms and reduce heterogeneity between studies, we propose a list of recommendations.\\nMethods: Two reviewers examined publications indexed by Scopus, IEEE, MEDLINE, EMBASE, the ACM Digital\\nLibrary, and the ACL Anthology. Publications reporting on NLP for mapping clinical text from EHRs to\\nontology concepts were included. Year, country, setting, objective, evaluation and validation methods, NLP\\nalgorithms, terminology systems, dataset size and language, performance measures, reference standard,\\ngeneralizability, operational use, and source code availability were extracted. The studies\\u0092 objectives were\\ncategorized by way of induction. These results were used to define recommendations.\\nResults: Two thousand three hundred fifty five unique studies were identified. Two hundred fifty six studies\\nreported on the development of NLP algorithms for mapping free text to ontology concepts. Seventy-seven\\ndescribed development and evaluation. Twenty-two studies did not perform a validation on unseen data and\\n68 studies did not perform external validation. Of 23 studies that claimed that their algorithm was\\ngeneralizable, 5 tested this by external validation. A list of sixteen recommendations regarding the usage of\\nNLP systems and algorithms, usage of data, evaluation and validation, presentation of results, and\\ngeneralizability of results was developed.\\n(Continued on next page)\\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article\\'s Creative Commons\\nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative Commons\\nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\ndata made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: m.g.kersloot@amsterdamumc.nl\\n1Amsterdam UMC, University of Amsterdam, Department of Medical\\nInformatics, Amsterdam Public Health Research Institute Castor EDC, Room\\nJ1B-109, PO Box 22700, 1100 DE Amsterdam, The Netherlands\\n2Castor EDC, Amsterdam, The Netherlands\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 \\nhttps://doi.org/10.1186/s13326-020-00231-z\\n(Continued from previous page)\\nConclusion: We found many heterogeneous approaches to the reporting on the development and evaluation of NLP\\nalgorithms that map clinical text to ontology concepts. Over one-fourth of the identified publications did not perform\\nan evaluation. In addition, over one-fourth of the included studies did not perform a validation, and 88% did not\\nperform external validation. We believe that our recommendations, alongside an existing reporting standard, will\\nincrease the reproducibility and reusability of future studies and NLP algorithms in medicine.\\nKeywords: Ontologies, Entity linking, Annotation, Concept mapping, Named-entity recognition, Natural language\\nprocessing, Evaluation studies, Recommendations for future studies\\nBackground\\nOne of the main activities of clinicians, besides providing\\ndirect patient care, is documenting care in the electronic\\nhealth record (EHR). Currently, clinicians document clin-\\nical findings and symptoms primarily as free-text descrip-\\ntions within clinical notes in the EHR since they are not\\nable to fully express complex clinical findings and nuances\\nof every patient in a structured format [1, 2]. These free-\\ntext descriptions are, amongst other purposes, of interest\\nfor clinical research [3, 4], as they cover more information\\nabout patients than structured EHR data [5]. However,\\nfree-text descriptions cannot be readily processed by a\\ncomputer and, therefore, have limited value in research\\nand care optimization.\\nOne method to make free text machine-processable is\\nentity linking, also known as annotation, i.e., mapping\\nfree-text phrases to ontology concepts that express the\\nphrases\\u0092 meaning. Ontologies are explicit formal specifica-\\ntions of the concepts in a domain and relations among\\nthem [6]. In the medical domain, SNOMED CT [7] and\\nthe Human Phenotype Ontology (HPO) [8] are examples\\nof widely used ontologies to annotate clinical data. After\\nthe data has been annotated, it can be reused by clinicians\\nto query EHRs [9, 10], to classify patients into different\\nrisk groups [11, 12], to detect a patient\\u0092s eligibility for clin-\\nical trials [13], and for clinical research [14].\\nNatural Language Processing (NLP) can be used to\\n(semi-)automatically process free text. The literature indi-\\ncates that NLP algorithms have been broadly adopted and\\nimplemented in the field of medicine [15, 16], including\\nalgorithms that map clinical text to ontology concepts\\n[17]. Unfortunately, implementations of these algorithms\\nare not being evaluated consistently or according to a pre-\\ndefined framework and limited availability of data sets and\\ntools hampers external validation [18].\\nTo improve and standardize the development and evalu-\\nation of NLP algorithms, a good practice guideline for\\nevaluating NLP implementations is desirable [19, 20].\\nSuch a guideline would enable researchers to reduce the\\nheterogeneity between the evaluation methodology and\\nreporting of their studies. Generic reporting guidelines\\nsuch as TRIPOD [21] for prediction models, STROBE\\n[22] for observational studies, RECORD [23] for studies\\nconducted using routinely-collected health data, and\\nSTARD [24] for diagnostic accuracy studies, are available,\\nbut are often not used in NLP research. This is presum-\\nably because some guideline elements do not apply to\\nNLP and some NLP-related elements are missing or un-\\nclear. We, therefore, believe that a list of recommenda-\\ntions for the evaluation methods of and reporting on\\nNLP studies, complementary to the generic reporting\\nguidelines, will help to improve the quality of future\\nstudies.\\nIn this study, we will systematically review the\\ncurrent state of the development and evaluation of\\nNLP algorithms that map clinical text onto ontology\\nconcepts, in order to quantify the heterogeneity of\\nmethodologies used. We will propose a structured list\\nof recommendations, which is harmonized from exist-\\ning standards and based on the outcomes of the re-\\nview, to support the systematic evaluation of the\\nalgorithms in future studies.\\nMethods\\nThis study consists of two phases: a systematic review of\\nthe literature and the formation of recommendations\\nbased on the findings of the review.\\nLiterature review\\nA systematic review of the literature was performed\\nusing the Preferred Reporting Items for Systematic re-\\nviews and Meta-Analyses (PRISMA) statement [25].\\nSearch strategy and study selection\\nWe searched Scopus, IEEE, MEDLINE, EMBASE, the As-\\nsociation for Computing Machinery (ACM) Digital Library,\\nand the Association for Computational Linguistics (ACL)\\nAnthology for the following keywords: Natural Language\\nProcessing, Medical Language Processing, Electronic Health\\nRecord, reports, charts, clinical notes, clinical text, medical\\nnotes, ontolog*, concept*, encod*, annotat*, code, and cod-\\ning. We excluded the words \\u0091reports\\u0092 and \\u0091charts\\u0092 in the\\nACL and ACM databases since these databases also contain\\npublications on non-medical subjects. The detailed search\\nstrategies for each database can be found in Additional file\\n2. We searched until December 19, 2019 and applied the\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 2 of 21\\nfilters \\u0093English\\u0094 and \\u0093has abstract\\u0094 for all databases. More-\\nover, we applied the filters \\u0093Medicine, Health Professions,\\nand Nursing\\u0094 for Scopus, the filters \\u0093Conferences\\u0094, \\u0093Jour-\\nnals\\u0094, and \\u0093Early Access Articles\\u0094 for IEEE, and the filter\\n\\u0093Article\\u0094 for Scopus and EMBASE. EndNote X9 [26] and\\nRayyan [27] were used to review and delete duplicates.\\nThe selection process consisted of three phases. In the\\nfirst phase, two independent reviewers with a Medical\\nInformatics background (MK, FP) individually assessed\\nthe resulting titles and abstracts and selected publica-\\ntions that fitted the criteria described below.\\nInclusion criteria were:\\n\\001 Medical language processing as the main topic of\\nthe publication\\n\\001 Use of EHR data, clinical reports, or clinical notes\\n\\001 Algorithm performs annotation\\n\\001 Publication is written in English\\nFig. 1 PRISMA flow diagram\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 3 of 21\\nSome studies do not describe the application of NLP in\\ntheir study by only listing NLP as the used method, instead\\nof describing its specific implementation. Additionally,\\nsome studies create their own ontology to perform NLP\\ntasks, instead of using an established, domain-accepted\\nontology. Both approaches limit the generalizability of the\\nstudy\\u0092s methods. Therefore, we defined the following exclu-\\nsion criteria:\\n\\001 Implementation was not described\\n\\001 Implementation does not use an existing established\\nontology for encoding\\nTable 1 Induced objective tasks with their definition and an example\\nInduced NLP task(s) Description Example\\nConcept detection 1 Assign ontology concepts to phrases in free\\ntext (i.e., entity linking or annotation)\\n\\u0093Systolic blood pressure\\u0094 can be represented as SNOMED-CT\\nconcept 271649006 | Systolic blood pressure (observable entity) |\\nEvent detection Detect events in free text \\u0093Patient visited the outpatient clinic in January 2020\\u0094 is an\\nevent of type Visit.\\nRelationship detection Detect semantic relationships between\\nconcepts in free text\\nThe concept Lung cancer in \\u0093This patient was diagnosed with\\nrecurrent lung cancer\\u0094 is related to the concept Recurrence.\\nText normalization Transform free text into a single canonical\\nform\\n\\u0093This patient was diagnosed with influenza last year.\\u0094 becomes\\n\\u0093This patient be diagnose with influenza last year.\\u0094\\nText summarization Create a short summary of free text and\\npossible restructure the text based on this\\nsummary\\n\\u0093Last year, this patient visited the clinic and was diagnosed with\\ndiabetes mellitus type 2, and in addition to his diabetes, the\\npatient was also diagnosed with hypertension\\u0094 becomes\\n\\u0093Last year, this patient was diagnosed with diabetes mellitus\\ntype 2 and hypertension\\u0094.\\nClassification Assign categories to free text A report containing the text \\u0093This patient is not diagnosed\\nyet\\u0094 will be assigned to the category Undiagnosed.\\nPrediction Create a predictive model based on free text Predict the outcome of the APACHE score based on the\\n(free-text) content in a patient chart.\\nIdentification Identify documents (e.g., reports or patient\\ncharts) that match a specific condition\\nbased on the contents of the document\\nFind all patient charts that describe patients with hypertension\\nand a BMI above 30.\\nSoftware development Develop new or build upon existing NLP\\nsoftware\\nA new algorithm was developed to map ontology concepts\\nto free text in clinical reports.\\nSoftware evaluation Evaluate the effectiveness of NLP software The mapping algorithm has an F-score of 0.874.\\n1.Also known as Medical Entity Linking and Medical Concept Normalization\\nTable 2 Induced objective categories with their definition and associated NLP task(s)\\nInduced category Induced NLP task(s) Definition\\nComputer-assisted coding Concept detection Perform semi-automated annotation (i.e., with a human in the loop)\\nInformation comparison Concept detection\\nEvent detection\\nRelationship detection\\nCompare extracted structured information to information available in free-text form\\nInformation enrichment Concept detection\\nEvent detection\\nRelationship detection\\nText normalization\\nText summarization\\nExtract structured information from free text and attach this new information to the source\\nInformation extraction Concept detection\\nEvent detection\\nRelationship detection\\nExtract structured information from free text\\nPrediction Classification\\nPrediction\\nIdentification\\nUse structured information to classify free-text reports, predict outcomes, or identify cases\\nSoftware development\\nand evaluation\\nSoftware development\\nSoftware evaluation\\nDevelop new NLP software or evaluate new or existing NLP software\\nText processing Text normalization\\nText summarization\\nTransform free text into a new, more comprehensible form\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 4 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\nA\\nfs\\nha\\nr\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nD\\nat\\na\\nW\\nar\\neh\\nou\\nse\\nD\\nat\\na\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n(C\\nPT\\n,H\\nC\\nPC\\nS,\\nIC\\nD\\n-\\n10\\n,I\\nC\\nD\\n10\\nC\\nM\\n/\\nIC\\nD\\n9C\\nM\\n,\\nLO\\nIN\\nC\\n,M\\neS\\nH\\n,S\\nN\\nO\\nM\\nED\\n-\\nC\\nT,\\nRx\\nN\\nor\\nm\\n)\\nN\\not\\nlis\\nte\\nd\\nN\\no,\\non\\nly\\nlin\\nks\\nto\\ncT\\nA\\nKE\\nS\\nso\\nur\\nce\\nco\\nde\\n[2\\n9]\\nA\\nln\\naz\\nza\\nw\\ni\\n20\\n16\\nU\\nK\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nPh\\nen\\noC\\nH\\nF\\nco\\nrp\\nus\\n1\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n0]\\nA\\ntu\\ntx\\na\\n20\\n18\\nSp\\nai\\nn\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nEH\\nR\\ndo\\ncu\\nm\\nen\\nts\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nN\\new\\nIC\\nD\\n(S\\nN\\nO\\nM\\nED\\n-C\\nT\\nfo\\nr\\nno\\nrm\\nal\\niz\\nat\\nio\\nn)\\nN\\not\\nye\\nt,\\nai\\nm\\nto\\nem\\nbe\\nd\\nit\\nin\\nhu\\nm\\nan\\n-s\\nup\\ner\\nvi\\nse\\nd\\nlo\\nop\\nN\\not\\nlis\\nte\\nd\\n[3\\n1]\\nBa\\nrr\\net\\nt\\n20\\n13\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nPa\\nlli\\nat\\niv\\ne\\nca\\nre\\nco\\nns\\nul\\nt\\nle\\ntt\\ner\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\no,\\nbu\\nt\\npl\\nan\\nne\\nd\\n[3\\n2]\\nBe\\nck\\ner\\n20\\n16\\nG\\ner\\nm\\nan\\ny\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSh\\nA\\nRe\\n/C\\nLE\\nF\\nco\\nrp\\nus\\n(2\\n01\\n3)\\n2\\nEx\\nis\\ntin\\ng\\nG\\ner\\nm\\nan\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT\\n(E\\nng\\nlis\\nh)\\n,\\nU\\nM\\nLS\\n(G\\ner\\nm\\nan\\n)\\nN\\not\\nye\\nt,\\nst\\nill\\nun\\nde\\nr\\nde\\nve\\nlo\\npm\\nen\\nt\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n3]\\nBe\\nck\\ner\\n20\\n19\\nG\\ner\\nm\\nan\\ny\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nof\\npa\\ntie\\nnt\\ns\\nw\\nith\\nkn\\now\\nn\\nco\\nlo\\nre\\nct\\nal\\nca\\nnc\\ner\\nO\\nw\\nn\\nG\\ner\\nm\\nan\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nYe\\ns,\\nle\\nd\\nto\\nim\\npr\\nov\\ned\\nqu\\nal\\nity\\nof\\nca\\nre\\nfo\\nr\\nco\\nlo\\nre\\nct\\nal\\npa\\ntie\\nnt\\ns\\nN\\not\\nlis\\nte\\nd\\n[3\\n4]\\nBe\\nja\\nn\\n20\\n15\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nan\\nd\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nda\\nta\\nse\\nt\\n(2\\n01\\n0)\\n3\\nO\\nw\\nn\\n+\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\no\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n5]\\nC\\nas\\ntr\\no\\n20\\n10\\nSp\\nai\\nn\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nw\\nith\\n\\u0091m\\nos\\nt\\nre\\nle\\nva\\nnt\\nin\\nfo\\nrm\\nat\\nio\\nn\\u0092\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n6]\\nC\\nat\\nlin\\ng\\n20\\n18\\nU\\nK\\nN\\no\\nSo\\nft\\nw\\nar\\ne\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nM\\nIM\\nIC\\n-II\\nId\\nat\\nas\\net\\n4\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nIC\\nD\\n-9\\n-C\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[3\\n7]\\nC\\nha\\npm\\nan\\n20\\n04\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nEm\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[3\\n8]\\nC\\nhe\\nn\\n20\\n16\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nan\\nd\\npr\\nog\\nre\\nss\\nno\\nte\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[3\\n9]\\nC\\nhi\\nar\\nam\\nel\\nlo\\n20\\n16\\nIta\\nly\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\n(c\\nar\\ndi\\nol\\nog\\ny,\\ndi\\nab\\net\\nol\\nog\\ny,\\nhe\\npa\\nto\\nlo\\ngy\\n,n\\nep\\nhr\\nol\\nog\\ny,\\nan\\nd\\non\\nco\\nlo\\ngy\\n)\\nO\\nw\\nn\\nIta\\nlia\\nn\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[4\\n0]\\nC\\nho\\nde\\ny\\n20\\n16\\nU\\nSA\\nSe\\nm\\nEv\\nal\\n(2\\n01\\n4)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nIC\\nU\\nD\\nat\\na:\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns,\\nEC\\nG\\n,\\nec\\nho\\n,a\\nnd\\nra\\ndi\\nol\\nog\\ny\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[4\\n1]\\nC\\nhu\\nng\\n20\\n05\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nEc\\nho\\nca\\nrd\\nio\\ngr\\nam\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nye\\nt,\\nit\\nw\\nill\\nbe\\nus\\ned\\nto\\npo\\npu\\nla\\nte\\na\\nre\\ngi\\nst\\nry\\nN\\not\\nlis\\nte\\nd\\n[4\\n2]\\nC\\nom\\nbi\\n20\\n18\\nIta\\nly\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nVi\\ngi\\nSe\\ngn\\n(a\\ndv\\ner\\nse\\ndr\\nug\\nre\\nac\\ntio\\nns\\n)\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nIta\\nlia\\nn\\n+\\nEn\\ngl\\nis\\nh\\nN\\new\\nM\\ned\\nD\\nRA\\nYe\\ns,\\nim\\npl\\nem\\nen\\nte\\nd\\nin\\nVi\\ngi\\nFa\\nrm\\nac\\no\\nPs\\neu\\ndo\\nco\\nde\\n[4\\n3]\\nD\\ne\\nBr\\nui\\njn\\n20\\n11\\nC\\nan\\nad\\na\\ni2\\nb2\\n/V\\nA\\n(2\\n01\\n0)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nH\\nos\\npi\\nta\\nld\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nan\\nd\\npr\\nog\\nre\\nss\\nre\\npo\\nrt\\ns\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[4\\n4]\\nD\\nei\\nss\\ner\\not\\nh\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSi\\nx\\nse\\nts\\nof\\nre\\nal\\npa\\ntie\\nnt\\nda\\nta\\nfro\\nm\\nfo\\nur\\ndi\\nffe\\nre\\nnt\\nm\\ned\\nic\\nal\\nce\\nnt\\ner\\ns.\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nH\\nPO\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[4\\n5]\\nD\\nem\\nne\\nr-\\nFu\\nsh\\nm\\nan\\n20\\n17\\nU\\nSA\\nN\\no\\nSo\\nft\\nw\\nar\\ne\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nBi\\noS\\nco\\npe\\n5 ,\\nN\\nC\\nBI\\ndi\\nse\\nas\\ne\\nco\\nrp\\nus\\n6 ,\\ni2\\nb2\\n/\\nVA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3 ,\\nSh\\nA\\nRe\\nco\\nrp\\nus\\n7 ,\\nLH\\nC\\nte\\nst\\nco\\nlle\\nct\\nio\\nn\\n(b\\nio\\nlo\\ngi\\nca\\nl/\\ncl\\nin\\nic\\nal\\njo\\nur\\nna\\nla\\nbs\\ntr\\nac\\nts\\n)\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nYe\\ns,\\nus\\ned\\nin\\not\\nhe\\nr\\npa\\npe\\nrs\\nid\\nen\\ntif\\nie\\nd\\nin\\nlit\\ner\\nat\\nur\\ne\\nse\\nar\\nch\\nYe\\ns\\n[4\\n6]\\nD\\niv\\nita\\n20\\n14\\nU\\nSA\\nPa\\nrt\\ns:\\ni2\\nb2\\n/V\\nA\\nSo\\nft\\nw\\nar\\ne\\nRa\\nnd\\nom\\nly\\nse\\nle\\nct\\ned\\ncl\\nin\\nic\\nal\\nre\\nco\\nrd\\ns\\nfro\\nm\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\n(le\\nve\\nl0\\n+\\n9)\\nYe\\ns,\\nus\\ned\\nby\\nVA\\nYe\\ns\\n[4\\n7]\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 5 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\n(C\\non\\ntin\\nue\\nd)\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\n(2\\n01\\n0)\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nth\\ne\\nm\\nos\\nt\\nfre\\nqu\\nen\\nt\\ndo\\ncu\\nm\\nen\\nt\\nty\\npe\\ns\\nIn\\nfo\\nrm\\nat\\nic\\ns\\nan\\nd\\nC\\nom\\npu\\ntin\\ng\\nIn\\nfra\\nst\\nru\\nct\\nur\\ne\\nD\\nua\\nrt\\ne\\n20\\n18\\nPo\\nrt\\nug\\nal\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nD\\nea\\nth\\nce\\nrt\\nifi\\nca\\nte\\ns,\\ncl\\nin\\nic\\nal\\nbu\\nlle\\ntin\\ns,\\nan\\nd\\nau\\nto\\nps\\ny\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nPo\\nrt\\nug\\nue\\nse\\nN\\new\\nIC\\nD\\n-1\\n0\\nYe\\ns,\\nus\\ned\\nby\\nPo\\nrt\\nug\\nes\\ne\\nM\\nin\\nis\\ntr\\ny\\nof\\nH\\nea\\nlth\\nfo\\nr\\nne\\nar\\nre\\nal\\n-t\\nim\\ne\\nde\\nat\\nh\\nca\\nus\\ne\\nsu\\nrv\\nei\\nlla\\nnc\\ne\\nN\\not\\nlis\\nte\\nd\\n[4\\n8]\\nFa\\nlis\\n20\\n19\\nU\\nK\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nM\\nIM\\nIC\\n-II\\nId\\nat\\nas\\net\\n4\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nIC\\nD\\n-9\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[4\\n9]\\nFe\\nrr\\não\\n20\\n13\\nPo\\nrt\\nug\\nal\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nIn\\npa\\ntie\\nnt\\nad\\nul\\nt\\nep\\nis\\nod\\nes\\nfro\\nm\\nth\\ne\\nEH\\nR\\nO\\nw\\nn\\nPo\\nrt\\nug\\nue\\nse\\nN\\new\\nIC\\nD\\n-9\\n-C\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n0]\\nG\\ner\\nbi\\ner\\n20\\n11\\nFr\\nan\\nce\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nom\\npu\\nte\\nriz\\ned\\nem\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nm\\ned\\nic\\nal\\nre\\nco\\nrd\\ns\\nO\\nw\\nn\\nFr\\nen\\nch\\nN\\new\\nIC\\nD\\n-1\\n0,\\nC\\nC\\nA\\nM\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nA\\nTC\\n,M\\neS\\nH\\n,I\\nC\\nPC\\n-2\\n,\\nD\\nC\\nR\\nN\\not\\nye\\nt,\\nw\\nill\\nbe\\nin\\nte\\ngr\\nat\\ned\\nin\\nto\\na\\nC\\nD\\nSS\\nN\\not\\nlis\\nte\\nd\\n[5\\n1]\\nG\\noi\\nco\\nec\\nhe\\na\\nSa\\nla\\nza\\nr\\n20\\n13\\nSp\\nai\\nn\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nD\\nia\\ngn\\nos\\ntic\\nte\\nxt\\nfro\\nm\\npa\\ntie\\nnt\\nre\\nco\\nrd\\ns\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nN\\new\\nIC\\nD\\n-9\\n-C\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n2]\\nH\\nam\\nid\\n20\\n13\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nN\\not\\nes\\nof\\nIra\\nq\\nan\\nd\\nA\\nfg\\nha\\nni\\nst\\nan\\nve\\nte\\nra\\nns\\nfro\\nm\\nth\\ne\\nVA\\nna\\ntio\\nna\\nlc\\nlin\\nic\\nal\\nda\\nta\\nba\\nse\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[5\\n3]\\nH\\nas\\nsa\\nnz\\nad\\neh\\n20\\n16\\nA\\nus\\ntr\\nal\\nia\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSh\\nA\\nRe\\n/C\\nLE\\nF\\nco\\nrp\\nus\\n(2\\n01\\n3)\\n2\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nap\\npl\\nic\\nab\\nle\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[5\\n4]\\nH\\nel\\nw\\ne\\n20\\n17\\nLe\\nba\\nno\\nn\\nN\\no\\nC\\nom\\npu\\nte\\nr-\\nas\\nsi\\nst\\ned\\nco\\ndi\\nng\\nM\\nIM\\nIC\\n-II\\nId\\nat\\nas\\net\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\n,I\\nC\\nD\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n5]\\nH\\ner\\nsh\\n20\\n01\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nRa\\ndi\\nol\\nog\\ny\\nim\\nag\\ne\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\no,\\nst\\nill\\nin\\nde\\nve\\nlo\\npm\\nen\\nt/\\nte\\nst\\nin\\ng\\nPs\\neu\\ndo\\nco\\nde\\n[5\\n6]\\nH\\noo\\nge\\nnd\\noo\\nrn\\n20\\n15\\nN\\net\\nhe\\nrla\\nnd\\ns\\nN\\no\\nPr\\ned\\nic\\ntio\\nn\\nC\\non\\nsu\\nlta\\ntio\\nn\\nno\\nte\\ns\\nof\\npa\\ntie\\nnt\\ns\\nin\\na\\npr\\nim\\nar\\ny\\nca\\nre\\nse\\ntt\\nin\\ng\\nO\\nw\\nn\\nD\\nut\\nch\\nN\\new\\nSN\\nO\\nM\\nED\\n-C\\nT,\\nU\\nM\\nLS\\n,I\\nC\\nPC\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n7]\\nJin\\nda\\nl\\n20\\n13\\nU\\nSA\\ni2\\nb2\\n(2\\n01\\n2)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n2)\\n8\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nM\\neS\\nH\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n8]\\nKa\\nng\\n20\\n09\\nKo\\nre\\na\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nO\\nw\\nn\\nKo\\nre\\nan\\nN\\new\\nKO\\nM\\nET\\n,U\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[5\\n9]\\nKe\\nrs\\nlo\\not\\n20\\n19\\nN\\net\\nhe\\nrla\\nnd\\ns\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\n(N\\non\\n-s\\nm\\nal\\nlc\\nel\\nl)\\nLu\\nng\\nca\\nnc\\ner\\nch\\nar\\nts\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[6\\n0]\\nKö\\nni\\ng\\n20\\n19\\nG\\ner\\nm\\nan\\ny\\nN\\no\\nSo\\nft\\nw\\nar\\ne\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nD\\nis\\nch\\nar\\nge\\nle\\ntt\\ner\\ns\\nfro\\nm\\nBA\\nSE\\n-II\\nst\\nud\\ny\\nO\\nw\\nn\\nG\\ner\\nm\\nan\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nW\\nin\\nge\\nrt\\n-N\\nom\\nen\\ncl\\nat\\nur\\ne\\nN\\no,\\nst\\nill\\nha\\ns\\nto\\npr\\nov\\ne\\nits\\nva\\nlu\\ne\\nN\\not\\nlis\\nte\\nd\\n[6\\n1]\\nLi\\n20\\n15\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nco\\nm\\npa\\nris\\non\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nan\\nd\\ndi\\nsc\\nha\\nrg\\ne\\npr\\nes\\ncr\\nip\\ntio\\nn\\nlis\\nts\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nRx\\nN\\nor\\nm\\nN\\not\\nye\\nt,\\npl\\nan\\ns\\nto\\nm\\nov\\ne\\nto\\npr\\nod\\nuc\\ntio\\nn\\nPs\\neu\\ndo\\nco\\nde\\n[6\\n2]\\nLi\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nEH\\nR\\nno\\nte\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nM\\ned\\nD\\nRA\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[6\\n3]\\nLi\\nng\\nre\\nn\\n20\\n16\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nSt\\nru\\nct\\nur\\ned\\nan\\nd\\nun\\nst\\nru\\nct\\nur\\ned\\nda\\nta\\nfro\\nm\\ntw\\no\\nEH\\nR\\nda\\nta\\nba\\nse\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,I\\nC\\nD\\n-9\\n,R\\nxN\\nor\\nm\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[1\\n2]\\nLi\\nu\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nfro\\nm\\ndi\\nffe\\nre\\nnt\\nin\\nst\\nitu\\ntio\\nns\\n+\\nPu\\nbM\\ned\\nC\\nas\\ne\\nre\\npo\\nrt\\nab\\nst\\nra\\nct\\ns\\nO\\nw\\nn\\n+\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nH\\nPO\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[6\\n4]\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 6 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\n(C\\non\\ntin\\nue\\nd)\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\nLo\\nw\\ne\\n20\\n09\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSi\\nng\\nle\\n-s\\npe\\nci\\nm\\nen\\npa\\nth\\nol\\nog\\ny\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[6\\n5]\\nLu\\no\\n20\\n14\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nPa\\nth\\nol\\nog\\ny\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nYe\\ns,\\ncu\\nrr\\nen\\ntly\\nw\\nor\\nki\\nng\\non\\npr\\noj\\nec\\nt\\nin\\nm\\nul\\ntip\\nle\\nho\\nsp\\nita\\nls\\nN\\not\\nlis\\nte\\nd\\n[6\\n6]\\nM\\ney\\nst\\nre\\n20\\n06\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nC\\nlin\\nic\\nal\\ndo\\ncu\\nm\\nen\\nts\\nfo\\nrm\\nad\\nul\\nt\\nin\\npa\\ntie\\nnt\\ns\\nin\\na\\nca\\nrd\\nio\\nva\\nsc\\nul\\nar\\nun\\nit\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n(le\\nve\\nl0\\n),\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nye\\nt,\\nte\\nst\\nin\\ng\\nin\\npr\\nac\\ntic\\ne\\nN\\not\\nlis\\nte\\nd\\n[6\\n7]\\nM\\ney\\nst\\nre\\n20\\n10\\nU\\nSA\\ni2\\nb2\\n(2\\n00\\n9)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nda\\nta\\nse\\nt\\n(2\\n00\\n9)\\n9\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\not\\nye\\nt,\\npo\\nss\\nib\\nle\\nin\\nte\\ngr\\nat\\nio\\nn\\nin\\nre\\nse\\nar\\nch\\nin\\nfra\\nst\\nru\\nct\\nur\\ne\\nN\\not\\nlis\\nte\\nd\\n[6\\n8]\\nM\\nin\\nar\\nd\\n20\\n11\\nFr\\nan\\nce\\ni2\\nb2\\n/V\\nA\\n(2\\n01\\n0)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[6\\n9]\\nM\\nis\\nhr\\na\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nfro\\nm\\nN\\nIH\\nC\\nlin\\nic\\nal\\nC\\nen\\nte\\nr\\nda\\nta\\nw\\nar\\neh\\nou\\nse\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n,H\\nPO\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[7\\n0]\\nN\\ngu\\nye\\nn\\n20\\n18\\nA\\nus\\ntr\\nal\\nia\\nN\\no\\nC\\nom\\npu\\nte\\nr-\\nas\\nsi\\nst\\ned\\nco\\ndi\\nng\\nH\\nos\\npi\\nta\\nlp\\nro\\ngr\\nes\\ns\\nno\\nte\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nSN\\nO\\nM\\nED\\nC\\nT,\\nIC\\nD\\n-1\\n0-\\nA\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n1]\\nO\\nel\\nlri\\nch\\n20\\n15\\nU\\nK\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nPu\\nbM\\ned\\nab\\nst\\nra\\nct\\ns,\\ncl\\nin\\nic\\nal\\ntr\\nia\\nl\\nin\\nfo\\nrm\\nat\\nio\\nn,\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3 ,\\nSH\\nA\\nRE\\n/C\\nLE\\nF\\n(2\\n01\\n3)\\n2\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[7\\n2]\\nPa\\ntr\\nic\\nk\\n20\\n11\\nA\\nus\\ntr\\nal\\nia\\ni2\\nb2\\n/V\\nA\\n(2\\n01\\n0)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n3]\\nPé\\nre\\nz\\n20\\n18\\nSp\\nai\\nn\\nN\\no\\nTe\\nxt\\npr\\noc\\nes\\nsi\\nng\\nSp\\non\\nta\\nne\\nou\\ns\\nD\\nTs\\nra\\nnd\\nom\\nly\\nse\\nle\\nct\\ned\\nen\\ntr\\nie\\ns\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nN\\new\\nIC\\nD\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n4]\\nRe\\nát\\neg\\nui\\n20\\n18\\nC\\nan\\nad\\na\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n00\\n8)\\n10\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nRx\\nN\\nor\\nm\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n5]\\nRo\\nbe\\nrt\\ns\\n20\\n11\\nU\\nSA\\ni2\\nb2\\n/V\\nA\\n(2\\n01\\n0)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,I\\nC\\nD\\n-9\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[7\\n6]\\nRo\\nus\\nse\\nau\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nco\\nm\\npa\\nris\\non\\nED\\nen\\nco\\nun\\nte\\nrs\\nfo\\nr\\npa\\ntie\\nnt\\ns\\nw\\nith\\nhe\\nad\\nac\\nhe\\ns\\nw\\nho\\nre\\nce\\niv\\ned\\nhe\\nad\\nC\\nT\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n:S\\nN\\nO\\nM\\nED\\nC\\nT,\\nRa\\ndL\\nex\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[7\\n7]\\nSa\\nvo\\nva\\n20\\n10\\nU\\nSA\\ni2\\nb2\\n(2\\n00\\n6,\\n20\\n08\\n)\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nSu\\nbs\\net\\nof\\ncl\\nin\\nic\\nal\\nno\\nte\\ns\\nfro\\nm\\nth\\ne\\nEM\\nR\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT,\\nRx\\nN\\nor\\nm\\nYe\\ns,\\nus\\ned\\nin\\not\\nhe\\nr\\npa\\npe\\nrs\\nid\\nen\\ntif\\nie\\nd\\nin\\nlit\\ner\\nat\\nur\\ne\\nse\\nar\\nch\\nYe\\ns\\n[7\\n8]\\nSh\\niv\\nad\\ne\\n20\\n15\\nU\\nSA\\ni2\\nb2\\n/U\\nTH\\nea\\nlth\\n(2\\n01\\n4)\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n4)\\n11\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[1\\n1]\\nSh\\noe\\nnb\\nill\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nEH\\nR\\nno\\nte\\ns\\nfro\\nm\\nhy\\npe\\nrt\\nen\\nsi\\non\\npa\\ntie\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\n,S\\nN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[7\\n9]\\nSo\\nhn\\n20\\n14\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nlin\\nic\\nal\\nno\\nte\\ns\\nw\\nith\\nm\\ned\\nic\\nat\\nio\\nn\\nm\\nen\\ntio\\nns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nRx\\nN\\nor\\nm\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[8\\n0]\\nSo\\nlti\\n20\\n08\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nen\\nric\\nhm\\nen\\nt\\nC\\nar\\ndi\\nol\\nog\\ny\\nam\\nbu\\nla\\nto\\nry\\npr\\nog\\nre\\nss\\nno\\nte\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[8\\n1]\\nSo\\nria\\nno\\n20\\n19\\nSp\\nai\\nn\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ncl\\nin\\nic\\nal\\nem\\ner\\nge\\nnc\\ny\\ndi\\nsc\\nha\\nrg\\ne\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nSp\\nan\\nis\\nh\\nN\\new\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nye\\nt\\nYe\\ns\\n[8\\n2]\\nSo\\nys\\nal\\n20\\n18\\nU\\nSA\\nPa\\nrt\\ns:\\ni2\\nb2\\nSo\\nft\\nw\\nar\\ne\\nD\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nfro\\nm\\nth\\ne\\ni2\\nb2\\n/V\\nA\\nO\\nw\\nn\\n+\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nYe\\ns,\\nus\\ned\\nby\\nva\\nrio\\nus\\nYe\\ns\\n[8\\n3]\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 7 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\n(C\\non\\ntin\\nue\\nd)\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\n(2\\n00\\n9\\n+\\n20\\n10\\n),\\nSh\\nA\\nRe\\n/C\\nLE\\nF\\n(2\\n01\\n3)\\n,S\\nem\\n-E\\nVA\\nL\\n(2\\n01\\n4)\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3 ,\\nou\\ntp\\nat\\nie\\nnt\\ncl\\nin\\nic\\nvi\\nsi\\nt\\nno\\nte\\ns,\\nm\\noc\\nk\\ncl\\nin\\nic\\nal\\ndo\\ncu\\nm\\nen\\nts\\nEx\\nis\\ntin\\ng\\nin\\nst\\nitu\\ntio\\nns\\nan\\nd\\nin\\ndu\\nst\\nria\\nl\\nen\\ntit\\nie\\ns\\nSp\\nas\\ni?\\n20\\n15\\nU\\nK\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nM\\nRI\\nre\\npo\\nrt\\ns\\nof\\npa\\ntie\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nTR\\nA\\nK,\\nU\\nM\\nLS\\n,M\\nED\\nC\\nIN\\n,\\nRa\\ndL\\nex\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[8\\n4]\\nSt\\nra\\nus\\ns\\n20\\n13\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nPa\\nth\\nol\\nog\\ny\\nre\\npo\\nrt\\ns\\nof\\nbr\\nea\\nst\\nan\\nd\\npr\\nos\\nta\\nte\\nca\\nnc\\ner\\npa\\ntie\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nSN\\nO\\nM\\nED\\nC\\nT\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[8\\n5]\\nSu\\nng\\n20\\n18\\nTa\\niw\\nan\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nas\\nes\\nof\\nad\\nul\\nt\\npa\\ntie\\nnt\\ns\\nw\\nith\\nA\\nIS\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[8\\n6]\\nTc\\nhe\\nch\\nm\\ned\\njie\\nv\\n20\\n18\\nFr\\nan\\nce\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nQ\\nua\\ner\\no\\n(F\\nre\\nnc\\nh\\nM\\nED\\nLI\\nN\\nE\\nab\\nst\\nra\\nct\\ntit\\nle\\ns\\n+\\nEM\\nEA\\ndr\\nug\\nla\\nbe\\nls\\n)+\\nC\\nép\\niD\\nC\\n(IC\\nD\\n-1\\n0\\nco\\ndi\\nng\\nof\\nde\\nat\\nh\\nce\\nrt\\nifi\\nca\\nte\\ns)\\nEx\\nis\\ntin\\ng\\nFr\\nen\\nch\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nte\\nrm\\nin\\nol\\nog\\nie\\ns\\n(IC\\nD\\n-1\\n0)\\nYe\\ns,\\nav\\nai\\nla\\nbl\\ne\\nin\\nSI\\nFR\\nBi\\noP\\nor\\nta\\nl\\nYe\\ns\\n[8\\n7]\\nTe\\nrn\\noi\\ns\\n20\\n18\\nFr\\nan\\nce\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nEn\\ndo\\nsc\\nop\\ny\\nre\\npo\\nrt\\ns\\nw\\nrit\\nte\\nn\\nbe\\ntw\\nee\\nn\\n20\\n15\\nan\\nd\\n20\\n16\\nO\\nw\\nn\\nFr\\nen\\nch\\nN\\new\\nC\\nC\\nA\\nM\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[8\\n8]\\nTr\\nav\\ner\\ns\\n20\\n04\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nC\\nhi\\nef\\nco\\nm\\npl\\nai\\nnt\\nte\\nxt\\nen\\ntr\\nie\\ns\\nfo\\nr\\nal\\nl\\nem\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nvi\\nsi\\nts\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[8\\n9]\\nTu\\nlk\\nen\\ns\\n20\\n19\\nBe\\nlg\\niu\\nm\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nYe\\ns\\n[9\\n0]\\nU\\nsu\\ni\\n20\\n18\\nJa\\npa\\nn\\nN\\no\\nPr\\ned\\nic\\ntio\\nn\\nEl\\nec\\ntr\\non\\nic\\nm\\ned\\nic\\nat\\nio\\nn\\nhi\\nst\\nor\\ny\\nda\\nta\\nfro\\nm\\nph\\nar\\nm\\nac\\ny\\nO\\nw\\nn\\nJa\\npa\\nne\\nse\\nN\\new\\nIC\\nD\\n-1\\n0\\nN\\not\\nye\\nt,\\nex\\npe\\nct\\nto\\nus\\ne\\nit\\nN\\not\\nlis\\nte\\nd\\n[9\\n1]\\nVa\\nltc\\nhi\\nno\\nv\\n20\\n19\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nRa\\ndi\\nol\\nog\\ny\\nre\\npo\\nrt\\ns,\\nem\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nno\\nte\\ns\\n+\\not\\nhe\\nr\\ncl\\nin\\nic\\nal\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT,\\nRa\\ndL\\nex\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[9\\n2]\\nW\\nad\\nia\\n20\\n18\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nC\\nhe\\nst\\nC\\nT\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT,\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[9\\n3]\\nW\\nal\\nke\\nr\\n20\\n19\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nTr\\nea\\ntm\\nen\\nt\\nsi\\nte\\ns\\nfro\\nm\\nEM\\nR\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[9\\n4]\\nXi\\ne\\n20\\n19\\nC\\nhi\\nna\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nM\\nIM\\nIC\\n-II\\nId\\nat\\nas\\net\\n4\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nIC\\nD\\n-9\\n-C\\nM\\n,I\\nC\\nD\\n-1\\n0\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[9\\n5]\\nXu\\n20\\n11\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nC\\nRC\\npa\\ntie\\nnt\\nca\\nse\\ns\\nfro\\nm\\nth\\ne\\nSy\\nnt\\nhe\\ntic\\nD\\ner\\niv\\nat\\niv\\ne\\nda\\nta\\nba\\nse\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\no,\\nst\\nill\\nun\\nde\\nr\\nde\\nve\\nlo\\npm\\nen\\nt\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[9\\n6]\\nYa\\nda\\nv\\n20\\n13\\nU\\nSA\\nN\\no\\nPr\\ned\\nic\\ntio\\nn\\nEm\\ner\\nge\\nnc\\ny\\nde\\npa\\nrt\\nm\\nen\\nt\\nC\\nT\\nim\\nag\\nin\\ng\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nYe\\ns,\\nco\\nm\\nm\\nan\\nd\\nlin\\ne\\nco\\nm\\nm\\nan\\nd\\n[9\\n7]\\nYa\\no\\n20\\n19\\nU\\nSA\\nN\\no\\nPr\\ned\\nic\\ntio\\nn\\ni2\\nb2\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n00\\n8)\\n10\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nPa\\nrt\\n(S\\nor\\nl)\\n[9\\n8]\\nZe\\nng\\n20\\n18\\nU\\nSA\\nN\\no\\nC\\nla\\nss\\nifi\\nca\\ntio\\nn\\nPr\\nog\\nre\\nss\\nno\\nte\\ns\\nan\\nd\\nbr\\nea\\nst\\nca\\nnc\\ner\\nsu\\nrg\\nic\\nal\\npa\\nth\\nol\\nog\\ny\\nre\\npo\\nrt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\n(+\\nex\\nis\\ntin\\ng)\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[9\\n9]\\nZh\\nan\\ng\\n20\\n13\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nco\\nrp\\nus\\n(2\\n01\\n0)\\n3\\nan\\nd\\nG\\nEN\\nIA\\nco\\nrp\\nus\\n(M\\nED\\nLI\\nN\\nE\\nab\\nst\\nra\\nct\\ns)\\nEx\\nis\\ntin\\ng\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nlis\\nte\\nd\\n[1\\n00\\n]\\nZh\\nou\\n20\\n06\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nRe\\nco\\nrd\\ns\\nof\\npa\\ntie\\nnt\\ns\\nw\\nith\\nbr\\nea\\nst\\nco\\nm\\npl\\nai\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nU\\nM\\nLS\\nN\\no,\\nst\\nill\\nun\\nde\\nr\\nde\\nve\\nlo\\npm\\nen\\nt\\nN\\not\\nlis\\nte\\nd\\n[1\\n01\\n]\\nZh\\nou\\n20\\n11\\nU\\nSA\\nN\\no\\nSo\\nft\\nw\\nar\\ne\\nC\\nO\\nPD\\nan\\nd\\nC\\nA\\nD\\npa\\ntie\\nnt\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nN\\new\\nSN\\nO\\nM\\nED\\nC\\nT,\\nRx\\nN\\nor\\nm\\n,\\nYe\\ns,\\nde\\nsc\\nrib\\ned\\nin\\not\\nhe\\nr\\nN\\not\\nlis\\nte\\nd\\n[1\\n02\\n]\\nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 8 of 21\\nTa\\nb\\nle\\n3\\nIn\\ncl\\nud\\ned\\npu\\nbl\\nic\\nat\\nio\\nns\\nan\\nd\\nth\\nei\\nr\\nfir\\nst\\nau\\nth\\nor\\n,y\\nea\\nr,\\ntit\\nle\\n,a\\nnd\\nco\\nun\\ntr\\ny\\n(C\\non\\ntin\\nue\\nd)\\nA\\nut\\nho\\nr\\nY\\nea\\nr\\nC\\nou\\nnt\\nry\\nC\\nha\\nlle\\nng\\ne\\nIn\\nd\\nuc\\ned\\nob\\nje\\nct\\niv\\ne\\nD\\nat\\na\\nor\\nig\\nin\\nD\\nat\\nas\\net\\nD\\nat\\na\\nla\\nng\\nua\\ng\\ne\\nU\\nse\\nd\\nsy\\nst\\nem\\nTe\\nrm\\n.S\\nys\\n.\\nIn\\nus\\ne\\nSo\\nur\\nce\\nco\\nd\\ne\\nRe\\nf\\nde\\nve\\nlo\\npm\\nen\\nt\\nan\\nd\\nev\\nal\\nua\\ntio\\nn\\nU\\nM\\nLS\\n,P\\nPL\\n,M\\nD\\nD\\n,H\\nL7\\nva\\nlu\\ne\\nse\\nts\\npa\\npe\\nr\\n(1\\n03\\n])\\nZh\\nou\\n20\\n14\\nU\\nSA\\nN\\no\\nIn\\nfo\\nrm\\nat\\nio\\nn\\nex\\ntr\\nac\\ntio\\nn\\nA\\ndm\\nis\\nsi\\non\\nno\\nte\\ns\\nan\\nd\\ndi\\nsc\\nha\\nrg\\ne\\nsu\\nm\\nm\\nar\\nie\\ns\\nO\\nw\\nn\\nEn\\ngl\\nis\\nh\\nEx\\nis\\ntin\\ng\\nSN\\nO\\nM\\nED\\nC\\nT,\\nH\\nL7\\nRo\\nle\\nC\\nod\\nes\\nN\\not\\nlis\\nte\\nd\\nN\\not\\nap\\npl\\nic\\nab\\nle\\n[1\\n03\\n]\\n1.\\nPh\\nen\\noC\\nH\\nF\\nco\\nrp\\nus\\n:n\\nar\\nra\\ntiv\\ne\\nre\\npo\\nrt\\ns\\nfr\\nom\\nel\\nec\\ntr\\non\\nic\\nhe\\nal\\nth\\nre\\nco\\nrd\\ns\\n(E\\nH\\nRs\\n)\\nan\\nd\\nlit\\ner\\nat\\nur\\ne\\nar\\ntic\\nle\\ns\\n2.\\nSh\\nA\\nRe\\n/C\\nLE\\nF\\nco\\nrp\\nus\\n(2\\n01\\n3)\\n:n\\nar\\nra\\ntiv\\ne\\ncl\\nin\\nic\\nal\\nre\\npo\\nrt\\ns\\n3.\\ni2\\nb2\\n/V\\nA\\nch\\nal\\nle\\nng\\ne\\nda\\nta\\nse\\nt\\n(2\\n01\\n0)\\n:d\\nis\\nch\\nar\\nge\\nsu\\nm\\nm\\nar\\nie\\ns\\nan\\nd\\npr\\nog\\nre\\nss\\nre\\npo\\nrt\\ns\\n4.\\nM\\nIM\\nIC\\n-II'</span>text15\n:   <span style=white-space:pre-wrap>'RESEARCH Open Access\\nOntological representation, classification\\nand data-driven computing of phenotypes\\nAlexandr Uciteli1,2* , Christoph Beger1,3, Toralf Kirsten2,4,5, Frank A. Meineke1,2 and Heinrich Herre1,2*\\nAbstract\\nBackground: The successful determination and analysis of phenotypes plays a key role in the diagnostic process,\\nthe evaluation of risk factors and the recruitment of participants for clinical and epidemiological studies. The\\ndevelopment of computable phenotype algorithms to solve these tasks is a challenging problem, caused by various\\nreasons. Firstly, the term \\u0091phenotype\\u0092 has no generally agreed definition and its meaning depends on context.\\nSecondly, the phenotypes are most commonly specified as non-computable descriptive documents. Recent\\nattempts have shown that ontologies are a suitable way to handle phenotypes and that they can support clinical\\nresearch and decision making.\\nThe SMITH Consortium is dedicated to rapidly establish an integrative medical informatics framework to provide\\nphysicians with the best available data and knowledge and enable innovative use of healthcare data for research\\nand treatment optimisation. In the context of a methodological use case \\u0091phenotype pipeline\\u0092 (PheP), a technology\\nto automatically generate phenotype classifications and annotations based on electronic health records (EHR) is\\ndeveloped. A large series of phenotype algorithms will be implemented. This implies that for each algorithm a\\nclassification scheme and its input variables have to be defined. Furthermore, a phenotype engine is required to\\nevaluate and execute developed algorithms.\\nResults: In this article, we present a Core Ontology of Phenotypes (COP) and the software Phenotype Manager\\n(PhenoMan), which implements a novel ontology-based method to model, classify and compute phenotypes from\\nalready available data. Our solution includes an enhanced iterative reasoning process combining classification tasks\\nwith mathematical calculations at runtime. The ontology as well as the reasoning method were successfully\\nevaluated with selected phenotypes including SOFA score, socio-economic status, body surface area and WHO BMI\\nclassification based on available medical data.\\nConclusions: We developed a novel ontology-based method to model phenotypes of living beings with the aim\\nof automated phenotype reasoning based on available data. This new approach can be used in clinical context,\\ne.g., for supporting the diagnostic process, evaluating risk factors, and recruiting appropriate participants for clinical\\nand epidemiological studies.\\nKeywords: Phenotype definition, Phenotype classification, Phenotype calculation, Phenotype ontology, Phenotype\\nreasoning\\n© The Author(s). 2020, corrected publication 2020. Open Access This article is licensed under a Creative Commons Attribution\\n4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as\\nlong as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\\nand indicate if changes were made. The images or other third party material in this article are included in the article\\'s Creative\\nCommons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\'s Creative\\nCommons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need\\nto obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\\nlicenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.\\n0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\\n* Correspondence: auciteli@imise.uni-leipzig.de; heinrich.herre@imise.uni-\\nleipzig.de\\n1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),\\nUniversity of Leipzig, Leipzig, Germany\\nFull list of author information is available at the end of the article\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 \\nhttps://doi.org/10.1186/s13326-020-00230-0\\nBackground\\nDespite its long ago introduction in 1909 by Wilhelm\\nJohannsen, the term \\u0091phenotype\\u0092 still has no generally\\nagreed definition [1]. Usually, a phenotype is consid-\\nered as an observable characteristic or trait of an or-\\nganism, such as its morphology, function, behaviour\\nor its biochemical and physiological properties [1\\u00963].\\nScheuermann et al. define a phenotype as a (combin-\\nation of) bodily feature(s) (physical components, bod-\\nily qualities or bodily processes) of an organism\\ndetermined by the interaction of its genetic make-up\\nand environment [4]. From the medical perspective,\\nclinical (clinically abnormal) and disease phenotypes\\n(clinical phenotype characterising a single disease) are\\nconsidered. According to Scheuermann et al., a dis-\\nease phenotype can exist without being observed. Ob-\\nserved bodily features that could be of clinical\\nrelevance are called \\u0091Sign\\u0092( \\u0093\\u0085 observed in a physical\\nexamination and is deemed by the clinician to be of\\nclinical significance\\u0094) or \\u0091Symptom\\u0092 ( \\u0093\\u0085 observed by\\nthe patient and is hypothesized by the patient to be a\\nrealization of a disease\\u0094) [4].\\nCorrect determination of phenotypes plays a key\\nrole for diagnosis of diseases, evaluation of risk fac-\\ntors and recruitment of patients for clinical and epi-\\ndemiological studies [5, 6]. One challenge is to\\ntranslate phenotype algorithms, which \\u0093are most com-\\nmonly represented as non-computable descriptive\\ndocuments and knowledge artifacts\\u0094 [7], into\\nmachine-readable form. This paper focuses on devel-\\noping a general phenotype representation model that\\ncan be used for data-driven phenotype computing,\\ni.e., software-supported determination of phenotypes\\nbased on the data of an organism. The model to be\\ndeveloped must support both the biological and the\\nmedical views of the phenotype notion. Recent at-\\ntempts have shown that ontologies are suitable to\\nhandle phenotypes and that they can support clinical\\nresearch and decision making [8\\u009610].\\nThere is a large ongoing initiative in Germany, the so\\ncalled German Medical Informatics Initiative (MII) [11,\\n12] that aims at making clinical data available for re-\\nsearch. Most German university hospitals participate in\\none of four funded consortia. Smart Medical Informa-\\ntion Technology for Healthcare (SMITH) is one of these\\nconsortia [13]. Within the ongoing SMITH project, a\\nphenotyping pipeline (PheP) will be established to sys-\\ntematically develop, evaluate and execute validated algo-\\nrithms and models for classifying and annotating patient\\ncare data. These annotations and derivatives will be pro-\\nvided for triggering alerts and actions, data sharing and\\ndeep analyses of patient care and outcomes. The general\\ndesign and concept of the SMITH phenotyping pipeline\\nis presented in [14].\\nIn this article, we propose a novel ontology-based\\nmethod to model and compute phenotypes. Our ap-\\nproach provides an extended reasoning combining\\nphenotypic data to derive complex phenotypes based on\\ncalculations and classifications.\\nMethods\\nPhenotypes can be derived from available data that may\\nhave been measured (quantitative data) or observed and\\nqualitatively described (categorical data). The data can,\\nfor example, come from Electronic Health Records\\n(EHR) (clinical data) or from a research database of a\\nclinical/epidemiological study (research data). In SMIT\\nH, the required EHR data will be integrated into a cen-\\ntral Health Data Storage (HDS) at each site. The inte-\\ngrated data is homogeneously represented in each HDS\\nusing HL7 FHIR [15] and can be queried utilising FHIR\\nSearch [16] (Fig. 1). Structured data from different\\nsource systems in hospitals as well as unstructured doc-\\numents will be extracted, transformed and loaded into\\nthe HDS. Natural Language Processing (NLP) techniques\\nare used to extract and transform relevant data from un-\\nstructured EHR documents into structured form. In\\nSMITH and the German Medical Informatics Initiative,\\nthe software tool ART-DECOR [17] is used to specify an\\noverarching global schema, the so-called core data set\\n[18]. The core data subsumes the minimal set of data el-\\nements that each site (i.e., University Hospital) needs to\\nprovide in a harmonised manner. In this way, data ele-\\nments are specified based on HL7 templates, their re-\\nspective value sets, referenced terminologies, exemplified\\nuse scenarios and data. These specifications are the basis\\nfor the ontology-based phenotype representation in our\\napproach.\\nFig. 1 Integration of the PhenoMan. The Metadata Manager models\\nbasic data elements using ART-DECOR. The Phenotype Designer\\nimports the ART-DECOR specification and develops phenotype\\nmodels (PheSO) utilising the PhenoMan Editor. The PhenoMan\\nrequests required input data from the FHIR Server, computes\\nphenotypes and writes the results back to the FHIR Server\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 2 of 17\\nHL7 FHIR and FHIR Search\\nHealthcare records are increasingly digitised. The\\nEHR must be discoverable and understandable. The\\npatient data must be structured and standardised to\\nsupport machine-based processing and automated\\nclinical decision making. The FHIR (Fast Healthcare\\nInteroperability Resources) specification is a HL7\\nstandard for modelling and exchanging healthcare in-\\nformation [15]. FHIR provides a base set of resource\\ntypes representing relevant clinical concepts that can\\nbe used to store and exchange data in order to solve\\na wide range of healthcare related problems. For\\neach resource type, the corresponding information\\ncontents and structure are specified. The resources\\ncan be used either by themselves or combined to\\ncomplex documents representing a coherent set of\\nhealthcare information [15]. The FHIR resource types\\ninclude, inter alia, \\u0091Patient\\u0092 (\\u0093Demographics and other\\nadministrative information about an individual or\\nanimal receiving care or other health-related ser-\\nvices.\\u0094), \\u0091Observation\\u0092 (\\u0093Measurements and simple as-\\nsertions made about a patient, device or other\\nsubject.\\u0094) and \\u0091Condition\\u0092 (\\u0093A clinical condition, prob-\\nlem, diagnosis, or other event, situation, issue, or clin-\\nical concept that has risen to a level of concern.\\u0094)\\n[15].\\nThe FHIR Search Framework [16] is part of the\\nHL7 FHIR standard and provides a range of opera-\\ntions and parameters (series of name = value pairs) to\\nsearch for existing FHIR resources in the underlying\\nrepository. In the simplest case, a search is executed\\nby performing a GET operation in the RESTful\\nframework:\\nGET [base-url]/[resource-type]?name = value&amp;...{&amp;_\\nformat = [mime-type]}}.\\ne.g., GET [base-url]/Patient?gender =male.\\nFor numeric parameter types (number, date or quan-\\ntity), a value range can be defined using a prefix to the\\nparameter value (e.g., gt = greater than, le = less or\\nequal).\\nThe \\u0091&amp;\\u0092 (AND) operator between single search criteria is\\nused to search for the intersection of resources that match\\nall criteria specified by each individual search parameter\\n(e.g., Patient?gender =male&amp;birthdate = gt1970). To search\\nfor resources with one of the specified parameter values\\n(OR), the values must be separated by a comma (e.g., Obser-\\nvation?code= http://loinc.org?3141-9, http://snomed.info/\\nsct?27113001, i.e., weight code from LOINC or SNOMED).\\nThe following query contains AND combinations of single\\ncriteria (code AND value-quantity) as well as OR linking of\\ncode values and can be used to search for weight observa-\\ntions where the weight is greater than 75 kg:\\nObservation?code= http://loinc.org?3141-9, http://snome-\\nd.info/sct?27113001&amp;value-quantity=gt75??kg.\\nART-DECOR\\nART-DECOR is an open-source tool suite that supports\\nthe creation and maintenance of HL7 templates, value\\nsets, scenarios and datasets [17]. To specify and hier-\\narchically structure required data elements (items, con-\\ncepts, variables) we use the Dataset Editor of ART-\\nDECOR. Data elements can possess several attributes,\\nsuch as name, description (in different languages) and\\nvalue domain (including data type, unit and possible\\nvalue set) (Fig. 2a).\\nOne of the most important components of a data\\nelement is its terminology associations. A termin-\\nology association defines the binding of dataset con-\\ncepts to relevant terminology [17]. To associate a\\ndata element with a terminology concept, the corre-\\nsponding code (including the URI or ID of the ter-\\nminology) must be specified. For instance, the\\nconcept \\u0091Fasting glucose [Mass/volume] in Serum or\\nPlasma\\u0092 from LOINC (URI: \\u0091http://loinc.org\\u0092) has the\\ncode \\u00911558\\u00966\\u0092 (Fig. 2a).\\nFurthermore, additional properties of data elements\\ncan be defined as key-value pairs. We use this function-\\nality to specify the mapping between the data element\\nand the corresponding FHIR resource type (e.g., for fast-\\ning glucose, key: \\u0091FHIR\\u0092, value: \\u0091Observation\\u0092) required\\nfor phenotype computing. Depending on the resource\\ntype, different FHIR Search parameters must be used to\\nquery the relevant FHIR resources. Moreover, the differ-\\nent structure of the resulting resources must be consid-\\nered to extract required data.\\nThe resulting dataset specification is available in XML\\nor JSON and can be parsed by our software.\\nOntological architecture\\nOur objective was to design the PhenoMan software\\naccording to the three-ontology method [19]. This\\nmethod is based on interactions of three different\\nkinds of ontologies: a task ontology (TO), a domain\\nontology (DO) and a top-level ontology (TLO). The\\nTO serves as the conceptual model for the software,\\nthe DO provides the domain-specific knowledge,\\nwhereas the TLO integrates the TO and the DO and\\nis used as foundation of them.\\nIn our case, the Core Ontology of Phenotypes (COP,\\nsee section \\'Core Ontology of Phenotypes (COP)\\') func-\\ntions as a TO. It describes the general structure of valid\\nphenotype specifications and thus enables the Pheno-\\nMan to create such specifications and to use them for\\nphenotype computing. Concrete phenotype specifica-\\ntions (domain-specific knowledge) are represented in\\nPhenotype Specification Ontologies (PheSO, see section\\n\\'Phenotype Specification Ontologies (PheSO)\\') playing\\nthe role of domain ontologies (DO) in our architecture.\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 3 of 17\\nFor the foundation of the TO we used the General\\nFormal Ontology (GFO) [20] as TLO. GFO has already\\nbeen successfully applied for a foundation of phenotype-\\nrelated notions. For instance, a novel approach to repre-\\nsent complex phenotypes in OWL was proposed im-\\nproving the consistency and expressiveness of formal\\nphenotype descriptions [10]. Another pillar of GFO for a\\ngrounding of phenotypes is the foundational ontology of\\nproperties, attributives and data (GFO-Data [21])\\nproviding an extensive classification of properties (and\\nattributives). In the current paper, we especially refer-\\nence the property notion of GFO (including distinction\\nbetween single and composite properties [22]) in our\\nphenotype representation model supporting data-driven\\nphenotype computing.\\nOne of the advantages of the three-ontology method is\\nthat the software only needs to implement the access to\\nentities (classes, properties) of the TO (COP), whereas\\nFig. 2 Mapping between ART-DECOR, PheSO, FHIR Subscription and FHIR Observation entities. a Specification of the data element \\u0091fasting\\nglucose\\u0092 in ART-DECOR. b Annotations of the corresponding class Fasting_Glucose after importing the ART-DECOR specification into the PheSO. c\\nSubscription generated for the class Fasting_Glucose. The criteria (FHIR Search query) is encoded. The original URL part is Observation?code=\\nhttp://loinc.org|1558-6. d Observation of fasting glucose provided by FHIR Server. The observation code, value, date and the referenced patient\\nare specified. (The same colour of the border indicates the mapping between the entities)\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 4 of 17\\nthe entities of the corresponding DO (PheSO) are proc-\\nessed dynamically. The PhenoMan uses the COP as an\\ninterface to access the PheSO entities.\\nAdditional requirements for the ontological modelling\\nwere:\\n\\u0096 Developing in OWL (using OWL API [23], HermiT\\n[24] and Openllet [25])\\n\\u0096 Modelling all attributes and relations that are\\nrelevant for reasoning as object or data properties\\n\\u0096 Modelling all attributes and relations that are not\\nrelevant for reasoning as annotations\\n\\u0096 Usage of general class axioms (based on \\u0091subclass\\nof\\u0092) instead of equivalence classes if only one\\ndirection (\\u0091is-a\\u0092 relation) is relevant for reasoning.\\nSoftware design\\nWe defined the following main requirements for the\\noverall system (Fig. 1):\\n\\u0096 The system must support a phenotype specification\\nproviding a GUI tool (see section \\'Specification of\\nphenotypes\\').\\n\\u0096 The phenotype specifications must be saved in a\\nstandardised ontology (see sections \\'Core Ontology\\nof Phenotypes (COP)\\' and \\'Phenotype Specification\\nOntologies (PheSO)\\').\\n\\u0096 The system must be able to correctly compute\\nphenotypes based on a phenotype specification\\n(ontology) and input data (see section \\'Classification\\nand calculation of phenotypes\\').\\n\\u0096 The system must support an additional\\nimplementation of mapping components for\\naccessing required data and metadata repositories.\\nExample components for metadata import from\\nART-DECOR as well as for interaction with FHIR\\nservers (e.g., SMITH HDS) must be implemented\\n(see sections \\'Data procurement\\' and \\'Transmission\\nof inferred phenotype classes to the FHIR Server\\').\\nThe PhenoMan accesses the FHIR Server, extracts\\nphenotype-specific data, computes the specified pheno-\\ntypes and writes the results back to the FHIR Server. For\\nthis purpose, the PhenoMan provides an API and acts as\\na web service (using Dropwizard [26]) (Fig. 1). The Phe-\\nnoMan is implemented in Java using OWL API [23] and\\ntwo reasoners, HermiT [24] and Openllet [25]. For cal-\\nculations we utilize the Java Expression Evaluator (Eva-\\nlEx) [27], but the integration of other libraries (e.g., for\\nexecuting R scripts) or rule systems (e.g., SWIRL or\\nDrools) is also possible. The EvalEx enables evaluating\\nmathematical and Boolean (inter alia, Boolean operators\\nand IF-THEN-ELSE structures) expressions and sup-\\nports defining custom functions and operators.\\nThe PhenoMan Editor1 is a desktop app, which is also\\ndeveloped with Java and bundled with the PhenoMan\\nAPI. It offers a graphical user interface based on Java\\nSwing to specify attributes of phenotype classes and cat-\\negories using appropriate form fields. On saving, form\\ncontent is transmitted to the PhenoMan API and written\\ninto the ontology. The editor can be executed on a local\\nmachine with a Java runtime environment 8 or higher\\nand was developed with the aim of rapidly defining\\nphenotype models.\\nEvaluation\\nAn evaluation of our approach was designed and con-\\nducted. The main objectives of the evaluation were to\\nprove:\\n1. Correct functioning of all software components\\n2. Faultless communication of the software with the\\nFHIR Server\\n3. Correctness of all provided phenotype specifications\\n4. Correct functioning of the overall system by\\ncomparison with a corresponding SPSS\\nimplementation of selected phenotypes.\\nWe evaluated the PhenoMan at different levels. Firstly,\\nwe tested all functionalities of the PhenoMan API (espe-\\ncially read/write in the ontology and computing pheno-\\ntypes) and the communication of the PhenoMan Service\\nwith the FHIR Server by a set of static JUnit tests using\\nfixtures (i.e., example PheSOs and patient data).\\nSecondly, each phenotype specification is shipped with\\na structured representation (spreadsheet) of test data (in-\\nput and output), such that the respective phenotype al-\\ngorithm can be automatically tested. The criterion for a\\nsuccessful execution of the JUnit tests was a match be-\\ntween the results calculated by PhenoMan based on pro-\\nvided input data and the corresponding output data.\\nFinally, we selected some test case algorithms/deriva-\\ntives (such as socio-economic status [28], body mass\\nindex [29], waist circumference and waist-hip ratio [30])\\nfrom the LIFE Adult study [31] running at the LIFE Re-\\nsearch Centre for Civilization Diseases, University of\\nLeipzig. There, derivatives are usually implemented by\\nepidemiologists, statisticians and other researchers using\\nthe statistics software SPSS [32] and R or are database\\n(SQL) queries and functions, which are automatically ex-\\necuted at night based on daily captured data. The result-\\ning data are directly stored within the LIFE research\\ndatabase in tabular form. More details about partici-\\npants, their invitation and consenting as well as\\n1Source code and releases of the PhenoMan Editor are available on\\nGitHub under the GPL-3.0 license: https://github.com/Onto-Med/Phe-\\nnoMan-Editor\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 5 of 17\\nexaminations, interviews, questionnaires and taken spe-\\ncimen can be found in [31]. We reproduced selected\\nSPSS derivatives using the PhenoMan and developed\\nparameterised JUnit tests to comparatively evaluate the\\naccuracy of the PhenoMan against the corresponding\\nSPSS implementation at the LIFE Research Centre. The\\ncriterion for a successful comparison was a match be-\\ntween the results calculated by PhenoMan and SPSS\\nsoftware for each dataset. The performance of our ap-\\nproach is not a critical issue in our use case (the pheno-\\ntype computing could run overnight).\\nThis evaluation included data of thousands of LIFE\\nparticipants.\\nResults\\nCore Ontology of Phenotypes (COP)\\nWe developed the Core Ontology of Phenotypes\\n(COP, Fig. 3) to model, classify and calculate pheno-\\ntypes based on instance data sets (e.g., of a patient).\\nIn this article, we consider a phenotype as a\\ndependent individual (in the sense of General Formal\\nOntology, GFO [20]), for example, the weight of a\\nspecific person. Hereinafter, abstract instantiable en-\\ntities that are instantiated by phenotypes are called\\nphenotype classes. For instance, the abstract property\\n\\u0091weight\\u0092 possesses individual weights as instances. We\\ndistinguish between single and composite properties,\\nand correspondingly, between single and composite\\nphenotypes. A composite property is defined as a\\nproperty that has single properties as parts [22].\\nBased on the definitions of single and composite\\nproperties [22], we define single phenotypes as single\\nproperties (e.g., age, weight, height) and composite\\nphenotypes as composite properties (e.g., height and\\nweight, BMI, SOFA score [33]) of an organism or of\\none of its subsystems. Properties of an organism are\\nconsidered as all documentable information about it,\\nwhereby the modeller is left to decide what is rele-\\nvant to the current situation. These can be, for ex-\\nample, observable characteristics or traits of an\\norganism [1\\u00963] or possible manifestations of clinical\\nphenotypes, such as signs, symptoms or dispositions\\n[4]. The corresponding data can be modelled using\\nthe FHIR Observation or Condition resources.\\nComposite phenotypes are divided into combined\\nand derived phenotypes. A combined phenotype is\\nonly a combination of corresponding phenotypes (e.g.,\\na combination of height and weight), whereas a de-\\nrived phenotype is an additional property (e.g., BMI)\\nderived from the corresponding phenotypes (height\\nand weight). In the framework of GFO we modelled\\nproperties using the class gfo: Property. In the present\\narticle, composite phenotype classes are modelled\\nusing a Boolean expression based on has_part relation\\n(e.g., weight and height: has_part some height and\\nhas_part some weight). Derived phenotype classes\\nadditionally define a calculation rule/mathematical\\nformula (e.g., BMI = weight [kg] / height [m]2). Fur-\\nthermore, combined phenotype classes can associate\\ncertain conditions with specific predefined values\\n(scores), which can be used, e.g., in further formulas.\\nFor example, if bilirubin value is greater than 12 mg/\\ndL, then the value 4 is used for the calculation of the\\nSOFA score [33].\\nAdditionally, we distinguish between restricted and\\nnon-restricted phenotype classes, depending on whether\\ntheir extensions (set of instances) are restricted to a cer-\\ntain range of individual phenotypes by defined condi-\\ntions or all instances are allowed. For example, the\\nphenotype class \\u0091age\\u0092 is instantiated by the ages of all liv-\\ning beings (non-restricted), whereas the phenotype class\\n\\u0091young age\\u0092 is instantiated by the ages of the young ones,\\ne.g., if the age is below 30 years (restricted).\\nPhenotype Specification Ontologies (PheSO)\\nWe consider a phenotype algorithm as a sequence of in-\\nstructions (1) to classify phenotypes (single or compos-\\nite) in phenotype classes or (2) to derive additional\\nproperties (derived phenotypes) from the phenotypes of\\nan organism. Phenotype algorithms can be implemented,\\nfor example, using a programming language or a statis-\\ntics software (e.g., SPSS or R). Our approach is to separ-\\nate the specification of phenotypes (models) from the\\nimplementation of corresponding algorithms. The COP\\nprovides a basic model to specify phenotypes in a stan-\\ndardised way, while the PhenoMan implements the gen-\\neral approach, common for all COP-based specifications.\\nIt is not our aim to completely model the EHR. Instead,\\nour approach can support the modelling and calculation\\nof selected phenotypes in a user-friendly standardised\\nmanner.\\nPhenotypes are modelled in Phenotype Specification\\nOntologies (PheSO) using the COP. The phenotype clas-\\nses and axioms (classification and calculation rules) con-\\ntained in the PheSO are used by PhenoMan to execute\\nthe corresponding phenotype algorithm. PheSOs are em-\\nbedded in the COP in such a way that the classes of the\\nPheSO are subclasses of the COP classes. Every PheSO\\nsubclass of the COP classes cop: Single_Phenotype, cop:\\nCombined_Phenotype or cop: Derived_Phenotype is a\\nphenotype class and is instantiated by phenotypes. The\\nFig. 3 Core Ontology of Phenotypes (COP)\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 6 of 17\\ndirect subclasses are non-restricted (e.g., Fasting_Glu-\\ncose, Fig. 4a), while the subclasses of the non-restricted\\nphenotype classes are restricted (e.g., Fasting_Glucose_\\nABNORMAL, i.e., fasting glucose is greater equal 125\\nmg/dL, Fig. 4a3).\\nPhenotype classes possess various common attributes\\n(e.g., labels, descriptions and codes of external concepts).\\nOther attributes vary depending on the type of the\\nphenotype class. The following are examples of such\\nattributes:\\n\\u0096 Non-restricted single phenotype (NSiP) class: unit of\\nmeasure and optional aggregate function.\\n\\u0096 Restricted single (RSiP) and derived phenotype\\n(RDeP) class: restriction.\\n\\u0096 Restricted combined phenotype (RCoP) class:\\nBoolean expression (based on RSiP, RCoP and RDeP\\nclasses) and optional score value.\\n\\u0096 Non-restricted derived phenotype (NDeP) class:\\nmathematical formula and Boolean expression\\nconsisting of AND-linked variables used in the for-\\nmula (NSiP and non-restricted combined phenotype\\n(NCoP) classes). If a NCoP class is used as a vari-\\nable, the RCoP classes (subclasses) of the NCoP class\\nmust have score values that should be used in the\\nformula.\\nSimple attributes of the phenotype classes are defined\\nas annotations. The logical relations between phenotype\\nclasses as well as range restrictions are represented in\\nOWL by anonymous equivalent classes or general class\\naxioms based on property restrictions.\\nPhenotype Manager (PhenoMan)\\nWe developed the software Phenotype Manager (Pheno-\\nMan), which implements a multistage reasoning ap-\\nproach combining standard reasoners (e.g., Pellet or\\nHermiT) and mathematical calculations. This section\\nbriefly outlines the main functionality of our solution.\\nSpecification of phenotypes\\nThe PhenoMan Editor is an interactive user interface for\\nmanaging and developing PheSOs. The user is able to\\ncreate a new PheSO or to load an existing ontology. The\\nFig. 4 Parts of the T2DM PheSO in Protégé. Middle: Phenotype classes (a Single, b Derived, c Combined). Left: Example annotations of the\\nphenotype classes. Right: Anonymous equivalent classes and general class axioms\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 7 of 17\\nPhenoMan Editor provides appropriate forms to browse,\\ncreate and edit categories and phenotype classes of the\\nontology. Value range restrictions, for example, are de-\\nfined by selecting a comparison operator and entering\\nthe corresponding values (Fig. 5). Boolean expressions\\nare built by drag-and-dropping the phenotype classes\\nfrom the left side into the expression form field and en-\\ntering relevant operators (Fig. 6). After submission, the\\nform data is transmitted to the PhenoMan API and is\\nstored in the actual PheSO.\\nFurthermore, an ART-DECOR specification (XML)\\nof relevant data elements can be imported in the\\nPheSO. For each data element, a NSiP class is gener-\\nated. All relevant attributes (name, codes, FHIR re-\\nsource type, data type, unit, etc.) specified in ART-\\nDECOR are defined as annotations of corresponding\\nclasses (Fig. 2a, b).\\nData procurement\\nAfter starting the PhenoMan Service, FHIR subscriptions\\n(rest-hooks) [34] are generated and transmitted to the\\nFHIR Server. The structure of the subscription resource\\nis very simple. The main parts of the resource are the\\ncriteria and the channel. The FHIR Server uses the cri-\\nteria (FHIR Search query) to determine resources for\\nwhich notifications have to be generated. When re-\\nsources are identified (after creating or updating) meet-\\ning the criteria, a notification is sent to the address\\n(\\u0091endpoint\\u0092) specified in the section \\u0091channel\\u0092.\\nFig. 5 Specification of the class Fasting_Glucose_ABNORMAL with the PhenoMan Editor form. We left out some of the metadata fields for\\nbetter visibility\\nFig. 6 Specification of the class T2DM_Case_3 with the PhenoMan Editor form\\nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 8 of 17\\nIn the configuration file of the PhenoMan, a directory\\ncontaining all available phenotype specifications (Phe-\\nSOs) as well as the address (URL) of the PhenoMan ser-\\nvice (including the PheSO name) are defined. For each\\nNSiP class of each available PheSO (in the defined direc-\\ntory) a subscription is created. To generate the subscrip-\\ntion criteria (FHIR Search query), the PhenoMan uses\\nthe resource type and codes specified in the correspond-\\ning NSiP class as annotations (Fig. 2b, c). The \\u0091endpoint\\u0092\\nattribute is automatically filled with the URL of the Phe-\\nnoMan service defined in the configuration file. The\\nremaining parts of the subscription resource (\\u0091status\\u0092,\\n\\u0091type\\u0092 and \\u0091payload\\u0092) take default values (\\u0091active\\u0092, \\u0091rest-\\nhook\\u0092 and \\u0091application/json\\u0092) (Fig. 2c).\\nAfter receiving a notification (including the\\ncomplete resource, Fig. 2d), the PhenoMan Service re-\\nquests further resources (for all other NSiP classes of\\nthe corresponding PheSO) using FHIR Search. The\\ngenerated FHIR Search queries are primarily based\\nupon the codes specified for the NSiP classes (simi-\\nlarly to subscription criteria), contain a reference to\\nthe patient and can additionally support possible ag-\\ngregate functions.\\nClassification and calculation of phenotypes\\nAfter receiving required resources, the PhenoMan starts\\ninferring phenotypes.\\nFirst, the relevant information is extracted from re-\\nceived resources and inserted into the ontology. On the\\none hand, the individual properties (single phenotypes)\\nare inserted as instances of the direct subclasses of cop:\\nSingle_Phenotype and the values are modelled as prop-\\nerty assertions based on the has_value relation. On the\\nother hand, a composite phenotype is defined as an in-\\nstance of the class cop: Composite_Phenotype, which\\ncombines all the single phenotype instances using prop-\\nerty assertions based on has_part relation. Then, our\\nmultistage reasoning algorithm is executed. The algo-\\nrithm consists of the following steps:\\n1. Classification step. A standard reasoner classifies the\\nexisting instances (assignment to classes).\\na. Single phenotype instances are classified in RSiP\\nclasses based on property restrictions.\\nb. The composite phenotype instance is classified\\nin RCoP classes based on the specified Boolean\\nexpression and inferred RSiP, RCoP and RDeP\\nclasses.\\nc. The composite phenotype instance is classified\\nin NDeP classes based on the specified Boolean\\nexpression and corresponding NSiP and NCoP\\nclasses. In this case, all variable values required\\nfor calculating formulas are present.\\nd. Available instances of NDeP classes\\n(representing calculated values) are classified in\\nRDeP classes based on proper'</span>\n\n",
            "text/latex": "\\begin{description*}\n\\item[text1] 'Althubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 \\textbackslash{}nhttps://doi.org/10.1186/s13326-019-0218-0\\textbackslash{}nRESEARCH Open Access\\textbackslash{}nCombining lexical and context features\\textbackslash{}nfor automatic ontology extension\\textbackslash{}nSara Althubaiti1,2, S¸enay Kafkas1,2 , Marwa Abdelhakim1,2 and Robert Hoehndorf1,2*\\textbackslash{}nAbstract\\textbackslash{}nBackground: Ontologies are widely used across biology and biomedicine for the annotation of databases. Ontology\\textbackslash{}ndevelopment is often a manual, time-consuming, and expensive process. Automatic or semi-automatic identification\\textbackslash{}nof classes that can be added to an ontology can make ontology development more efficient.\\textbackslash{}nResults: We developed a method that uses machine learning and word embeddings to identify words and phrases\\textbackslash{}nthat are used to refer to an ontology class in biomedical Europe PMC full-text articles. Once labels and synonyms of a\\textbackslash{}nclass are known, we use machine learning to identify the super-classes of a class. For this purpose, we identify lexical\\textbackslash{}nterm variants, use word embeddings to capture context information, and rely on automated reasoning over\\textbackslash{}nontologies to generate features, and we use an artificial neural network as classifier. We demonstrate the utility of our\\textbackslash{}napproach in identifying terms that refer to diseases in the Human Disease Ontology and to distinguish between\\textbackslash{}ndifferent types of diseases.\\textbackslash{}nConclusions: Our method is capable of discovering labels that refer to a class in an ontology but are not present in an\\textbackslash{}nontology, and it can identify whether a class should be a subclass of some high-level ontology classes. Our approach\\textbackslash{}ncan therefore be used for the semi-automatic extension and quality control of ontologies. The algorithm, corpora and\\textbackslash{}nevaluation datasets are available at https://github.com/bio-ontology-research-group/ontology-extension.\\textbackslash{}nKeywords: Disease ontology, Embeddings, Neural network\\textbackslash{}nBackground\\textbackslash{}nThe biomedical community has spent significant\\textbackslash{}nresources to develop biomedical ontologies which con-\\textbackslash{}ntain and define the basic classes and relations that occur\\textbackslash{}nwithin a domain. Biomedical ontologies are developed by\\textbackslash{}ndomain experts and are often developed in conjunction\\textbackslash{}nwith the needs arising in literature-based curation of\\textbackslash{}nbiological databases.\\textbackslash{}nManual curation of databases based on literature is a\\textbackslash{}nvery time-consuming task due to the massive amounts of\\textbackslash{}nliterature, and automated methods have been developed\\textbackslash{}nearly on to aid in curation {[}1{]}. One of the key tasks in\\textbackslash{}ncomputational support for literature curation is the auto-\\textbackslash{}nmatic concept recognition of mentions of ontology classes\\textbackslash{}nin text {[}2{]}. An ontology class is an intensionally defined\\textbackslash{}n*Correspondence: robert.hoehndorf@kaust.edu.sa\\textbackslash{}n1Computational Bioscience Research Center, King Abdullah University of\\textbackslash{}nScience and Technology, 23955-6900 Thuwal, Saudi Arabia\\textbackslash{}n2Computer, Electrical and Mathematical Sciences and Engineering Division,\\textbackslash{}nKing Abdullah University of Science and Technology, 23955-6900 Thuwal,\\textbackslash{}nSaudi Arabia\\textbackslash{}nentity that has a formal descriptionwithin an ontology and\\textbackslash{}naxioms that determine its relation with other classes {[}3{]}. In\\textbackslash{}nnatural language, multiple terms and phrases can be used\\textbackslash{}nto refer to an ontology class {[}4{]}, and the formal depen-\\textbackslash{}ndencies within an ontology further determine whether a\\textbackslash{}nterm refers to a class or not (i.e., whether a term refers to\\textbackslash{}na particular class may depend on background knowledge,\\textbackslash{}nin particular subclass relations, contained in an ontol-\\textbackslash{}nogy). For example, the Disease Ontology (DO) {[}5{]} declares\\textbackslash{}nPrediabetes syndrome (DOID:11716) to be a subclass\\textbackslash{}nof Diabetes mellitus (DOID:9351), and based on this\\textbackslash{}ninformation we assume that any reference to, or mention\\textbackslash{}nof, Prediabetes syndrome is also a reference to Diabetes\\textbackslash{}nmellitus (with respect to DO).\\textbackslash{}nThere are several text mining systems designed for\\textbackslash{}nontology concept recognition in text. These methods are\\textbackslash{}neither based on lexical methods and therefore applicable\\textbackslash{}nto a wide range of ontologies {[}6, 7{]} or they are domain-\\textbackslash{}nspecific and rely on machine learning {[}8{]}. Text mining\\textbackslash{}n© The Author(s). 2020 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\\textbackslash{}nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\\textbackslash{}nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the\\textbackslash{}nCreative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver\\textbackslash{}n(http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 2 of 13\\textbackslash{}nbased-methods can also be used to automatically or semi-\\textbackslash{}nautomatically construct and extend ontologies {[}9, 10{]}. For\\textbackslash{}nexample, Lee et al. {[}11{]} focus on text mining of relations\\textbackslash{}nthat are asserted in text between mentions of ontology\\textbackslash{}nclasses that has been used to refine ontology classes in the\\textbackslash{}nGene Ontology (GO) {[}12{]}. Text mining can also be used to\\textbackslash{}nsuggest new subclasses and sibling classes in ontologies,\\textbackslash{}nfor exampleWächter and Schroeder {[}13{]} carried out a text\\textbackslash{}nmining based-system from different text sources which is\\textbackslash{}nused for extending OBO ontologies by semi-automatically\\textbackslash{}ngenerating terms, definitions and parent\\textbackslash{}u0096child relations.\\textbackslash{}nXiang et al. {[}14{]} have developed a pattern-based system\\textbackslash{}nfor generating and annotating a large number of ontology\\textbackslash{}nterms, following ontology design patterns and providing\\textbackslash{}nlogical axioms that may be added to an ontology. Recently,\\textbackslash{}nclustering based on statistical co-occurrence measures\\textbackslash{}nwere also used to extend ontologies {[}15{]}.\\textbackslash{}nHere, we introduce a novel method relying on machine\\textbackslash{}nlearning to identify whether a word used in text refers\\textbackslash{}nto a class that could be included in a particular ontol-\\textbackslash{}nogy. Essentially, our method classifies terms to determine\\textbackslash{}nif they are usually mentioned in the same context as the\\textbackslash{}nlabels and synonyms of classes in an ontology (which are\\textbackslash{}nused as seeds to train the classifier); this classifier can then\\textbackslash{}nbe applied to unseen terms. Furthermore, our method can\\textbackslash{}nalso be used to expand ontologies by suggesting terms that\\textbackslash{}nare mentioned within the same context as specific classes\\textbackslash{}nin an ontology.\\textbackslash{}nWe demonstrate the utility of our method in identi-\\textbackslash{}nfying words referring to diseases from DO in full text\\textbackslash{}narticles. We select the DO because the labels and syn-\\textbackslash{}nonyms of DO classes are relatively easy to detect in text\\textbackslash{}nand a large number of computational methods rely on\\textbackslash{}naccess to a comprehensive disease ontology {[}16\\textbackslash{}u009619{]}. Our\\textbackslash{}nmethod achieves highly accurate (F-score > 90\\%) and\\textbackslash{}nrobust results, is capable of recognizing multiple different\\textbackslash{}nclasses including those defined formally through logical\\textbackslash{}noperators, and combines dictionary-based and context-\\textbackslash{}nbased features; therefore, our method is also capable of\\textbackslash{}nfinding new words that refer to a class. We manually\\textbackslash{}nevaluate the results and suggest several additions to the\\textbackslash{}nDO.\\textbackslash{}nMethods\\textbackslash{}nBuilding a disease dictionary\\textbackslash{}nWe built a dictionary from the labels and synonyms\\textbackslash{}nof classes in the Disease Ontology (DO), downloaded\\textbackslash{}non 5 February 2018 from http://disease-ontology.org/\\textbackslash{}ndownloads/. The dictionary consisted of 21,788 terms\\textbackslash{}nbelonging to 6,831 distinct disease classes from DO. We\\textbackslash{}nutilized the dictionary with the Whatizit tool {[}20{]} and\\textbackslash{}nannotated the ontology class mentions along with their\\textbackslash{}nidentifiers in approximately 1.6 million open access full-\\textbackslash{}ntext articles from the Europe PMC database {[}21{]} (http://\\textbackslash{}neuropepmc.org/ftp/archive/v.2017.06/) and generated a\\textbackslash{}ncorpus annotated with mentions of classes in DO. We\\textbackslash{}npreprocessed the corpus by removing stop words such as\\textbackslash{}n\\textbackslash{}u0093the\\textbackslash{}u0094, \\textbackslash{}u0093a\\textbackslash{}u0094, and \\textbackslash{}u0093is\\textbackslash{}u0094 as well as some punctuation characters.\\textbackslash{}nGenerating context-based features\\textbackslash{}nWe use Word2Vec {[}22{]} to generate word embedding.\\textbackslash{}nSpecifically, we use a skip-gram model which aims to find\\textbackslash{}nword representations that are useful for predicting the\\textbackslash{}nsurrounding words in a given sentence or a document\\textbackslash{}nconsisting of sequence of words; w1,w2, ...,wK . The objec-\\textbackslash{}ntive is to maximize the average log probability using the\\textbackslash{}nfollowing formula:\\textbackslash{}nV (w) = 1K\\textbackslash{}nK?\\textbackslash{}nk=1\\textbackslash{}nK?\\textbackslash{}n?c?j?c;j \\textbackslash{}003=0\\textbackslash{}nlog p(wK+j\\textbar{}wK ) (1)\\textbackslash{}nwhere word vectors V (w) are computed by averaging over\\textbackslash{}nthe number of words K and c is the size of the training\\textbackslash{}ncontext. We generated the word embedding by using the\\textbackslash{}ndefault parameter settings of theWord2Vec gensim imple-\\textbackslash{}nmentation: vector size (dimensionality) of 100, window\\textbackslash{}nsize 5, minimum occurrence count of 5, and we use a\\textbackslash{}nskip-gram (sg) model.\\textbackslash{}nSupervised training\\textbackslash{}nWe carried out a set of experiments to choose the optimal\\textbackslash{}ntraining algorithm to design our model. In our experi-\\textbackslash{}nments we used default parameters for the training algo-\\textbackslash{}nrithms but different hidden layers for Artificial Neural\\textbackslash{}nNetworks (ANNs) {[}23{]}. Our experiments show that the\\textbackslash{}nANN model outperforms an SVM model {[}24{]} (see Addi-\\textbackslash{}ntional file 1: Table 1 for full details), and our model\\textbackslash{}nperforms best with 200 neurons in a single hidden layer\\textbackslash{}n(we tested a single hidden layer with a size of 10, 50,\\textbackslash{}n100, and 200 neurons). We report results accordingly to a\\textbackslash{}nmodel with 200 neurons in the remainder of this work. In\\textbackslash{}nANNs, multiple neurons are organized in layers. Typically,\\textbackslash{}ndifferent layers perform different kinds of transforma-\\textbackslash{}ntions on their inputs {[}25{]}. In our experiments, we used\\textbackslash{}nan ANN with an input layer of different sizes, a single\\textbackslash{}nhidden layer that uses a sigmoid activation function, and\\textbackslash{}nan output layer that differs based on the experiment. We\\textbackslash{}ntrain each classifier in a supervised manner, using 10-fold\\textbackslash{}nstratified cross-validation. Additionally, we report testing\\textbackslash{}nperformance on an independent 20\\% testing set which\\textbackslash{}nwe generated by randomly removing data points before\\textbackslash{}ntraining.\\textbackslash{}nRecognizing ontology classes in text\\textbackslash{}nWe used two approaches to recognize the mention of\\textbackslash{}nontology classes in text. Our first approach relies solely on\\textbackslash{}nlabels and synonyms of the classes within a given ontol-\\textbackslash{}nogy O and can be used to determine whether a word refer\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 3 of 13\\textbackslash{}nto a class in O. We first obtain an ontology O in the Web\\textbackslash{}nOntology Language (OWL) {[}26{]} format and extract a list\\textbackslash{}nof class labels and synonyms L from O; we further utilize\\textbackslash{}na text corpus T as input to our method. Then, we gener-\\textbackslash{}nate word embeddings (i.e., vector-space encodings of the\\textbackslash{}ncontexts in which a word occurs) for all words in our text\\textbackslash{}ncorpus T and train a supervised machine learning model\\textbackslash{}nto classify whether a word refers to a class in O or not\\textbackslash{}n(using the L\\textbackslash{}u0092s words as positive training instances and all\\textbackslash{}nothers as negative instances).\\textbackslash{}nFigure 1 illustrates the workflow of our first approach.\\textbackslash{}nOur method is generic and can, in principle, be applied\\textbackslash{}nto any ontology as long as the ontology provides labels\\textbackslash{}n(or synonyms), these labels can be identified in text, and\\textbackslash{}nthe ontology from which the labels are extracted is more\\textbackslash{}nor less limited to a single domain. For example, refer-\\textbackslash{}nence ontologies in the OBO Foundry {[}27{]} are usually\\textbackslash{}nsingle domain ontologies and therefore suitable for our\\textbackslash{}nmethod. Ontologies that would not be suitable are appli-\\textbackslash{}ncation ontologies that cover multiple domains, such as the\\textbackslash{}nExperimental Factor Ontology (EFO) {[}28{]} (although our\\textbackslash{}nmethods can be applied to parts of it). It is most useful to\\textbackslash{}nextend an existing ontology with new labels, synonyms, or\\textbackslash{}nclasses.\\textbackslash{}nIn our second approach, we rely on annotations from\\textbackslash{}nthe Whatizit tool {[}20{]} to identify the mention of ontology\\textbackslash{}nclasses in text and determine their specific superclasses in\\textbackslash{}nan ontology. Our approach takes an ontology O in OWL\\textbackslash{}nformat, a set of ontology classes S = \\{C1, ...,Cn\\}, and a\\textbackslash{}ncorpus of text T as inputs.\\textbackslash{}nThis approach first uses Whatizit as a named entity\\textbackslash{}nrecognition and normalization tool to normalize class\\textbackslash{}nlabels and synonyms in text by replacing all mentions\\textbackslash{}nof a class with the class identifier (i.e., the class URI).\\textbackslash{}nWe annotate 15,183 distinct terms using Whatizit; the\\textbackslash{}ntotal dictionary consists of 21,788 terms (derived from\\textbackslash{}nthe labels and synonyms of classes in DO). We then train\\textbackslash{}nWord2Vec model that captures the context of the men-\\textbackslash{}ntion of the class and generates a vector space embedding\\textbackslash{}nfor that class. Given such vector space embeddings for\\textbackslash{}na set of classes in O, we use the vector space embed-\\textbackslash{}ndings as input to a machine learningmethod that classifies\\textbackslash{}nwhether another class appears in a similar context. We\\textbackslash{}nuse this method to determine if a class should belong the\\textbackslash{}nsuperclass of C in O. Figure 2 illustrates the workflow of\\textbackslash{}nthis approach.\\textbackslash{}nThe main difference between the two approaches is that\\textbackslash{}nthe first approach broadly identifies terms or words that\\textbackslash{}nrefer to classes within a domain (as defined by the sum\\textbackslash{}nof classes within an ontology) while the second approach\\textbackslash{}ncan determine whether a term or word refers to a class\\textbackslash{}nthat should appear as a subclass of a more specific ontol-\\textbackslash{}nogy class. Both methods generate \\textbackslash{}u0093seed\\textbackslash{}u0094 words in text and\\textbackslash{}nthen use these seeds first to generate context-based fea-\\textbackslash{}ntures (through Word2Vec) and use these context-based\\textbackslash{}nfeatures in a supervised machine learning classifier.\\textbackslash{}nManual analysis process\\textbackslash{}nWe manually evaluate some of our findings. The manual\\textbackslash{}nevaluation is based on the medical expert knowledge of\\textbackslash{}nthe evaluator who is a trained clinician, and supplemented\\textbackslash{}nby literature search to validate some findings or resolve\\textbackslash{}nconflicts. Mainly, results were confirmed by searching\\textbackslash{}nfor review papers that characterize a condition. Overall,\\textbackslash{}nFig. 1 Label-based workflow. The workflow describes how words (in red) are classified as disease or \\textbackslash{}u0093other\\textbackslash{}u0094\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 4 of 13\\textbackslash{}nFig. 2 Annotation-based workflow. In this workflow, we first normalize the mentions of disease classes in the corpus and then apply Word2Vec to\\textbackslash{}ngenerate embeddings for classes, not merely words\\textbackslash{}nmanual curation following the suggestions by our classi-\\textbackslash{}nfier took 10-15 min per sample (which included identify-\\textbackslash{}ning related classes in the DO and drafting an explanation\\textbackslash{}nfor cases which disagree with the DO).\\textbackslash{}nResults\\textbackslash{}nBroad classification of domain-specific terms: application\\textbackslash{}nto diseases\\textbackslash{}nOur method is a workflow that can be used to identify\\textbackslash{}nwhether a term or phrase commonly refers to a class\\textbackslash{}nthat may be included in a domain-specific ontology as a\\textbackslash{}nlabel, synonym, or a new class. To achieve this goal, we\\textbackslash{}nuse the existing labels and synonyms within a domain-\\textbackslash{}nontology as \\textbackslash{}u0093seeds\\textbackslash{}u0094 to train a machine learning classifier\\textbackslash{}nthat determines whether a new term is sufficiently similar\\textbackslash{}nto an existing label or synonym and may therefore also be\\textbackslash{}nincluded in the ontology. We represent terms primarily by\\textbackslash{}nthe context in which they occur within a large corpus of\\textbackslash{}ntext; we useWord2Vec {[}22{]} for this purpose.We then train\\textbackslash{}nan Artificial Neural Network classifier in a supervised\\textbackslash{}nmanner to distinguish between the terms already included\\textbackslash{}nwithin a domain ontology (and therefore expected to refer\\textbackslash{}nto a particular kind of phenomena) and randomly chosen\\textbackslash{}nterms not included in the ontology (and therefore most\\textbackslash{}nlikely not referring to a phenomenon within the domain\\textbackslash{}nof the ontology).\\textbackslash{}nWe demonstrate our method using the Human Dis-\\textbackslash{}nease Ontology (DO) {[}5{]} and applying it to the terms\\textbackslash{}noccurring in a large corpus of full-text biomedical articles\\textbackslash{}n(see \\textbackslash{}u0093Methods\\textbackslash{}u0094). First, we tested whether our approach is\\textbackslash{}ncapable of identifying words that refer to the Disease class\\textbackslash{}n(DOID:4), i.e., whether our method can detect terms\\textbackslash{}nthat refer to a disease. We generated word embeddings\\textbackslash{}nfor every disease terms and other words in our corpus of\\textbackslash{}nfull-text articles.\\textbackslash{}nFigure 3 illustrates the distribution of the terms refer-\\textbackslash{}nring to a diseases in DO and other words mentioned in\\textbackslash{}nour corpus which do not belong to DO using the t-SNE\\textbackslash{}ndimensionality reduction {[}29{]}. We can see that the terms\\textbackslash{}nare clearly different and should be separable through a\\textbackslash{}nmachine learning system.\\textbackslash{}nTherefore, we trained a machine learning model to\\textbackslash{}nrecognize whether a word refers to the disease or not\\textbackslash{}nusing the word embeddings as input. We split the vec-\\textbackslash{}ntor space embeddings into a training and testing dataset\\textbackslash{}nand consider all embeddings referring to disease as pos-\\textbackslash{}nitive instances and all others as negatives. We do not\\textbackslash{}napply any filtering before selecting the positive or negative\\textbackslash{}nsamples. We randomly select negatives equal to the num-\\textbackslash{}nber of positives (7,932 positives and 7,932 negatives). We\\textbackslash{}nwithhold 20\\% of randomly chosen positive and negative\\textbackslash{}ninstances for testing, train a model on the remaining 80\\%\\textbackslash{}nthrough 10-fold cross validation, and report the perfor-\\textbackslash{}nmance results on the 20\\% test set. Evaluated on the testing\\textbackslash{}nset, we can distinguish between disease and non-disease\\textbackslash{}nterms with an F-score of 95\\% and AUC of 96\\% (see Table 1\\textbackslash{}nand Figure 4).\\textbackslash{}nTo better understand the source of errors and whether\\textbackslash{}nour approach can be used to reliably extend ontolo-\\textbackslash{}ngies (either with additional labels and synonyms, or new\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 5 of 13\\textbackslash{}nFig. 3 a) The visualization of the embeddings using the t-SNE for binary-classification task b) The visualization of the embeddings using the t-SNE\\textbackslash{}nfor classifying infectious diseases. c) The visualization of the embeddings using the t-SNE for classifying anatomical diseases. d) The visualization of\\textbackslash{}nthe embeddings using the t-SNE for classifying the combination of infectious and anatomical diseases\\textbackslash{}nclasses), we performed a manual analysis on a set of 20\\textbackslash{}nfalse positive samples out of 197 which are not the label or\\textbackslash{}nsynonym of a disease class DO but are classified as disease\\textbackslash{}nby our classifier (see Table 2). We found that the majority\\textbackslash{}nof the 20 false positive samples refer to either diseases or\\textbackslash{}nphenotypes (where phenotypes are the observable char-\\textbackslash{}nacteristics of an organism that may occur manifestations,\\textbackslash{}nor signs and symptoms, of a disease, but do not constitute\\textbackslash{}na disease on its own). For example, Aphthosis is a pre-\\textbackslash{}ndiction of our method which refers to a human disorder\\textbackslash{}nthat is not currently in the DO; the majority of false pos-\\textbackslash{}nitives are disease-related terms that do not explicitly refer\\textbackslash{}nto a disease. For example, we predictedmal-absorption as\\textbackslash{}na disease term which may refer to a phenotype in some\\textbackslash{}ncontexts. Our findings indicate that an ANN classifier\\textbackslash{}ncan identify known terms referring to diseases, and can\\textbackslash{}nfurther suggest novel terms which may prove useful for\\textbackslash{}nontology development and extension.\\textbackslash{}nFine-grained classification: distinguishing between groups\\textbackslash{}nof diseases\\textbackslash{}nAs our method showed capability to identify terms refer-\\textbackslash{}nring to a disease, we next tested whether our method can\\textbackslash{}nalso distinguish between different types of diseases. For\\textbackslash{}nthis purpose, we used the embeddings generated from a\\textbackslash{}npre-processed corpus in which we normalize all mentions\\textbackslash{}nTable 1 F-score and AUC for our four experiments using different hidden layer sizes\\textbackslash{}nClassification Hidden layer sizes 10 50 100 200\\textbackslash{}nNumber of classes F-score AUC F-score AUC F-score AUC F-score AUC\\textbackslash{}nDiseases 2 94.65\\% 95.31\\% 94.83\\% 95.97\\% 95.32\\% 96.06\\% 94.49\\% 95.99\\%\\textbackslash{}nInfectious disease 5 95.65\\% 95.01\\% 96.01\\% 95.74\\% 95.43\\% 95.22\\% 95.68\\% 96.42\\%\\textbackslash{}nAnatomical disease 13 69.18\\% 77.22\\% 70.15\\% 80.24\\% 70.20\\% 76.98\\% 72.00\\% 85.11\\%\\textbackslash{}nInfectious + anatomical diseases 17 71.07\\% 84.75\\% 73.13\\% 84.03\\% 72.61\\% 84.98\\% 72.67\\% 83.66\\%\\textbackslash{}nThe values in bold represent the highest AUC and F-score within each experiments\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 6 of 13\\textbackslash{}nFig. 4 ROC curves for each experiment (Diseases, Infectious disease, Anatomical disease and a combination of Infectious disease + Anatomical disease)\\textbackslash{}nof a disease in our corpus using Whatizit tool. The dis-\\textbackslash{}nease dictionary that we utilized with Whatizit includes a\\textbackslash{}ntotal of 21,788 terms (labels and synonyms) from DO. We\\textbackslash{}nfound that 15,183 of these 21,788 terms appeared in our\\textbackslash{}ncorpus and we generate an embedding vector for each of\\textbackslash{}nthem. We then first trained a neural network model to\\textbackslash{}nrecognize whether a disease-term refers to the Infectious\\textbackslash{}nDisease (DOID:0050117) class or not, and furthermore\\textbackslash{}nwhether our method is able to distinguish between the\\textbackslash{}nfour different types of infectious disease in DO (i.e., bac-\\textbackslash{}nterial, fungal, parasitic, or viral infectious disease). As\\textbackslash{}ntraining data, we used the word embeddings generated for\\textbackslash{}nDO classes, and we used the Elk reasoner to split them\\textbackslash{}ninto four types of infectious diseases, and an additional\\textbackslash{}nclass for diseases that are not a subclass of Infectious Dis-\\textbackslash{}nease in DO. We randomly select 20\\% of the disease in\\textbackslash{}nDO as validation set and train the neural network classi-\\textbackslash{}nfier using 10-fold cross-validation on the remaining 80\\%\\textbackslash{}nto separate diseases into one of the five classes (non-\\textbackslash{}ninfectious, bacterial, fungal, parasitic and viral infections).\\textbackslash{}nTable 1 shows the performance achieved on the validation\\textbackslash{}nset.While the performance is less than predicting whether\\textbackslash{}na term refers to a disease, our classifier can distinguish\\textbackslash{}nbetween specific disease classes.\\textbackslash{}nWe manually analyzed a set of 20 false positive samples\\textbackslash{}nout of 38 which are not a subclass of Infectious disease in\\textbackslash{}nthe DO but are classified as infectious by our classifier (see\\textbackslash{}nTable 3). We found that 7 of these 20 cases can be sug-\\textbackslash{}ngested to be subclasses of the specific infectious disease\\textbackslash{}nthey have been classified with but do not have a subclass\\textbackslash{}nrelation asserted or inferred in DO. For example, the term\\textbackslash{}nsyphilitic meningitis (DOID:10073) is a disease that our\\textbackslash{}nmethod classify as a bacterial infectious disease but it is\\textbackslash{}nnot classified as infectious in the DO.\\textbackslash{}nMoreover, to test the strength of our method to distin-\\textbackslash{}nguish between disease classes, we further trained a neural\\textbackslash{}nnetwork model to distinguish between the 12 different\\textbackslash{}nsubclasses of Disease of anatomical entity (DOID:7), as\\textbackslash{}nwell as an additional class for diseases not classified as\\textbackslash{}nsubclasses of Disease of anatomical entity. We used the\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 7 of 13\\textbackslash{}nTable 2 Manually analyzed disease terms predicted as disease\\textbackslash{}nTerm Manual analysis result Explanation for the suggested diseases\\textbackslash{}nFACTO other -\\textbackslash{}nleucoencephalopathy other -\\textbackslash{}nAphthosis Disease A disease refers to a condition with repetitive mucosal ulcers\\textbackslash{}n{[}30, 31{]}.\\textbackslash{}nDesmoid other -\\textbackslash{}nmetapneumovirus other -\\textbackslash{}nTracheobronchomalacia Disease A rare condition with abnormal flaccidity of both the trachea\\textbackslash{}nand the bronchi which results in possibility of narrowing or\\textbackslash{}ncollapse of the airway {[}32\\textbackslash{}u009634{]}.\\textbackslash{}nRESLES Disease A rare condition characterized by transient lesions in the cen-\\textbackslash{}ntral part of the splenium of the corpus callosum (SCC), followed\\textbackslash{}nby complete reversibility on follow-up magnetic resonance\\textbackslash{}nimaging (MRI) after a variable period. It coincides with different\\textbackslash{}ndiseases {[}35, 36{]}.\\textbackslash{}nmal-absorption other -\\textbackslash{}nacroparesthesias other -\\textbackslash{}nlimb-shaking other -\\textbackslash{}npineocytomas Disease A rare disease that has an Orphanet ID: ORPHA:251912. It is\\textbackslash{}none of the pineal parenchymal tumors and is considered the\\textbackslash{}nleast aggressive one {[}37, 38{]}.\\textbackslash{}nhypomineralisation other -\\textbackslash{}nneurognathostomiasis Disease It is a severe form of human gnathostomiasis, DOID:11379,\\textbackslash{}nwhich can lead to disease and death, it involves the nervous\\textbackslash{}nsystem {[}39\\textbackslash{}u009641{]}.\\textbackslash{}nMetastasis other -\\textbackslash{}nmyelomatosis Disease A type of cancer that begins in plasma cells that produce anti-\\textbackslash{}nbodies. It could be one of the synonyms of multiple myeloma\\textbackslash{}nDOID:9538 {[}42, 43{]}.\\textbackslash{}nAMRF Disease An OMIM disease, OMIM:254900 {[}44{]}.\\textbackslash{}narthralgia other -\\textbackslash{}nfibrodentinoma Disease Fibrodentinoma is a benign odontogenic tumor that occurs\\textbackslash{}nin children and young adults. The disease name usually is\\textbackslash{}nrepresented as \\textbackslash{}u0093Ameloblastic Fibrodentinoma\\textbackslash{}u0094 {[}45, 46{]}.\\textbackslash{}ninfantile-ataxia other -\\textbackslash{}nknowlesi other -\\textbackslash{}nThe terms in bold represent the correctly validated terms (by a clinician) that classified as diseases terms using our method (in Diseases classification experiment).\\textbackslash{}nsame method to split the classes in training and test set as\\textbackslash{}nbefore. Results are shown in Table 1 and demonstrate that\\textbackslash{}nour method can also be useful to classify diseases in their\\textbackslash{}nanatomical sub-systems.\\textbackslash{}nWe manually analyzed a set of 20 false positive samples\\textbackslash{}nout of 127 which are not a subclass of Anatomical dis-\\textbackslash{}nease in the DO but are classified as being a subclass of a\\textbackslash{}nparticular anatomical system disease by our classifier (see\\textbackslash{}nTable 4). We found that 12 of the 20 false positives can be\\textbackslash{}nsuggested to be subclasses of the specific anatomical sys-\\textbackslash{}ntem disease they have been classified with but do not have\\textbackslash{}nsuch a subclass relation asserted or inferred in DO. For\\textbackslash{}nexample, we classify Narcolepsy (DOID:8986) as a Ner-\\textbackslash{}nvous system anatomical disease, and this may be added as\\textbackslash{}na new subclass axiom to DO.\\textbackslash{}nAs it is often inconvenient to train separate classifiers,\\textbackslash{}nwe also combined both tasks and trained a multi-class\\textbackslash{}nclassifier to classify disease classes either as infectious or\\textbackslash{}nanatomical, or as other disease. We evaluate the perfor-\\textbackslash{}nmance of this combined model (see Table 1), and our\\textbackslash{}nmachine learning system achieves an AUC up to 84\\% (see\\textbackslash{}nFigure 4). These results demonstrate it may be possible to\\textbackslash{}nidentify new subclasses, although the performance drops\\textbackslash{}nwhen we increase the complexity of the classification\\textbackslash{}nproblem by distinguishing between more subclasses.\\textbackslash{}nDiscussion\\textbackslash{}nWe developed a method to automatically expand ontolo-\\textbackslash{}ngies in the biomedical domain with new classes, syn-\\textbackslash{}nonyms, or axioms. We demonstrate the utility of our\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 8 of 13\\textbackslash{}nTable 3 Sample of manually analyzed disease terms predicted as infectious disease\\textbackslash{}nDisease terms Ontology class assigned\\textbackslash{}nby ANN\\textbackslash{}nManual analysis result Suggested additional\\textbackslash{}nclassification\\textbackslash{}nDOID Explanation\\textbackslash{}nPelizaeus-Merzbacher\\textbackslash{}ndisease\\textbackslash{}nViral infectious disease Non-infectious (inherited\\textbackslash{}ndisorder)\\textbackslash{}n- - -\\textbackslash{}nKaposi\\textbackslash{}u0092s sarcoma Viral infectious disease Viral infectious disease herpes simplex DOID:8566 The disease is caused by\\textbackslash{}nHuman herpesvirus 8\\textbackslash{}nwhich is Herpesviridae\\textbackslash{}ninfection.\\textbackslash{}nmaxillary sinusitis Bacterial infectious disease Bacterial infectious disease\\textbackslash{}n(usually start viral and\\textbackslash{}nprogress to either\\textbackslash{}nbacterial or fungal)\\textbackslash{}n- - It is an infection in the\\textbackslash{}nmaxillary sinuses which\\textbackslash{}ncould be due to different\\textbackslash{}netiology, one of them is\\textbackslash{}nbacterial {[}47{]}.\\textbackslash{}nkeratosis follicularis Bacterial infectious disease Non-infectious (genetic\\textbackslash{}ndisease)\\textbackslash{}n- - -\\textbackslash{}nchronic rheumatic\\textbackslash{}npericarditis\\textbackslash{}nViral infectious disease The condition is triggered\\textbackslash{}nby autoimmune reaction\\textbackslash{}nto infection, mainly group\\textbackslash{}nA streptococci.\\textbackslash{}n- - -\\textbackslash{}ngastroparesis Viral infectious disease In most cases the nerve is\\textbackslash{}ndamaged by diabetes or\\textbackslash{}nsurgery, however, a viral\\textbackslash{}ninfection might be a cause\\textbackslash{}n- - A condition in which the\\textbackslash{}nstomach suffers from\\textbackslash{}nparesis that affects the\\textbackslash{}nfood movement to the\\textbackslash{}nsmall intestine {[}48, 49{]}.\\textbackslash{}nosmotic diarrhea Bacterial infectious disease symptom - - -\\textbackslash{}nfamilial cold\\textbackslash{}nautoinflammatory\\textbackslash{}nsyndrome\\textbackslash{}nViral infectious disease Non-infectious (inherited\\textbackslash{}ndisease)\\textbackslash{}n- - -\\textbackslash{}nangular cheilitis Fungal infectious disease Etiology is controversial,\\textbackslash{}nmost commonly fungal or\\textbackslash{}nbacterial.\\textbackslash{}n- - Ambiguous.\\textbackslash{}nBinder syndrome Viral infectious disease Congenital disease - - -\\textbackslash{}nhypohidrosis Bacterial infectious disease Multi-causal - - -\\textbackslash{}nSjogren\\textbackslash{}u0092s syndrome Viral infectious disease autoimmune disease - - -\\textbackslash{}nmedian rhomboid\\textbackslash{}nglossitis\\textbackslash{}nFungal infectious disease Etiology is controversial,\\textbackslash{}nhowever it is considered\\textbackslash{}nas a variant of orallesion\\textbackslash{}nassociated with candida\\textbackslash{}ninfection {[}50{]}.\\textbackslash{}n- - Ambiguous.\\textbackslash{}nGoodpasture syndrome Viral infectious disease autoimmune disease - - -\\textbackslash{}nsyphilitic meningitis Bacterial infectious disease Bacterial infectious disease syphilis DOID:4166 Considering the same\\textbackslash{}nconcept of etiology, both\\textbackslash{}ndiseases are caused by\\textbackslash{}nbacterial infection\\textbackslash{}n(Treponema pallidum).\\textbackslash{}nacute diarrhea Viral infectious disease symptom - - -\\textbackslash{}nWHIM syndrome Bacterial infectious disease Congenital disease - - -\\textbackslash{}nerythrasma Fungal infectious disease Bacterial infection disease - - -\\textbackslash{}nchronic wasting disease Parasitic infectious disease Neurodegenerative\\textbackslash{}ndisorder\\textbackslash{}n- - -\\textbackslash{}nscarlet fever Bacterial infectious disease Bacterial infectious disease rheumatic fever DOID:1586 The disease is caused by\\textbackslash{}nGroup A bacteria of the\\textbackslash{}ngenus Streptococcus,\\textbackslash{}nsame causative agent for\\textbackslash{}nRheumatic fever.\\textbackslash{}nThe terms in bold represent the correctly validated terms (by a clinician) that classified as infectious diseases terms using our method (in Infectious disease classification\\textbackslash{}nexperiment).\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 9 of 13\\textbackslash{}nTable 4 Sample of manually analyzed disease terms classified as affecting particular anatomical systems\\textbackslash{}nDisease terms Ontology\\textbackslash{}nclass\\textbackslash{}nOntology\\textbackslash{}nclass\\textbackslash{}nassigned by\\textbackslash{}nANN\\textbackslash{}nManual analysis\\textbackslash{}nresult\\textbackslash{}nSuggested\\textbackslash{}nadditional\\textbackslash{}nclassification\\textbackslash{}nDOID Explanation\\textbackslash{}nTimothy\\textbackslash{}nsyndrome\\textbackslash{}ngenetic\\textbackslash{}ndisease\\textbackslash{}ncardiovascular\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nCannot specify\\textbackslash{}n(affect multiple\\textbackslash{}nparts)\\textbackslash{}n- - -\\textbackslash{}nFamilial periodic\\textbackslash{}nparalysis\\textbackslash{}ndisease of\\textbackslash{}nmetabolism\\textbackslash{}ncardiovascular\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nmusculoskeletal\\textbackslash{}nsystem disease\\textbackslash{}n- - -\\textbackslash{}nHyperprolactinemiadisease of\\textbackslash{}nmetabolism\\textbackslash{}nendocrine\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nendocrine system\\textbackslash{}ndisease\\textbackslash{}npituitary gland\\textbackslash{}ndisease\\textbackslash{}nDOID:53 The pituitary gland is\\textbackslash{}nthe endocrine gland\\textbackslash{}nresponsible for\\textbackslash{}nsecreting prolactin.\\textbackslash{}nAngiokeratoma\\textbackslash{}ncircumscriptum\\textbackslash{}ndisease of\\textbackslash{}ncellular\\textbackslash{}nproliferation\\textbackslash{}ngastrointestinal\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}ncardiovascular\\textbackslash{}nsystem disease\\textbackslash{}n- - -\\textbackslash{}nZollinger-\\textbackslash{}nEllison\\textbackslash{}nsyndrome\\textbackslash{}nsyndrome gastrointestinal\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}ngastrointestinal\\textbackslash{}nsystem disease\\textbackslash{}npeptic ulcer disease DOID:750 It is a disease that\\textbackslash{}naffects either\\textbackslash{}npancreas, duodenum,\\textbackslash{}nor both of them. Both\\textbackslash{}norgans are pats of the\\textbackslash{}nGIT system. The\\textbackslash{}ndisease pathology\\textbackslash{}nis mainly excessive\\textbackslash{}ngastrin secretion with\\textbackslash{}nsubsequent peptic\\textbackslash{}nulcers.\\textbackslash{}nPolycystic liver\\textbackslash{}ndisease\\textbackslash{}ngenetic\\textbackslash{}ndisease\\textbackslash{}ngastrointestinal\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}ngastrointestinal\\textbackslash{}nsystem disease\\textbackslash{}nliver disease DOID:409 It is a genetic disorder\\textbackslash{}nthat affects primarily\\textbackslash{}nthe liver.\\textbackslash{}nBilirubin\\textbackslash{}nmetabolic\\textbackslash{}ndisorder\\textbackslash{}ndisease of\\textbackslash{}nmetabolism\\textbackslash{}nhematopoietic\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nhematopoietic\\textbackslash{}nsystem disease\\textbackslash{}nkernicterus due to\\textbackslash{}nisoimmunization\\textbackslash{}nDOID:12043 Bilirubin disorder\\textbackslash{}ncould be a result of\\textbackslash{}nblood pathology,\\textbackslash{}nsame as for the\\textbackslash{}nmentioned\\textbackslash{}nclassification\\textbackslash{}nDOID:12043.\\textbackslash{}nAlpha\\textbackslash{}nthalassemia\\textbackslash{}ngenetic\\textbackslash{}ndisease\\textbackslash{}nhematopoietic\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nhematopoietic\\textbackslash{}nsystem disease\\textbackslash{}nhemoglobinopathy DOID:2860 The disease is mainly a\\textbackslash{}nhemoglobin\\textbackslash{}ndisorder with\\textbackslash{}nhematological\\textbackslash{}nphenotypes.\\textbackslash{}nKabuki syndrome syndrome immune sys-\\textbackslash{}ntem disease\\textbackslash{}nNot anatomical\\textbackslash{}n- multisystems\\textbackslash{}n- - -\\textbackslash{}nAmyloidosis disease of\\textbackslash{}nmetabolism\\textbackslash{}nimmune sys-\\textbackslash{}ntem disease\\textbackslash{}nNot anatomical -\\textbackslash{}nmultisystems\\textbackslash{}n- - -\\textbackslash{}nFatty liver disease disease of\\textbackslash{}nmetabolism\\textbackslash{}nmusculoskeletal\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}ngastrointestinal\\textbackslash{}nsystem disease\\textbackslash{}n- - -\\textbackslash{}nRenal-hepatic-\\textbackslash{}npancreatic\\textbackslash{}ndysplasia\\textbackslash{}nphysical\\textbackslash{}ndisorder\\textbackslash{}nmusculoskeletal\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nCannot specify\\textbackslash{}n(affect multiple\\textbackslash{}nparts)\\textbackslash{}n- - -\\textbackslash{}nRadioulnar syn-\\textbackslash{}nostosis\\textbackslash{}nphysical\\textbackslash{}ndisorder\\textbackslash{}nmusculoskeletal\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nmusculoskeletal\\textbackslash{}nsystem disease\\textbackslash{}nbone development\\textbackslash{}ndisease/Synostosis\\textbackslash{}nDOID:0080006/\\textbackslash{}nDOID:11971\\textbackslash{}nThere is already an\\textbackslash{}nentity in the DO for\\textbackslash{}nsynostosis under\\textbackslash{}nbone development\\textbackslash{}ndisease.\\textbackslash{}nHypophosphatasia genetic\\textbackslash{}ndisease\\textbackslash{}nmusculoskeletal\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nmusculoskeletal\\textbackslash{}nsystem disease\\textbackslash{}nbone remodeling\\textbackslash{}ndisease\\textbackslash{}nDOID:0080005 We could suggest\\textbackslash{}nan additional\\textbackslash{}nclassification based\\textbackslash{}non the main affected\\textbackslash{}nsystem. Our\\textbackslash{}nsuggestive\\textbackslash{}nclassification is\\textbackslash{}nmusculoskeletal since\\textbackslash{}nAlthubaiti et al. Journal of Biomedical Semantics            (2020) 11:1 Page 10 of 13\\textbackslash{}nTable 4 Sample of manually analyzed disease terms classified as affecting particular anatomical systems (Continued)\\textbackslash{}nthe disease is mainly\\textbackslash{}naffecting\\textbackslash{}nmineralization of\\textbackslash{}nthe bone with\\textbackslash{}nphenotypes similar to\\textbackslash{}nthose of Rickets\\textbackslash{}nDOID:10609.\\textbackslash{}nNarcolepsy disease of mental\\textbackslash{}nhealth\\textbackslash{}nnervous\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nnervous system\\textbackslash{}ndisease\\textbackslash{}n* * *\\textbackslash{}nAceruloplasminemia disease of metabolism nervous\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nnervous system\\textbackslash{}ndisease\\textbackslash{}nneurodegeneration\\textbackslash{}nwith brain iron\\textbackslash{}naccumulation\\textbackslash{}nDOID:0110734 The disease main\\textbackslash{}npathophysiology is\\textbackslash{}neither the absence\\textbackslash{}nor dysfunction of\\textbackslash{}nceruloplasmin with\\textbackslash{}nsubsequent iron\\textbackslash{}naccumulation in\\textbackslash{}nvarious organ, mainly\\textbackslash{}nthe brain.\\textbackslash{}nGlomangiomatosis disease of cellular pro-\\textbackslash{}nliferation\\textbackslash{}nnervous\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}ncardiovascular\\textbackslash{}nsystem disease\\textbackslash{}n- - -\\textbackslash{}nDeafness-dystonia-\\textbackslash{}noptic neuronopathy\\textbackslash{}nsyndrome\\textbackslash{}ndisease of metabolism nervous sys-\\textbackslash{}ntem disease\\textbackslash{}nnervous system\\textbackslash{}ndisease\\textbackslash{}nnervous system\\textbackslash{}ndisease; since it\\textbackslash{}ncovers many\\textbackslash{}nsubclasses to\\textbackslash{}nwhich we can map\\textbackslash{}nmany aspects of\\textbackslash{}nthis disease\\textbackslash{}nDOID:863 The disease\\textbackslash{}u0092s\\textbackslash{}nphenotypes reflect\\textbackslash{}nneurological affection\\textbackslash{}nofmultiple parts in the\\textbackslash{}nnervous system.\\textbackslash{}nTrophoblastic\\textbackslash{}nneoplasm\\textbackslash{}ndisease of cellular\\textbackslash{}nproliferation\\textbackslash{}nreproductive\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nreproductive\\textbackslash{}nsystem disease\\textbackslash{}nFemale\\textbackslash{}nreproductive organ\\textbackslash{}ncancer\\textbackslash{}nDOID:120 The term refers to the\\textbackslash{}ngroup of\\textbackslash{}nmalignant neoplasms\\textbackslash{}nthat consist of\\textbackslash{}nabnormal\\textbackslash{}nproliferation of\\textbackslash{}ntrophoblastic tissues\\textbackslash{}nsimilar to\\textbackslash{}nchoriocarcinoma\\textbackslash{}nDOID:3596 and\\textbackslash{}ngestational\\textbackslash{}ntrophoblastic\\textbackslash{}nneoplasia\\textbackslash{}nDOID:3590.\\textbackslash{}nCryptorchidism physical disorder reproductive\\textbackslash{}nsystem\\textbackslash{}ndisease\\textbackslash{}nreproductive\\textbackslash{}nsystem disease\\textbackslash{}ntesticular disease DOID:2519 The term refers to\\textbackslash{}nundescended testicle.\\textbackslash{}n*Nacrolepsy: is classified as a sleep disorde'\n\\item[text2] 'RESEARCH Open Access\\textbackslash{}nTemporal information extraction from\\textbackslash{}nmental health records to identify duration\\textbackslash{}nof untreated psychosis\\textbackslash{}nNatalia Viani1*, Joyce Kam1, Lucia Yin1, André Bittar1, Rina Dutta1,2, Rashmi Patel1,2, Robert Stewart1,2 and\\textbackslash{}nSumithra Velupillai1\\textbackslash{}nAbstract\\textbackslash{}nBackground: Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health,\\textbackslash{}nas longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about\\textbackslash{}nwhen psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic\\textbackslash{}nhealth records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information\\textbackslash{}nunderlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research.\\textbackslash{}nNatural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant\\textbackslash{}ninformation in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are\\textbackslash{}nlikely to include the information of interest. Next, temporal information extraction methods are needed to identify'\n\\item[text3] 'RESEARCH Open Access\\textbackslash{}nDisclosing Main authors and Organisations\\textbackslash{}ncollaborations in bioprinting through\\textbackslash{}nnetwork maps analysis\\textbackslash{}nLeonardo Azael García-García1* and Marisela Rodríguez-Salvador2\\textbackslash{}nAbstract\\textbackslash{}nBackground: Scientific activity for 3D bioprinting has increased over the past years focusing mainly on fully functional\\textbackslash{}nbiological constructs to overcome issues related to organ transplants. This research performs a scientometric analysis on\\textbackslash{}nbioprinting based on a competitive technology intelligence (CTI) cycle, which assesses scientific documents to establish\\textbackslash{}nthe publication rate of science and technology in terms of institutions, patents or journals. Although analyses of\\textbackslash{}npublications can be observed in the literature, the identification of the most influential authors and affiliations has not\\textbackslash{}nbeen addressed. This study involves the analysis of authors and affiliations, and their interactions in a global framework.\\textbackslash{}nWe use network collaboration maps and Betweenness Centrality (BC) to identify of the most prominent actors in\\textbackslash{}nbioprinting, enhancing the CTI analysis.\\textbackslash{}nResults: 2088 documents were retrieved from Scopus database from 2007 to 2017, disclosing an exponential growth\\textbackslash{}nwith an average publication increase of 17.5\\% per year. A threshold of five articles with ten or more cites was\\textbackslash{}nestablished for authors, while the same number of articles but cited five or more times was set for affiliations. The\\textbackslash{}nauthor with more publications was Atala A. (36 papers and a BC = 370.9), followed by Khademhosseini A. (30\\textbackslash{}ndocuments and a BC = 2104.7), and Mironov (30 documents and BC = 2754.9). In addition, a small correlation was\\textbackslash{}nobserved between the number of collaborations and the number of publications. Furthermore, 1760 institutions with a\\textbackslash{}nmedian of 10 publications were found, but only 20 within the established threshold. 30\\% of the 20 institutions had an\\textbackslash{}nexternal collaboration, and institutions located in and close to the life science cluster in Massachusetts showed a strong\\textbackslash{}ncooperation. The institution with more publications was the Harvard Medical School, 61 publications, followed by the\\textbackslash{}nBrigham and Women\\textbackslash{}u0092s hospital, 46 papers, and the Massachusetts Institute of Technology with 37 documents.\\textbackslash{}nConclusions: Network map analysis and BC allowed the identification of the most influential authors working on\\textbackslash{}nbioprinting and the collaboration between institutions was found limited. This analysis of authors and affiliations and\\textbackslash{}ntheir collaborations offer valuable information for the identification of potential associations for bioprinting researches\\textbackslash{}nand stakeholders.\\textbackslash{}nKeywords: Network map analysis, Betweenness centrality, Bioprinting, Text mining, Collaboration analysis,\\textbackslash{}nscientometrics, competitive technology intelligence\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\textbackslash{}nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\textbackslash{}nchanges were made. The images or other third party material in this article are included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\textbackslash{}npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\textbackslash{}nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\textbackslash{}ndata made available in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}n* Correspondence: L.A.Garcia-Garcia@sussex.ac.uk\\textbackslash{}n1University of Sussex, School of Engineering and Informatics, Falmer,\\textbackslash{}nBrighton, UK\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics\\textbackslash{}n           (2020) 11:3 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-0219-z\\textbackslash{}nBackground\\textbackslash{}nResearch articles are public documents that report scien-\\textbackslash{}ntific advancements to share knowledge and promote devel-\\textbackslash{}nopment in science. These documents contain fundamental\\textbackslash{}ninformation regarding not only to research but also to the\\textbackslash{}norganizations and authors involved. This data is of interest\\textbackslash{}nto identify leading organizations and to map scientific\\textbackslash{}ncollaborations.\\textbackslash{}nScientometric tools such as co-citation analysis, biblio-\\textbackslash{}ngraphic coupling, or co-author analysis can help to achieve\\textbackslash{}nthese goals. Co-citation analysis and bibliographic coupling\\textbackslash{}nare mainly used to measure the flow of information based\\textbackslash{}non the documents selected by authors, while co-author\\textbackslash{}nanalysis is more focused on the analysis of collaboration be-\\textbackslash{}ntween authors, taking into consideration the social aspect\\textbackslash{}nof the research collaboration. Furthermore, co-author ana-\\textbackslash{}nlysis has been proved to be useful to determine the multi\\textbackslash{}nand interdisciplinary of the institutions and their collabora-\\textbackslash{}ntions {[}1{]}. Co-author analysis requires information related to\\textbackslash{}nauthors\\textbackslash{}u0092 aliases, affiliations, publications, areas of research,\\textbackslash{}nand their collaborations. This information can be obtained\\textbackslash{}nfrom digital libraries (DL) aimed to create systems for the\\textbackslash{}nidentification of authors such as ORCID, which was created\\textbackslash{}nby non-profit organizations, or ResearcherID, Scopus,\\textbackslash{}nPubMED or Web of Science, which are companies that are\\textbackslash{}ndeveloping their unique identifiers for authors {[}2\\textbackslash{}u00964{]}. When\\textbackslash{}nevaluating advances in science and technology, names of\\textbackslash{}nauthors and affiliations become major indicators, as 1) their\\textbackslash{}nnumber of citations by peers correlates to their acknow-\\textbackslash{}nledgment as influential on their area of research {[}5{]} and 2)\\textbackslash{}ncontributes to determining the specific disciplines involved\\textbackslash{}nin the research {[}1{]}, both are important elements to nurture\\textbackslash{}nthe decision-making process. In this sense, Competitive\\textbackslash{}nIntelligence (CI) acquires a relevant role, through the defin-\\textbackslash{}nition, collection, analysis, and presentation of relevant infor-\\textbackslash{}nmation {[}6{]}. The CI process can be further enhanced by\\textbackslash{}nincorporating feedback form experts to validate the infor-\\textbackslash{}nmation obtained {[}7{]}. CI is fundamental to research and de-\\textbackslash{}nvelopment (R\\&D), including products or processes with\\textbackslash{}nradical novelty, such as bioprinting.\\textbackslash{}nBioprinting is an emerging technology, a variant of addi-\\textbackslash{}ntive manufacturing that involves the fabrication of 3D con-\\textbackslash{}nstructs for living tissues and organs {[}8, 9{]}. This discipline\\textbackslash{}nis growing at an accelerated pace, involving branches of\\textbackslash{}nknowledge such as biology and engineering. Bioprinting\\textbackslash{}nhas been developed to assist the needs of a fast-growing\\textbackslash{}npopulation. This technique has potential social and eco-\\textbackslash{}nnomic impacts {[}10, 11{]}, including a huge effect in organ\\textbackslash{}ntransplants, where one of the main objectives is the print-\\textbackslash{}ning of functional biological structures to help in the short-\\textbackslash{}nage of organs, thus overcoming long waiting lists and\\textbackslash{}nissues related to the transplanted organs such as rejection\\textbackslash{}n{[}10\\textbackslash{}u009612{]}. Although there have been significant signs of pro-\\textbackslash{}ngress in the past years, there are some areas of research to\\textbackslash{}nbe explored in this incipient technology {[}11{]}. Since acad-\\textbackslash{}nemy and industry have acknowledged that bioprinting will\\textbackslash{}nhave a significant impact on the health-care sector in the\\textbackslash{}nfollowing years, the identification of technology trends in\\textbackslash{}n3D bioprinting {[}13, 14{]}, including potential printing tech-\\textbackslash{}nniques {[}15{]}, becomes crucial to stay competitive and to\\textbackslash{}ndevelop new technologies in this field. With this aim,\\textbackslash{}nRodriguez-Salvador et al. {[}7{]} performed a patentometric\\textbackslash{}nand scientometric analysis in bioprinting to identify trends\\textbackslash{}nand to explore the knowledge landscape of this technology.\\textbackslash{}nIn addition, they also identified the most prolific institu-\\textbackslash{}ntions, being the MIT (113 publications) the number one,\\textbackslash{}nfollowed by Nanyang Technology University (103 publica-\\textbackslash{}ntions), and Tsinghua University (93 publications); They\\textbackslash{}nalso found that the three first countries with more publica-\\textbackslash{}ntions were USA with 1491, followed by China with 744,\\textbackslash{}nand Germany with 377 {[}7{]}. These analyses are mostly\\textbackslash{}nbased on the frequency of documents by affiliation and\\textbackslash{}ncountry, and no inclusion or exclusion terms were set. The\\textbackslash{}ninsights obtained can be enhanced with the identification\\textbackslash{}nof the leading scientists and their field of expertise, thus\\textbackslash{}ndistinguishing the principal areas of current research and\\textbackslash{}ndetermining potential opportunities for R\\&D.\\textbackslash{}nIn order to unveil scientific and technological trends, it is\\textbackslash{}nimportant to face big volumes of information using text\\textbackslash{}nmining. This activity can be applied to identify and extract\\textbackslash{}npotentially useful information from texts. It combines tools\\textbackslash{}nsuch as machine learning, artificial intelligence, and statis-\\textbackslash{}ntics to analyse large amounts of both structured and un-\\textbackslash{}nstructured data. The information obtained can contribute\\textbackslash{}nto understanding patterns in data by making use of tools\\textbackslash{}nsuch as text categorization, text clustering, information ex-\\textbackslash{}ntraction, among others {[}16{]}. Information retrieval, word\\textbackslash{}nfrequency analysis, word distribution, pattern recognition,\\textbackslash{}nand visualisation techniques are some of the most frequent\\textbackslash{}npractices {[}17{]}. As a conclusion, text mining adds important\\textbackslash{}nvalue to the pattern recognition by structuring the content\\textbackslash{}nof data from textual sources for research, data analysis,\\textbackslash{}nbusiness or competitive intelligence (CI) {[}17\\textbackslash{}u009619{]}.\\textbackslash{}nA fundamental topic for the CI is the determination of\\textbackslash{}nkey players, such as the main organizations and authors in-\\textbackslash{}nvolved in scientific advancements. Network analysis can be\\textbackslash{}nused to identify the collaboration in a visual pattern, where\\textbackslash{}neither the authors or affiliations are represented by nodes\\textbackslash{}nand their collaborations can be seen as the connection\\textbackslash{}namong them. Moreover, the nodes with common attributes\\textbackslash{}nof interest for the analysis can be grouped using clusteriza-\\textbackslash{}ntion. Clusterization allows to group components with simi-\\textbackslash{}nlar characteristics, such as research topics or techniques.\\textbackslash{}nWhen clustering collaborations, the closer the nodes in au-\\textbackslash{}nthors or affiliations network maps, the more similarities\\textbackslash{}nthey share {[}5, 20{]}. Furthermore, collaboration analysis can\\textbackslash{}nbe strengthened with the assessment of the Betweenness\\textbackslash{}nCentrality (BC) to determine the level of association of the\\textbackslash{}nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 2 of 13\\textbackslash{}nnodes according to their position in the network. A\\textbackslash{}nstraightforward measurement of the association level can\\textbackslash{}nbe the connectivity, but it fails to disclose the importance\\textbackslash{}nof a node. To overcome this, BC measure can be calculated\\textbackslash{}nto evaluate the importance of a node and its social inter-\\textbackslash{}naction in a network as this measure counts the number of\\textbackslash{}nregions in the map connected by each element, setting\\textbackslash{}ntheir importance in the flow of information {[}21, 22{]}.\\textbackslash{}nScientometric and patentometric techniques have been\\textbackslash{}nused recently to analyse the number publications per year,\\textbackslash{}nthe main authors, and organizations to determine the main\\textbackslash{}nadvancements in bioprinting (methods, materials, etc.)\\textbackslash{}n{[}23\\textbackslash{}u009625{]}. Scientometric and text mining can be used to de-\\textbackslash{}ntect the authors and affiliations with more publications\\textbackslash{}nand more influence in bioprinting. This information can\\textbackslash{}nbe an input for people looking for well-known experts in\\textbackslash{}nbioprinting or state-of-the-art developments in the field.\\textbackslash{}nTo achieve the main goal of this paper, a customised\\textbackslash{}nsearch query was used to gather documents from Scopus.\\textbackslash{}nThe query included keywords highly used in the most\\textbackslash{}ncited papers on bioprinting. Two network maps of collab-\\textbackslash{}norations, one for authors and one for affiliations, were\\textbackslash{}ngenerated and analysed. Further analyses were carried out\\textbackslash{}nto estimate the BC measurement, and the relationship be-\\textbackslash{}ntween number of publications and the number of collabo-\\textbackslash{}nrations. These parameters were used for the identification\\textbackslash{}nof the most prolific (those with more publications on this\\textbackslash{}ntopic) and important authors and institutions involved in\\textbackslash{}nthe publications of advances in bioprinting.\\textbackslash{}nThis analysis is the first attempt to undertake a quantita-\\textbackslash{}ntive analysis using a network analysis approach and the\\textbackslash{}ncalculation of centrality measurements to strengthen the\\textbackslash{}nCI methodologies. The findings enhance the perception of\\textbackslash{}nthe importance of collaborations among institutions for\\textbackslash{}nthe generation of high-quality scientific outcomes and for\\textbackslash{}nthe dissemination of the knowledge generated, helping\\textbackslash{}nboth researchers and stakeholders in the identification of\\textbackslash{}npotential opportunities for research and collaboration.\\textbackslash{}nMethods\\textbackslash{}nThis paper is focused on the network analysis of authors\\textbackslash{}nand institutions from scientific publications in bioprinting.\\textbackslash{}nThe analysis comprises both, a network analysis on the\\textbackslash{}ncollaboration among institutions and one that deals with\\textbackslash{}nthe collaboration among authors. The network maps were\\textbackslash{}ngenerated in Gephi, an open-source software for network\\textbackslash{}nanalysis {[}26\\textbackslash{}u009634{]}. Betweenness centrality was calculated\\textbackslash{}nfor both, authors and institutions\\textbackslash{}u0092 collaborations.\\textbackslash{}nThe adequate identification of specific keywords on the\\textbackslash{}ntopic of interest is a determining step in the search strat-\\textbackslash{}negy, as they contribute to the appropriate establishment of\\textbackslash{}nthe search queries. A preliminary search in Scopus using\\textbackslash{}nonly the term bioprinting with no period of time defined\\textbackslash{}nwas the first stage of this research. Scopus was selected for\\textbackslash{}nthe information retrieval as this is a major scientific data-\\textbackslash{}nbase that includes information from more than 20,000 sci-\\textbackslash{}nentific journals {[}35{]}. The ten most cited papers identified\\textbackslash{}nthrough this search were selected, as they have been\\textbackslash{}nacknowledged as referents for the topic. Table 1, García-\\textbackslash{}nGarcía{[}36{]}, shows the ten articles that formed the first set\\textbackslash{}nof documents. These papers were used to identify the key-\\textbackslash{}nwords to form the search queries. A text mining program\\textbackslash{}nwas specially coded to carry out the text-mining of these\\textbackslash{}npublications, thus determining the most relevant keywords\\textbackslash{}non the topic. With a broader range of terms and their syn-\\textbackslash{}nonyms we guarantee the inclusion of a wide range of pub-\\textbackslash{}nlications compared to searches performed using only the\\textbackslash{}nterm bioprinting. Three different types of keywords were\\textbackslash{}nsearched in the whole text of the papers, being 1) the most\\textbackslash{}nfrequent terms, 2) terms containing the word bio, and 3)\\textbackslash{}nthe collocations, which are the juxtapositions of two words\\textbackslash{}nwith a greater frequency. A cleaning of terms was accom-\\textbackslash{}nplished manually afterward to sort them out according to\\textbackslash{}nspecialized language of the subject. The identified key-\\textbackslash{}nwords were separated by subtopics (i. e. technology,\\textbackslash{}nprocess, and application) to form the search queries. A set\\textbackslash{}nof 23 searches were performed with the selected termin-\\textbackslash{}nology prior to the development of the definite query.\\textbackslash{}nThese searches were used to identify the correct grouping\\textbackslash{}nof terms and the exclusion terms.\\textbackslash{}nThe search query was formed using the keywords previ-\\textbackslash{}nously identified in combination with Boolean and proxim-\\textbackslash{}nity operators, and exclusion terms. For this stage, the\\textbackslash{}ndefinite search was carried out by defining the period of\\textbackslash{}ntime, from 1 January 2000 to 15 November 2017 (when\\textbackslash{}nthe information collection was concluded). The main\\textbackslash{}nquery is observed in the appendix A1. The collection activ-\\textbackslash{}nity involved the use of the query to search in title, abstract\\textbackslash{}nand keywords. A quick review of titles and abstracts of the\\textbackslash{}ndocuments found was carried out to discard those not\\textbackslash{}nrelated to bioprinting.\\textbackslash{}nThe bibliographic information of the documents identi-\\textbackslash{}nfied in Scopus was retrieved and exported in a CSV format\\textbackslash{}nto be cleaned and analysed. A cleaning process and the\\textbackslash{}ncomplete normalization of the data was carried out to\\textbackslash{}nstandardize authors and affiliations names. We performed\\textbackslash{}na manual name disambiguation for both authors and affili-\\textbackslash{}nations. The two authors analysed manually all the names\\textbackslash{}non each one of the publications gathered. Every time a\\textbackslash{}nsimilar name was observed, name disambiguation was car-\\textbackslash{}nried out by looking to the full name, affiliation, and e-mail.\\textbackslash{}nThe level of agreement on the disambiguation performed\\textbackslash{}nby the authors was measured using Cohen\\textbackslash{}u0092s kappa {[}37{]}.\\textbackslash{}nCo-author analysis was limited exclusively to the informa-\\textbackslash{}ntion of the publications gathered and we did not require\\textbackslash{}nfurther information from available DLs.\\textbackslash{}nThe analysis to identify the most influential authors\\textbackslash{}nand affiliations was carried out by setting a threshold for\\textbackslash{}nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 3 of 13\\textbackslash{}nTa\\textbackslash{}nb\\textbackslash{}nle\\textbackslash{}n1\\textbackslash{}nC\\textbackslash{}nom\\textbackslash{}npa\\textbackslash{}nris\\textbackslash{}non\\textbackslash{}nof\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nto\\textbackslash{}np\\textbackslash{}nte\\textbackslash{}nn\\textbackslash{}nci\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}npa\\textbackslash{}npe\\textbackslash{}nrs\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nSc\\textbackslash{}nop\\textbackslash{}nus\\textbackslash{}nob\\textbackslash{}nta\\textbackslash{}nin\\textbackslash{}ned\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nse\\textbackslash{}nar\\textbackslash{}nch\\textbackslash{}nof\\textbackslash{}n`b\\textbackslash{}nio\\textbackslash{}npr\\textbackslash{}nin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}u0092\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npe\\textbackslash{}nd\\textbackslash{}nse\\textbackslash{}nar\\textbackslash{}nch\\textbackslash{}nqu\\textbackslash{}ner\\textbackslash{}ny\\textbackslash{}nin\\textbackslash{}ntit\\textbackslash{}nle\\textbackslash{}ns,\\textbackslash{}nab\\textbackslash{}nst\\textbackslash{}nra\\textbackslash{}nct\\textbackslash{}ns,\\textbackslash{}nor\\textbackslash{}nke\\textbackslash{}nyw\\textbackslash{}nor\\textbackslash{}nds\\textbackslash{}nTo\\textbackslash{}np\\textbackslash{}nte\\textbackslash{}nn\\textbackslash{}nre\\textbackslash{}nsu\\textbackslash{}nlts\\textbackslash{}nus\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nke\\textbackslash{}nyw\\textbackslash{}nor\\textbackslash{}nd\\textbackslash{}nbi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}n{[}3\\textbackslash{}n6{]}\\textbackslash{}nTo\\textbackslash{}np\\textbackslash{}nte\\textbackslash{}nn\\textbackslash{}nar\\textbackslash{}ntic\\textbackslash{}nle\\textbackslash{}ns\\textbackslash{}nus\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npe\\textbackslash{}nd\\textbackslash{}nse\\textbackslash{}nar\\textbackslash{}nch\\textbackslash{}nst\\textbackslash{}nrin\\textbackslash{}ng\\textbackslash{}nTi\\textbackslash{}ntle\\textbackslash{}nA\\textbackslash{}nut\\textbackslash{}nho\\textbackslash{}nrs\\textbackslash{}nYe\\textbackslash{}nar\\textbackslash{}nSo\\textbackslash{}nur\\textbackslash{}nce\\textbackslash{}nC\\textbackslash{}nite\\textbackslash{}ns\\textbackslash{}nTi\\textbackslash{}ntle\\textbackslash{}nA\\textbackslash{}nut\\textbackslash{}nho\\textbackslash{}nr\\textbackslash{}nYe\\textbackslash{}nar\\textbackslash{}nSo\\textbackslash{}nur\\textbackslash{}nce\\textbackslash{}nC\\textbackslash{}nite\\textbackslash{}ns\\textbackslash{}n1\\textbackslash{}n3D\\textbackslash{}nbi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nof\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nor\\textbackslash{}nga\\textbackslash{}nns\\textbackslash{}n{[}3\\textbackslash{}n8{]}\\textbackslash{}n.\\textbackslash{}nM\\textbackslash{}nur\\textbackslash{}nph\\textbackslash{}ny,\\textbackslash{}nS.\\textbackslash{}nV.\\textbackslash{}n,\\textbackslash{}nA\\textbackslash{}nta\\textbackslash{}nla\\textbackslash{}n,A\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nN\\textbackslash{}nat\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nBi\\textbackslash{}not\\textbackslash{}nec\\textbackslash{}nhn\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}n32\\textbackslash{}n(8\\textbackslash{}n),\\textbackslash{}npp\\textbackslash{}n.7\\textbackslash{}n73\\textbackslash{}n\\textbackslash{}u00967\\textbackslash{}n85\\textbackslash{}n14\\textbackslash{}n98\\textbackslash{}n3D\\textbackslash{}nbi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nof\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}nes\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nor\\textbackslash{}nga\\textbackslash{}nns\\textbackslash{}n{[}3\\textbackslash{}n8{]}\\textbackslash{}n.\\textbackslash{}nM\\textbackslash{}nur\\textbackslash{}nph\\textbackslash{}ny\\textbackslash{}nS.\\textbackslash{}nV.\\textbackslash{}n,A\\textbackslash{}nta\\textbackslash{}nla\\textbackslash{}nA\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nN\\textbackslash{}nat\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nBi\\textbackslash{}not\\textbackslash{}nec\\textbackslash{}nhn\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}n32\\textbackslash{}n(8\\textbackslash{}n),\\textbackslash{}npp\\textbackslash{}n.\\textbackslash{}n77\\textbackslash{}n3\\textbackslash{}u0096\\textbackslash{}n78\\textbackslash{}n5.\\textbackslash{}n14\\textbackslash{}n98\\textbackslash{}n2\\textbackslash{}nSc\\textbackslash{}naf\\textbackslash{}nfo\\textbackslash{}nld\\textbackslash{}n-fr\\textbackslash{}nee\\textbackslash{}nva\\textbackslash{}nsc\\textbackslash{}nul\\textbackslash{}nar\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nen\\textbackslash{}ngi\\textbackslash{}nne\\textbackslash{}ner\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nus\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nbi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}n{[}3\\textbackslash{}n9{]}\\textbackslash{}n.\\textbackslash{}nN\\textbackslash{}nor\\textbackslash{}not\\textbackslash{}nte\\textbackslash{}n,C\\textbackslash{}n.,\\textbackslash{}nM\\textbackslash{}nar\\textbackslash{}nga\\textbackslash{}n,\\textbackslash{}nF.\\textbackslash{}nS.\\textbackslash{}n,N\\textbackslash{}nik\\textbackslash{}nla\\textbackslash{}nso\\textbackslash{}nn,\\textbackslash{}nL.\\textbackslash{}nE.\\textbackslash{}n,\\textbackslash{}nFo\\textbackslash{}nrg\\textbackslash{}nac\\textbackslash{}ns,\\textbackslash{}nG\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n09\\textbackslash{}nBi\\textbackslash{}nom\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n30\\textbackslash{}n(3\\textbackslash{}n0)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n59\\textbackslash{}n10\\textbackslash{}n\\textbackslash{}u00965\\textbackslash{}n91\\textbackslash{}n7\\textbackslash{}n60\\textbackslash{}n0\\textbackslash{}nM\\textbackslash{}nic\\textbackslash{}nro\\textbackslash{}nsc\\textbackslash{}nal\\textbackslash{}ne\\textbackslash{}nte\\textbackslash{}nch\\textbackslash{}nno\\textbackslash{}nlo\\textbackslash{}ngi\\textbackslash{}nes\\textbackslash{}nfo\\textbackslash{}nr\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nen\\textbackslash{}ngi\\textbackslash{}nne\\textbackslash{}ner\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nbi\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}n{[}4\\textbackslash{}n0{]}\\textbackslash{}n.\\textbackslash{}nKh\\textbackslash{}nad\\textbackslash{}nem\\textbackslash{}nho\\textbackslash{}nss\\textbackslash{}nei\\textbackslash{}nni\\textbackslash{}nA\\textbackslash{}n.,\\textbackslash{}nLa\\textbackslash{}nng\\textbackslash{}ner\\textbackslash{}nR.\\textbackslash{}n,B\\textbackslash{}nor\\textbackslash{}nen\\textbackslash{}nst\\textbackslash{}nei\\textbackslash{}nn\\textbackslash{}nJ.,\\textbackslash{}nVa\\textbackslash{}nca\\textbackslash{}nnt\\textbackslash{}niJ\\textbackslash{}n.P\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n06\\textbackslash{}nPr\\textbackslash{}noc\\textbackslash{}n.N\\textbackslash{}nat\\textbackslash{}nl.\\textbackslash{}nAc\\textbackslash{}nad\\textbackslash{}n.\\textbackslash{}nSc\\textbackslash{}ni.\\textbackslash{}nU\\textbackslash{}n.S\\textbackslash{}n.A\\textbackslash{}n.,\\textbackslash{}n10\\textbackslash{}n3\\textbackslash{}n(8\\textbackslash{}n),\\textbackslash{}npp\\textbackslash{}n.2\\textbackslash{}n48\\textbackslash{}n0\\textbackslash{}u0096\\textbackslash{}n24\\textbackslash{}n87\\textbackslash{}n.\\textbackslash{}n11\\textbackslash{}n63\\textbackslash{}n3\\textbackslash{}n3D\\textbackslash{}nbi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nof\\textbackslash{}nva\\textbackslash{}nsc\\textbackslash{}nul\\textbackslash{}nar\\textbackslash{}niz\\textbackslash{}ned\\textbackslash{}n,\\textbackslash{}nhe\\textbackslash{}nte\\textbackslash{}nro\\textbackslash{}nge\\textbackslash{}nne\\textbackslash{}nou\\textbackslash{}ns\\textbackslash{}nce\\textbackslash{}nll-\\textbackslash{}nla\\textbackslash{}nde\\textbackslash{}nn\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nns\\textbackslash{}ntr\\textbackslash{}nuc\\textbackslash{}nts\\textbackslash{}n{[}2\\textbackslash{}n4{]}\\textbackslash{}n.\\textbackslash{}nKo\\textbackslash{}nle\\textbackslash{}nsk\\textbackslash{}ny,\\textbackslash{}nD\\textbackslash{}n.B\\textbackslash{}n.,\\textbackslash{}nTr\\textbackslash{}nub\\textbackslash{}ny,\\textbackslash{}nR.\\textbackslash{}nL.\\textbackslash{}n,G\\textbackslash{}nla\\textbackslash{}ndm\\textbackslash{}nan\\textbackslash{}n,A\\textbackslash{}n.S\\textbackslash{}n.,\\textbackslash{}nH\\textbackslash{}nom\\textbackslash{}nan\\textbackslash{}n,K\\textbackslash{}n.A\\textbackslash{}n.,\\textbackslash{}nLe\\textbackslash{}nw\\textbackslash{}nis\\textbackslash{}n,J\\textbackslash{}n.A\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nAd\\textbackslash{}nva\\textbackslash{}nnc\\textbackslash{}ned\\textbackslash{}nM\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n26\\textbackslash{}n(1\\textbackslash{}n9)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n31\\textbackslash{}n24\\textbackslash{}n\\textbackslash{}u00963\\textbackslash{}n13\\textbackslash{}n0\\textbackslash{}n58\\textbackslash{}n8\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}ntr\\textbackslash{}nan\\textbackslash{}nsp\\textbackslash{}nla\\textbackslash{}nnt\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nof\\textbackslash{}na\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne-\\textbackslash{}nen\\textbackslash{}ngi\\textbackslash{}nne\\textbackslash{}ner\\textbackslash{}ned\\textbackslash{}nai\\textbackslash{}nrw\\textbackslash{}nay\\textbackslash{}n{[}4\\textbackslash{}n1{]}\\textbackslash{}n.\\textbackslash{}nM\\textbackslash{}nac\\textbackslash{}nch\\textbackslash{}nia\\textbackslash{}nrin\\textbackslash{}niP\\textbackslash{}n.,\\textbackslash{}nJu\\textbackslash{}nng\\textbackslash{}neb\\textbackslash{}nlu\\textbackslash{}nth\\textbackslash{}nP.\\textbackslash{}n,\\textbackslash{}nG\\textbackslash{}no\\textbackslash{}nT.\\textbackslash{}n,A\\textbackslash{}nsn\\textbackslash{}nag\\textbackslash{}nhi\\textbackslash{}nM\\textbackslash{}n.A\\textbackslash{}n.,\\textbackslash{}nRe\\textbackslash{}nes\\textbackslash{}nL.\\textbackslash{}nE.\\textbackslash{}n,\\textbackslash{}nC\\textbackslash{}nog\\textbackslash{}nan\\textbackslash{}nT.\\textbackslash{}nA\\textbackslash{}n.,\\textbackslash{}nD\\textbackslash{}nod\\textbackslash{}nso\\textbackslash{}nn\\textbackslash{}nA\\textbackslash{}n.,\\textbackslash{}nM\\textbackslash{}nar\\textbackslash{}nto\\textbackslash{}nre\\textbackslash{}nll\\textbackslash{}nJ.,\\textbackslash{}nBe\\textbackslash{}nlli\\textbackslash{}nni\\textbackslash{}nS.\\textbackslash{}n,P\\textbackslash{}nar\\textbackslash{}nni\\textbackslash{}ngo\\textbackslash{}ntt\\textbackslash{}no\\textbackslash{}nP.\\textbackslash{}nP.\\textbackslash{}n,\\textbackslash{}nD\\textbackslash{}nic\\textbackslash{}nki\\textbackslash{}nns\\textbackslash{}non\\textbackslash{}nS.\\textbackslash{}nC\\textbackslash{}n.,\\textbackslash{}nH\\textbackslash{}nol\\textbackslash{}nla\\textbackslash{}nnd\\textbackslash{}ner\\textbackslash{}nA\\textbackslash{}n.P\\textbackslash{}n.,\\textbackslash{}nM\\textbackslash{}nan\\textbackslash{}nte\\textbackslash{}nro\\textbackslash{}nS.\\textbackslash{}n,C\\textbackslash{}non\\textbackslash{}nco\\textbackslash{}nni\\textbackslash{}nM\\textbackslash{}n.T\\textbackslash{}n.,\\textbackslash{}nBi\\textbackslash{}nrc\\textbackslash{}nha\\textbackslash{}nll\\textbackslash{}nM\\textbackslash{}n.A\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n08\\textbackslash{}nTh\\textbackslash{}ne\\textbackslash{}nLa\\textbackslash{}nnc\\textbackslash{}net\\textbackslash{}n37\\textbackslash{}n2\\textbackslash{}n(9\\textbackslash{}n65\\textbackslash{}n5)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n20\\textbackslash{}n23\\textbackslash{}n\\textbackslash{}u00962\\textbackslash{}n03\\textbackslash{}n0.\\textbackslash{}n10\\textbackslash{}n14\\textbackslash{}n4\\textbackslash{}nPr\\textbackslash{}nin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}npr\\textbackslash{}not\\textbackslash{}not\\textbackslash{}nyp\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nof\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}nes\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nsc\\textbackslash{}naf\\textbackslash{}nfo\\textbackslash{}nld\\textbackslash{}ns\\textbackslash{}n{[}2\\textbackslash{}n3{]}\\textbackslash{}n.\\textbackslash{}nD\\textbackslash{}ner\\textbackslash{}nby\\textbackslash{}n,B\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n12\\textbackslash{}nSc\\textbackslash{}nie\\textbackslash{}nnc\\textbackslash{}ne\\textbackslash{}n33\\textbackslash{}n8\\textbackslash{}n(6\\textbackslash{}n10\\textbackslash{}n9)\\textbackslash{}n,\\textbackslash{}npp\\textbackslash{}n.9\\textbackslash{}n21\\textbackslash{}n\\textbackslash{}u00969\\textbackslash{}n26\\textbackslash{}n51\\textbackslash{}n0\\textbackslash{}nM\\textbackslash{}nec\\textbackslash{}nha\\textbackslash{}nni\\textbackslash{}nca\\textbackslash{}nlp\\textbackslash{}nro\\textbackslash{}npe\\textbackslash{}nrt\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nce\\textbackslash{}nll\\textbackslash{}ncu\\textbackslash{}nltu\\textbackslash{}nra\\textbackslash{}nlr\\textbackslash{}nes\\textbackslash{}npo\\textbackslash{}nns\\textbackslash{}ne\\textbackslash{}nof\\textbackslash{}npo\\textbackslash{}nly\\textbackslash{}nca\\textbackslash{}npr\\textbackslash{}nol\\textbackslash{}nac\\textbackslash{}nto\\textbackslash{}nne\\textbackslash{}nsc\\textbackslash{}naf\\textbackslash{}nfo\\textbackslash{}nld\\textbackslash{}ns\\textbackslash{}nde\\textbackslash{}nsi\\textbackslash{}ngn\\textbackslash{}ned\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nfa\\textbackslash{}nbr\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}ned\\textbackslash{}nvi\\textbackslash{}na\\textbackslash{}nfu\\textbackslash{}nse\\textbackslash{}nd\\textbackslash{}nde\\textbackslash{}npo\\textbackslash{}nsi\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nm\\textbackslash{}nod\\textbackslash{}nel\\textbackslash{}nlin\\textbackslash{}ng\\textbackslash{}n{[}4\\textbackslash{}n2{]}\\textbackslash{}n.\\textbackslash{}nH\\textbackslash{}nut\\textbackslash{}nm\\textbackslash{}nac\\textbackslash{}nhe\\textbackslash{}nr\\textbackslash{}nD\\textbackslash{}n.W\\textbackslash{}n.,\\textbackslash{}nSc\\textbackslash{}nha\\textbackslash{}nnt\\textbackslash{}nz\\textbackslash{}nT.\\textbackslash{}n,\\textbackslash{}nZe\\textbackslash{}nin\\textbackslash{}nI.,\\textbackslash{}nN\\textbackslash{}ng\\textbackslash{}nK.\\textbackslash{}nW\\textbackslash{}n.,\\textbackslash{}nTe\\textbackslash{}noh\\textbackslash{}nS.\\textbackslash{}nH\\textbackslash{}n.,\\textbackslash{}nTa\\textbackslash{}nn\\textbackslash{}nK.\\textbackslash{}nC\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n01\\textbackslash{}nJo\\textbackslash{}nur\\textbackslash{}nna\\textbackslash{}nlo\\textbackslash{}nf\\textbackslash{}nBi\\textbackslash{}nom\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nM\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}nRe\\textbackslash{}nse\\textbackslash{}nar\\textbackslash{}nch\\textbackslash{}n55\\textbackslash{}n(2\\textbackslash{}n),\\textbackslash{}npp\\textbackslash{}n.2\\textbackslash{}n03\\textbackslash{}n\\textbackslash{}u00962\\textbackslash{}n16\\textbackslash{}n.\\textbackslash{}n93\\textbackslash{}n9\\textbackslash{}n5\\textbackslash{}nA\\textbackslash{}ndd\\textbackslash{}niti\\textbackslash{}nve\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}nuf\\textbackslash{}nac\\textbackslash{}ntu\\textbackslash{}nrin\\textbackslash{}ng\\textbackslash{}nof\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}nes\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nor\\textbackslash{}nga\\textbackslash{}nns\\textbackslash{}n{[}4\\textbackslash{}n3{]}\\textbackslash{}n.\\textbackslash{}nM\\textbackslash{}nel\\textbackslash{}nch\\textbackslash{}nel\\textbackslash{}ns,\\textbackslash{}nF.\\textbackslash{}nP.\\textbackslash{}nW\\textbackslash{}n.,\\textbackslash{}nD\\textbackslash{}nom\\textbackslash{}nin\\textbackslash{}ngo\\textbackslash{}ns,\\textbackslash{}nM\\textbackslash{}n.A\\textbackslash{}n.N\\textbackslash{}n.,\\textbackslash{}nKl\\textbackslash{}nei\\textbackslash{}nn,\\textbackslash{}nT.\\textbackslash{}nJ.,\\textbackslash{}nBa\\textbackslash{}nrt\\textbackslash{}nol\\textbackslash{}no,\\textbackslash{}nP.\\textbackslash{}nJ.,\\textbackslash{}nH\\textbackslash{}nut\\textbackslash{}nm\\textbackslash{}nac\\textbackslash{}nhe\\textbackslash{}nr,\\textbackslash{}nD\\textbackslash{}n.W\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n12\\textbackslash{}nPr\\textbackslash{}nog\\textbackslash{}nre\\textbackslash{}nss\\textbackslash{}nin\\textbackslash{}nPo\\textbackslash{}nly\\textbackslash{}nm\\textbackslash{}ner\\textbackslash{}nSc\\textbackslash{}nie\\textbackslash{}nnc\\textbackslash{}ne\\textbackslash{}n37\\textbackslash{}n(8\\textbackslash{}n),\\textbackslash{}npp\\textbackslash{}n.\\textbackslash{}n10\\textbackslash{}n79\\textbackslash{}n\\textbackslash{}u00961\\textbackslash{}n10\\textbackslash{}n4\\textbackslash{}n49\\textbackslash{}n5\\textbackslash{}nSo\\textbackslash{}nlid\\textbackslash{}nfre\\textbackslash{}nef\\textbackslash{}nor\\textbackslash{}nm\\textbackslash{}nfa\\textbackslash{}nbr\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nof\\textbackslash{}nth\\textbackslash{}nre\\textbackslash{}ne-\\textbackslash{}ndi\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nsi\\textbackslash{}non\\textbackslash{}nal\\textbackslash{}nsc\\textbackslash{}naf\\textbackslash{}nfo\\textbackslash{}nld\\textbackslash{}ns\\textbackslash{}nfo\\textbackslash{}nr\\textbackslash{}nen\\textbackslash{}ngi\\textbackslash{}nne\\textbackslash{}ner\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nre\\textbackslash{}npl\\textbackslash{}nac\\textbackslash{}nem\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}nes\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nor\\textbackslash{}nga\\textbackslash{}nns\\textbackslash{}n{[}4\\textbackslash{}n4{]}\\textbackslash{}n.\\textbackslash{}nLe\\textbackslash{}non\\textbackslash{}ng\\textbackslash{}nK.\\textbackslash{}nF.\\textbackslash{}n,C\\textbackslash{}nhe\\textbackslash{}nah\\textbackslash{}nC\\textbackslash{}n.M\\textbackslash{}n.,\\textbackslash{}nC\\textbackslash{}nhu\\textbackslash{}na\\textbackslash{}nC\\textbackslash{}n.K\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n03\\textbackslash{}nBi\\textbackslash{}nom\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n24\\textbackslash{}n(1\\textbackslash{}n3)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n23\\textbackslash{}n63\\textbackslash{}n\\textbackslash{}u00962\\textbackslash{}n37\\textbackslash{}n8.\\textbackslash{}n73\\textbackslash{}n9\\textbackslash{}n6\\textbackslash{}n25\\textbackslash{}nth\\textbackslash{}nan\\textbackslash{}nni\\textbackslash{}nve\\textbackslash{}nrs\\textbackslash{}nar\\textbackslash{}ny\\textbackslash{}nar\\textbackslash{}ntic\\textbackslash{}nle\\textbackslash{}n:E\\textbackslash{}nng\\textbackslash{}nin\\textbackslash{}nee\\textbackslash{}nrin\\textbackslash{}ng\\textbackslash{}nhy\\textbackslash{}ndr\\textbackslash{}nog\\textbackslash{}nel\\textbackslash{}ns\\textbackslash{}nfo\\textbackslash{}nr\\textbackslash{}nbi\\textbackslash{}nof\\textbackslash{}nab\\textbackslash{}nric\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}n{[}4\\textbackslash{}n5{]}\\textbackslash{}n.\\textbackslash{}nM\\textbackslash{}nal\\textbackslash{}nda\\textbackslash{}n,J\\textbackslash{}n.,\\textbackslash{}nVi\\textbackslash{}nss\\textbackslash{}ner\\textbackslash{}n,J\\textbackslash{}n.,\\textbackslash{}nM\\textbackslash{}nel\\textbackslash{}nch\\textbackslash{}nel\\textbackslash{}ns,\\textbackslash{}nF.\\textbackslash{}nP.\\textbackslash{}n,G\\textbackslash{}nro\\textbackslash{}nll,\\textbackslash{}nJ.,\\textbackslash{}nH\\textbackslash{}nut\\textbackslash{}nm\\textbackslash{}nac\\textbackslash{}nhe\\textbackslash{}nr,\\textbackslash{}nD\\textbackslash{}n.W\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nAd\\textbackslash{}nva\\textbackslash{}nnc\\textbackslash{}ned\\textbackslash{}nM\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n25\\textbackslash{}n(3\\textbackslash{}n6)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n50\\textbackslash{}n11\\textbackslash{}n\\textbackslash{}u00965\\textbackslash{}n02\\textbackslash{}n8\\textbackslash{}n46\\textbackslash{}n5\\textbackslash{}nSt\\textbackslash{}nem\\textbackslash{}nce\\textbackslash{}nll-\\textbackslash{}nba\\textbackslash{}nse\\textbackslash{}nd\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nen\\textbackslash{}ngi\\textbackslash{}nne\\textbackslash{}ner\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nsi\\textbackslash{}nlk\\textbackslash{}nbi\\textbackslash{}nom\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n{[}4\\textbackslash{}n6{]}\\textbackslash{}n.\\textbackslash{}nW\\textbackslash{}nan\\textbackslash{}ng\\textbackslash{}nY.\\textbackslash{}n,K\\textbackslash{}nim\\textbackslash{}nH\\textbackslash{}n.-J\\textbackslash{}n.,\\textbackslash{}nVu\\textbackslash{}nnj\\textbackslash{}nak\\textbackslash{}n-\\textbackslash{}nN\\textbackslash{}nov\\textbackslash{}nak\\textbackslash{}nov\\textbackslash{}nic\\textbackslash{}nG\\textbackslash{}n.,\\textbackslash{}nKa\\textbackslash{}npl\\textbackslash{}nan\\textbackslash{}nD\\textbackslash{}n.L\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n06\\textbackslash{}nBi\\textbackslash{}nom\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n27\\textbackslash{}n(3\\textbackslash{}n6)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n60\\textbackslash{}n64\\textbackslash{}n\\textbackslash{}u00966\\textbackslash{}n08\\textbackslash{}n2.\\textbackslash{}n65\\textbackslash{}n7\\textbackslash{}n7\\textbackslash{}nA\\textbackslash{}n3D\\textbackslash{}nbi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nsy\\textbackslash{}nst\\textbackslash{}nem\\textbackslash{}nto\\textbackslash{}npr\\textbackslash{}nod\\textbackslash{}nuc\\textbackslash{}ne\\textbackslash{}nhu\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}n-s\\textbackslash{}nca\\textbackslash{}nle\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nns\\textbackslash{}ntr\\textbackslash{}nuc\\textbackslash{}nts\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nst\\textbackslash{}nru\\textbackslash{}nct\\textbackslash{}nur\\textbackslash{}nal\\textbackslash{}nin\\textbackslash{}nte\\textbackslash{}ngr\\textbackslash{}nity\\textbackslash{}n{[}4\\textbackslash{}n7{]}\\textbackslash{}n.\\textbackslash{}nKa\\textbackslash{}nng\\textbackslash{}n,H\\textbackslash{}n.-W\\textbackslash{}n.,\\textbackslash{}nLe\\textbackslash{}ne,\\textbackslash{}nS.\\textbackslash{}nJ.,\\textbackslash{}nKo\\textbackslash{}n,I\\textbackslash{}n.K\\textbackslash{}n.,\\textbackslash{}nYo\\textbackslash{}no,\\textbackslash{}nJ.J\\textbackslash{}n.,\\textbackslash{}nA\\textbackslash{}nta\\textbackslash{}nla\\textbackslash{}n,A\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nN\\textbackslash{}nat\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nBi\\textbackslash{}not\\textbackslash{}nec\\textbackslash{}nhn\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}n34\\textbackslash{}n(3\\textbackslash{}n),\\textbackslash{}npp\\textbackslash{}n.3\\textbackslash{}n12\\textbackslash{}n\\textbackslash{}u00963\\textbackslash{}n19\\textbackslash{}n46\\textbackslash{}n6\\textbackslash{}nSc\\textbackslash{}naf\\textbackslash{}nfo\\textbackslash{}nld\\textbackslash{}n-fr\\textbackslash{}nee\\textbackslash{}nva\\textbackslash{}nsc\\textbackslash{}nul\\textbackslash{}nar\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nen\\textbackslash{}ngi\\textbackslash{}nne\\textbackslash{}ner\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nus\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nbi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}n{[}3\\textbackslash{}n8{]}\\textbackslash{}n.\\textbackslash{}nN\\textbackslash{}nor\\textbackslash{}not\\textbackslash{}nte\\textbackslash{}nC\\textbackslash{}n.,\\textbackslash{}nM\\textbackslash{}nar\\textbackslash{}nga\\textbackslash{}nF.\\textbackslash{}nS.\\textbackslash{}n,\\textbackslash{}nN\\textbackslash{}nik\\textbackslash{}nla\\textbackslash{}nso\\textbackslash{}nn\\textbackslash{}nL.\\textbackslash{}nE.\\textbackslash{}n,F\\textbackslash{}nor\\textbackslash{}nga\\textbackslash{}ncs\\textbackslash{}nG\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n09\\textbackslash{}nBi\\textbackslash{}nom\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n30\\textbackslash{}n(3\\textbackslash{}n0)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n59\\textbackslash{}n10\\textbackslash{}n\\textbackslash{}u00965\\textbackslash{}n91\\textbackslash{}n7\\textbackslash{}n60\\textbackslash{}n0\\textbackslash{}n8\\textbackslash{}nPr\\textbackslash{}nin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nth\\textbackslash{}nre\\textbackslash{}ne-\\textbackslash{}ndi\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nsi\\textbackslash{}non\\textbackslash{}nal\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nan\\textbackslash{}nal\\textbackslash{}nog\\textbackslash{}nue\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nde\\textbackslash{}nce\\textbackslash{}nllu\\textbackslash{}nla\\textbackslash{}nriz\\textbackslash{}ned\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}nel\\textbackslash{}nlu\\textbackslash{}nla\\textbackslash{}nr\\textbackslash{}nm\\textbackslash{}nat\\textbackslash{}nrix\\textbackslash{}nbi\\textbackslash{}noi\\textbackslash{}nnk\\textbackslash{}n{[}4\\textbackslash{}n8{]}\\textbackslash{}n.\\textbackslash{}nPa\\textbackslash{}nti,\\textbackslash{}nF.\\textbackslash{}n,J\\textbackslash{}nan\\textbackslash{}ng,\\textbackslash{}nJ.,\\textbackslash{}nH\\textbackslash{}na,\\textbackslash{}nD\\textbackslash{}n.-H\\textbackslash{}n.,\\textbackslash{}nKi\\textbackslash{}nm\\textbackslash{}n,D\\textbackslash{}n.-H\\textbackslash{}n.,\\textbackslash{}nC\\textbackslash{}nho\\textbackslash{}n,D\\textbackslash{}n.-W\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nN\\textbackslash{}nat\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nCo\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nun\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nns\\textbackslash{}n53\\textbackslash{}n,9\\textbackslash{}n35\\textbackslash{}n41\\textbackslash{}n2\\textbackslash{}nO\\textbackslash{}nrg\\textbackslash{}nan\\textbackslash{}npr\\textbackslash{}nin\\textbackslash{}ntin\\textbackslash{}ng:\\textbackslash{}nTi\\textbackslash{}nss\\textbackslash{}nue\\textbackslash{}nsp\\textbackslash{}nhe\\textbackslash{}nro\\textbackslash{}nid\\textbackslash{}ns\\textbackslash{}nas\\textbackslash{}nbu\\textbackslash{}nild\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nbl\\textbackslash{}noc\\textbackslash{}nks\\textbackslash{}n{[}4\\textbackslash{}n9{]}\\textbackslash{}n.\\textbackslash{}nM\\textbackslash{}niro\\textbackslash{}nno\\textbackslash{}nv\\textbackslash{}nV.\\textbackslash{}n,V\\textbackslash{}nis\\textbackslash{}nco\\textbackslash{}nnt\\textbackslash{}niR\\textbackslash{}n.P\\textbackslash{}n.,\\textbackslash{}nKa\\textbackslash{}nsy\\textbackslash{}nan\\textbackslash{}nov\\textbackslash{}nV.\\textbackslash{}n,F\\textbackslash{}nor\\textbackslash{}nga\\textbackslash{}ncs\\textbackslash{}nG\\textbackslash{}n.,\\textbackslash{}nD\\textbackslash{}nra\\textbackslash{}nke\\textbackslash{}nC\\textbackslash{}n.J.\\textbackslash{}n,M\\textbackslash{}nar\\textbackslash{}nkw\\textbackslash{}nal\\textbackslash{}nd\\textbackslash{}nR.\\textbackslash{}nR.\\textbackslash{}n20\\textbackslash{}n09\\textbackslash{}nBi\\textbackslash{}nom\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n30\\textbackslash{}n(1\\textbackslash{}n2)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n21\\textbackslash{}n64\\textbackslash{}n\\textbackslash{}u00962\\textbackslash{}n17\\textbackslash{}n4.\\textbackslash{}n59\\textbackslash{}n4\\textbackslash{}n9\\textbackslash{}nTi\\textbackslash{}nss\\textbackslash{}nue\\textbackslash{}nen\\textbackslash{}ngi\\textbackslash{}nne\\textbackslash{}ner\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nby\\textbackslash{}nse\\textbackslash{}nlf-\\textbackslash{}nas\\textbackslash{}nse\\textbackslash{}nm\\textbackslash{}nbl\\textbackslash{}ny\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nbi\\textbackslash{}no-\\textbackslash{}npr\\textbackslash{}nin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nof\\textbackslash{}nliv\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nce\\textbackslash{}nlls\\textbackslash{}n{[}5\\textbackslash{}n0{]}\\textbackslash{}n.\\textbackslash{}nJa\\textbackslash{}nka\\textbackslash{}nb,\\textbackslash{}nK.\\textbackslash{}n,N\\textbackslash{}nor\\textbackslash{}not\\textbackslash{}nte\\textbackslash{}n,C\\textbackslash{}n.,\\textbackslash{}nM\\textbackslash{}nar\\textbackslash{}nga\\textbackslash{}n,F\\textbackslash{}n.,\\textbackslash{}nVu\\textbackslash{}nnj\\textbackslash{}nak\\textbackslash{}n-\\textbackslash{}nN\\textbackslash{}nov\\textbackslash{}nak\\textbackslash{}nov\\textbackslash{}nic\\textbackslash{}n,G\\textbackslash{}n.,\\textbackslash{}nFo\\textbackslash{}nrg\\textbackslash{}nac\\textbackslash{}ns,\\textbackslash{}nG\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n10\\textbackslash{}nBi\\textbackslash{}nof\\textbackslash{}nab\\textbackslash{}nric\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}n2\\textbackslash{}n(2\\textbackslash{}n),0\\textbackslash{}n22\\textbackslash{}n00\\textbackslash{}n1\\textbackslash{}n29\\textbackslash{}n0\\textbackslash{}n3D\\textbackslash{}nbi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nof\\textbackslash{}nva\\textbackslash{}nsc\\textbackslash{}nul\\textbackslash{}nar\\textbackslash{}niz\\textbackslash{}ned\\textbackslash{}n,\\textbackslash{}nhe\\textbackslash{}nte\\textbackslash{}nro\\textbackslash{}nge\\textbackslash{}nne\\textbackslash{}nou\\textbackslash{}ns\\textbackslash{}nce\\textbackslash{}nll-\\textbackslash{}nla\\textbackslash{}nde\\textbackslash{}nn\\textbackslash{}ntis\\textbackslash{}nsu\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nns\\textbackslash{}ntr\\textbackslash{}nuc\\textbackslash{}nts\\textbackslash{}n{[}2\\textbackslash{}n4{]}\\textbackslash{}n.\\textbackslash{}nKo\\textbackslash{}nle\\textbackslash{}nsk\\textbackslash{}ny\\textbackslash{}nD\\textbackslash{}n.B\\textbackslash{}n.,\\textbackslash{}nTr\\textbackslash{}nub\\textbackslash{}ny\\textbackslash{}nR.\\textbackslash{}nL.\\textbackslash{}n,\\textbackslash{}nG\\textbackslash{}nla\\textbackslash{}ndm\\textbackslash{}nan\\textbackslash{}nA\\textbackslash{}n.S\\textbackslash{}n.,\\textbackslash{}nBu\\textbackslash{}nsb\\textbackslash{}nee\\textbackslash{}nT.\\textbackslash{}nA\\textbackslash{}n.,\\textbackslash{}nH\\textbackslash{}nom\\textbackslash{}nan\\textbackslash{}nK.\\textbackslash{}nA\\textbackslash{}n.,\\textbackslash{}nLe\\textbackslash{}nw\\textbackslash{}nis\\textbackslash{}nJ.A\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nAd\\textbackslash{}nva\\textbackslash{}nnc\\textbackslash{}ned\\textbackslash{}nM\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}n26\\textbackslash{}n(1\\textbackslash{}n9)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n31\\textbackslash{}n24\\textbackslash{}n\\textbackslash{}u00963\\textbackslash{}n13\\textbackslash{}n0\\textbackslash{}n58\\textbackslash{}n8\\textbackslash{}n10\\textbackslash{}n3D\\textbackslash{}nBi\\textbackslash{}nop\\textbackslash{}nrin\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nof\\textbackslash{}nhe\\textbackslash{}nte\\textbackslash{}nro\\textbackslash{}nge\\textbackslash{}nne\\textbackslash{}nou\\textbackslash{}ns\\textbackslash{}nao\\textbackslash{}nrt\\textbackslash{}nic\\textbackslash{}nva\\textbackslash{}nlv\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nnd\\textbackslash{}nui\\textbackslash{}nts\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nal\\textbackslash{}ngi\\textbackslash{}nna\\textbackslash{}nte\\textbackslash{}n/g\\textbackslash{}nel\\textbackslash{}nat\\textbackslash{}nin\\textbackslash{}nhy\\textbackslash{}ndr\\textbackslash{}nog\\textbackslash{}nel\\textbackslash{}n{[}5\\textbackslash{}n1{]}\\textbackslash{}n.\\textbackslash{}nD\\textbackslash{}nua\\textbackslash{}nn,\\textbackslash{}nB.\\textbackslash{}n,H\\textbackslash{}noc\\textbackslash{}nka\\textbackslash{}nda\\textbackslash{}ny,\\textbackslash{}nL.\\textbackslash{}nA\\textbackslash{}n.,\\textbackslash{}nKa\\textbackslash{}nng\\textbackslash{}n,K\\textbackslash{}n.H\\textbackslash{}n.,\\textbackslash{}nBu\\textbackslash{}ntc\\textbackslash{}nhe\\textbackslash{}nr,\\textbackslash{}nJ.T\\textbackslash{}n.\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nJo\\textbackslash{}nur\\textbackslash{}nna\\textbackslash{}nlo\\textbackslash{}nfB\\textbackslash{}nio\\textbackslash{}nm\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nM\\textbackslash{}nat\\textbackslash{}ner\\textbackslash{}nia\\textbackslash{}nls\\textbackslash{}nRe\\textbackslash{}nse\\textbackslash{}nar\\textbackslash{}nch\\textbackslash{}n-\\textbackslash{}nPa\\textbackslash{}nrt\\textbackslash{}nA\\textbackslash{}n10\\textbackslash{}n1\\textbackslash{}nA\\textbackslash{}n(5\\textbackslash{}n),\\textbackslash{}npp\\textbackslash{}n.\\textbackslash{}n12\\textbackslash{}n55\\textbackslash{}n\\textbackslash{}u00961\\textbackslash{}n26\\textbackslash{}n4\\textbackslash{}n24\\textbackslash{}n4\\textbackslash{}nBi\\textbackslash{}nnd\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nco\\textbackslash{}nnd\\textbackslash{}nen\\textbackslash{}nsa\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nof\\textbackslash{}npl\\textbackslash{}nas\\textbackslash{}nm\\textbackslash{}nid\\textbackslash{}nD\\textbackslash{}nN\\textbackslash{}nA\\textbackslash{}non\\textbackslash{}nto\\textbackslash{}nfu\\textbackslash{}nnc\\textbackslash{}ntio\\textbackslash{}nna\\textbackslash{}nliz\\textbackslash{}ned\\textbackslash{}nca\\textbackslash{}nrb\\textbackslash{}non\\textbackslash{}nna\\textbackslash{}nno\\textbackslash{}ntu\\textbackslash{}nbe\\textbackslash{}ns:\\textbackslash{}nTo\\textbackslash{}nw\\textbackslash{}nar\\textbackslash{}nd\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nns\\textbackslash{}ntr\\textbackslash{}nuc\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nof\\textbackslash{}nna\\textbackslash{}nno\\textbackslash{}ntu\\textbackslash{}nbe\\textbackslash{}n-b\\textbackslash{}nas\\textbackslash{}ned\\textbackslash{}nge\\textbackslash{}nne\\textbackslash{}nde\\textbackslash{}nliv\\textbackslash{}ner\\textbackslash{}ny\\textbackslash{}nve\\textbackslash{}nct\\textbackslash{}nor\\textbackslash{}ns\\textbackslash{}n{[}5\\textbackslash{}n2{]}\\textbackslash{}n.\\textbackslash{}nSi\\textbackslash{}nng\\textbackslash{}nh\\textbackslash{}nR.\\textbackslash{}n,P\\textbackslash{}nan\\textbackslash{}nta\\textbackslash{}nro\\textbackslash{}ntt\\textbackslash{}no\\textbackslash{}nD\\textbackslash{}n.,\\textbackslash{}nM\\textbackslash{}ncC\\textbackslash{}nar\\textbackslash{}nth\\textbackslash{}ny\\textbackslash{}nD\\textbackslash{}n.,\\textbackslash{}nC\\textbackslash{}nha\\textbackslash{}nlo\\textbackslash{}nin\\textbackslash{}nO\\textbackslash{}n.,\\textbackslash{}nH\\textbackslash{}noe\\textbackslash{}nbe\\textbackslash{}nke\\textbackslash{}nJ.,\\textbackslash{}nPa\\textbackslash{}nrt\\textbackslash{}nid\\textbackslash{}nos\\textbackslash{}nC\\textbackslash{}n.D\\textbackslash{}n.,\\textbackslash{}nBr\\textbackslash{}nia\\textbackslash{}nnd\\textbackslash{}nJ.-\\textbackslash{}nP.\\textbackslash{}n,P\\textbackslash{}nra\\textbackslash{}nto\\textbackslash{}nM\\textbackslash{}n.,\\textbackslash{}nBi\\textbackslash{}nan\\textbackslash{}nco\\textbackslash{}nA\\textbackslash{}n.,\\textbackslash{}nKo\\textbackslash{}nst\\textbackslash{}nar\\textbackslash{}nel\\textbackslash{}nos\\textbackslash{}nK.\\textbackslash{}n20\\textbackslash{}n05\\textbackslash{}nJo\\textbackslash{}nur\\textbackslash{}nna\\textbackslash{}nlo\\textbackslash{}nft\\textbackslash{}nhe\\textbackslash{}nAm\\textbackslash{}ner\\textbackslash{}nic\\textbackslash{}nan\\textbackslash{}nCh\\textbackslash{}nem\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nSo\\textbackslash{}nci\\textbackslash{}net\\textbackslash{}ny\\textbackslash{}n12\\textbackslash{}n7\\textbackslash{}n(1\\textbackslash{}n2)\\textbackslash{}n,p\\textbackslash{}np.\\textbackslash{}n43\\textbackslash{}n88\\textbackslash{}n\\textbackslash{}u00964\\textbackslash{}n39\\textbackslash{}n6.\\textbackslash{}n57\\textbackslash{}n4\\textbackslash{}nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 4 of 13\\textbackslash{}neach analysis. A threshold of five documents cited at\\textbackslash{}nleast ten times was set for authors, while for the institu-\\textbackslash{}ntions we selected those with five documents cited at least\\textbackslash{}nfive times. These inclusion parameters were based on\\textbackslash{}nthe median number of cites for the whole set of docu-\\textbackslash{}nments, which was 10.33, and the median number of pub-\\textbackslash{}nlications per author was 5.76. For institutions, the mean\\textbackslash{}nnumber of publications was 10 with the same median of\\textbackslash{}ncites for the documents, 10.33; however, only three affili-\\textbackslash{}nations were within the threshold, hence the median for\\textbackslash{}ncitations and documents was reduced to 5 to include\\textbackslash{}nmore affiliations.\\textbackslash{}nTo identify the most prolific authors, the top ten au-\\textbackslash{}nthors with more frequency within the threshold defined\\textbackslash{}nwere selected and a Pearson correlation was computed to\\textbackslash{}ndetermine the relationship existing between the number\\textbackslash{}nof publications and the number of co-authors. The au-\\textbackslash{}nthors were clustered by the similarity of areas of research\\textbackslash{}nin the network maps and those with higher networking\\textbackslash{}nwere identified by BC calculation. The number of times a\\textbackslash{}nnode is taken as a connection for the shortest paths\\textbackslash{}nbetween two other nodes can be estimated through BC,\\textbackslash{}nwhich measures the node\\textbackslash{}u0092s connection to different groups\\textbackslash{}non a network map, being of a higher value the one who\\textbackslash{}nconnects more groups {[}53{]}. The BC is obtained using the\\textbackslash{}nequation {[}53{]}:\\textbackslash{}nCB vð Þ ¼\\textbackslash{}nX\\textbackslash{}nv?s?t\\textbackslash{}n?st vð Þ\\textbackslash{}n?st\\textbackslash{}nWhere ?st is the total of shortest paths from node s to\\textbackslash{}nnode t, and ?st (v) is the number of those paths that go\\textbackslash{}nthrough v.\\textbackslash{}nThe information within the threshold was imported\\textbackslash{}ninto VOSviewer, a software for data analysis and visual-\\textbackslash{}nisation {[}54, 55{]}, to perform the network map analysis.\\textbackslash{}nThe authors or institutions are represented by nodes or\\textbackslash{}nvertex in the network maps, and their connections are\\textbackslash{}nrepresented by links or edges; in this document, the\\textbackslash{}nterms are used indistinctly to refer to authors or affilia-\\textbackslash{}ntions and their connections. Two undirected network\\textbackslash{}nmaps were constructed from two matrices, representing\\textbackslash{}nonly the correlation and not causality. A matrix of au-\\textbackslash{}nthors and a matrix of affiliations were generated using\\textbackslash{}nthe visualisation of singularities (VOS) of the VOSviewer\\textbackslash{}nsoftware {[}55{]}. The clustering was performed in VOS-\\textbackslash{}nviewer, computed using the default Field Independent\\textbackslash{}nClustering Model (FICM) {[}55{]}. The statistical analysis to\\textbackslash{}ndetermine the BC of the nodes forming both maps was\\textbackslash{}nperformed in Gephi. The final step of the analysis was\\textbackslash{}nthe consultation with experts in 3D bioprinting to valid-\\textbackslash{}nate the results. Experts from UK and Asia were selected\\textbackslash{}nbased on their international presence and impact in the\\textbackslash{}nfield considering elements such as their number of cites,\\textbackslash{}npublications, projects, and their availability. Instead of\\textbackslash{}nproviding the experts with a list of authors found on the\\textbackslash{}nresults of this research, we asked them to provide a list\\textbackslash{}nof authors working on bioprinting according to their\\textbackslash{}nown criteria. This method was used to reduce bias in\\textbackslash{}ntheir selection, as they provided a list acknowledging\\textbackslash{}ntheir peers based on their own experience. Is it worth\\textbackslash{}nmentioning that the experts requested anonymity, there-\\textbackslash{}nfore, we can only provide professional details of three of\\textbackslash{}nthem at the time they were consulted. One of the ex-\\textbackslash{}nperts was affiliated to the School of materials at the Uni-\\textbackslash{}nversity of Manchester and had more than 10,000 Scopus\\textbackslash{}ncitations. A second expert was affiliated to the Faculty of\\textbackslash{}nEngineering at the University of Nottingham and had\\textbackslash{}nmore than 760 citations. A third one was affiliated to the\\textbackslash{}nSingapore Centre for 3D printing at Nanyang Technol-\\textbackslash{}nogy University with more than 14,000 citations.\\textbackslash{}nResults\\textbackslash{}nFrom the initial search, where the ten most cited articles in\\textbackslash{}nbioprinting from Scopus were considered, the top-cited\\textbackslash{}narticle is 3D bioprinting of tissue and organs {[}37{]}. This is a\\textbackslash{}nreview of different techniques used in bioprinting cited\\textbackslash{}n1498 times, as seen in Table 1; the second most cited art-\\textbackslash{}nicle is Scaffold-free vascular tissue engineering using bio-\\textbackslash{}nprinting {[}38{]}. This article describes a fully biological\\textbackslash{}nmethod to fabricate tubular vascular grafts and has been\\textbackslash{}ncited less than 50\\% of the first author, 600 times; the third\\textbackslash{}npaper, entitled 3D bioprinting of vascularized heterogeneous\\textbackslash{}ncell-laden tissue constructs {[}24{]} was cited 446 times and\\textbackslash{}ndescribes methods to generate vascularized tissue con-\\textbackslash{}nstructs. The second and third papers are focused on one of\\textbackslash{}nthe biggest challenges faced to print fully functional organs,\\textbackslash{}nthe fabrication of scaffold-free blood vessels with mechan-\\textbackslash{}nical properties close to the naturally grown vessels. Five of\\textbackslash{}nthe ten articles were published in journals related to mate-\\textbackslash{}nrials, four of them in general science journals (Nature Bio-\\textbackslash{}ntechnology, Nature Communications, and Science), and\\textbackslash{}none in the journal of Biofabrication, as can be observed in\\textbackslash{}nTable 1. The results of the searches in Scopus using only\\textbackslash{}nthe term bioprinting and those obtained using the search\\textbackslash{}nquery developed are listed and compared in Table 1. It can\\textbackslash{}nbe observed that the paper entitled 3D bioprinting of tissue\\textbackslash{}nand organs still in first place in both results. The second\\textbackslash{}npaper listed in the results from the search string is Micro-\\textbackslash{}nscale technologies for tissue engineering biology by Khadem-\\textbackslash{}nhosseini et al. {[}39{]} with 77\\% of the cites of the most first\\textbackslash{}npublication, 1163, followed by the paper Clinical trans-\\textbackslash{}nplantation of a tissue-engineered airway by Macchiarini\\textbackslash{}net al. {[}40{]}, published in the Lancet. Interestingly, the first\\textbackslash{}nthree papers are published in three major journals covering\\textbackslash{}nbiology and medical-related science, and six out of the 10\\textbackslash{}npapers on this search were published in journals related to\\textbackslash{}nmaterials and one in chemical engineering.\\textbackslash{}nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 5 of 13\\textbackslash{}nUsing the previously defined search query a total of\\textbackslash{}n2088 publications were found from 2007 to November\\textbackslash{}n15 of 2017 (when information collection activity ended).\\textbackslash{}nFigure 1 shows the number of publications per year,\\textbackslash{}nthere is a remarkable growth, where the highest number\\textbackslash{}nof publications is 339 for 2017.\\textbackslash{}nAfter the data selection and cleaning, a total of 228\\textbackslash{}nauthors were found within the threshold of at least 5 docu-\\textbackslash{}nments with 10 or more citations. 89 of the authors were\\textbackslash{}nfound with repeated surnames. A Cohen\\textbackslash{}u0092s kappa (?) of\\textbackslash{}n0.62 was obtained for the agreement on the author name\\textbackslash{}ndisambiguation. Values from 0.61 to 0.8 are ranked as\\textbackslash{}nGood {[}36{]}. A collaboration was observed in 93\\% of the au-\\textbackslash{}nthors, being 792 the total number of connections in the\\textbackslash{}nmap. Regarding affiliations, a total of 20 organizations fall\\textbackslash{}ninto the inclusion threshold, from which only 30\\% had an\\textbackslash{}nexternal collaboration.\\textbackslash{}nFrom the analysis, only ten authors were found to have\\textbackslash{}nmore than 18 documents, as seen in Fig. 2, where the\\textbackslash{}nnumber of documents and the number of co-authors for\\textbackslash{}neach of them are shown. The author with more documents\\textbackslash{}nfalling in the threshold defined is Atala A. with 36 docu-\\textbackslash{}nments and 13 co-authors. The following author, Khadem-\\textbackslash{}nhosseini A., had a total of 30 documents and more than\\textbackslash{}ndouble of collaborations for the first author, 27 co-authors,\\textbackslash{}nbeing the one with more connections. Mironov V. was in\\textbackslash{}nthird place with 30 documents, and 20 co-authors. A Pear-\\textbackslash{}nson correlation analysis was performed to determine the\\textbackslash{}nrelationship between documents and co-authors, and a\\textbackslash{}nweak positive correlation was observed, as the Pearson cor-\\textbackslash{}nrelation coefficient had a value r = 0.29 for the top ten au-\\textbackslash{}nthors, stating a lack of relationship between the number of\\textbackslash{}nco-authors and the number of publications.\\textbackslash{}nFigure 3 shows the network map of the author\\textbackslash{}u0092s col-\\textbackslash{}nlaboration, where the nodes\\textbackslash{}u0092 size is proportional to their\\textbackslash{}nBC value. The nodes representing the authors were\\textbackslash{}ngrouped in a total of 17 clusters. From the BC calcula-\\textbackslash{}ntion, the most prolific author, Atala A., was at the Wake\\textbackslash{}nForest Institute for Regenerative Medicine from the\\textbackslash{}nWake Forest University School of Medicine, Winston\\textbackslash{}nSalem, United States when the information was gathered\\textbackslash{}n(15 November 2017). According to Scopus altmetrics,\\textbackslash{}nthis author had an h-index of 89, 850 documents pub-\\textbackslash{}nlished, and a total of 17,376 citations working with 150\\textbackslash{}nco-authors at the time of the analysis (see Table 2). On\\textbackslash{}nthe other hand, under the inclusion terms, this author\\textbackslash{}npublished a total of 36 documents on the topic analysed,\\textbackslash{}nhaving 13 connections, 2851 citations, and a between-\\textbackslash{}nness centrality value of 370.9.\\textbackslash{}nThe second most prolific author found is Khademhos-\\textbackslash{}nseini Ali L.I., affiliated with the Brigham and Women\\textbackslash{}u0092s\\textbackslash{}nHospital, Department of Medicine, Boston, United States,\\textbackslash{}nwhen the data was collected. This author had an h-index\\textbackslash{}nof 88, a total of 645 papers, with a total of 16,704 citations\\textbackslash{}nand 150 co-authors, as stated in the Scopus altmetrics.\\textbackslash{}nConsidering the inclusion terms, this author accounted\\textbackslash{}nfor 30 documents, 27 connections, 3047 citations, and a\\textbackslash{}nbetweenness centrality of 2104.9 (see Table 2).\\textbackslash{}nFig. 1 Publications per year in bioprinting\\textbackslash{}nGarcía-García and Rodríguez-Salvador Journal of Biomedical Semantics            (2020) 11:3 Page 6 of 13\\textbackslash{}nThe third author was Mironov V., from the Laboratory\\textbackslash{}nfor Biotechnological Research \\textbackslash{}u00913D bioprinting solutions\\textbackslash{}u0092,\\textbackslash{}nMoscow, Russian federation. The Scopus altmetrics showed\\textbackslash{}nthat this author had 105 papers, 3231 cites, and an h-index\\textbackslash{}nof 31, co-authoring with 150 people. In this analysis, the au-\\textbackslash{}nthor accounted for 30 documents, 20 links, 1009 citations,\\textbackslash{}nand a betweenness centrality of 2754.9 (see Table 2).\\textbackslash{}nAccording to the network map and the BC calculations,\\textbackslash{}nMironov V. was stated as the author with a higher influ-\\textbackslash{}nence in the knowledge flow of the collaboration network, as\\textbackslash{}nit had the higher BC, followed by Khademhosseini A. While\\textbackslash{}nMironov was affiliated to a biotechnological research\\textbackslash{}nlaboratory, Atala and Khademhosseini were associated to\\textbackslash{}ntwo of the top ten research departments in bioprinting\\textbackslash{}nfound on this analysis.\\textbackslash{}nThe authors ranked by the experts were compared\\textbackslash{}nwith the most influential authors disclosed in this study,\\textbackslash{}nas it can be seen from Table 3.\\textbackslash{}nThree of the ten top authors in this scientometric study\\textbackslash{}nwere considered as influential by the experts consulted,\\textbackslash{}nAtala A., Mironov V., and Wei Sun; who were listed among\\textbackslash{}nthe top five authors in both cases. The top three authors\\textbackslash{}nfrom this study, who are listed in Table 3, are also the main\\textbackslash{}ninfluential authors with a higher BC (see Table 2).\\textbackslash{}nInstitutions\\textbackslash{}u0092 research efforts can be better estimated by\\textbackslash{}nthe number and the quality of their publications, therefore\\textbackslash{}nthe affiliations wit'\n\\item[text4] 'Keet Journal of Biomedical Semantics            (2020) 11:4 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00224-y\\textbackslash{}nDATABASE Open Access\\textbackslash{}nThe African wildlife ontology tutorial\\textbackslash{}nontologies\\textbackslash{}nC. Maria Keet\\textbackslash{}nAbstract\\textbackslash{}nBackground: Most tutorial ontologies focus on illustrating one aspect of ontology development, notably language\\textbackslash{}nfeatures and automated reasoners, but ignore ontology development factors, such as emergent modelling guidelines\\textbackslash{}nand ontological principles. Yet, novices replicate examples from the exercise they carry out. Not providing good\\textbackslash{}nexamples holistically causes the propagation of sub-optimal ontology development, which may negatively affect the\\textbackslash{}nquality of a real domain ontology.\\textbackslash{}nResults: We identified 22 requirements that a good tutorial ontology should satisfy regarding subject domain, logics\\textbackslash{}nand reasoning, and engineering aspects. We developed a set of ontologies about African Wildlife to serve as tutorial\\textbackslash{}nontologies. A majority of the requirements have been met with the set of African Wildlife Ontology tutorial ontologies,\\textbackslash{}nwhich are introduced in this paper. The African Wildlife Ontology is mature and has been used yearly in an ontology\\textbackslash{}nengineering course or tutorial since 2010 and is included in a recent ontology engineering textbook with relevant\\textbackslash{}nexamples and exercises.\\textbackslash{}nConclusion: The African Wildlife Ontology provides a wide range of options concerning examples and exercises for\\textbackslash{}nontology engineering well beyond illustrating just language features and automated reasoning. It assists in\\textbackslash{}ndemonstrating tasks concerning ontology quality, such as alignment to a foundational ontology and satisfying\\textbackslash{}ncompetency questions, versioning, and multilingual ontologies.\\textbackslash{}nKeywords: Ontology engineering, Tutorial ontology, African wildlife\\textbackslash{}nBackground\\textbackslash{}nThe amount of educational material to learn about ontolo-\\textbackslash{}ngies is increasing gradually, and there is material for dif-\\textbackslash{}nferent target audiences, including domain experts, applied\\textbackslash{}nphilosophers, computer scientists and software develop-\\textbackslash{}ners, and practitioners. These materials may include a tuto-\\textbackslash{}nrial ontology to illustrate concepts and principles and may\\textbackslash{}nbe used for exercises. There are no guidelines as to what\\textbackslash{}nsuch a tutorial ontology should be about and should look\\textbackslash{}nlike. The two most popular tutorial ontologies are about\\textbackslash{}nwine and pizza, which are not ideal introductory subject\\textbackslash{}ndomains on closer inspection (discussed below), they are\\textbackslash{}nlimited to OWLDL only, and are over 15 years old by now,\\textbackslash{}nCorrespondence: mkeet@cs.uct.ac.za\\textbackslash{}nDepartment of Computer Science, University of Cape Town, 18 University\\textbackslash{}nAvenue, Rondebosch, Cape Town, South Africa\\textbackslash{}nhence, neither taking into consideration the more recent\\textbackslash{}ninsights in ontology engineering nor the OWL 2 standard\\textbackslash{}nwith its additional features.\\textbackslash{}nConsidering subject domains in the most closely related\\textbackslash{}narea, conceptual modelling for relational databases, there\\textbackslash{}nis a small set of universes of discourse that are used in\\textbackslash{}nteaching throughout the plethora of teaching materials\\textbackslash{}navailable: the video/DVD/book rentals, employees at a\\textbackslash{}ncompany, a university, and, to a lesser extent, flights and\\textbackslash{}nairplanes. Neither of these topics for databases lend them-\\textbackslash{}nselves well for ontologies, for the simple reason that the\\textbackslash{}ntwo have different purposes. It does raise the question as\\textbackslash{}nto what would be suitable and, more fundamentally, what\\textbackslash{}nit is that makes some subject domain suitable but not\\textbackslash{}nanother, and, underlying that, what the requirements are\\textbackslash{}nfor an ontology to be a good tutorial ontology.\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\textbackslash{}ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\textbackslash{}nmade. The images or other third party material in this article are included in the article\\textbackslash{}u0092s Creative Commons licence, unless\\textbackslash{}nindicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}u0092s Creative Commons licence and your\\textbackslash{}nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\textbackslash{}nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\textbackslash{}nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\textbackslash{}navailable in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 2 of 11\\textbackslash{}nTable 1 Summary of main extant tutorial ontologies\\textbackslash{}nOntology Year Stated aim Content Language Modelling issues Automated\\textbackslash{}nreasoning\\textbackslash{}nCurrent OE (e.g.,\\textbackslash{}nODP, FO)\\textbackslash{}nwine 2001 novice \\textbackslash{}u0091all aspects\\textbackslash{}u0092\\textbackslash{}n(methodology,\\textbackslash{}nmodelling, reasoning)\\textbackslash{}nfor OE\\textbackslash{}nsomewhat generic,\\textbackslash{}nrepetitive, limited\\textbackslash{}nextensibility\\textbackslash{}nframes; the\\textbackslash{}nwine.owl in\\textbackslash{}nOWL DL is\\textbackslash{}nbased on it\\textbackslash{}nmultiple (e.g., class vs\\textbackslash{}ninstance, hasX)\\textbackslash{}nyes no\\textbackslash{}npizza 2004 Protégé user guide,\\textbackslash{}nalso illustrate OWL\\textbackslash{}nand reasoning\\textbackslash{}nsomewhat generic,\\textbackslash{}nrepetitive, limited\\textbackslash{}nextensibility\\textbackslash{}nOWL DL multiple (e.g., hasX,\\textbackslash{}nFO commitment, lack\\textbackslash{}nof domain \\& range)\\textbackslash{}nyes no\\textbackslash{}nuniversity 2005 illustrate OWL and\\textbackslash{}nreasoning\\textbackslash{}ngeneric, CDM (cf.\\textbackslash{}nontology) scope,\\textbackslash{}nvery small\\textbackslash{}n<OWL DL\\textbackslash{}n(ALCIN)\\textbackslash{}nmultiple (e.g., XorY,\\textbackslash{}nnaming of\\textbackslash{}nindividuals)\\textbackslash{}nyes no\\textbackslash{}nzooAnimals 2011 illustrate DL\\&OWL\\textbackslash{}nand some GoodOD\\textbackslash{}nmodelling guidelines\\textbackslash{}ngeneric, a lot of\\textbackslash{}ndetail, easily\\textbackslash{}nextensible\\textbackslash{}n<OWL 2 DL\\textbackslash{}n(SHO)\\textbackslash{}nfew yes partially\\textbackslash{}n(BioTopLite)\\textbackslash{}nfamily\\textbackslash{}nhistory\\textbackslash{}n2013 illustrate OWL 2 DL\\textbackslash{}nand reasoning\\textbackslash{}nspecific to author\\textbackslash{}u0092s\\textbackslash{}nfamily, not extensible\\textbackslash{}nOWL 2 DL multiple (e.g., hasX,\\textbackslash{}nFO commitment, lack\\textbackslash{}nof domain \\& range)\\textbackslash{}nerror no\\textbackslash{}nshirt 2015 illustrate the design of\\textbackslash{}nthe FMA\\textbackslash{}ngeneric, structure\\textbackslash{}nspecific to FMA,\\textbackslash{}nrepetitive, not\\textbackslash{}nextensible\\textbackslash{}n<OWL 2 DL\\textbackslash{}n(ALCIQ)\\textbackslash{}nfew (lack of domain\\textbackslash{}n\\& range)\\textbackslash{}nnone very limited\\textbackslash{}n(reference\\textbackslash{}nontology)\\textbackslash{}nAbbreviations: OE: ontology engineering; ODP: ontology design pattern; FO: foundational ontology; CDM: conceptual data model; FMA: foundational model of anatomy;\\textbackslash{}nOWL DL is SHOIN(D) and OWL 2 DL is SROIQ(D) in DL notation\\textbackslash{}nIn this paper, we will first analyse existing tutorial\\textbackslash{}nontologies and highlight some issues. We then proceed\\textbackslash{}nto formulate a preliminary, first, list of requirements that\\textbackslash{}ntutorial ontologies should meet. The African Wildlife\\textbackslash{}nOntology (AWO) tutorial ontologies are then introduced\\textbackslash{}nbriefly and held against the requirements. The scope of\\textbackslash{}nthis paper is thus to introduce the AWO tutorial ontolo-\\textbackslash{}ngies and to frame it in that context. Finally, we discuss and\\textbackslash{}nconclude.\\textbackslash{}nTutorial ontologies: issues and comparison\\textbackslash{}nThere are several tutorial ontologies, which are sum-\\textbackslash{}nmarised in Table 1 and discussed in this subsection; the\\textbackslash{}nnext subsection that summarises the problems.\\textbackslash{}nOf the six tutorial ontologies considered in detail, two\\textbackslash{}nare popular, being the Wine Ontology and the Pizza\\textbackslash{}nontology, since they are part of the W3C OWL guide\\textbackslash{}nand designed for the most popular ontology development\\textbackslash{}nenvironment (Protégé), respectively. They have various\\textbackslash{}nshortcomings as tutorial ontologies, however, especially\\textbackslash{}nconcerning modelling practices or styles (see also {[}1{]}).\\textbackslash{}nThe Wine ontology in its current form emanates from\\textbackslash{}nthe \\textbackslash{}u0093Ontology development 101\\textbackslash{}u0094 tutorial {[}2{]} with its\\textbackslash{}nframes and slots that was subsequently transferred into\\textbackslash{}nOWL1 and used in the \\textbackslash{}u0093OWL guide\\textbackslash{}u0094 {[}3{]}, which is a W3C\\textbackslash{}nRecommendation. While the guide contains some good\\textbackslash{}nsuggestions, such as that \\textbackslash{}u0093Synonyms for the same con-\\textbackslash{}ncept do not represent different classes\\textbackslash{}u0094 {[}2{]}, there are also\\textbackslash{}n1http://www.w3.org/TR/2003/PR-owl-guide-20031209/wine\\textbackslash{}nmodelling issues, notably that the ontology is replete with\\textbackslash{}nthe class-as-instance error that is promoted by the incor-\\textbackslash{}nrect statement in the tutorial \\textbackslash{}u0093Individual instances are the\\textbackslash{}nmost specific concepts represented in a knowledge base.\\textbackslash{}u0094\\textbackslash{}n{[}2{]} (e.g., TaylorPort as instance of Port and MalbecGrape\\textbackslash{}nas instance of Grape instead of as subclass of ), and the\\textbackslash{}nsub-optimal object property naming scheme of \\textbackslash{}u0091hasX\\textbackslash{}u0092 ,\\textbackslash{}nsuch as adjacentRegion between two Regions rather than\\textbackslash{}nthe reusable and generic adjacent. Further, it uses differ-\\textbackslash{}nent desiderata in the direct subclassing of wine such as the\\textbackslash{}nlikes of Bordeaux and Loire (region-based) and Chardon-\\textbackslash{}nnay and Cabernet Sauvignon (grape-based), and then\\textbackslash{}nthere are other criteria, like DessertWine (food pairing-\\textbackslash{}nbased grouping) and \\textbackslash{}u0091wine descriptor\\textbackslash{}u0092 ones (DryWine,\\textbackslash{}nRedWine, TableWine), This does make it interesting for\\textbackslash{}nshowing classification reasoning (except the undesirable\\textbackslash{}ndeduction that DryWine ? TableWine), but is not ideal\\textbackslash{}nfrom amodelling viewpoint. Further, from a tutorial view-\\textbackslash{}npoint: there are many repetitions, such as very many\\textbackslash{}nwineries, which distract from the principles, and it lacks\\textbackslash{}nannotations.\\textbackslash{}nThe Pizza ontology tutorial was created for the Pro-\\textbackslash{}ntégé user manual and OWL DL ontology language {[}4{]}.\\textbackslash{}nIt reflects the state of the art at that time, yet much has\\textbackslash{}nhappened over the past 15 years. For instance, there are\\textbackslash{}nnew OWL 2 features and there are foundational ontolo-\\textbackslash{}ngies that provide guidance for representing attributes (cf.\\textbackslash{}nPizza\\textbackslash{}u0092s ValuePartition). Pizza\\textbackslash{}u0092s DomainConcept throws a\\textbackslash{}nlearner straight into philosophical debates, which may not\\textbackslash{}nbe useful to start with, and, for all practical purposes,\\textbackslash{}nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 3 of 11\\textbackslash{}nduplicates owl:Thing. Like the Wine ontology, it has\\textbackslash{}nthe \\textbackslash{}u0091hasX\\textbackslash{}u0092 naming scheme for object properties, such as\\textbackslash{}nhasTopping, including the name of the class it is sup-\\textbackslash{}nposed to relate to, which is a quirk that is a combination\\textbackslash{}nof a workaround for not having qualified number restric-\\textbackslash{}ntions (anOWL 1 artefact) and of a sub-optimal ontological\\textbackslash{}nanalysis of the relation (in casu, of how the toppings really\\textbackslash{}nrelate to the rest of the pizza) that reduces chance of\\textbackslash{}nontology reuse and alignment. Also, this propagates into\\textbackslash{}nstudents\\textbackslash{}u0092 modelling approaches: students\\textbackslash{}u0092 ontologies in\\textbackslash{}nearlier instances of the author\\textbackslash{}u0092s course on ontology engi-\\textbackslash{}nneering included, among others, a sandwich ontology with\\textbackslash{}nhasFilling, an electrical circuit board ontology with hasIso-\\textbackslash{}nlator, furniture with hasHeadboard. Modelling issues\\textbackslash{}nare compounded by the statement \\textbackslash{}u0093we generally advise\\textbackslash{}nagainst doing {[}domain and range declarations{]}\\textbackslash{}u0094 in the\\textbackslash{}ntutorial documentation. When one aims to get novices\\textbackslash{}nto use Protégé and OWL so as not get too many error\\textbackslash{}nwith the automated reasoners, that might make sense,\\textbackslash{}nbut ontologically, fewer constraints make an ontology less\\textbackslash{}ngood because it admits more unintended models. Finally,\\textbackslash{}nit has repetitive content to show features, which may be\\textbackslash{}ndistracting, and, as with Wine, there is only one \\textbackslash{}u0091final\\textbackslash{}u0092\\textbackslash{}nontology, despite that multiple releases are common in\\textbackslash{}npractice.\\textbackslash{}nOther tutorial ontologies include Family History,\\textbackslash{}nzooAnimals, University, and Shirt. Family History {[}5{]} is\\textbackslash{}ndeveloped by the same group as Pizza and aims to teach\\textbackslash{}nabout advanced OWL 2 features and maximise the use\\textbackslash{}nof inferencing. Loading it in Protégé 5.2 results in three\\textbackslash{}npunning errors, since it mixes three object properties with\\textbackslash{}nannotation properties (affecting 32 axioms), which is dis-\\textbackslash{}nallowed, and trying to classify it without the three anno-\\textbackslash{}ntation properties returned an OutOfMemoryError (on a\\textbackslash{}nMacBookPro, 2.6 GHz and 8GB of memory), which is not\\textbackslash{}nideal to start a tutorial with. Concerning modelling issues,\\textbackslash{}nParentOfRobert illustrates one can use individuals in class\\textbackslash{}nexpressions, but just that the language allows it, does not\\textbackslash{}nmean it is ontologically a good idea that must be taught.\\textbackslash{}nIt also has the \\textbackslash{}u0091hasX\\textbackslash{}u0092 semantic weakness, very few anno-\\textbackslash{}ntations, DomainEntity being subsumed by owl:Thing,\\textbackslash{}nand multiple data properties. In contrast to Pizza and\\textbackslash{}nWine, all the declared instances are instances and the\\textbackslash{}nontology has different versions as one goes along in the\\textbackslash{}nchapters. It has some subject domain aspects descending\\textbackslash{}ninto politics, which would render it unsuitable for teach-\\textbackslash{}ning in several countries, such as stating that Sex? Female\\textbackslash{}nunionsq Male (enforcing a gender binary) and that Person \\textbackslash{}004\\textbackslash{}n? 2 hasParent.Person (multiple constructions are possible\\textbackslash{}nbiologically, societally, and legally).\\textbackslash{}nThe remaining tutorial ontologies have been developed\\textbackslash{}nby different \\textbackslash{}u0091schools\\textbackslash{}u0092 of views on ontology engineering\\textbackslash{}n(OE), which is readily apparent in their scope and con-\\textbackslash{}ntent. The zooAnimals tutorial ontology {[}6{]} comes closest\\textbackslash{}nto our aims for a versatile tutorial ontology, demonstrat-\\textbackslash{}ning multiple OWL features, avoiding modelling issues\\textbackslash{}nsuch as class vs instance, and it is informed by a top-\\textbackslash{}ndomain ontology (BioTop) as well as deep philosophical\\textbackslash{}nnotions such as dispositions. It puts them all together\\textbackslash{}ninto one ontology instead of gradual extensions, how-\\textbackslash{}never, which is off-putting at a novice stage. One may\\textbackslash{}nquibble about some of the content, such as simplifica-\\textbackslash{}ntions that Plant ? ?hasProperPart.Chloroplast (notably,\\textbackslash{}nsome parasitic plants and all myco-heterotrophic plants\\textbackslash{}ndo not have chloroplasts) and there are unintended unde-\\textbackslash{}nsirable deductions\\textbackslash{}u0097i.e., logically implied, but incorrect\\textbackslash{}nontologically\\textbackslash{}u0097such as marineAnimal \\textbackslash{}004 Omnivore since\\textbackslash{}nnot all such animals are omnivores. Any simplified \\textbackslash{}u0091com-\\textbackslash{}nmon generic subject domain\\textbackslash{}u0092 is likely to have some short-\\textbackslash{}ncuts that are not 100\\% scientifically accurate, and it may be\\textbackslash{}na fine line between tutorial approximation and modelling\\textbackslash{}nmistake.\\textbackslash{}nThe University ontology focuses on illustrating OWL\\textbackslash{}nfeatures and automated reasoning, rather than modelling.\\textbackslash{}nFor instance, it has AcademicStaff with sibling NonAca-\\textbackslash{}ndemicStaff where a \\textbackslash{}u0093non-X\\textbackslash{}u0094 complement class is sub-\\textbackslash{}noptimal, especially when there is a term for it. The repre-\\textbackslash{}nsentation of Student \\textbackslash{}004 Person is an advanced modelling\\textbackslash{}naspect that can be improved upon with a separate branch\\textbackslash{}nfor roles played by an object. The Computer Science\\textbackslash{}nOntology was based on the University Ontology tuto-\\textbackslash{}nrial and contains artificial classes, like unions of classes\\textbackslash{}n(ProfessorinHCIorAI) and underspecified or incorrect indi-\\textbackslash{}nviduals like AI and HCI (e.g., some course instance would\\textbackslash{}nbe CS\\_AI-sem1-2018 instead).\\textbackslash{}nThe Shirt ontology is a tutorial ontology to explain the\\textbackslash{}nstructure and organisation of the Foundational Model of\\textbackslash{}nAnatomy in a simpler way2 and therefore does not have\\textbackslash{}nthe hasX naming scheme for object properties, it has no\\textbackslash{}ndata properties and no instances. It has many annotations\\textbackslash{}nwith explanations of the entities. There are no inferences.\\textbackslash{}nRegarding suitability of the subject domains of the\\textbackslash{}nontologies assessed, they are mixed. Wine misses many\\textbackslash{}nwine-producing regions in the Americas (e.g., Chile), in\\textbackslash{}nEurope (e.g. Spain, Bulgaria), and elsewhere (e.g., South\\textbackslash{}nAfrica) and Pizza lacks varieties beyond Italian and Amer-\\textbackslash{}nican ones, and both are served regularly in a relatively\\textbackslash{}nsmall part of the world, therewith reducing their appeal\\textbackslash{}ninternationally. Family history and a university as subject\\textbackslash{}ndomains veer too easily into the area of database design for\\textbackslash{}na single application, rather than application-independent\\textbackslash{}ngeneric knowledge for an ontology. Shirts and zoo animals\\textbackslash{}ndo not have these shortcomings.\\textbackslash{}nFinally, more or less related textbooks were consid-\\textbackslash{}nered {[}7\\textbackslash{}u009611{]}. Only the \\textbackslash{}u0093Semantic Web for the working\\textbackslash{}n2http://xiphoid.biostr.washington.edu/fma/shirt\\_ontology/shirt\\_ontology\\_1.\\textbackslash{}nphp\\textbackslash{}nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 4 of 11\\textbackslash{}nOntologist\\textbackslash{}u0094 (2nd ed.) has sample files for the book\\textbackslash{}u0092s many\\textbackslash{}nsmall examples3 with two reoccurring subject domains,\\textbackslash{}nbeing English literature and products.\\textbackslash{}nProblems to address\\textbackslash{}nThe previous section described several problems with\\textbackslash{}nexisting tutorial ontologies. Notably, the recurring short-\\textbackslash{}ncomings are that\\textbackslash{}ni) good modelling practices are mostly ignored in\\textbackslash{}nfavour of demonstrating language features,\\textbackslash{}nautomated reasoning, and tools\\textbackslash{}nii) when good modelling practices and at least some\\textbackslash{}nrecent ontology engineering advances are included, it\\textbackslash{}nfalls short on language features and gradual\\textbackslash{}nextensions.\\textbackslash{}nThis has a negative effect on learning about ontology\\textbackslash{}ndevelopment, for tutorial ontology practices are nonethe-\\textbackslash{}nless seen by students as so-called \\textbackslash{}u0091model answers\\textbackslash{}u0092 even if\\textbackslash{}nit were not intended to have that function.\\textbackslash{}nThe ontology survey does not reveal what may be the\\textbackslash{}ncharacteristics of a good tutorial ontology and, to the best\\textbackslash{}nof our knowledge, there is no such list of comprehen-\\textbackslash{}nsive criteria for tutorial ontologies specifically. Schober\\textbackslash{}net al. {[}6{]} propose a partial list with seven high-level\\textbackslash{}ncontent requirements indeed, such as a common sense\\textbackslash{}nknowledge subject domain that lends itself well to demon-\\textbackslash{}nstrate the \\textbackslash{}u0093classic\\textbackslash{}u0094 modelling challenges, but it omits the\\textbackslash{}nessential components of logic, reasoning, and engineering\\textbackslash{}nrequirements. Scoping it more broadly, one could con-\\textbackslash{}nsider modelling guidelines and automated checkers for\\textbackslash{}nproduction level ontologies, such as {[}12\\textbackslash{}u009615{]}. They can\\textbackslash{}ninform the development of tutorial ontologies, in partic-\\textbackslash{}nular to avoid such issues as the class vs. instance error in\\textbackslash{}nthe provided sample ontology, but that is different from\\textbackslash{}neducating students about the foundations and reasons for\\textbackslash{}nsuch guidelines starting from a basic level of modelling\\textbackslash{}nto more advanced issues. For instance, disjointness and\\textbackslash{}ncovering constraints among subclasses of a parent class is\\textbackslash{}nindeed desirable together with coherent criteria to declare\\textbackslash{}na taxonomy {[}15{]}, but that does not let students observe or\\textbackslash{}nexperience mistakes, i.e., learn what is suboptimal or does\\textbackslash{}nnot work and why. A tutorial ontology also would have\\textbackslash{}nto be able to accommodate common pitfalls and grad-\\textbackslash{}nual quality improvements, among other things, which are\\textbackslash{}nnot covered by the general guidelines. Also, general guide-\\textbackslash{}nlines tend to follow one commitment over another\\textbackslash{}u0097e.g.,\\textbackslash{}nthe GoodOD guidelines favour a realist approach with\\textbackslash{}nthe BFO foundational ontology\\textbackslash{}u0097but for teaching OE in\\textbackslash{}ngeneral, students need learn to be cognisant of multiple\\textbackslash{}npossible commitments, their consequences when choos-\\textbackslash{}ning one or the other, and have at least one practical\\textbackslash{}n3http://www.workingontologist.org/Examples.zip; Last accessed: 26-11-2018.\\textbackslash{}nexample of such a difference to illustrate it, which general\\textbackslash{}nguidelines do not provide.\\textbackslash{}nPotential benefits of the African wildlife ontology tutorial\\textbackslash{}nontologies\\textbackslash{}nIn order to address these problems, we introduce the\\textbackslash{}nAfrican Wildlife Ontology (AWO). The AWO has been\\textbackslash{}ndeveloped and extended over 8 years. It meets a range of\\textbackslash{}ndifferent tutorial ontology requirements, notably regard-\\textbackslash{}ning subject domain, use of language features and auto-\\textbackslash{}nmated reasoning, and its link with foundational ontologies\\textbackslash{}non the one hand and engineering on the other. It aims to\\textbackslash{}ntake a principled approach to tutorial ontology develop-\\textbackslash{}nment, which thereby not only may assist a learner, but,\\textbackslash{}nmoreover from a scientific viewpoint, it might serve as a\\textbackslash{}nstarting point for tutorial ontology creation or improve-\\textbackslash{}nment more broadly, and therewith in the future contribute\\textbackslash{}nto an experimental analysis of tutorial ontology qual-\\textbackslash{}nity. This could benefit educational material for ontology\\textbackslash{}ndevelopment.\\textbackslash{}nAlso, educationally, there is some benefit to \\textbackslash{}u0091reusing\\textbackslash{}u0092\\textbackslash{}nthe same ontology to illustrate a range of aspects, rather\\textbackslash{}nthan introducing many small ad hoc examples, for then\\textbackslash{}nlater in a course, it makes it easier for the learners to see\\textbackslash{}nthe advances they have made. This is also illustrated with\\textbackslash{}noffering multiple versions of the ontology, which clearly\\textbackslash{}nindicate different types of increments.\\textbackslash{}nFinally, the AWO can be used on its own or together\\textbackslash{}nwith the textbook \\textbackslash{}u0093An Introduction to Ontology Engineer-\\textbackslash{}ning\\textbackslash{}u0094 {[}16{]}, which contains examples, tasks and exercises\\textbackslash{}nwith the AWO.\\textbackslash{}nConstruction and content\\textbackslash{}nThe construction of the AWO tutorial ontologies has gone\\textbackslash{}nthrough an iterative development process since 2010. This\\textbackslash{}ninvolved various extensions and improvements by design,\\textbackslash{}nmainly to address the increasing amount of requirements\\textbackslash{}nto meet, and maintenance issues, such as resolving link\\textbackslash{}nrot of an imported ontology. Rather than describing the\\textbackslash{}nprocess of the iterative development cycles, we present\\textbackslash{}nhere a \\textbackslash{}u0091digest\\textbackslash{}u0092 version of it. First, a set of tutorial ontol-\\textbackslash{}nogy requirements are presented together, then a brief\\textbackslash{}noverview of the AWO content is described, and subse-\\textbackslash{}nquently we turn to which of these requirements are met\\textbackslash{}nby the AWO.\\textbackslash{}nOE tutorial ontology requirements\\textbackslash{}nTutorials on ontologies may have different foci and it\\textbackslash{}nis unlikely that an ontology used for a specific tutorial\\textbackslash{}nwill meet all requirements. The ontology should meet the\\textbackslash{}nneeds for that tutorial or course, and that should be stated\\textbackslash{}nclearly. As such, this list is intended to serve as a set of con-\\textbackslash{}nsiderations when developing a tutorial ontology. Each item\\textbackslash{}neasily can take up a paragraph of explanation. We refrain\\textbackslash{}nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 5 of 11\\textbackslash{}nfrom this by assuming the reader of this paper is suffi-\\textbackslash{}nciently well-versed in ontology engineering and seeking\\textbackslash{}ninformation on tutorial ontologies. For indicative purpose,\\textbackslash{}nthe requirements are categorised under three dimensions:\\textbackslash{}nthe subject domain of the ontology, logics \\& reasoning,\\textbackslash{}nand engineering factors.\\textbackslash{}nSubject domain\\textbackslash{}nThe tutorial ontology\\textbackslash{}u0092s subject domain, also called uni-\\textbackslash{}nverse of discourse, should be versatile to be able to cater\\textbackslash{}nplausibly for a range of modelling aspects. We specify\\textbackslash{}nseven requirements for it, as follows.\\textbackslash{}n1. It should be general and commonsensical domain\\textbackslash{}nknowledge, so as to be sufficiently intuitive for\\textbackslash{}nnon-experts to be able to understand content and\\textbackslash{}nadd knowledge. Optionally, it may be an enjoyable\\textbackslash{}nsubject domain to make it look more interesting and,\\textbackslash{}nperhaps, also uncontroversial4 to increase chance of\\textbackslash{}nuse across different settings and cultures.\\textbackslash{}n2. The content should be not wrong ontologically,\\textbackslash{}nneither regarding how things are represented (e.g.,\\textbackslash{}nno classes as instances) nor the subject domain\\textbackslash{}nsemantics (e.g., whales are mammals, not fish).\\textbackslash{}n3. It needs to be sufficiently international or\\textbackslash{}ncross-cultural so that experimentation with a\\textbackslash{}nscenario with multiple natural languages for\\textbackslash{}nmultilingual ontologies is plausible.\\textbackslash{}n4. Its contents should demonstrate diverse aspects\\textbackslash{}nsuccinctly when illustrating a point (in contrast to\\textbackslash{}nbeing repetitive in content).\\textbackslash{}n5. It needs to be sufficiently versatile to illustrate the\\textbackslash{}nmultiple aspects in ontology development (see\\textbackslash{}nbelow), including the use of core relations (e.g.,\\textbackslash{}nmereology).\\textbackslash{}n6. It should permit extension to knowledge that\\textbackslash{}nrequires features beyond Description Logics-based\\textbackslash{}nOWL species, so as to demonstrate representation\\textbackslash{}nlimitations and pointers to possible directions of\\textbackslash{}nsolutions (e.g., temporal aspects, non-monotonicity,\\textbackslash{}nfull first-order logic).\\textbackslash{}n7. The subject domain should be able to possibly be\\textbackslash{}nused in a range of use case scenarios (database\\textbackslash{}nintegration, science, NLP, and so on).\\textbackslash{}nLogics \\& reasoning\\textbackslash{}nSince a core feature of ontologies is their logic under-\\textbackslash{}npinning, a tutorial ontology thus also will need to meet\\textbackslash{}ncriteria for the representation language and automated\\textbackslash{}nreasoning over it. They are as follows.\\textbackslash{}n4Recent political issues include complaints with subject domains of exercises\\textbackslash{}nthat perpetuate stereotypes and simplifications, such as, but not limited to, the\\textbackslash{}ngender binary, who can marry whom, and gendered subject domains\\textbackslash{}nperceived to reside at the edges of the spectrum.\\textbackslash{}nI. The ontology should be represented in a logic that has\\textbackslash{}ntool support for modelling and automated reasoning.\\textbackslash{}nII. The ontology should be represented in a logic that\\textbackslash{}nhas tool support for \\textbackslash{}u0091debugging\\textbackslash{}u0092 features that\\textbackslash{}n\\textbackslash{}u0091explain\\textbackslash{}u0092 the deductions, meaning at least showing\\textbackslash{}nthe subset of axioms involved in a deduction.\\textbackslash{}nIII. It should permit simple classification examples and\\textbackslash{}neasy examples for showing unsatisfiability and\\textbackslash{}ninconsistency, such that it does not involve more\\textbackslash{}nthan 2-3 axioms in the explanation, and also longer\\textbackslash{}nones for an intermediate level.\\textbackslash{}nIV. The standard reasoning tasks should terminate fairly\\textbackslash{}nfast (< 5 s) for most basic exercises with the\\textbackslash{}nontology, with the \\textbackslash{}u0091standard\\textbackslash{}u0092 reasoning tasks being\\textbackslash{}nsubsumption/classification, satisfiability, consistency,\\textbackslash{}nquerying and instance retrieval.\\textbackslash{}nV. The representation language should offer some way\\textbackslash{}nof importing or linking ontologies into a network of\\textbackslash{}nontologies.\\textbackslash{}nVI. The language should be expressive enough to\\textbackslash{}ndemonstrate advanced modelling features (e.g.,\\textbackslash{}nirreflexivity and role composition).\\textbackslash{}nVII. The logic should be intuitive for the modelling\\textbackslash{}nexamples at least at the start; e.g., if there is a need for\\textbackslash{}nternary relations, then the logic should permit\\textbackslash{}nn-aries with n ? 3 so that it can be represented as\\textbackslash{}nsuch, rather than as an approximation with a\\textbackslash{}nreification and a workaround pattern.\\textbackslash{}nEngineering and development tasks\\textbackslash{}nAn ontology is an artefact, which has to be built and\\textbackslash{}nmaintained. To this end, there are multiple approaches,\\textbackslash{}nmethodologies, methods, and software tools of which at\\textbackslash{}nleast a subset will have to become part of an ontologist\\textbackslash{}u0092s\\textbackslash{}ntoolbox. We identified eight broad requirements:\\textbackslash{}nA. At least some ontology development methods and\\textbackslash{}ntools should be able to use the ontology, be used for\\textbackslash{}nimprovement of the ontology, etc.\\textbackslash{}nB. The ontology needs to permit short/simple\\textbackslash{}ncompetency questions (CQs) and may permit long\\textbackslash{}nand complicated CQs, which are formulated for the\\textbackslash{}nontology\\textbackslash{}u0092s content and where some can be answered\\textbackslash{}non the ontology and others cannot.\\textbackslash{}nC. At least some of the top-level classes in the hierarchy\\textbackslash{}nshould be straight-forward enough to be easily linked\\textbackslash{}nto a leaf category from a foundational ontology (e.g.,\\textbackslash{}nAnimal is clearly a physical object, but the ontological\\textbackslash{}nstatus of Algorithm is not immediately obvious).\\textbackslash{}nD. It should be relatable to, or usable with, or else at\\textbackslash{}nleast amenable to the use of, ontology design\\textbackslash{}npatterns, be they content patterns or other types.\\textbackslash{}nE. It is beneficial if there is at least one ontology\\textbackslash{}nsufficiently related to its contents, so that it can be\\textbackslash{}nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 6 of 11\\textbackslash{}nused for tasks such as comparison, alignment, and\\textbackslash{}nontology imports.\\textbackslash{}nF. It is beneficial if there are relevant related\\textbackslash{}nnon-ontological resources that could be used for\\textbackslash{}nbottom-up ontology development.\\textbackslash{}nG. It should be able to show ontology quality\\textbackslash{}nimprovements gradually, stored in different files.\\textbackslash{}nH. It should not violate basic ontology design principles\\textbackslash{}n(e.g., classes and relations vs. implementation\\textbackslash{}ndecisions with data properties and data types when\\textbackslash{}nrepresenting qualities of entities, such as an animal\\textbackslash{}u0092s\\textbackslash{}nweight).\\textbackslash{}nWhile this list may turn out not to be exhaustive in the\\textbackslash{}nnear future, it is expected to be sufficient for introduc-\\textbackslash{}ntory levels of ontology development tutorials and courses.\\textbackslash{}nEither way, this list of requirements are already hard\\textbackslash{}nto meet in one single ontology. For instance, simplicity\\textbackslash{}n(Items 3, III, and B) vs. complicated extensions and onto-\\textbackslash{}nlogical precision (Items 6 and C) cannot be fully met\\textbackslash{}nat once. On the flip side, some requirements are closely\\textbackslash{}nrelated or overlap, such as design principles (Item H) and\\textbackslash{}nnot being wrong ontologically (Item 2) since some of the\\textbackslash{}nformer are informed by the latter.\\textbackslash{}nContent of the AWO \\textbackslash{}u0096 at a glance\\textbackslash{}nThe principal content of the AWO is, in the first stage\\textbackslash{}nat least, \\textbackslash{}u0091intuitive\\textbackslash{}u0092 knowledge about African wildlife.\\textbackslash{}nThis subject domain originated from an early Semantic\\textbackslash{}nWeb book ({[}8{]}, Section 4.3.1) that was restructured and\\textbackslash{}nextended slightly for its first, basic version; see Table 2\\textbackslash{}nand Fig. 1. It has descriptions of typical wildlife animals,\\textbackslash{}nsuch as Lion and Elephant, and what they eat, including\\textbackslash{}nImpala (a type of antelope), and Twig or Leaf, respectively.\\textbackslash{}nBasic extensions in the simple version of the ontology\\textbackslash{}n(v1) include plant parts, so as to demonstrate parthood\\textbackslash{}nand its transitivity, and carnivore vs. herbivore, which\\textbackslash{}nmake it easy to illustrate disjointness, subsumption rea-\\textbackslash{}nsoning, and unsatisfiable classes, and carnivorous plants\\textbackslash{}nto demonstrate logical consequences of declaring domain\\textbackslash{}nand range axioms 5. Most elements have been annotated\\textbackslash{}nwith informal descriptions, and several annotations link to\\textbackslash{}ndescriptions on Wikipedia.\\textbackslash{}nLike the aforementioned Family History ontology, there\\textbackslash{}nare several versions of the AWO that reflect different\\textbackslash{}nstages of learning. In the case of the AWO, this is not\\textbackslash{}nspecifically with respect to OWL language features, but\\textbackslash{}none of notions of ontology quality and where one is in\\textbackslash{}nthe learning process. For instance, version 1a contains\\textbackslash{}nanswers to several competency questions\\textbackslash{}u0097i.e., quality\\textbackslash{}n5in casu, declaring eats too restrictively with as domain only Animal: then\\textbackslash{}neither it will result in an unsatisfiable CarnivorousPlant (if Animal and Plant are\\textbackslash{}ndeclared disjoint) or it will result in the undesirable deduction that\\textbackslash{}nCarnivorousPlant \\textbackslash{}004 Animal\\textbackslash{}nrequirements that an ontology ought to meet {[}17{]}\\textbackslash{}u0097that\\textbackslash{}nwere formulated for Exercise 5.1 in the \\textbackslash{}u0093Methods and\\textbackslash{}nmethodologies\\textbackslash{}u0094 chapter of {[}16{]}. Versions 2 and 3, on\\textbackslash{}nthe other hand, have the AWO aligned to the DOLCE\\textbackslash{}nand BFO foundational ontologies, respectively, whose dif-\\textbackslash{}nferences and merits are discussed in Chapter 6 of the\\textbackslash{}ntextbook, ensuring discussion of refinements in ontologi-\\textbackslash{}ncal precision with, e.g., processes and dispositions (e.g., an\\textbackslash{}nEating class with participating objects cf. an eats object\\textbackslash{}nproperty). Their respective versions with the answers to\\textbackslash{}nthe related exercises have the name appended with an \\textbackslash{}u0091a\\textbackslash{}u0092\\textbackslash{}nas well. Version 4 has some contents \\textbackslash{}u0091cleaned up\\textbackslash{}u0092, par-\\textbackslash{}ntially based on what the OOPS! tool {[}14{]} detected; it uses\\textbackslash{}nmore advanced language features; and takes steps in the\\textbackslash{}ndirection of adhering to science more precisely with finer\\textbackslash{}ngranularity, such as type of carnivores and distinguishing\\textbackslash{}nbetween types of roots.\\textbackslash{}nThere are also four versions in different natural lan-\\textbackslash{}nguages, being in isiZulu, Afrikaans, Dutch, and Spanish,\\textbackslash{}nwhich mainly serve the purpose of illustrating issues with\\textbackslash{}nmultilingual settings of ontology use, which relates to\\textbackslash{}ncontent in Chapter 9 of the textbook.\\textbackslash{}nAWO against the requirements\\textbackslash{}nThe AWO meets most of the requirements (see Table 3).\\textbackslash{}nConcerning the subject domain, the content is general,\\textbackslash{}nversatile, not wrong, sufficiently international, and not\\textbackslash{}nrepetitive (Items 1-4). The AWO includes the core rela-\\textbackslash{}ntion of parthood for, especially, plants and their parts, with\\textbackslash{}noptional straightforward extensions with the participation\\textbackslash{}nrelation (e.g., animals participating in a Chasing event)\\textbackslash{}nand membership (animal collectives, such as Herd; see v4\\textbackslash{}nof the AWO), therewithmeeting Item 5. Representation of\\textbackslash{}nrelevant domain knowledge beyond Description Logics-\\textbackslash{}nbased OWL species (Item 6) could include information\\textbackslash{}nabout temporal segregation of foraging or commensal-\\textbackslash{}nism, inclusion of species with distinct successive phases\\textbackslash{}nwith substantial morphological changes (e.g., Caterpil-\\textbackslash{}nlar/Butterfly), and the notion of rigidity between what an\\textbackslash{}nobject is and the role it plays (e.g., Lion playing the role of\\textbackslash{}nPredator; see v4 of the AWO). The subject domain is also\\textbackslash{}nfertile ground for exceptions that may be represented with\\textbackslash{}nnon-monotonic logics; typical examples are that, gener-\\textbackslash{}nally, birds fly and plants have chlorophyl, but not all of\\textbackslash{}nthem (e.g., the penguin and the dodder, respectively). Use\\textbackslash{}ncase scenarios (Item 7) may be, among others, science\\textbackslash{}nof African wildlife, activism on endangered species, and\\textbackslash{}napplications such as a database integration and manage-\\textbackslash{}nment system for zoos and for tourism websites.\\textbackslash{}nRegarding the logics and reasoning, the AWO is rep-\\textbackslash{}nresented in OWL {[}19{]}, and thus has ample tooling sup-\\textbackslash{}nport for knowledge representation, reasoning, and basic\\textbackslash{}ndebugging/explanation, with ontology development envi-\\textbackslash{}nronment tools such as Protégé (Items I-III). The AWO\\textbackslash{}nKeet Journal of Biomedical Semantics            (2020) 11:4 Page 7 of 11\\textbackslash{}nTable 2 AWO ontologies, with their main differences\\textbackslash{}nFile name Difference\\textbackslash{}nAfricanWildlifeOntology.xml This is the file from http://www.iro.umontreal.ca/\\textasciitilde{}lapalme/ift6281/OWL/AfricanWildlifeOntology.xml,\\textbackslash{}nthat was based on the description in {[}8{]}\\textbackslash{}nAfricanWildlifeOntologyWeb.owl AfricanWildlifeOntology.xml + changed the extension to .owl and appended the name\\textbackslash{}nwith Web. This ontology gave at the time (in 2010) a load error in the then current version of Protégé\\textbackslash{}ndue to the use of Collection in the definition of Herbivore\\textbackslash{}nAfricanWildlifeOntology0.owl AfricanWildlifeOntologyWeb.owl + that section on Collection removed\\textbackslash{}nAfricanWildlifeOntology1.owl AfricanWildlifeOntology0.owl + several classes and object properties were added (up to\\textbackslash{}nSRI DL expressiveness), more annotations, URI updated (described in Example 4.1 in {[}16{]})\\textbackslash{}nAfricanWildlifeOntology1a.owl AfricanWildlifeOntology1.owl + new content for a selection of the CQs in Exercise 5.1 in\\textbackslash{}n{[}16{]} (its CQ5, CQ8) and awo\\_12 of the CQ dataset {[}18{]})\\textbackslash{}nAfricanWildlifeOntology2.owl AfricanWildlifeOntology1.owl + OWL-ised DOLCE (Dolce-Lite.owl) was imported\\textbackslash{}nand aligned\\textbackslash{}nAfricanWildlifeOntology2a.owl AfricanWildlifeOntology2.owl + answers to the questions in Example 6.2 in {[}16{]} on\\textbackslash{}nfoundational ontology alignment\\textbackslash{}nAfricanWildlifeOntology3.owl AfricanWildlife'\n\\item[text5] 'Nguyen et al. Journal of Biomedical Semantics            (2020) 11:5 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00221-1\\textbackslash{}nRESEARCH Open Access\\textbackslash{}nNeural side effect discovery from user\\textbackslash{}ncredibility and experience-assessed online\\textbackslash{}nhealth discussions\\textbackslash{}nVan-Hoang Nguyen* , Kazunari Sugiyama, Min-Yen Kan and Kishaloy Halder\\textbackslash{}nAbstract\\textbackslash{}nBackground: Health 2.0 allows patients and caregivers to conveniently seek medical information and advice via\\textbackslash{}ne-portals and online discussion forums, especially regarding potential drug side effects. Although online health\\textbackslash{}ncommunities are helpful platforms for obtaining non-professional opinions, they pose risks in communicating\\textbackslash{}nunreliable and insufficient information in terms of quality and quantity. Existing methods in extracting user-reported\\textbackslash{}nadverse drug reactions (ADRs) in online health forums are not only insufficiently accurate as they disregard user\\textbackslash{}ncredibility and drug experience, but are also expensive as they rely on supervised ground truth annotation of\\textbackslash{}nindividual statement. We propose a NEural ArchiTecture for Drug side effect prediction (NEAT), which is optimized on\\textbackslash{}nthe task of drug side effect discovery based on a complete discussion while being attentive to user credibility and\\textbackslash{}nexperience, thus, addressing the mentioned shortcomings. We train our neural model in a self-supervised fashion\\textbackslash{}nusing ground truth drug side effects from mayoclinic.org. NEAT learns to assign each user a score that is\\textbackslash{}ndescriptive of their credibility and highlights the critical textual segments of their post.\\textbackslash{}nResults: Experiments show that NEAT improves drug side effect discovery from online health discussion by 3.04\\%\\textbackslash{}nfrom user-credibility agnostic baselines, and by 9.94\\% from non-neural baselines in term of F1. Additionally, the latent\\textbackslash{}ncredibility scores learned by the model correlate well with trustworthiness signals, such as the number of \\textbackslash{}u0093thanks\\textbackslash{}u0094\\textbackslash{}nreceived by other forum members, and improve credibility heuristics such as number of posts by 0.113 in term of\\textbackslash{}nSpearman\\textbackslash{}u0092s rank correlation coefficient. Experience-based self-supervised attention highlights critical phrases such as\\textbackslash{}nmentioned side effects, and enhances fully supervised ADR extraction models based on sequence labelling by 5.502\\%\\textbackslash{}nin terms of precision.\\textbackslash{}nConclusions: NEAT considers both user credibility and experience in online health forums, making feasible a\\textbackslash{}nself-supervised approach to side effect prediction for mentioned drugs. The derived user credibility and attention\\textbackslash{}nmechanism are transferable and improve downstream ADR extraction models. Our approach enhances automatic\\textbackslash{}ndrug side effect discovery and fosters research in several domains including pharmacovigilance and clinical studies.\\textbackslash{}nKeywords: Online health communities, Drug side effect discovery, Credibility analysis, Deep learning, Natural\\textbackslash{}nlanguage processing\\textbackslash{}n*Correspondence: vhnguyen@u.nus.edu\\textbackslash{}nSchool of Computing, National University of Singapore, 13 Computing Drive,\\textbackslash{}n117417 Singapore, Singapore\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\textbackslash{}ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\textbackslash{}nmade. The images or other third party material in this article are included in the article\\textbackslash{}u0092s Creative Commons licence, unless\\textbackslash{}nindicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}u0092s Creative Commons licence and your\\textbackslash{}nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\textbackslash{}nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\textbackslash{}nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\textbackslash{}navailable in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 2 of 16\\textbackslash{}nBackground\\textbackslash{}nSeeking medical opinions from online health commu-\\textbackslash{}nnities has become popular: 71\\% of adults aged 18\\textbackslash{}u009629\\textbackslash{}n(equivalent to 59\\% of all U.S. adults) reported consulting\\textbackslash{}nonline health websites for opinions {[}1{]}. These opinions\\textbackslash{}ncome from an estimated twenty to one hundred thousand\\textbackslash{}nhealth-related websites {[}2{]}, inclusive of online health com-\\textbackslash{}nmunities that network patients with each other to pro-\\textbackslash{}nvide information and social support {[}3{]}. Platforms such\\textbackslash{}nas HealthBoards1 and MedHelp2 feature users report-\\textbackslash{}ning their own health experiences, inclusive of their self-\\textbackslash{}nreviewed drugs and medical treatments. Hence, they are\\textbackslash{}nvaluable sources for researchers {[}4, 5{]}.\\textbackslash{}nAlthough patients use these platforms to access valuable\\textbackslash{}ninformation about drug reactions, there are challenges\\textbackslash{}nto their effective, large-scale use. There is lexical varia-\\textbackslash{}ntion where users describe the same side effect differently.\\textbackslash{}nFor example, dizziness can be expressed as giddiness or\\textbackslash{}nmy head is spinning, posing difficulty to most feature-\\textbackslash{}nbased or keyword matching approaches. Separately, there\\textbackslash{}nare valid concerns regarding credibility of user-generated\\textbackslash{}ncontents to be harvested at large in which research has\\textbackslash{}nshown to be of variable quality and should be approached\\textbackslash{}nwith caution {[}6\\textbackslash{}u00969{]}. One proxy indicator for information\\textbackslash{}nquality is the author\\textbackslash{}u0092s trustworthiness {[}10{]}. In the con-\\textbackslash{}ntext of social media or online forums, user trustworthiness\\textbackslash{}nis often approximated via ratings from other users, i.e.,\\textbackslash{}nnumber of thanks or upvotes {[}11{]}, or via their consistency\\textbackslash{}nof reporting credible information {[}12, 13{]}. In addition to\\textbackslash{}ncredibility, forum members also offer expertise thanks to\\textbackslash{}ntheir own experience \\textbackslash{}u0096 with prescriptions in particular \\textbackslash{}u0096\\textbackslash{}nand facilitate responses to drug queries {[}14{]}. For instance,\\textbackslash{}nwhile reporting expected side effects for a specific treat-\\textbackslash{}nment, patients with long-term use of certain drugs can be\\textbackslash{}na complementary source of information:\\textbackslash{}nWhile my experience of 10 years is with Paxil, I expect that Zoloft will be\\textbackslash{}nthe same. You should definitely feel better within 2 weeks. One way I found to\\textbackslash{}nmake it easier to sleep was to get lots of exercize {[}sic{]}. Walk or run or whatever\\textbackslash{}nto burn off that anxiety. \\textbackslash{}u0096 User 3690.\\textbackslash{}nThe above is an answer to a thread asking for expected\\textbackslash{}nside effects for depression treatment with Zoloft.\\textbackslash{}nUser 3690\\textbackslash{}u0092s history of active discussion on other anti-\\textbackslash{}ndepressants such as Lexapro and Xanax lends credibil-\\textbackslash{}nity to them being an authority on depression treatments.\\textbackslash{}nWe noticed that Zoloft (mentioned in the thread)\\textbackslash{}nshares many common side effects with the other two\\textbackslash{}nanti-depressants: \\textbackslash{}u0093changed behavior,\\textbackslash{}u0094 \\textbackslash{}u0093dry mouth,\\textbackslash{}u0094 and\\textbackslash{}n\\textbackslash{}u0093sleepiness or unusual drowsiness.\\textbackslash{}u0094 as illustrated in Table 1.\\textbackslash{}nMany such examples suggest that drugs which are often\\textbackslash{}nprescribed together for the same treatment, such as anti-\\textbackslash{}ndepressants, are likely to be discussed within a same\\textbackslash{}n1https://www.healthboards.com/\\textbackslash{}n2https://medhelp.org/\\textbackslash{}nTable 1 Side effects of anti-depressants\\textbackslash{}nDrugs Side effects\\textbackslash{}nLexapro chills, constipation, cough, decreased appetite, decreased\\textbackslash{}nsexual desire, diarrhea, drymouth, joint pain, muscle\\textbackslash{}nache, tingling feeling, sleepiness or unusual\\textbackslash{}ndrowsiness, unusual dream, sweating, ...\\textbackslash{}nXanax abdominal or stomach pain, muscle weakness , changed\\textbackslash{}nbehavior, chills, cough, decreased appetite, decreased\\textbackslash{}nurine, diarrhea, difficult bowel movement, cough, dry\\textbackslash{}nmouth, tingling feeling, sleepiness or unusual\\textbackslash{}ndrowsiness, slurred speech, sweating, yellow eye,..\\textbackslash{}nZoloft changed behavior, decreased sexual desire, diarrhea,\\textbackslash{}ndrymouth, heartburn, sleepiness or unusual\\textbackslash{}ndrowsiness, sweating,..\\textbackslash{}nThe Drugs and Side effects columns respectively list the anti-depressants and their\\textbackslash{}nside effects extracted from a drug\\textbackslash{}u0096side effect database. Side effects in common\\textbackslash{}namong those listed are bold\\textbackslash{}nthread and share common side effects. In addition, users\\textbackslash{}nwho have experienced certain drug reactions are more\\textbackslash{}noutspoken and active on those discussions involving drugs\\textbackslash{}nof similar side effects. These signals arise from the rich\\textbackslash{}ncontext of online health information; hence, we expect\\textbackslash{}nsystems to explore beyond individual statements. Specif-\\textbackslash{}nically, they should consider the complete discussion con-\\textbackslash{}ntent as well as the global experience of each involved users,\\textbackslash{}nin order to discover drug side effects or extract adverse\\textbackslash{}ndrug reactions (ADRs).\\textbackslash{}nWe argue that modeling user expertise from experi-\\textbackslash{}nenced side effects is more robust compared against gen-\\textbackslash{}neral user profile and engagement features {[}13, 14{]}, as user\\textbackslash{}nexpertise provides more meaningful signals for side effect\\textbackslash{}ndiscovery. To the best of our knowledge, there is no pre-\\textbackslash{}nvious work that incorporates user expertise in side effect\\textbackslash{}ndiscovery in discussion forums at either the thread or\\textbackslash{}npost level. In this work, given online health discussions,\\textbackslash{}nwe propose a novel end-to-end neural architecture that\\textbackslash{}njointly models each author\\textbackslash{}u0092s credibility, their global expe-\\textbackslash{}nrience and their post\\textbackslash{}u0092s textual content to discover the\\textbackslash{}nside effect of unseen drugs. We optimize the model on\\textbackslash{}na self-supervised task of predicting side effect of men-\\textbackslash{}ntioned drugs for complete threads, where ground truth\\textbackslash{}nis accessible. Our key observation is that users can be\\textbackslash{}ngrouped into clusters that share the same expertise or\\textbackslash{}ninterest in certain drugs, possibly due to their common\\textbackslash{}ntreatment or medical history. We incorporate this crit-\\textbackslash{}nical observation into our user model in representing a\\textbackslash{}npost\\textbackslash{}u0092s content via a cluster-sensitive attention mechanism\\textbackslash{}n{[}15{]}. We also follow general definition of truth discov-\\textbackslash{}nery and let the model learn a credibility score that is\\textbackslash{}nunique to every user and descriptive of their trustworthi-\\textbackslash{}nness. Our experiments include an overall ablation study\\textbackslash{}nto validate the significance of each model component.\\textbackslash{}nThis paper extends our former work {[}16{]} by conducting\\textbackslash{}na correlation study that analyzes the representativeness of\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 3 of 16\\textbackslash{}nlearned credibility scores and a comparison between our\\textbackslash{}nself-supervised attention-based approach and traditional\\textbackslash{}nsupervised sequence labeling approaches on side effect\\textbackslash{}nmention extraction.\\textbackslash{}nWe summarize our contributions as follows:\\textbackslash{}n\\textbackslash{}u0095 We propose a NEural ArchiTecture, NEAT, that\\textbackslash{}ncaptures 1) user expertise and 2) credibility, 3) the\\textbackslash{}nsemantic content of individual posts and 4) the\\textbackslash{}ncomplete discussion thread, to improve side effect\\textbackslash{}ndiscovery from online health discussions. NEAT\\textbackslash{}u0092s\\textbackslash{}nmain means of user credibility and experience\\textbackslash{}nassessment can be easily adopted by various neural\\textbackslash{}nattentional encoders {[}17, 18{]}.\\textbackslash{}n\\textbackslash{}u0095 We formulate a self-supervised task of side effect\\textbackslash{}nprediction of mentioned drugs for the proposed\\textbackslash{}nnetwork to jointly optimize its components.\\textbackslash{}n\\textbackslash{}u0095 We conduct experiments to verify the validity of our\\textbackslash{}nlearned credibility and the robustness of\\textbackslash{}nself-supervised attention-based extraction,\\textbackslash{}ncomparing against traditional supervised sequence\\textbackslash{}nlabeling baselines.\\textbackslash{}nRelated work\\textbackslash{}nWe first review existing approaches to drug side effect\\textbackslash{}ndiscovery from health forums and social media. Next, we\\textbackslash{}nexamine how these works incorporate user credibility and\\textbackslash{}nexpertise in their learning objective. Finally, we justify\\textbackslash{}nour choice of neural architecture by discussing its mod-\\textbackslash{}neling capability of context-rich structures such as online\\textbackslash{}ndiscussion.\\textbackslash{}nDrug Side Effect Discovery. Existing methods for drug\\textbackslash{}ndiscovery from online content extract drugs at post and\\textbackslash{}nstatement level. ADR mining systems typically include a\\textbackslash{}nnamed entity recognition (NER) model and a relationship\\textbackslash{}nor semantic role labeling model {[}19, 20{]}. Recent neu-\\textbackslash{}nral approaches address lexical variation in user-generated\\textbackslash{}ncontent \\textbackslash{}u0096 the difficulty faced by traditional keyword\\textbackslash{}nmatching and rule-based approaches \\textbackslash{}u0096 to improve recog-\\textbackslash{}nnition and labeling components {[}21, 22{]}. Distributed word\\textbackslash{}nrepresentations {[}23, 24{]} constructed from context can\\textbackslash{}ncapture semantics based on the hypothesis that syn-\\textbackslash{}nonyms often share similar contextual words. For example,\\textbackslash{}n\\textbackslash{}u0093headache\\textbackslash{}u0094 and \\textbackslash{}u0093cephalea\\textbackslash{}u0094 will have close representations\\textbackslash{}nif they share contextual words such as \\textbackslash{}u0093head\\textbackslash{}u0094 or \\textbackslash{}u0093pain\\textbackslash{}u0094.\\textbackslash{}nApproaches to sub-word embedding {[}25, 26{]} model the\\textbackslash{}nmorphology of words by leveraging sub-word or charac-\\textbackslash{}nter information. These representations are naturally inte-\\textbackslash{}ngrated into neural sequential models {[}17, 18, 27{]} that are\\textbackslash{}nsensitive to syntactic order. However, supervised sequence\\textbackslash{}nlabeling or mention extraction approaches require labo-\\textbackslash{}nrious annotations at the word (token) level, and are\\textbackslash{}nonly capable of discovering side effects that are explic-\\textbackslash{}nitly present in the text. Expert supervision or additional\\textbackslash{}nsemantic matching models are also required to map such\\textbackslash{}nrecognized text segments to standardized vocabularies or\\textbackslash{}nthesaurii {[}28{]}. In contrast, our proposed self-supervised\\textbackslash{}ntask formulation discovers the aggregated side effects of\\textbackslash{}nmentioned drugs for each community discussion by con-\\textbackslash{}nsidering the whole thread\\textbackslash{}u0092s content. The list of discussed\\textbackslash{}ndrugs are tagged by forummoderators or obtained by pat-\\textbackslash{}ntern matching. This learning design not only effectively\\textbackslash{}nalleviates the need for expensive, finer-grained annota-\\textbackslash{}ntions but also allows for the prediction of side effects not\\textbackslash{}nexplicitly mentioned in the discussion.\\textbackslash{}nUser Credibility and Expertise Integration. Credi-\\textbackslash{}nbility is of the utmost concern in large-scale knowl-\\textbackslash{}nedge harvesting {[}8, 29, 30{]}. Previous work on side effect\\textbackslash{}ndiscovery from individual statements or posts derive\\textbackslash{}ninformation credibility by verifying a statement\\textbackslash{}u0092s men-\\textbackslash{}ntioned side effects against ground truth drug side effect\\textbackslash{}ndatabases, and assess associated user credibility by mea-\\textbackslash{}nsuring the percentage of a user\\textbackslash{}u0092s credible statements\\textbackslash{}n{[}13, 31{]}. In contrast, our approach to side effect discov-\\textbackslash{}nery from discussions by jointly modeling multiple posts\\textbackslash{}nand authors eschews the assessment of statement cred-\\textbackslash{}nibility and derives user credibility differently. We assign\\textbackslash{}neach user a positive score that is used to weight their\\textbackslash{}npost content in representing the discussion\\textbackslash{}u0092s holistic con-\\textbackslash{}ntent. Suchweighted summation is detailedmathematically\\textbackslash{}nin Appendix 1 to conform to the general principle of\\textbackslash{}ntruth discovery, where sources providing credible infor-\\textbackslash{}nmation should be assigned higher credibility scores, and\\textbackslash{}nthe information that is supported by credible sources will\\textbackslash{}nbe regarded as true {[}10{]}. Although our dataset does not\\textbackslash{}nprovide any ground truth for user trustworthiness, we fol-\\textbackslash{}nlowed the previous usage of ratings or upvotes in online\\textbackslash{}nforums and adopted the number of \\textbackslash{}u0093thanks\\textbackslash{}u0094 received from\\textbackslash{}nother forum members {[}11{]} as our proxy for user trust-\\textbackslash{}nworthiness. Previous works have modeled user expertise\\textbackslash{}nbased on user profiles such as demographics; activity\\textbackslash{}nfeatures such as posting frequency and posting pattern\\textbackslash{}nthrough time series and network analysis {[}13, 14{]}. As\\textbackslash{}nshown in an earlier example in Section 1, modeling user\\textbackslash{}nexpertise from previously experienced side effects better\\textbackslash{}ncaptures author authoritativeness for certain side effects.\\textbackslash{}nIt is also universally applicable to any online platform.\\textbackslash{}nModeling Online Discussion Content and Structure\\textbackslash{}nAs our work makes use of the rich topographical proper-\\textbackslash{}nties of online communities, we briefly review approaches\\textbackslash{}nfor modeling textual content and post-thread discus-\\textbackslash{}nsion structure. Previous works use probabilistic graphical\\textbackslash{}nmodels implicitly to represent textual content (especially,\\textbackslash{}ntopicmodeling) as bags-of-words {[}28, 32{]} or inventories of\\textbackslash{}nstylistic and linguistic features {[}13{]}. Such lightweight rep-\\textbackslash{}nresentation are well-suited in moderately short contexts,\\textbackslash{}ni.e., sentences or posts. However, in terms of modeling\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 4 of 16\\textbackslash{}nlong discussions consisting of multiple posts, state-of-the-\\textbackslash{}nart models for Community Question Answering (CQA)\\textbackslash{}nfeature hierarchical neural architectures {[}33\\textbackslash{}u009635{]}. In term\\textbackslash{}nof encoding text, sequential encoders such as Long Short-\\textbackslash{}nTermMemory (LSTM) {[}36{]} or Convolutional Neural Net-\\textbackslash{}nworks (CNN) {[}18{]} are capable of encoding long-term\\textbackslash{}ndependencies and semantic expressiveness by leverag-\\textbackslash{}ning word embeddings. In terms of encoding hierarchical\\textbackslash{}nstructures such as community discussions consisting of\\textbackslash{}npost- and thread-level features, neural architectures allow\\textbackslash{}nfor straightforward and efficient integration of multiple\\textbackslash{}nlearning objectives. In addition, our neural architecture,\\textbackslash{}nNEAT, incorporates attention mechanism that focuses on\\textbackslash{}nessential phrases while encoding post content, and joint\\textbackslash{}nuser credibility learning while optimizing for the side\\textbackslash{}neffect discovery objective.\\textbackslash{}nMethods\\textbackslash{}nBasic Terminology. To ensure a consistent representa-\\textbackslash{}ntion, we define some terms and formalize them as follows:\\textbackslash{}n\\textbackslash{}u0095 A drug d has a set of side effects,\\textbackslash{}nSd = \\{s1, s2, . . . , s\\textbar{}Sd\\textbar{}\\}\\textbackslash{}u0095 A post p is a message in online forums and contains a\\textbackslash{}nsequence of words. Each post p belongs to the set of\\textbackslash{}nall online forum posts P and is written by a user u\\textbackslash{}nand belongs to a thread t.\\textbackslash{}n\\textbackslash{}u0095 A user u is a member of an online forum and\\textbackslash{}nparticipates in a list of threads, i.e.,\\textbackslash{}nTu = \\{t1, t2, . . . , t\\textbar{}Tu\\textbar{}\\} by writing at least one post in\\textbackslash{}neach thread. We use the terms user and author, as\\textbackslash{}nwell as user experience and user expertise\\textbackslash{}ninterchangeably. Each user belongs to the set of all\\textbackslash{}nonline forum users U and is characterized by their\\textbackslash{}ncredibility and expertise. Credibility wu of user u\\textbackslash{}nreflects the probability of user u provide trustworthy\\textbackslash{}nor helpful information, and is approximated from the\\textbackslash{}nnumber of \\textbackslash{}u0093thanks\\textbackslash{}u0094 given from other forummembers.\\textbackslash{}n\\textbackslash{}u0095 A thread t (see Table 2) is an ordered collection of\\textbackslash{}npost\\textbackslash{}u0096user pairs,\\textbackslash{}nQt = \\{(p1,u1) , (p2,u2) , . . . ,\\textbackslash{}n(\\textbackslash{}np\\textbar{}Qt \\textbar{},u\\textbar{}Qt \\textbar{}\\textbackslash{}n)\\}.\\textbackslash{}nEach thread discusses the treatment for a particular\\textbackslash{}ncondition and entails a list of prescribed drugs\\textbackslash{}nDt = \\{d1, d2, . . . , d\\textbar{}Dt \\textbar{}\\}. Hence, every thread has a list\\textbackslash{}nof aggregated potential side effects defined as\\textbackslash{}nSt = Sd1 ? Sd2 · · · ? Sd\\textbar{}Dt \\textbar{} .\\textbackslash{}nTask Definition. Drug side effect discovery from discus-\\textbackslash{}nsions is the task of assigning the most relevant subset of\\textbackslash{}npotential side effects to threads discussing certain drugs,\\textbackslash{}nfrom a large collection of side effects. We view the drug\\textbackslash{}nside effect discovery problem as a multi-label classifica-\\textbackslash{}ntion task. In our setting, an instance of item\\textbackslash{}u0096label is a\\textbackslash{}ntuple\\textbackslash{}n(xt , y\\textbackslash{}n)\\textbackslash{}nwhere xt is the feature vector of thread t\\textbackslash{}nderived from its list of post\\textbackslash{}u0096user pairs Qt and y is the side\\textbackslash{}neffect label vector i.e., y ? \\{0, 1\\}\\textbar{}S\\textbar{}, where \\textbar{}S\\textbar{} is the number\\textbackslash{}nof possible side effect labels. Given training instances, we\\textbackslash{}ntrain our classifier to predict the list of drug side effects in\\textbackslash{}nunseen threads discussing unseen drugs.\\textbackslash{}nFormal Hypothesis. Given a thread t with Qt , we\\textbackslash{}nhypothesize that considering the credibility and experi-\\textbackslash{}nence of user u ? (p,u) ? Qt improves the quality of feature\\textbackslash{}nrepresentation in thread t, resulting in better drug side\\textbackslash{}neffect discovery performance.\\textbackslash{}nSelf-supervised Drug Side Effect Discovery. We pro-\\textbackslash{}npose a self-supervised learning objective. Instead of\\textbackslash{}nrelying on the identical and independently distributed\\textbackslash{}nassumption of fully supervised learning, we construct\\textbackslash{}nthe dataset from threads that can discuss a set of com-\\textbackslash{}nmon drugs. We look up the side effects of these men-\\textbackslash{}ntioned drugs via a drug\\textbackslash{}u0096side effect medical database\\textbackslash{}nobtained from Mayo Clinic portal. Our self-supervised\\textbackslash{}ntask explores discussion-based side effect discovery which\\textbackslash{}nalleviates the need for finer-grain annotation compared\\textbackslash{}nagainst existing approach of statement-based side effect\\textbackslash{}ndiscovery. We also propose our neural architecture,\\textbackslash{}nTable 2 A sample discussion thread from an online health community\\textbackslash{}nUser IDs Posts Mentioned drugs Aggregated side effects\\textbackslash{}n3690 While my experience of 10 years is with Paxil,\\textbackslash{}nI expect that Zoloft will be the same. You\\textbackslash{}nshould definitely feel better within 2 weeks.\\textbackslash{}nOne way I found to make it easier to sleep\\textbackslash{}nwas to get lots of exercize. Walk or run or\\textbackslash{}nwhatever to burn off that anxiety.\\textbackslash{}nZoloft, Paxil changed behavior, decreased sexual desire,\\textbackslash{}ndiarrhea, dry mouth, heart-burn, sleepiness\\textbackslash{}nor unusual drowsiness,...\\textbackslash{}n26521 I\\textbackslash{}u0092ve heard of people going \\textbackslash{}u0093cold turkey\\textbackslash{}u0094 and\\textbackslash{}nhaving withdrawal at 6 months! Please, get\\textbackslash{}nin contact with a doctor ASAP! \\textbackslash{}u0093common\\textbackslash{}nsymptoms include dizziness, electric shock-\\textbackslash{}nlike sensations, sweating, nausea, insomnia,\\textbackslash{}ntremor, confusion, nightmares and vertigo\\textbackslash{}u0094\\textbackslash{}nThe User IDs and Posts columns respectively list the IDs of users involved in the discussions and their messages. The Mentioned drugs and Aggregated side effects columns\\textbackslash{}nrespectively list the explicitly discussed drugs and their combined side effects\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 5 of 16\\textbackslash{}nNEAT, that jointly models user credibility, expertise and\\textbackslash{}ntext content with attention while optimizing for the self-\\textbackslash{}nsupervised objective. The network has three major com-\\textbackslash{}nponents: 1) user expertise representation with rich multi-\\textbackslash{}ndimensional vectors; 2) cluster-sensitive attention being\\textbackslash{}ncapable of focusing on relevant phases for post con-\\textbackslash{}ntent encoding improvement; and 3) credibility weighting\\textbackslash{}nmechanism which effectively learns to assign credibility\\textbackslash{}nscore to each user, based on their content. We discuss its\\textbackslash{}nimplementation in the following sections. Figure 1 shows\\textbackslash{}nthe detailed network architecture of our model.\\textbackslash{}nUser Expertise Representation (UE). We embed each\\textbackslash{}nuser u ? U as a vector vu so that the vector captures user\\textbackslash{}nu\\textbackslash{}u0092s experience with certain side effects. As each user u par-\\textbackslash{}nticipates in the threads Tu, entailing a list of experienced\\textbackslash{}nside effects, we derive user side effect experience vector\\textbackslash{}nv?u ? R\\textbar{}S\\textbar{} where S is the set of all possible side effects\\textbackslash{}nand v?ui = nui where user u has discussed ith side effect\\textbackslash{}nin nui threads. We obtain a user drug experience matrix\\textbackslash{}nM? ? R\\textbar{}U\\textbar{}×\\textbar{}S\\textbar{} where jth row of M? denotes user side\\textbackslash{}neffect experience vector of jth user. To avoid learning from\\textbackslash{}nsparse multi-hot encoded representations and to improve\\textbackslash{}nFig. 1 The neural architecture of our proposed NEAT. The wu and vu boxes denote Credibility Weight (CW) component and User Expertise (UE)\\textbackslash{}ncomponent. The yellow boxes and blue boxes denote Cluster Attention (CA) component and neural text encoders with attention. The highlighted\\textbackslash{}nwords in red denoted the text segments that are being attended by the encoder. The ×, ?, and ? symbols denote the multiplication, summation,\\textbackslash{}nand sigmoid, respectively\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 6 of 16\\textbackslash{}nthe model\\textbackslash{}u0092s scalability with the number of side effects, we\\textbackslash{}nperform dimensionality reduction, specifically principal\\textbackslash{}ncomponent analysis (PCA) {[}37{]}, to our experience matrix\\textbackslash{}nM? obtained from training set. Figure 2 shows percentage\\textbackslash{}nof variance explained versus number of included principal\\textbackslash{}ncomponents. Since our PCA plots do not show signifi-\\textbackslash{}ncant improved percentage of variance explained beyond\\textbackslash{}n100 components, we use g = 100 components, reduc-\\textbackslash{}ning our original M? ? R\\textbar{}U\\textbar{}×\\textbar{}S\\textbar{} to user expertise matrix\\textbackslash{}nM ? R\\textbar{}U\\textbar{}×g .\\textbackslash{}nUser Cluster Attention (CA).We make an assumption\\textbackslash{}nvia observations that users in online health communities\\textbackslash{}ncan be effectively grouped into clusters based on their\\textbackslash{}nprevious side effect experience. The advantages of clus-\\textbackslash{}ntering users is twofold: First, since users in the same\\textbackslash{}nclusters share certain parameters, they are jointly mod-\\textbackslash{}neled and more active forum members leverage less active\\textbackslash{}nones. Second, clustering efficiently reduces the number\\textbackslash{}nof parameters to learn and improves optimization and\\textbackslash{}ngeneralization. We apply K-means \\textbackslash{}u0096 a distance-based\\textbackslash{}nunsupervised clustering algorithm {[}38{]} \\textbackslash{}u0096 to binary-valued\\textbackslash{}nuser experience vectors v?u after normalization. By using\\textbackslash{}ncosine similarity, the algorithm effectively groups users\\textbackslash{}nwith a high number of co-occurred side effects in the same\\textbackslash{}ncluster. To determine the number of clusters c, we plot\\textbackslash{}nthe silhouette scores against the number of clusters and\\textbackslash{}nobserve the sharp drop after c = 7 (Fig. 3). The average\\textbackslash{}nsilhouette score is 0.57 for our choice of c = 7, indi-\\textbackslash{}ncating that users are moderately matched to their own\\textbackslash{}ngroups and separated from other groups. The top 5 most\\textbackslash{}ncommon side effects in each clusters are shown in Table 3.\\textbackslash{}nIn the larger domain of natural language processing,\\textbackslash{}nattention has become an integral part for modeling text\\textbackslash{}nsequences {[}39, 40{]}. By learning to focus on essential text\\textbackslash{}nsegments, attention allows text encoders to capture long\\textbackslash{}nterm semantic dependencies with regard to auxiliary con-\\textbackslash{}ntextual information {[}41, 42{]}. In our related task of ADR\\textbackslash{}nmentions extraction, attention has been adopted recently\\textbackslash{}nin neural sequence labelling models {[}21, 43{]}, resulting\\textbackslash{}nin promising improvement. Inspired by the concept, we\\textbackslash{}nenhance text encoding with user expertise attention. Even\\textbackslash{}nthough the attention is adjusted to the non-extractive self-\\textbackslash{}nsupervised task of thread-level drug side effect discovery,\\textbackslash{}nwe hypothesize that our model learns to highlight the\\textbackslash{}nmentioned accurate side effects, and can be used as a\\textbackslash{}nself-supervised baseline for side effect extraction. Based\\textbackslash{}non the previously obtained clustering results, we assign a\\textbackslash{}nlearnable cluster attention vector for each user group and\\textbackslash{}nincorporate their expertise into the text encoding process.\\textbackslash{}nPost Content Encoding. NEAT takes the content of a\\textbackslash{}nthread t as input, which is a list of post\\textbackslash{}u0096user pairs Qt .\\textbackslash{}nPost pi of pair (pi,ui) ? Qt consists of a sequence of\\textbackslash{}nwords xpi = \\{w1, . . . ,wn\\} with length n. We seek to rep-\\textbackslash{}nresent a post pi as a vector vp that effectively captures\\textbackslash{}nits semantics through an encoding function f (xpi) mod-\\textbackslash{}neled by a neural text encoding module (the blue boxes\\textbackslash{}nin Fig. 1). We embed each word into a low dimensional\\textbackslash{}nvector and transform the post into a sequence of word\\textbackslash{}nvectors \\{vw1 , vw2 , . . . , vwn\\}. Each word vector is initialized\\textbackslash{}nusing pre-trained GloVe {[}24{]} embeddings, and each out-\\textbackslash{}nof-vocabulary word vector is initialized randomly. We\\textbackslash{}nmake use of modularity \\textbackslash{}u0096 a major advantage of neural\\textbackslash{}nFig. 2 Principal component analysis on user experience vectors. The horizontal axis denotes the number of principal components chosen for PCA,\\textbackslash{}nwhile the vertical axis denotes their percentage of variance explained. We notice that the percentage of variance explained does not increase\\textbackslash{}nsignificantly after 100 principal components\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 7 of 16\\textbackslash{}nFig. 3 Silhouette scores for User Clustering. The horizontal axis denotes the number of clusters chosen for K-means clustering, while the vertical axis\\textbackslash{}ndenotes their the silhouette scores. We notice that the silhouette scores drop sharply after 7 clusters.\\textbackslash{}narchitectures \\textbackslash{}u0096 and design the post content encoder as\\textbackslash{}na standalone component that can be easily updated with\\textbackslash{}nany state-of-the-art text encoder. In this work, we pro-\\textbackslash{}nvide two neural text encoders: long-short term memory\\textbackslash{}n(LSTM, see Fig. 4) {[}36{]} and convolutional neural net-\\textbackslash{}nworks (CNN, see Fig. 5) {[}18{]}, both of which incorporates\\textbackslash{}nattention mechanism.\\textbackslash{}nA bi-directional LSTM encodes the word vector\\textbackslash{}nsequence and outputs two sequences of hidden states: a\\textbackslash{}nforward sequence, Hf = hf1,hf2, . . . ,hfn that starts from\\textbackslash{}nthe beginning of the text; and a backward sequence,Hb =\\textbackslash{}nhb1,hb2, . . . ,hbn that starts from the end of the text. Formany\\textbackslash{}nsequence encoding tasks, knowing both past (left) and\\textbackslash{}nfuture (right) contexts has proven to be effective {[}44{]}. The\\textbackslash{}nstates hfi ,hbj ? Re of the forward and backward sequences\\textbackslash{}nare computed as follows:\\textbackslash{}nhfi = LSTM(hfi?1, vwi), hbj = LSTM(hbj+1, vwj),\\textbackslash{}nwhere e is the number of encoder units, and hfi ,hbj are the\\textbackslash{}nith and jth hidden state vector of the forward (f ) and back-\\textbackslash{}nward (b) sequence. We derive the cluster attention vector\\textbackslash{}nas vai ? Re for each user ci, from which the weights of\\textbackslash{}neach hidden state hfj and hbj based on their similarity with\\textbackslash{}nthe attention vector are:\\textbackslash{}nwaj =\\textbackslash{}nexp(vaihj)?n\\textbackslash{}nl=1 exp(vaihl)\\textbackslash{}n. (1)\\textbackslash{}nThe intuition behind Eq. (1), inspired by Luong et al.\\textbackslash{}n{[}39{]}, is that hidden states which are similar to the atten-\\textbackslash{}ntion vector vai should be paid more attention to; hence\\textbackslash{}nare weighted higher during document encoding. vai is\\textbackslash{}nadjusted during training to capture hidden states that are\\textbackslash{}nsignificant in forming the final post representation. waj\\textbackslash{}nis then used to compute forward and backward weighted\\textbackslash{}nfeature vectors:\\textbackslash{}nhf =\\textbackslash{}nn?\\textbackslash{}nj\\textbackslash{}nwajh\\textbackslash{}nf\\textbackslash{}nj , hb =\\textbackslash{}nn?\\textbackslash{}nj\\textbackslash{}nwajhbj . (2)\\textbackslash{}nWe concatenate the forward and backward vectors to\\textbackslash{}nobtain a single vector, following previous bi-directional\\textbackslash{}nLSTM practice {[}45{]}.\\textbackslash{}nTable 3 Most common experienced side effects for each user\\textbackslash{}ncluster ci (i = 1 to 7)\\textbackslash{}nCluster Most common experienced side effects\\textbackslash{}nc1 vision blurred, yellow skin, vision double, yellow eye,\\textbackslash{}nnose stuffy\\textbackslash{}nc2 headache, itch, stomach pain, weak, nausea\\textbackslash{}nc3 itch, irritate, headache, pain abdominal, stomach\\textbackslash{}ncramp\\textbackslash{}nc4 bad taste, nausea, tiredness, irritate, mouth ulcer\\textbackslash{}nc5 skin red, itch, rash skin, skin peeling, burning skin\\textbackslash{}nc6 sneezing, nose runny, nose stuffy, decrease sexual\\textbackslash{}ndesire, pain breast\\textbackslash{}nc7 nausea, stomach pain, vomit, diarrhea, pain\\textbackslash{}nabdominal\\textbackslash{}nThe left column lists the names of 7 clusters, and the right column describes the\\textbackslash{}nmost common experienced side effects of users in each cluster\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 8 of 16\\textbackslash{}nFig. 4 LSTM-based encoder with cluster attention. The × and + cells denote the attention-weighted summation described in Eq. (2). The C cell\\textbackslash{}ndenotes the concatenation of the forward, hf , and backward, hb , hidden states\\textbackslash{}nOur choice of CNN-based encoder is based on prior\\textbackslash{}nwork {[}18, 46{]}. A convolution block k consists of two sub-\\textbackslash{}ncomponents: a convolution layer and a cluster attention\\textbackslash{}nlayer. In the convolution layer, a kernel of window s\\textbackslash{}n(0 < s < n) of weight W is used to generate\\textbackslash{}nthe hidden representation hkj for the word embeddings\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 9 of 16\\textbackslash{}nFig. 5 CNN-based Encoder with Cluster Attention. The × and + cells denote the attention-weighted summation described in Eq. 2. The C cell\\textbackslash{}ndenotes the concatenation of the final hidden states of K convolution blocks\\textbackslash{}nNguyen et al. Journal of Biomedical Semantics            (2020) 11:5 Page 10 of 16\\textbackslash{}n\\{vwi?s+1 , · · · , vwi\\} as:\\textbackslash{}nhkj = CONV (W , \\{vwi?s+1 , · · · , vwi\\}) (3)\\textbackslash{}nwhere CONV (·) is the convolution operation described\\textbackslash{}nin {[}18{]}. In the cluster attention layer, we first derive the\\textbackslash{}nattention weight waj for each hidden representation hkj\\textbackslash{}nsimilarly to the LSTM-based encoder. Attention weighted\\textbackslash{}npooling is used to obtain the convolution block output as\\textbackslash{}nfollows:\\textbackslash{}nhk =\\textbackslash{}nn?\\textbackslash{}nj\\textbackslash{}nwajhkj (4)\\textbackslash{}nSince we use multiple convolution blocks of different\\textbackslash{}nkernel sizes, the final post representation is the concate-\\textbackslash{}nnation of K block outputs hk .\\textbackslash{}nThread Content Encoding with Credibility Weights\\textbackslash{}n(CW). For every post\\textbackslash{}u0096user pair (pi,ui) at thread t, we\\textbackslash{}nfirst compute feature vector vpi for post pi. NEAT then\\textbackslash{}nconcatenates this post\\textbackslash{}u0096user representation with user ui\\textbackslash{}u0092s\\textbackslash{}nexpertise vector vui to form post\\textbackslash{}u0096user complex vector vpui .\\textbackslash{}nThis post\\textbackslash{}u0096user complex is weighted by a user credibil-\\textbackslash{}nity ewui , where wui initially set to 0 per user and updated\\textbackslash{}nwhile training for the self-supervised side effect discovery\\textbackslash{}nobjective. We implement credibility learning according to\\textbackslash{}nthe general intuition from the truth discovery literature:\\textbackslash{}nusers who give quality posts, on which the model can\\textbackslash{}nsolely base to make correct predictions, are given a higher\\textbackslash{}ncredibility. We also exploit this credibility score to encode\\textbackslash{}nthe thread representation by placing emphasis on the con-\\textbackslash{}ntent of credible users. A representation of a thread that\\textbackslash{}nmeets the above description is the weighted sum of each\\textbackslash{}npost\\textbackslash{}u0096user complex vector:\\textbackslash{}nvt =\\textbackslash{}nn?\\textbackslash{}ni=1\\textbackslash{}nvp?ui =\\textbackslash{}nn?\\textbackslash{}ni=1\\textbackslash{}newui vpui (5)\\textbackslash{}nMulti-label Prediction:NEAT feeds the thread content\\textbackslash{}nrepresentation vt through a fully connected layer whose\\textbackslash{}noutputs can be computed as follows:\\textbackslash{}nst = W tanh(vt) + b, (6)\\textbackslash{}nwhere W and b are weights and biases of the layer. The\\textbackslash{}noutput vector st ? R\\textbar{}S\\textbar{} is finally passed through a sigmoid\\textbackslash{}nactivation function ?(·), and trained using cross-entropy\\textbackslash{}nloss L defined as follows:\\textbackslash{}nL = 1\\textbar{}T \\textbar{}\\textbackslash{}n\\textbar{}T \\textbar{}?\\textbackslash{}nt=1\\textbackslash{}n\\{yt · log(? (st)) + (1 ? yt) · log(1 ? ?(st))\\}\\textbackslash{}n+ ?1\\textbackslash{}n??\\textbackslash{}nu\\textbackslash{}nv2u + ?2\\textbackslash{}n?\\textbackslash{}ni\\textbackslash{}n\\textbar{}wui \\textbar{}\\textbackslash{}n(7)\\textbackslash{}nWe adopt regularization that penalizes the training loss\\textbackslash{}nwith the user experience matrix\\textbackslash{}u0092s L2 norm by a fac-\\textbackslash{}ntor of ?1 and the user credibility vector wu\\textbackslash{}u0092s L1 norm\\textbackslash{}nby a factor of ?2. The loss function is differentiable,\\textbackslash{}nthus trainable with the Adam optimizer {[}47{]}. During\\textbackslash{}nour gradient-based learning, user ui\\textbackslash{}u0092s credibility score\\textbackslash{}nwui is updated by calculating ?L?wui by back-propagation\\textbackslash{}n(see Appendix 1).\\textbackslash{}nResults\\textbackslash{}nWe conduct experiments to validate the effectiveness of\\textbackslash{}nour proposed model. We design an ablation study to high-\\textbackslash{}nlight the effectiveness of each component of NEAT in\\textbackslash{}nour self-supervised side effect prediction. In addition, we\\textbackslash{}nexpand our previous work {[}16{]}. More specifically,\\textbackslash{}n1. We verify the representativeness of the learned\\textbackslash{}ncredi'\n\\item[text6] 'Gleim et al. Journal of Biomedical Semantics            (2020) 11:6 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00223-z\\textbackslash{}nRESEARCH Open Access\\textbackslash{}nEnabling ad-hoc reuse of private data\\textbackslash{}nrepositories through schema extraction\\textbackslash{}nLars Christoph Gleim1* , Md Rezaul Karim1,2, Lukas Zimmermann3, Oliver Kohlbacher3,4,5,6,7,\\textbackslash{}nHolger Stenzhorn3,8, Stefan Decker1,2 and Oya Beyan1,2\\textbackslash{}nAbstract\\textbackslash{}nBackground: Sharing sensitive data across organizational boundaries is often significantly limited by legal and ethical\\textbackslash{}nrestrictions. Regulations such as the EU General Data Protection Rules (GDPR) impose strict requirements concerning\\textbackslash{}nthe protection of personal and privacy sensitive data. Therefore new approaches, such as the Personal Health Train\\textbackslash{}ninitiative, are emerging to utilize data right in their original repositories, circumventing the need to transfer data.\\textbackslash{}nResults: Circumventing limitations of previous systems, this paper proposes a configurable and automated schema\\textbackslash{}nextraction and publishing approach, which enables ad-hoc SPARQL query formulation against RDF triple stores\\textbackslash{}nwithout requiring direct access to the private data. The approach is compatible with existing Semantic Web-based\\textbackslash{}ntechnologies and allows for the subsequent execution of such queries in a safe setting under the data provider\\textbackslash{}u0092s\\textbackslash{}ncontrol. Evaluation with four distinct datasets shows that a configurable amount of concise and task-relevant schema,\\textbackslash{}nclosely describing the structure of the underlying data, was derived, enabling the schema introspection-assisted\\textbackslash{}nauthoring of SPARQL queries.\\textbackslash{}nConclusions: Automatically extracting and publishing data schema can enable the introspection-assisted creation of\\textbackslash{}ndata selection and integration queries. In conjunction with the presented system architecture, this approach can\\textbackslash{}nenable reuse of data from private repositories and in settings where agreeing upon a shared schema and encoding a\\textbackslash{}npriori is infeasible. As such, it could provide an important step towards reuse of data from previously inaccessible\\textbackslash{}nsources and thus towards the proliferation of data-driven methods in the biomedical domain.\\textbackslash{}nKeywords: Semantic web, Linked data, RDF, SPARQL, Schema extraction, Privacy, Data access, Distributed systems,\\textbackslash{}nQuery design, Personal health train, FAIR data\\textbackslash{}nBackground\\textbackslash{}nData-driven methods play an increasingly important role\\textbackslash{}nfor cost-efficient and timely research results and effec-\\textbackslash{}ntive decision support {[}2{]} throughout numerous domain\\textbackslash{}nsuch as economics {[}3{]}, education {[}4{]}, manufacturing {[}5{]},\\textbackslash{}nhealthcare and life sciences {[}6\\textbackslash{}u00968{]}.\\textbackslash{}nAt the same time, the data that build the founda-\\textbackslash{}ntion of these models oftentimes underlies strict sharing\\textbackslash{}n*Correspondence: gleim@cs.rwth-aachen.de\\textbackslash{}nThis work is an extended version of a paper previously published at the\\textbackslash{}nSeWeBMeDA-2018 workshop {[}1{]}.\\textbackslash{}n1Informatik 5, RWTH Aachen University, Ahornstr. 55, 52062 Aachen, Germany\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nrequirements. For example, in the sensitive healthcare\\textbackslash{}ndomain, although first responders, hospitals, and many\\textbackslash{}nother stakeholders already collect valuable data for data-\\textbackslash{}ndriven research and treatment today, large portions of this\\textbackslash{}ndata remain inaccessible to the majority of stakeholders\\textbackslash{}n\\textbackslash{}u0096 largely due to ethical, administrative, legal and political\\textbackslash{}nhurdles that render data sharing infeasible {[}9{]}. In prac-\\textbackslash{}ntice, this leads to an inability to access large amounts of\\textbackslash{}ndata crucial for a variety of tasks such as the optimiza-\\textbackslash{}ntion of decision support systems, first response systems\\textbackslash{}nand data-driven research. At the core of this issue lies the\\textbackslash{}nlack of an effective mechanism to allow for data access\\textbackslash{}nin a legally certain, sustainable and cost-efficient manner\\textbackslash{}nwithout extensive delays.\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\textbackslash{}ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\textbackslash{}nmade. The images or other third party material in this article are included in the article\\textbackslash{}u0092s Creative Commons licence, unless\\textbackslash{}nindicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}u0092s Creative Commons licence and your\\textbackslash{}nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\textbackslash{}nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\textbackslash{}nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\textbackslash{}navailable in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 2 of 15\\textbackslash{}nFor example, learning health systems, allowing for data-\\textbackslash{}ndriven research on sensitive data such as electronic health\\textbackslash{}nrecords (EHRs), have long been said to bear the poten-\\textbackslash{}ntial to \\textbackslash{}u0093fill major knowledge gaps about health care costs,\\textbackslash{}nthe benefits and risks of drugs and procedures, geo-\\textbackslash{}ngraphic variations, environmental health influences, the\\textbackslash{}nhealth of special populations, and personalized medicine.\\textbackslash{}u0094\\textbackslash{}n{[}10{]}. While a variety of such systems have been proposed\\textbackslash{}n{[}10\\textbackslash{}u009613{]}, practical implementation has so far not become\\textbackslash{}na reality, likely due to the aforementioned hurdles.\\textbackslash{}nIn order to enable data economy in privacy-sensitive\\textbackslash{}ndomains and effective reuse of existing data and research,\\textbackslash{}nnovel approaches are emerging to overcome these limi-\\textbackslash{}ntations. One of those approaches is the Personal Health\\textbackslash{}nTrain (PHT) framework {[}14{]}, which aims to bring algo-\\textbackslash{}nrithms and statistical models to data sources, rather than\\textbackslash{}nsharing data with the third parties such as researchers.\\textbackslash{}nThe main benefit of this approach is its ability of utilizing\\textbackslash{}nall the data, including the sensitive and private informa-\\textbackslash{}ntion, without data having to leave the original data source.\\textbackslash{}nA key challenge of this approach is that data users (such as\\textbackslash{}nresearchers) are required to develop their models without\\textbackslash{}nhaving a grasp of the actual data. Unless there are univer-\\textbackslash{}nsally agreed information models and data set descriptions,\\textbackslash{}nthere is a need to create and communicate a schema \\textbackslash{}u0096 that\\textbackslash{}nis information about the structure of the data \\textbackslash{}u0096 to enable\\textbackslash{}nwriting queries for heterogeneous data resources.\\textbackslash{}nThis work is embedded in our ongoing efforts support-\\textbackslash{}ning data reuse in healthcare environments and conducted\\textbackslash{}nas part of the SMITH {[}15{]} and DIFUTURE {[}16{]} projects.\\textbackslash{}nThe key contributions of this paper consist of an auto-\\textbackslash{}nmated approach for extracting task-relevant schema from\\textbackslash{}nRDF data sources for the efficient formulation of data\\textbackslash{}nselection and integration queries without direct access to\\textbackslash{}nthe data and a corresponding integration with an infor-\\textbackslash{}nmation system architecture that allows for the subsequent\\textbackslash{}nevaluation of that query in a secure enclave.\\textbackslash{}nIn the following, we describes some related work and\\textbackslash{}nthe basic foundations of our approach. Subsequently, we\\textbackslash{}noutline the motivation of our research, as well as the key\\textbackslash{}nchallenges of schema extraction from sensitive data with-\\textbackslash{}nout sacrificing privacy, followed by the description of our\\textbackslash{}nproposed schema extraction approach from existing data\\textbackslash{}nin the methods section. We then present a number of\\textbackslash{}nevaluation results of the proposed data selection and inte-\\textbackslash{}ngrationmethodology, based on the schema extracted from\\textbackslash{}na sample use case. After a discussion of our results, we fin-\\textbackslash{}nish with a conclusion of our results and a short outlook of\\textbackslash{}ndirections for future work.\\textbackslash{}nRelated work\\textbackslash{}nIn order to facilitate knowledge discovery for both\\textbackslash{}nhumans and machines, the FAIR data principles {[}17{]}\\textbackslash{}nhave been proposed: A set of guiding principles to make\\textbackslash{}nresearch and scientific data Findable, Accessible, Interop-\\textbackslash{}nerable, and Re-usable. These guidance principles promise\\textbackslash{}nto help in the discovery, access, integration and analysis of\\textbackslash{}ntask-appropriate scientific data and associated algorithms\\textbackslash{}nandworkflows. Thus, FAIR is gaining a lot of attention and\\textbackslash{}nincreasing adoption.\\textbackslash{}nCore to realizing these principles are Semantic Web\\textbackslash{}nTechnologies {[}18{]}, which provide a framework for data\\textbackslash{}nsharing and reuse by making the semantics of data\\textbackslash{}nmachine interpretable. Particularly the directed, graph-\\textbackslash{}nbased data model RDF {[}19\\textbackslash{}u009621{]} (built entirely upon the\\textbackslash{}nnotion of statements, i.e. data in the form of subject\\textbackslash{}npredicate object triples) in conjunction with formal\\textbackslash{}nconceptualizations of information models, semantics and\\textbackslash{}nencoding conventions in RDF vocabularies and ontologies\\textbackslash{}ntakes an important role.\\textbackslash{}nAs such, RDF Schema (RDFS) {[}22{]} and the Web Ontol-\\textbackslash{}nogy Language (OWL) {[}23{]} provide a proven framework in\\textbackslash{}norder to describe (but not necessarily enforce) the struc-\\textbackslash{}nture and semantics of data. Substantially, RDFS introduces\\textbackslash{}nthe concepts of classes and properties as well as basic rela-\\textbackslash{}ntions between them. OWL \\textbackslash{}u0096 a computational logic-based\\textbackslash{}nlanguage \\textbackslash{}u0096 extends upon these concepts in order to repre-\\textbackslash{}nsent rich and complex knowledge about things, groups of\\textbackslash{}nthings, and relations between them.\\textbackslash{}nIn the context of this work, we use the term \\textbackslash{}u0091schema\\textbackslash{}u0092\\textbackslash{}nto refer to the semantic and structural annotation of data\\textbackslash{}nusing especially these two vocabularies.\\textbackslash{}nOn the other hand, the classical notion of schema as\\textbackslash{}nthe formal definition of the shape that data needs to com-\\textbackslash{}nply with in order to be valid (i.e. schema validation and\\textbackslash{}nenforcement) also exists in the Semantic Web with the\\textbackslash{}nShape Expression Language (ShEx) {[}24{]} and the Shapes\\textbackslash{}nConstraint Language (SHACL) {[}25{]}. At this time, there\\textbackslash{}nare however no established ways of sharing data shapes\\textbackslash{}nthrough public repositories and as such, in practice, they\\textbackslash{}nare only adapted in isolated deployments.\\textbackslash{}nNevertheless, using RDFS and OWL, it is possible to\\textbackslash{}ncreate domain-specific, optionally interoperable vocabu-\\textbackslash{}nlaries and ontologies, which may declare e.g. term or con-\\textbackslash{}ncept equivalences and dependencies between each other\\textbackslash{}nand subsequently enable interoperability across individual\\textbackslash{}nencodings.\\textbackslash{}nKey to realizing the semantics described in RDFS\\textbackslash{}nand OWL vocabularies is the inference or entailment\\textbackslash{}nof implicit knowledge (inferred triples) that follow from\\textbackslash{}nexplicit knowledge (dataset triples) via the semantics\\textbackslash{}ndescribed in the corresponding vocabularies. Figure 1\\textbackslash{}nillustrates some of the inferred triples that follow from\\textbackslash{}nthe formal RDFS and OWL entailment semantics {[}26{]}.\\textbackslash{}nHere we assume the namespaces ex and snomed to be\\textbackslash{}ndefined1.\\textbackslash{}n1Likely the definitions would be http://example.org/ and http://purl.\\textbackslash{}nbioontology.org/ontology/SNOMEDCT/\\textbackslash{}nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 3 of 15\\textbackslash{}nFig. 1 Illustration of several effects of entailment support of a SPARQL endpoint\\textbackslash{}nIn this example, only a small number of triples is con-\\textbackslash{}ntained in the actual dataset, while the majority of knowl-\\textbackslash{}nedge is inferred using RDFS and OWL semantics. Notably\\textbackslash{}neach resource is either a class, a property or an individual,\\textbackslash{}ni.e. either schema or data.\\textbackslash{}nPopular examples of RDFS and OWL vocabularies\\textbackslash{}ninclude the Ontology for Biomedical Investigation (OBI)\\textbackslash{}n{[}27{]} in the biology and healthcare domain, the GoodRela-\\textbackslash{}ntions ontology {[}28{]} in eBusiness and theDCAT vocabulary\\textbackslash{}n{[}29{]}, which is used for the general purpose metadata\\textbackslash{}nannotation of datasets and data catalogs.\\textbackslash{}nIn the context of eHealth systems, support for the\\textbackslash{}nSemantic Web is becoming more and more promi-\\textbackslash{}nnent with candidates such as the multilingual thesaurus\\textbackslash{}nSNOMEDCT {[}30{]}, ongoing research efforts into an RDF\\textbackslash{}nspecification of HL7 FHIR {[}31{]}, as well as the establish-\\textbackslash{}nment of clear guidelines for dataset descriptions such as\\textbackslash{}nthe HCLS Community Profile {[}32{]}.\\textbackslash{}nVarious high-quality catalogs of freely reusable vocabu-\\textbackslash{}nlaries exist, allowing for the easy discovery of suitable ter-\\textbackslash{}nminology to semantically annotate data. Examples include\\textbackslash{}nthe Linked Open Vocabulary (LOV) {[}33\\textbackslash{}u009635{]} and the Bio-\\textbackslash{}nPortal {[}36\\textbackslash{}u009638{]} project.\\textbackslash{}nThe related idea of using schema export and import for\\textbackslash{}nfederated data access date back to as early as 1985 {[}39{]} but\\textbackslash{}nit is only recently that the idea has receivedmore attention\\textbackslash{}nin the context of the Semantic Web.\\textbackslash{}nKellou-Menouer et al. {[}40{]} propose a schema discov-\\textbackslash{}nery approach based on hierarchical clustering instead of\\textbackslash{}ndata annotations thus leading to an approximate schema.\\textbackslash{}nFlorenzano et al. {[}41{]}, Lohmann et al. {[}42, 43{]} and\\textbackslash{}nDudá\\textbackslash{}u009a et al. {[}44{]} introduce approaches focused on schema\\textbackslash{}nextraction for visualization of the data structure but do\\textbackslash{}nnot consider publishing or reuse of the extracted schema.\\textbackslash{}nBenedetti et al. {[}45, 46{]} propose an interesting related\\textbackslash{}napproach for schema extraction, visualization and query\\textbackslash{}ngeneration but do not consider interoperability issues and\\textbackslash{}nrely on custom mechanisms for schema storage.\\textbackslash{}nMotivation\\textbackslash{}nRecently, Jochems et al. {[}47{]} and Deist et al. {[}48{]}\\textbackslash{}nintroduced two related promising Semantic Web-based\\textbackslash{}napproaches in the context of the PHT initiative, founded\\textbackslash{}non the key concept of bringing research to the data rather\\textbackslash{}nthan bringing data to the research. As such the underly-\\textbackslash{}ning information system architecture enables learning from\\textbackslash{}nprivacy sensitive data without the data ever crossing orga-\\textbackslash{}nnizational boundaries, maintaining control over the data,\\textbackslash{}npreserving data privacy and thereby overcoming legal and\\textbackslash{}nethical issues common to other forms of data exchanges.\\textbackslash{}nThe general approach of this underlying system can be\\textbackslash{}noutlined as follows:\\textbackslash{}n1 Initially, both the client and data provider agree upon\\textbackslash{}na set of attributes or features, such that all\\textbackslash{}nparticipating data providers have corresponding\\textbackslash{}nsources of (privacy sensitive) data.\\textbackslash{}n2 Then each data provider encodes their data using an\\textbackslash{}n(also agreed upon) ontology or vocabulary,\\textbackslash{}nconverting it into RDF representation. This process\\textbackslash{}nyields proper Linked Data {[}49{]} and thus enables\\textbackslash{}nsemantic interoperability {[}50{]}.\\textbackslash{}n3 The resulting RDF data is deployed to a private triple\\textbackslash{}nstore at each location, providing a private SPARQL\\textbackslash{}n{[}51{]} query endpoint, which is not directly accessible\\textbackslash{}nby the client.\\textbackslash{}n4 A SPARQL data query is then formulated based on\\textbackslash{}nthe previously agreed upon encoding and a\\textbackslash{}ncorresponding distributable processing algorithm\\textbackslash{}ndefined.\\textbackslash{}n5 The shared query is then executed locally at each\\textbackslash{}ndata provider against their respective triple stores\\textbackslash{}nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 4 of 15\\textbackslash{}nand the returned data processed using the\\textbackslash{}ncorresponding algorithm.\\textbackslash{}n6 The local results are then combined into a global one.\\textbackslash{}n7 Depending on the approach, steps 5 and 6 may be\\textbackslash{}nfurther iterated.\\textbackslash{}nWhile these approaches \\textbackslash{}u0096 introduced in the context\\textbackslash{}nof the PHT initiative \\textbackslash{}u0096 work well when multiple parties\\textbackslash{}nagree on jointly collecting, encoding and evaluating data\\textbackslash{}nin advance \\textbackslash{}u0096 such as is the case for conducting individual\\textbackslash{}ncoordinated studies \\textbackslash{}u0096 they solve the issue of interoperabil-\\textbackslash{}nity by agreeing on a single shared knowledge representa-\\textbackslash{}ntion and encoding methodology a priori (steps 1-3 in the\\textbackslash{}nabove process). In an optimal setting where agreeing on a\\textbackslash{}nsingle shared and global information model and encoding,\\textbackslash{}nreuse of diverse and existing data could always be directly\\textbackslash{}naccomplished with this approach.\\textbackslash{}nHowever, to our knowledge, so far all corresponding\\textbackslash{}nefforts have been unsuccessful. At the time of writing\\textbackslash{}nthe popular https://fairsharing.org/ portal indexes 1084\\textbackslash{}ndatabases using 1183 standards, suggesting that in prac-\\textbackslash{}ntice, each collected dataset and domain much rather tends\\textbackslash{}nto introduce its own encoding methodology.\\textbackslash{}nAdditionally, RDF datasets de facto often combine terms\\textbackslash{}nfrom multiple vocabularies and ontologies, sometimes\\textbackslash{}ndeviating from the originally intended information mod-\\textbackslash{}nels and encodings.\\textbackslash{}nThus when trying to reuse diverse existing data, a proper\\textbackslash{}nunderstanding of the real structure of the available data \\textbackslash{}u0096\\textbackslash{}ni.e. the schema of the data \\textbackslash{}u0096 is indispensable. For a client\\textbackslash{}nwithout direct access to the data, this information is how-\\textbackslash{}never typically not available, since its acquisition inherently\\textbackslash{}nrelies upon inspection of the structure of the data.\\textbackslash{}nApproaches, such as the PHT, depend upon ad-hoc\\textbackslash{}ndata selection and integration facilities (step 4 of the\\textbackslash{}nPHT approach, corresponding to the first two steps of\\textbackslash{}nthe classical Knowledge Discovery in Databases (KDD)\\textbackslash{}nprocess {[}52{]}) for the efficient and effective extraction of\\textbackslash{}nknowledge from private data sources. In order to enable\\textbackslash{}nthe usage of such an approach with diverse existing\\textbackslash{}ndata, suitable methods for the extraction and distribution\\textbackslash{}ntask-specific schema, tailored specifically for the purpose\\textbackslash{}nof enabling ad-hoc data selection and integration, are\\textbackslash{}nneeded.\\textbackslash{}nMethods\\textbackslash{}nIn this section, we propose an automated approach for\\textbackslash{}nextracting task-specific schema from RDF data sources in\\textbackslash{}norder to enable the efficient formulation of SPARQL data\\textbackslash{}nselection and integration queries without direct access to\\textbackslash{}nthe data. First, we describe basic requirements for the\\textbackslash{}nextracted schema, as well as the fundamental idea of the\\textbackslash{}nschema extraction technique before subsequently intro-\\textbackslash{}nducing a number of extensions, in order to support for\\textbackslash{}nmore generally applicable schema extraction methodol-\\textbackslash{}nogy. We discuss the trade-offs to be made between differ-\\textbackslash{}nent versions of the schema extraction approach and finally\\textbackslash{}nshow how the extracted schema can be used further for\\textbackslash{}nthe data selection and integration.\\textbackslash{}nIn the context of RDF data, the fundamental knowl-\\textbackslash{}nedge required for the creation of SPARQL queries for\\textbackslash{}ndata selection and integration consists of the various\\textbackslash{}nrdf:type objects, the rdf:Property predicates and the struc-\\textbackslash{}ntural relations between them. This information can itself\\textbackslash{}nbe represented using Semantic Web Standards, such as\\textbackslash{}nRDFS, OWL, ShEx or SHACL.\\textbackslash{}nWhile shape languages such as ShEx and SHACL\\textbackslash{}nare natural candidates for representing prescriptive data\\textbackslash{}nschema, they are designed specifically for the validation of\\textbackslash{}nclearly structured individual data shapes and to commu-\\textbackslash{}nnicate explicit graph patterns. As such they are however\\textbackslash{}nnot equally well suited for the formalization of the flexible\\textbackslash{}nschema of entire semi-structured datasets.\\textbackslash{}nRDFS on the other hand provides a simple and descrip-\\textbackslash{}ntive structural annotation of the relationships between\\textbackslash{}nproperties and classes and as such serves as a promising\\textbackslash{}ncandidate for the task at hand.\\textbackslash{}nWhile OWL further extends RDFS with a pow-\\textbackslash{}nerful set of description logic-based modeling prim-\\textbackslash{}nitives, the corresponding semantic complexity adds\\textbackslash{}nsignificant overhead to the schema extraction pro-\\textbackslash{}ncess. Especially since the extracted schema is only\\textbackslash{}nmeant to be used for query authoring and explic-\\textbackslash{}nitly not for reasoning, in the context of this work\\textbackslash{}nwe generally restrict our effort to extracting schema\\textbackslash{}nusing RDFS and the OWL owl:equivalentClass,\\textbackslash{}nowl:equivalentProperty and owl:sameAs pred-\\textbackslash{}nicates, which we deem most relevant in order to enable\\textbackslash{}ninteroperability and the effective formulation of selection\\textbackslash{}nand integration queries.\\textbackslash{}nEspecially in order to ensure interoperability with\\textbackslash{}nexisting Semantic Web technologies and compatibility\\textbackslash{}nwith standard Semantic Web tools, such as schema-\\textbackslash{}nintrospection-assisted SPARQL query builders, the\\textbackslash{}nextracted schema should thus be available as a simple\\textbackslash{}nRDFS and OWL vocabulary via a SPARQL endpoint.\\textbackslash{}nSchema-introspection refers to the process of examin-\\textbackslash{}ning the schema definition to determine which types of\\textbackslash{}nentities exist, which properties are defined upon them and\\textbackslash{}nsubsequently, what can be queried for. Since the schema\\textbackslash{}nneeded to create data queries (e.g. using SPARQL) only\\textbackslash{}ncontains basic structural information about the original\\textbackslash{}ndata, it also conveys far less privacy critical information\\textbackslash{}nthan exposing the actual data. As such it can be published\\textbackslash{}npublicly without privacy concerns in many scenarios.\\textbackslash{}nIn the following, we describe an automated approach for\\textbackslash{}nschema extraction fromRDF data which allows for the for-\\textbackslash{}nmulation of data selection and integration queries without\\textbackslash{}nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 5 of 15\\textbackslash{}ndirect access to the data and the subsequent evaluation of\\textbackslash{}nthat query in a secure enclave.\\textbackslash{}nSchema extraction\\textbackslash{}nWe propose an approach for schema extraction based\\textbackslash{}non exploiting key characteristics of RDF, RDFS, and\\textbackslash{}nOWL. RDF data encoded in compliance with correspond-\\textbackslash{}ning vocabularies inherently include metadata about their\\textbackslash{}nsemantics and structural relationships.\\textbackslash{}nFor the schema extraction, the rdf:type relation plays\\textbackslash{}nthe key role, as it declares data points to be instances of\\textbackslash{}nspecific data types or, according to RDFS terminology and\\textbackslash{}nsemantics {[}53{]}, classes. Anything that is a type in the sense\\textbackslash{}nof occurring as the target of this relation should thus auto-\\textbackslash{}nmatically becomes part of the schema as an entity of type\\textbackslash{}nrdfs:Class. Additionally, any property relation (that\\textbackslash{}nis any identifier occurring in the predicate position of a\\textbackslash{}nsubject-predicate-object triple) which occurs in the data\\textbackslash{}nshould be included as an entity of type rdf:Property.\\textbackslash{}nFinally all directly describing properties of these classes\\textbackslash{}nand properties should be included as well. For the scope\\textbackslash{}nof this work, we assume that all data in the private data\\textbackslash{}nrepository is sensitive and should remain private.\\textbackslash{}nEntailment supported schema extraction Assuming\\textbackslash{}nperfect conditions, namely proper inclusion of all used\\textbackslash{}nvocabularies into the triple store, correct usage of those\\textbackslash{}nvocabularies, as well as OWL entailment {[}26{]} support of\\textbackslash{}nthe SPARQL endpoint providing access to the data, the\\textbackslash{}nentire schema of a given RDF data set can be extracted\\textbackslash{}nusing a single simple SPARQL CONSTRUCT query as\\textbackslash{}ndepicted in Listing 1.\\textbackslash{}n\\textbackslash{}002\\textbackslash{}nCONSTRUCT \\{?s ?p ?o\\}\\textbackslash{}nWHERE \\{\\textbackslash{}n\\{{[}{]} ?s {[}{]}\\}\\textbackslash{}nUNION \\{{[}{]} a ?s\\} .\\textbackslash{}n?s ?p ?o .\\textbackslash{}n\\}\\textbackslash{}n\\textbackslash{}003 \\textbackslash{}004\\textbackslash{}nListing 1 SPARQL schema extraction query relying on proper\\textbackslash{}nentailment support of the endpoint.\\textbackslash{}nNote that we explicitly define the relevant subset of all\\textbackslash{}navailable schema information to be that which is actually\\textbackslash{}nused in the data, i.e. the instantiated schema, and thus only\\textbackslash{}nextract that.\\textbackslash{}nThe preceding query constructs an RDF graph (line 1)\\textbackslash{}ncontaining all the directly describing triples ?s ?p ?o\\textbackslash{}nthat occur in the tripe store but having only the following\\textbackslash{}nsubjects:\\textbackslash{}n1 Instantiated RDF properties ?s (line 3) which\\textbackslash{}naccording to RDF 1.1 Semantics {[}53{]} are any IRI used\\textbackslash{}nin predicate position (c.f. rdfD2).\\textbackslash{}n2 Instantiated RDFS classes ?s (line 4) via their\\textbackslash{}noccurrence as the object of a triple with rdf:type\\textbackslash{}nas the predicate. The fact that these are RDFS classes\\textbackslash{}nfollows directly from the RDFS axiomatic triple\\textbackslash{}nrdf:type rdfs:range rdfs:Class . in\\textbackslash{}nconjunction with RDFS entailment pattern rdfs3\\textbackslash{}n{[}53{]}.\\textbackslash{}nAccording to the SPARQL entailment regime, all the\\textbackslash{}nsubclass relationships, transitive properties, equivalences\\textbackslash{}netc. used in the data are automatically materialized (i.e.\\textbackslash{}nincluded in the dataset as inferred knowledge as illus-\\textbackslash{}ntrated in Fig. 1) and thus resolved and included too (c.f.\\textbackslash{}n{[}53, 54{]}).\\textbackslash{}nIt should be noted that the query only extracts direct\\textbackslash{}nproperties (i.e. triples ?s ?p ?o directly related to the\\textbackslash{}nsubject ?s) and as such, some complex constraints such\\textbackslash{}nas OWL disjointness axioms are not included in the\\textbackslash{}nextracted schema. However, as stated before, for the task\\textbackslash{}nof query formulation we consider this to be sufficient.\\textbackslash{}nDirectly instantiated schema Since in practice few\\textbackslash{}nSPARQL endpoints actually support any kind of entail-\\textbackslash{}nment and usually do not materialize implicit triples, the\\textbackslash{}napplicability of this basic approach is limited. While the\\textbackslash{}noriginal query can theoretically also be executed with-\\textbackslash{}nout entailment support, it does not guarantee that all\\textbackslash{}nused properties and classes are annotated accordingly\\textbackslash{}nas rdf:Property and rdf:Class and completely\\textbackslash{}nignores any resource ?s that lacks further describing\\textbackslash{}ntriples ?s ?p ?o.\\textbackslash{}nThus, in the following we introduce several revisions\\textbackslash{}nof the initial extraction query 1 that allow us to reintro-\\textbackslash{}nduce the missing triples without relying upon entailment\\textbackslash{}nsupport. Additionally, many datasets de facto employ\\textbackslash{}nterms from a number of different vocabularies and ontolo-\\textbackslash{}ngies and deviate from the originally intended informa-\\textbackslash{}ntion model. Since the availability of information about\\textbackslash{}ndomain and range of the different properties employed\\textbackslash{}nin the dataset is especially relevant in order to assist the\\textbackslash{}nquery creation process, we further explicitly construct\\textbackslash{}nrdfs:domain and rdfs:range statements according\\textbackslash{}nto the property\\textbackslash{}u0092s respective usage in the dataset.\\textbackslash{}nIn scenarios where it is sufficient to consider only those\\textbackslash{}ntypes and properties that are directly used in the dataset\\textbackslash{}nor where no information whatsoever about the employed\\textbackslash{}nvocabularies is available, it can be reasonable to disregard\\textbackslash{}nthe inference generalizations and equivalences entirely.\\textbackslash{}nListing 2 proposes a SPARQL query for the extraction\\textbackslash{}nof a corresponding schema, which closely reflects the\\textbackslash{}nstructure of the underlying data and works even if the\\textbackslash{}ndefinitions of the employed ontologies are unavailable.\\textbackslash{}nFor this and all further queries, we assume standard\\textbackslash{}nSPARQL namespace and prefix definitions as specified by\\textbackslash{}nthe World Wide Web Consortium\\textbackslash{}u0092s OWL and SPARQL\\textbackslash{}nspecifications {[}53, 55{]}.\\textbackslash{}nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 6 of 15\\textbackslash{}n\\textbackslash{}002\\textbackslash{}nCONSTRUCT \\{\\textbackslash{}n?predicate ?a ?b; a rdf:Property;\\textbackslash{}nrdfs:domain ?pDomain; rdfs:range ?\\textbackslash{}npRange.\\textbackslash{}n?concept ?c ?d; a rdfs:Class.\\textbackslash{}n\\} WHERE \\{\\textbackslash{}n?s ?predicate ?o.\\textbackslash{}nOPTIONAL \\{?s a ?pDomain\\}\\textbackslash{}nOPTIONAL \\{?o a ?pRange\\}\\textbackslash{}nOPTIONAL \\{?predicate ?a ?b\\}\\textbackslash{}n{[}{]} a ?concept\\textbackslash{}nFILTER(!isBlank(?concept))\\textbackslash{}nOPTIONAL \\{?concept ?c ?d\\}\\textbackslash{}n\\}\\textbackslash{}n\\textbackslash{}003 \\textbackslash{}004\\textbackslash{}nListing 2 Basic SPARQL schema extraction query which only\\textbackslash{}ndiscovers RDFS classes and properties directly instantiated in the\\textbackslash{}nqueried dataset.\\textbackslash{}nAnalogously to query 1, we detect predicates as any\\textbackslash{}nInternationalized Resource Identifier (IRI) used in predi-\\textbackslash{}ncate position (line 5) and classes as IRIs used as objects of\\textbackslash{}nRDF type triples (line 9). We also include any additional\\textbackslash{}ninformation directly relating to those subjects that might\\textbackslash{}nbe available in the dataset (lines 8 and 11). To explicitly\\textbackslash{}nconstruct rdfs:domain and rdfs:range information\\textbackslash{}nof the predicates, we further determine the rdf:type\\textbackslash{}nof each subject (line 6) and object (line 7), if available.\\textbackslash{}nAdditionally we filter out any class declarations without\\textbackslash{}nan own identifier (line 10) to avoid potential referenc-\\textbackslash{}ning issues with the extracted schema. Lastly we construct\\textbackslash{}nthe schema graph as all discovered predicates (explicitly\\textbackslash{}ntyped as rdf:Property) and their related informa-\\textbackslash{}ntion (line 2) and all discovered classes (explicitly typed as\\textbackslash{}nrdfs:Class) and their related information (line 3).\\textbackslash{}nWhen applying this extraction approach to the dataset\\textbackslash{}ndepicted in Fig. 1, we end up with the schema depicted in\\textbackslash{}nFig. 2 where classes are highlighted in blue and properties\\textbackslash{}nin green (i.e. with implicit rdf:type triples).\\textbackslash{}nSubsequently, in this exemplary use case, following the\\textbackslash{}nextracted schema closely one could query for instances\\textbackslash{}nof the ex:Patient class and their corresponding prop-\\textbackslash{}nerty ex:treatedAt, which however perfectly reflects\\textbackslash{}nthe available dataset without inferred knowledge.\\textbackslash{}nIt should be noted, that this extracted schema is explic-\\textbackslash{}nitly not suited for triple entailment according to RDFS\\textbackslash{}nsemantics, due to the conjunctive nature of multiple\\textbackslash{}nrdfs:domain and rdfs:range definitions on prop-\\textbackslash{}nerties (c.f. RDFS entailment patterns rdfs2 and rdfs3\\textbackslash{}n{[}53{]}). A semantically correct alternative would be the\\textbackslash{}nusage of Schema.org\\textbackslash{}u0092s schema:domainIncludes and\\textbackslash{}nschema:rangeIncludes properties in line 2, instead\\textbackslash{}nof their RDFS equivalents. However, since RDFS domain\\textbackslash{}nand range semantics are implemented in a variety of tools\\textbackslash{}nfor schema exploration, visualization and assisted query\\textbackslash{}nauthoring {[}56\\textbackslash{}u009658{]}, while schema.org semantics are not\\textbackslash{}nequally well supported, we deliberately defer semantic\\textbackslash{}ncorrectness to a closer representation of the underlying\\textbackslash{}ndata\\textbackslash{}u0092s structure.\\textbackslash{}nLocally inferred schema In order to re-include previ-\\textbackslash{}nously inferred information such as additional types and\\textbackslash{}nclasses due to sub-property, subclass, domain, range or\\textbackslash{}nequivalence relationships, we can extract the relevant\\textbackslash{}nschema directly from the data and the full definitions of\\textbackslash{}nthe employed ontologies using the SPARQL 1.1 Property\\textbackslash{}nPaths {[}59{]} feature, independent of entailment support or\\textbackslash{}nstatement materialization on the endpoint.\\textbackslash{}nA corresponding SPARQL query is depicted in Listing 3.\\textbackslash{}n\\textbackslash{}002\\textbackslash{}nWHERE \\{\\textbackslash{}n?s ?x ?o. OPTIONAL \\{?s a ?pDomain\\}\\textbackslash{}nOPTIONAL \\{?o a ?pRange\\}\\textbackslash{}n?x (rdfs:subPropertyOf\\textbar{}owl:\\textbackslash{}nequivalentProperty\\textbar{}\\textasciicircum{}owl:\\textbackslash{}nequivalentProperty\\textbackslash{}n\\textbar{}owl:sameAs\\textbar{}\\textasciicircum{}owl:sameAs)* ?predicate\\textbackslash{}nOPTIONAL \\{?predicate ?a ?b\\}\\textbackslash{}n\\{?predicate (rdfs:range\\textbar{}rdfs:domain) ?y\\}\\textbackslash{}nUNION \\{{[}{]} a ?y\\}\\textbackslash{}n?y (rdfs:subClassOf\\textbar{}owl:equivalentClass\\textbar{}\\textasciicircum{}\\textbackslash{}nowl:equivalentClass\\textbar{}owl:\\textbackslash{}nsameAs\\textbar{}\\textasciicircum{}owl:sameAs)* ?concept\\textbackslash{}nFILTER(!isBlank(?concept))\\textbackslash{}nOPTIONAL \\{?concept ?c ?d\\}\\}\\textbackslash{}n\\textbackslash{}003 \\textbackslash{}004\\textbackslash{}nListing 3 Extended WHERE clause of schema extraction query 2\\textbackslash{}nemploying SPARQL 1.1 Property Paths to emulate RDFS\\textbackslash{}nspecialization, domain and range semantics, as well as OWL\\textbackslash{}nequivalence entailment.\\textbackslash{}nThe query constructs a graph, which in addition to all\\textbackslash{}ninstantiated RDFS classes and RDF properties (and their\\textbackslash{}ndirect properties) includes generalizations and equivalent\\textbackslash{}nresources of those via RDFS and OWL semantics.\\textbackslash{}nFor both properties and classes, we resolve corre-\\textbackslash{}nsponding generalizations directly using the relevant\\textbackslash{}nRDFS entailment patterns (rdfs5, rdfs7, rdfs9, rdfs11)\\textbackslash{}n{[}53{]} and concept equivalences using OWL\\textbackslash{}u0092s owl:\\textbackslash{}nequivalentClass, owl:equivalentProperty\\textbackslash{}nand owl:sameAs predicates {[}54{]} in lines 5 and 9. While\\textbackslash{}nowl:sameAs is only supposed to be used for the decla-\\textbackslash{}nration of equivalence between individuals, it is commonly\\textbackslash{}nmisused in practice and as such deliberately included in\\textbackslash{}nthis query.\\textbackslash{}nrdfs:Class annotations are further inferred follow-\\textbackslash{}ning RDFS entailment rules rdfs2 and rdfs3 {[}53{]} from\\textbackslash{}nrdfs:domain and rdfs:range properties declared\\textbackslash{}non instantiated rdf:Property resources (line 7).\\textbackslash{}nWhen applying this extraction approach to the dataset\\textbackslash{}ndepicted in Fig. 1, we end up with the relevant schema\\textbackslash{}ndepicted in Fig. 3. As before, classes are highlighted in blue\\textbackslash{}nand properties in green.\\textbackslash{}nFollowing the extracted schema, it is now also possible\\textbackslash{}nto query for instances of the hospital and person classes, as\\textbackslash{}nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 7 of 15\\textbackslash{}nFig. 2 Directly instantiated schema extracted from Example 1\\textbackslash{}nwell as a number of equivalent SNOMEDCT vocabulary\\textbackslash{}nterms.\\textbackslash{}nEmploying terminology services\\textbackslash{}nIn practice, individual SPARQL endpoints providing\\textbackslash{}naccess to individual datasets cannot be (and are not) bur-\\textbackslash{}ndened with serving all vocabularies and terminologies\\textbackslash{}nused in the dataset and related to those. That is the pur-\\textbackslash{}npose of specialized terminology services and vocabulary\\textbackslash{}ncatalogs, such as the aforementioned LOV and BioPortal\\textbackslash{}nprojects.\\textbackslash{}nIn order to resolve equivalences and generalizations\\textbackslash{}nacross vocabularies, it is thus possible to make use of the\\textbackslash{}nSPARQL 1.1 Federated Query protocol {[}60, 61{]} in order\\textbackslash{}nto entail additional schema triples using external termi-\\textbackslash{}nnology services. The query depicted in Listing 4 employs\\textbackslash{}nfederated queries to the SPARQL endpoint http://\\textbackslash{}nexample.org/terminology in order to accomplish\\textbackslash{}nthis. The query further explicitly filters out all subject that\\textbackslash{}nare blank nodes in order to avoid renaming and resolu-\\textbackslash{}ntion issues between blank nodes from different sources\\textbackslash{}n(c.f. {[}60{]}).\\textbackslash{}nWhile the approach follows the same principles as the\\textbackslash{}npreviously introduced local inference (c.f. Listing 3), here\\textbackslash{}neach inference step also includes results from the exter-\\textbackslash{}nnal terminology service. As such, following the exam-\\textbackslash{}nple from before, the extracted schema would now also\\textbackslash{}ninclude all inferred knowledge from the SNOMEDCT\\textbackslash{}nvocabulary as well as any vocabulary known to the\\textbackslash{}nterminology service that declares equivalences with\\textbackslash{}nSNOMEDCT.\\textbackslash{}nIn some cases, such as with rare diseases, even the\\textbackslash{}nlimited communication with remote terminology services\\textbackslash{}nmight affect data privacy, since the instantiation of cer-\\textbackslash{}ntain very rare classes or predicates might in itself reveal\\textbackslash{}nprivate data. In such cases a local terminology service can\\textbackslash{}nbe employed, i.e. by creating a local deployment of the\\textbackslash{}nLOV service or by providing local copies of the relevant\\textbackslash{}nfull vocabularies. Nevertheless, sharing of the extracted\\textbackslash{}nschema in such cases may still require additional consid-\\textbackslash{}nerations.\\textbackslash{}nUnfortunately, current implementations of federated\\textbackslash{}nSPARQL queries still typically incur large performance\\textbackslash{}npenalties by using suboptimal resolution strategies. As\\textbackslash{}nsuch, in practice, it is often helpful tomanually decompose\\textbackslash{}nthe single query into multiple query steps. An exem-\\textbackslash{}nplary four-step approach using the SPARQL 1.1 UPDATE\\textbackslash{}nconstruct {[}62, 63{]} can be found in the supplementary\\textbackslash{}nFig. 3 Locally inferred relevant schema extracted from example 1\\textbackslash{}nGleim et al. Journal of Biomedical Semantics            (2020) 11:6 Page 8 of 15\\textbackslash{}nmaterials2, which also includes performance optimized\\textbackslash{}nreformulations of the other queries.\\textbackslash{}n\\textbackslash{}002\\textbackslash{}nWHERE \\{\\textbackslash{}n?s ?x ?o.\\textbackslash{}nOPTIONAL \\{?s a ?pDomain\\}\\textbackslash{}nOPTIONAL \\{?o a ?pRange\\}\\textbackslash{}n\\{?x (rdfs:subPropertyOf\\textbar{}owl:'\n\\item[text7] 'Grabar et al. Journal of Biomedical Semantics            (2020) 11:7 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00225-x\\textbackslash{}nRESEARCH Open Access\\textbackslash{}nCAS: corpus of clinical cases in French\\textbackslash{}nNatalia Grabar1,2*\\textbackslash{}u0086, Clément Dalloux3\\textbackslash{}u0086 and Vincent Claveau3\\textbackslash{}u0086\\textbackslash{}nAbstract\\textbackslash{}nBackground: Textual corpora are extremely important for various NLP applications as they provide information\\textbackslash{}nnecessary for creating, setting and testing those applications and the corresponding tools. They are also crucial for\\textbackslash{}ndesigning reliablemethods and reproducible results. Yet, in some areas, such as themedical area, due to confidentiality\\textbackslash{}nor to ethical reasons, it is complicated or even impossible to access representative textual data. We propose the CAS\\textbackslash{}ncorpus built with clinical cases, such as they are reported in the published scientific literature in French.\\textbackslash{}nResults: Currently, the corpus contains 4,900 clinical cases in French, totaling nearly 1.7M word occurrences. Some\\textbackslash{}nclinical cases are associated with discussions. A subset of the whole set of cases is enriched with morpho-syntactic\\textbackslash{}n(PoS-tagging, lemmatization) and semantic (the UMLS concepts, negation, uncertainty) annotations. The corpus is\\textbackslash{}nbeing continuously enriched with new clinical cases and annotations. The CAS corpus has been compared with\\textbackslash{}nsimilar clinical narratives. When computed on tokenized and lowercase words, the Jaccard index indicates that the\\textbackslash{}nsimilarity between clinical cases and narratives reaches up to 0.9727.\\textbackslash{}nConclusion: We assume that the CAS corpus can be effectively exploited for the development and testing of NLP\\textbackslash{}ntools and methods. Besides, the corpus will be used in NLP challenges and distributed to the research community.\\textbackslash{}nKeywords: Medical area, Natural language processing, Corpus with clinical cases, Morpho-syntactic and semantic\\textbackslash{}nannotation, Sustainability, Reproducibility\\textbackslash{}nBackground\\textbackslash{}nTextual corpora are central for various NLP applications\\textbackslash{}nas they provide information necessary for creating, set-\\textbackslash{}nting, testing and validating these applications, the cor-\\textbackslash{}nresponding tools, and the results. Yet, in some areas,\\textbackslash{}ndue to confidentiality or to ethical reasons, it is compli-\\textbackslash{}ncated or even impossible to access representative textual\\textbackslash{}ndata typically created and used by the actors of these\\textbackslash{}nareas. For instance, medical and legal areas are concerned\\textbackslash{}nwith these issues: in the legal area, information on law-\\textbackslash{}nsuits and trials remains confidential, while in the medical\\textbackslash{}narea, medical confidentiality must be respected by the\\textbackslash{}nmedical staff. In both situations, personal data cannot\\textbackslash{}nbe made publicly available, which prevents corpora from\\textbackslash{}n*Correspondence: natalia.grabar@univ-lille.fr\\textbackslash{}n\\textbackslash{}u0086Natalia Grabar, Clé Dalloux and Vincent Claveau contributed equally to this\\textbackslash{}nwork.\\textbackslash{}n1CNRS, UMR 8163, F-59000 Lille, France\\textbackslash{}n2Univ. Lille, UMR 8163 - STL - Savoirs Textes Langage, F-59000 Lille, France\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nbeing released and makes experiments non-reproducible\\textbackslash{}nby other researchers and with other methods. To face such\\textbackslash{}nsituations, Natural Language Processing (NLP) proposes\\textbackslash{}nspecific methods and tools. Hence, for several years now,\\textbackslash{}nanonymization and de-identification methods and tools\\textbackslash{}nhave been made available and provide competitive and\\textbackslash{}nreliable results {[}1\\textbackslash{}u00964{]} reaching up to 90\\% precision and\\textbackslash{}nrecall. But it may still be difficult to access de-identified\\textbackslash{}ndocuments and use them for research. One reason is that\\textbackslash{}nthere is a risk of re-identification of people, and more\\textbackslash{}nparticularly of patients {[}5, 6{]} because medical histories\\textbackslash{}ncan be unique. In consequence, the application of de-\\textbackslash{}nidentification tools on personal data often does not permit\\textbackslash{}nto make the data freely available and usable within the\\textbackslash{}nresearch context.\\textbackslash{}nYet, there is a real need for the development of methods\\textbackslash{}nand tools for several applications suited for such restricted\\textbackslash{}nareas. For instance, in the medical area, it is impor-\\textbackslash{}ntant to design suitable tools for information retrieval and\\textbackslash{}nextraction, for recruiting patients for clinical trials, for\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\textbackslash{}ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\textbackslash{}nmade. The images or other third party material in this article are included in the article\\textbackslash{}u0092s Creative Commons licence, unless\\textbackslash{}nindicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}u0092s Creative Commons licence and your\\textbackslash{}nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\textbackslash{}nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\textbackslash{}nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\textbackslash{}navailable in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}nGrabar et al. Journal of Biomedical Semantics            (2020) 11:7 Page 2 of 10\\textbackslash{}nperforming several other important tasks such as index-\\textbackslash{}ning, study of temporality, negation, etc. {[}7\\textbackslash{}u009613{]}. Another\\textbackslash{}nimportant issue is related to the reliability of tools and\\textbackslash{}nto the reproducibility of study results across similar data\\textbackslash{}nfrom different sources. The scientific research and clin-\\textbackslash{}nical communities are indeed increasingly coming under\\textbackslash{}ncriticism for the lack of reproducibility in the biomedical\\textbackslash{}narea {[}14\\textbackslash{}u009616{]}, but notice that, for instance, psychology is\\textbackslash{}nconcerned with this issue as well {[}17\\textbackslash{}u009619{]}. The first step\\textbackslash{}ntowards the reproducibility of results is the availability of\\textbackslash{}nfreely usable tools and corpora. In the current contribu-\\textbackslash{}ntion, we are mainly concerned with the construction of\\textbackslash{}nfreely available corpora for the medical domain. Yet, we\\textbackslash{}nare aware that sharing tools and methods is also impor-\\textbackslash{}ntant. We assume that availability of corpora may boost the\\textbackslash{}ndesign and dissemination of other resources, methods and\\textbackslash{}ntools for biomedical tasks and applications.\\textbackslash{}nThe purpose of our work is to introduce the CAS cor-\\textbackslash{}npus, that contains clinical cases in French such as those\\textbackslash{}npublished in scientific literature or used in the education\\textbackslash{}nand training of medical students. In what follows, we first\\textbackslash{}npresent some existing studies onmedical corpora creation\\textbackslash{}n(\\textbackslash{}u0093Existing work: freely available clinical corpora\\textbackslash{}u0094), high-\\textbackslash{}nlighting corpora which are freely available for research.\\textbackslash{}nWe then present the methods used for building, annota-\\textbackslash{}ntion and analysis of the CAS corpus with clinical cases in\\textbackslash{}nFrench (\\textbackslash{}u0093Methods\\textbackslash{}u0094). The results are presented in \\textbackslash{}u0093Results\\textbackslash{}u0094\\textbackslash{}nand discussed in \\textbackslash{}u0093Discussion\\textbackslash{}u0094. We conclude with some\\textbackslash{}ndirections for future work (\\textbackslash{}u0093Conclusion\\textbackslash{}u0094 sections). The\\textbackslash{}nwork presented in this article is an extended and updated\\textbackslash{}nversion of our previous publication {[}20{]}.\\textbackslash{}nExisting work: freely available clinical corpora\\textbackslash{}nWithin the medical area, we can distinguish two main\\textbackslash{}ntypes of medical corpora: scientific and clinical.\\textbackslash{}n\\textbackslash{}u0095 Scientific corpora are issued from scientific\\textbackslash{}npublications and reporting. Such corpora are\\textbackslash{}nbecoming increasingly available to researchers thanks\\textbackslash{}nto recent and less recent initiatives dedicated to open\\textbackslash{}npublication, such as those promoted by the NLM\\textbackslash{}n(National Library of Medicine) through the PUBMED\\textbackslash{}nportal1 and specifically dedicated to the biomedical\\textbackslash{}narea, and by the HAL2 and ISTEX3 initiatives, which\\textbackslash{}nprovide generic portals for accessing scientific\\textbackslash{}npublications from various areas, including medicine.\\textbackslash{}nSuch corpora contain scientific publications that\\textbackslash{}ndescribe research studies: motivation, methods,\\textbackslash{}nresults and issues on precise research questions.\\textbackslash{}nOther portals may also provide access to scientific\\textbackslash{}nliterature aimed at specific purposes, namely indexing\\textbackslash{}n1https://www.ncbi.nlm.nih.gov/pubmed\\textbackslash{}n2https://hal.archives-ouvertes.fr/\\textbackslash{}n3https://www.istex.fr/\\textbackslash{}nreliable literature, such as proposed by HON {[}21{]},\\textbackslash{}nCISMEF {[}22{]}, and other similar initiatives {[}23{]}. Some\\textbackslash{}nexisting scientific corpora also provide annotations\\textbackslash{}nand categorizations, such as PoS-tagging {[}24{]} and\\textbackslash{}nnegation {[}25{]}. These are often built for the purposes\\textbackslash{}nof shared tasks {[}26, 27{]}.\\textbackslash{}n\\textbackslash{}u0095 Clinical corpora are related to hospital and clinical\\textbackslash{}nevents of patients. Such corpora typically contain\\textbackslash{}ndocuments that describe medical history of patients\\textbackslash{}nand the medical care they are undergoing. This kind\\textbackslash{}nof corpora is typically created and used in clinical\\textbackslash{}ncontext as part of the healthcare process. Even after\\textbackslash{}nde-identification, it is complicated to obtain free\\textbackslash{}naccess to this kind of medical data and, for this\\textbackslash{}nreason, there are very few clinical corpora freely\\textbackslash{}navailable for research.\\textbackslash{}nIn our work, we are mainly interested in clinical cor-\\textbackslash{}npora: the proposed literature review of the existing work\\textbackslash{}nis aimed at clinical corpora that are freely available for\\textbackslash{}nresearch. We present here the main existing clinical cor-\\textbackslash{}npora:\\textbackslash{}n\\textbackslash{}u0095 MIMIC (Medical Information Mart for Intensive\\textbackslash{}nCare), now available in its third version, provides the\\textbackslash{}nlargest available set of structured and unstructured\\textbackslash{}nclinical data in English. MIMIC III is a single-center\\textbackslash{}ndatabase comprising information pertaining to\\textbackslash{}npatients admitted in critical care units at a large\\textbackslash{}ntertiary care hospital. Those data include vital signs,\\textbackslash{}nmedications, laboratory measurements, observations\\textbackslash{}nand notes charted by care providers, fluid balance,\\textbackslash{}nprocedure codes, diagnostic codes, imaging reports,\\textbackslash{}nhospital length of stay, survival data, and more. The\\textbackslash{}ndatabase supports applications including academic\\textbackslash{}nand industrial research, quality improvement\\textbackslash{}ninitiatives, and higher education coursework {[}28{]}.\\textbackslash{}nThose data are widely used by researchers, for\\textbackslash{}ninstance for predicting mortality {[}29, 30{]}, for\\textbackslash{}ndiagnosis identification and encoding {[}31, 32{]}, for\\textbackslash{}nstudies on temporality {[}33{]} or for identifying similar\\textbackslash{}nclinical notes {[}34{]}, to cite just a few existing studies.\\textbackslash{}nData from these corpora are also used in challenges,\\textbackslash{}nsuch as i2b2, n2c2 and CLEF-eHEALTH.\\textbackslash{}n\\textbackslash{}u0095 i2b2 (Informatics for Integrating Biology and the\\textbackslash{}nBedside)4 is an NIH-funded initiative promoting the\\textbackslash{}ndevelopment and test of NLP tools for\\textbackslash{}nEnglish-language documents with the purpose of\\textbackslash{}nhealthcare improvement. In order to enhance the\\textbackslash{}nability of NLP tools to process fine-grained\\textbackslash{}ninformation from clinical records, i2b2 challenges\\textbackslash{}nprovide sets of fully de-identified clinical notes\\textbackslash{}nenriched with specific annotations {[}9, 11, 35{]}, such as:\\textbackslash{}n4https://www.i2b2.org/NLP/DataSets/Main.php\\textbackslash{}nGrabar et al. Journal of Biomedical Semantics            (2020) 11:7 Page 3 of 10\\textbackslash{}nde-identification, smoking status, medication-related\\textbackslash{}ninformation, semantic relations between entities, or\\textbackslash{}ntemporality. The clinical corpora and their\\textbackslash{}nannotations built for the i2b2 NLP challenges are\\textbackslash{}navailable now for general research purposes.\\textbackslash{}n\\textbackslash{}u0095 n2c2 (National NLP Clinical Challenges),5 held in\\textbackslash{}n2018 and 2019, also address the processing of\\textbackslash{}nEnglish-language clinical documents. These\\textbackslash{}nchallenges are dedicated to other typical tasks when\\textbackslash{}nhandling clinical documents: inclusion of patients in\\textbackslash{}nclinical trials, detection of adverse-drug events,\\textbackslash{}ncomputing of textual semantic similarity, concept\\textbackslash{}nnormalization, and extraction of family history.\\textbackslash{}n\\textbackslash{}u0095 CLEF-eHEALTH challenges6 held in 2013 and 2014\\textbackslash{}nprovide annotations for disorder detection and\\textbackslash{}nabbreviation normalization. In 2016 the focus was on\\textbackslash{}nstructuring Australian free-text nurse notes. Finally,\\textbackslash{}nin 2016 and 2017 death reports in French, provided\\textbackslash{}nby the CépiDc,7 have been processed for death cause\\textbackslash{}nextraction.\\textbackslash{}n\\textbackslash{}u0095 eHealth-KD 2019 challenge8 targets human language\\textbackslash{}nmodelling in a scenario in which electronic health\\textbackslash{}ndocuments in Spanish could be machine readable\\textbackslash{}nfrom a semantic point of view. The two proposed\\textbackslash{}ntasks are: identification and classification of key\\textbackslash{}nphrases, and detection of semantic relations between\\textbackslash{}nthese key phrases.\\textbackslash{}nFinally, medical data, close to those handled in the clini-\\textbackslash{}ncal context, can be found in clinical trials protocols. One\\textbackslash{}nexample is the corpus of clinical trials annotated with\\textbackslash{}ninformation on numerical values in English {[}36{]}, and on\\textbackslash{}nnegation in French and Brazilian Portuguese {[}37, 38{]}.\\textbackslash{}nMethods\\textbackslash{}nWe first describe the specificity of the sources and clinical\\textbackslash{}ncases from which the CAS corpus was created (\\textbackslash{}u0093Building\\textbackslash{}nthe corpus\\textbackslash{}u0094), then the annotation rationale (\\textbackslash{}u0093Annotation\\textbackslash{}nof the corpus\\textbackslash{}u0094), and the principles of its comparison with\\textbackslash{}nsimilar clinical narratives from Rennes University Hospi-\\textbackslash{}ntal (\\textbackslash{}u0093Comparison with clinical narratives\\textbackslash{}u0094 sections).\\textbackslash{}nBuilding the corpus\\textbackslash{}nThe CAS corpus in French contains clinical cases as\\textbackslash{}npublished in scientific literature, legal or training mate-\\textbackslash{}nrial. Hence, it is built using material freely available\\textbackslash{}nin online sources. The collected clinical cases are pub-\\textbackslash{}nlished in different journals and websites from French-\\textbackslash{}nspeaking countries in various continents. Those clinical\\textbackslash{}n5https://n2c2.dbmi.hms.harvard.edu/\\textbackslash{}n6https://sites.google.com/site/shareclefehealth/\\textbackslash{}n7http://www.cepidc.inserm.fr/\\textbackslash{}n8https://knowledge-learning.github.io/ehealthkd-2019\\textbackslash{}ncases are related to various medical specialties (e.g. cardi-\\textbackslash{}nology, urology, oncology, obstetrics, pulmonology, gastro-\\textbackslash{}nenterology...).\\textbackslash{}nThe purpose of clinical cases is to describe clinical sit-\\textbackslash{}nuations for real de-identified or fake patients. Common\\textbackslash{}nclinical cases are typically part of education programs\\textbackslash{}nused for training medical students, while rare cases are\\textbackslash{}nusually shared through scientific publications to illustrate\\textbackslash{}nless common clinical situations. As for clinical cases which\\textbackslash{}ncan be found in legal sources, they usually report on situ-\\textbackslash{}nations which became complicated due to various reasons\\textbackslash{}nemanating from different healthcare levels: medical doc-\\textbackslash{}ntor, healthcare team, institution, health system and their\\textbackslash{}ninteractions.\\textbackslash{}nSimilarly to clinical documents, the content of clinical\\textbackslash{}ncases depends on the clinical situations that are illustrated,\\textbackslash{}nand on the disorders, but also on the purpose of the pre-\\textbackslash{}nsented cases: description of diagnoses, treatments or pro-\\textbackslash{}ncedures, evolution, family history, adverse-drug reactions,\\textbackslash{}nexpected audience, etc.\\textbackslash{}nData in published clinical cases are de-identified by the\\textbackslash{}nauthors prior to their publication. Besides, publication\\textbackslash{}nis usually done with the written permission of patients.\\textbackslash{}nThe case reports can be related to any medical situa-\\textbackslash{}ntion (diagnosis, treatment, procedure, follow-up...), to any\\textbackslash{}nspecialty and to any disorder. The typical structure of\\textbackslash{}nscientific publications with clinical cases starts by intro-\\textbackslash{}nducing the clinical situation, then one or more clinical\\textbackslash{}ncases are presented to support the situation. Schemes,\\textbackslash{}nimaging, examination results, patient history, lab results,\\textbackslash{}nclinical evolution, treatment, etc. can also be provided for\\textbackslash{}nthe illustration of clinical cases. Finally, those clinical cases\\textbackslash{}nare discussed. Hence, such cases may present an exten-\\textbackslash{}nsive description of medical problems. Such publications\\textbackslash{}ngather medical information related to clinical discourse\\textbackslash{}n(clinical cases) and to scientific discourse (introduction'\n\\item[text8] 'RESEARCH Open Access\\textbackslash{}nStructuring, reuse and analysis of electronic\\textbackslash{}ndental data using the Oral Health and\\textbackslash{}nDisease Ontology\\textbackslash{}nWilliam D. Duncan1,2* , Thankam Thyvalikakath2,3, Melissa Haendel4, Carlo Torniai5, Pedro Hernandez6, Mei Song7,\\textbackslash{}nAmit Acharya8, Daniel J. Caplan9, Titus Schleyer2,10\\textbackslash{}u0086 and Alan Ruttenberg11\\textbackslash{}u0086\\textbackslash{}nAbstract\\textbackslash{}nBackground: A key challenge for improving the quality of health care is to be able to use a common framework to\\textbackslash{}nwork with patient information acquired in any of the health and life science disciplines. Patient information\\textbackslash{}ncollected during dental care exposes many of the challenges that confront a wider scale approach. For example, to\\textbackslash{}nimprove the quality of dental care, we must be able to collect and analyze data about dental procedures from\\textbackslash{}nmultiple practices. However, a number of challenges make doing so difficult. First, dental electronic health record\\textbackslash{}n(EHR) information is often stored in complex relational databases that are poorly documented. Second, there is not\\textbackslash{}na commonly accepted and implemented database schema for dental EHR systems. Third, integrative work that\\textbackslash{}nattempts to bridge dentistry and other settings in healthcare is made difficult by the disconnect between\\textbackslash{}nrepresentations of medical information within dental and other disciplines\\textbackslash{}u0092 EHR systems. As dentistry increasingly\\textbackslash{}nconcerns itself with the general health of a patient, for example in increased efforts to monitor heart health and\\textbackslash{}nsystemic disease, the impact of this disconnect becomes more and more severe.\\textbackslash{}nTo demonstrate how to address these problems, we have developed the open-source Oral Health and Disease\\textbackslash{}nOntology (OHD) and our instance-based representation as a framework for dental and medical health care\\textbackslash{}ninformation. We envision a time when medical record systems use a common data back end that would make\\textbackslash{}ninteroperating trivial and obviate the need for a dedicated messaging framework to move data between systems.\\textbackslash{}nThe OHD is not yet complete. It includes enough to be useful and to demonstrate how it is constructed. We\\textbackslash{}ndemonstrate its utility in an analysis of longevity of dental restorations. Our first narrow use case provides a\\textbackslash{}nprototype, and is intended demonstrate a prospective design for a principled data backend that can be used\\textbackslash{}nconsistently and encompass both dental and medical information in a single framework.\\textbackslash{}n(Continued on next page)\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\textbackslash{}nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\textbackslash{}nchanges were made. The images or other third party material in this article are included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\textbackslash{}npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\textbackslash{}nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\textbackslash{}ndata made available in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}n* Correspondence: wdduncan@gmail.com\\textbackslash{}n\\textbackslash{}u0086Titus Schleyer and Alan Ruttenberg are joint senior authors\\textbackslash{}n1National Center for Ontological Research, Buffalo, NY, USA\\textbackslash{}n2Center for Biomedical Informatics, Regenstrief institute, Inc., Indianapolis, IN,\\textbackslash{}nUSA\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00222-0\\textbackslash{}n(Continued from previous page)\\textbackslash{}nResults: The OHD contains over 1900 classes and 59 relationships. Most of the classes and relationships were\\textbackslash{}nimported from existing OBO Foundry ontologies. Using the LSW2 (LISP Semantic Web) software library, we translated\\textbackslash{}ndata from a dental practice\\textbackslash{}u0092s EHR system into a corresponding Web Ontology Language (OWL) representation based\\textbackslash{}non the OHD framework. The OWL representation was then loaded into a triple store, and as a proof of concept, we\\textbackslash{}naddressed a question of clinical relevance \\textbackslash{}u0096 a survival analysis of the longevity of resin filling restorations. We\\textbackslash{}nprovide queries using SPARQL and statistical analysis code in R to demonstrate how to perform clinical research\\textbackslash{}nusing a framework such as the OHD, and we compare our results with previous studies.\\textbackslash{}nConclusions: This proof-of-concept project translated data from a single practice. By using dental practice data, we\\textbackslash{}ndemonstrate that the OHD and the instance-based approach are sufficient to represent data generated in real-\\textbackslash{}nworld, routine clinical settings. While the OHD is applicable to integration of data from multiple practices with\\textbackslash{}ndifferent dental EHR systems, we intend our work to be understood as a prospective design for EHR data storage\\textbackslash{}nthat would simplify medical informatics. The system has well-understood semantics because of our use of BFO-\\textbackslash{}nbased realist ontology and its representation in OWL. The data model is a well-defined web standard.\\textbackslash{}nKeywords: Ontology, Dental health, Informatics, Electronic heath record, OWL, SPARQL\\textbackslash{}nBackground\\textbackslash{}nA key challenge for improving the quality of healthcare\\textbackslash{}nis to be able to use a common framework to work with\\textbackslash{}npatient information acquired in any of the health and life\\textbackslash{}nscience disciplines. The patient information collected\\textbackslash{}nduring dental care exposes many of the challenges that\\textbackslash{}nconfront a wider scale approach. Within dentistry, a key\\textbackslash{}naspect for improving the quality of care is the ability to\\textbackslash{}ncollect and analyze data about oral health conditions\\textbackslash{}nand procedures, such as the longevity of fillings, the fre-\\textbackslash{}nquency of patient checkups, and incidence of tooth loss.\\textbackslash{}nRecent reports estimate that 73.8\\% of solo practitioners\\textbackslash{}nand 78.7\\% of group practitioners in the U.S. use a com-\\textbackslash{}nputer to manage some, and 14.3 and 15.9\\%, respectively,\\textbackslash{}nall patient information on a computer {[}1{]} . In conse-\\textbackslash{}nquence, we now have the opportunity to study dental\\textbackslash{}nhealth services and perform outcomes research using\\textbackslash{}nlarge amounts of secondary data obtained from geo-\\textbackslash{}ngraphically dispersed dental practices {[}2{]}.\\textbackslash{}nLarge secondary datasets could help us more easily\\textbackslash{}nstudy diseases in a sizable samples with increased statis-\\textbackslash{}ntical power, track patients for an extended period of\\textbackslash{}ntime, provide valid and representative samples, supply\\textbackslash{}ncorrelates not commonly collected in an oral health set-\\textbackslash{}nting, collect data in real time and ascertain potential\\textbackslash{}nconfounders {[}2{]}.\\textbackslash{}nAnalyzing data from electronic health records (EHR),\\textbackslash{}nhowever, presents a number of challenges. First, dental\\textbackslash{}nEHR information is often stored in relational databases\\textbackslash{}nthat are poorly documented and have complex relations\\textbackslash{}nbetween tables. This makes extracting and analyzing\\textbackslash{}ndata from even a single practice\\textbackslash{}u0092s system difficult. Sec-\\textbackslash{}nond, dental EHR database schemas vary depending on\\textbackslash{}nthe vendor who developed the system. This adds diffi-\\textbackslash{}nculty when integrating data from multiple practices.\\textbackslash{}nThird, information is not always encoded in the same\\textbackslash{}nway. For example, a tooth encoded as number (e.g.,\\textbackslash{}ntooth \\textbackslash{}u00916\\textbackslash{}u0092) or as a character array in which the index pos-\\textbackslash{}nition of a character represents the tooth (e.g., the \\textbackslash{}u0091Y\\textbackslash{}u0092 in\\textbackslash{}nthe character array \\textbackslash{}u0091NNNNNYNNNNNNNNNNNN\\textbackslash{}nNNNNNNNNNNNNNN\\textbackslash{}u0092 represents a right upper sec-\\textbackslash{}nondary canine tooth, i.e., tooth \\textbackslash{}u00916\\textbackslash{}u0092). Last, dental EHR sys-\\textbackslash{}ntems are typically only loosely specified. So, outside of a\\textbackslash{}ncommon core of structures for the oral cavity and its\\textbackslash{}nparts, there is wide variation in how information such as\\textbackslash{}nspecific types of materials, details of methods, instru-\\textbackslash{}nments, and general patient health is represented. Much\\textbackslash{}nof this information is either semi-or unstructured text.\\textbackslash{}nWhile we focus here on dental EHRs, these same prob-\\textbackslash{}nlems are endemic in other EHR systems.\\textbackslash{}nTo demonstrate how to address some of these problems,\\textbackslash{}nwe have developed the Oral Health and Disease Ontology\\textbackslash{}n(OHD) as a common framework for representing dental\\textbackslash{}nhealth information embedded in a larger framework ad-\\textbackslash{}nequate to accommodate structured representation that\\textbackslash{}ngoes beyond that in current dental EHR systems and ex-\\textbackslash{}ntends into general medicine. The OHD contains terms for\\textbackslash{}nrepresenting anatomical structures (e.g., distal surface of\\textbackslash{}ntooth), dental procedures (e.g., tooth extraction), and oral\\textbackslash{}nconditions (e.g., caries), as well as relations between terms\\textbackslash{}n(e.g., distal surface is part of tooth). The OHD\\textbackslash{}u0092s structure\\textbackslash{}nprovides a common representation of the entities that\\textbackslash{}nEHR data is about, without being designed in a way that\\textbackslash{}nunintentionally limits it to only dental health data. This\\textbackslash{}nmakes it possible to use the OHD as framework for inte-\\textbackslash{}ngrating inhomogeneous data from disparate database sys-\\textbackslash{}ntems and support representations for future systems.\\textbackslash{}nUsing the OHD\\textbackslash{}u0092s terms and relations, information from\\textbackslash{}nmultiple dental EHRs can now be translated into OWL 2\\textbackslash{}n{[}3{]} statements, stored in a semantic database or triple\\textbackslash{}nstore, and queried using SPARQL {[}4{]} to extract informa-\\textbackslash{}ntion for analysis.\\textbackslash{}nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 2 of 19\\textbackslash{}nAs a proof of concept, we have translated dental EHR\\textbackslash{}ndata from a single dental practice, and performed a sur-\\textbackslash{}nvival analysis of the longevity of resin filling restorations.\\textbackslash{}nThe proof of concept demonstrates both aspects of the\\textbackslash{}nOHD that are specific to dentistry (e.g. teeth, restora-\\textbackslash{}ntions and other procedures) as well as aspects that would\\textbackslash{}nremain unchanged in a general medical context, such as\\textbackslash{}ndemographic correlates. The output of this analysis is\\textbackslash{}ndiscussed in the Results section. In the Discussion sec-\\textbackslash{}ntion we describe the potential for wider application.\\textbackslash{}nRelated work\\textbackslash{}nThe work in this paper expands upon previous work de-\\textbackslash{}nveloping the OHD {[}5, 6{]}, and provides a more detailed\\textbackslash{}nexplication of the OHD\\textbackslash{}u0092s structure. It differs from previ-\\textbackslash{}nous ontology work, such as PeriO {[}7{]} and BigMouth {[}8{]},\\textbackslash{}nin two respects.\\textbackslash{}nFirst, it focuses on the domain of dental anatomy and\\textbackslash{}nprocedures rather than genomic information. Second,\\textbackslash{}nthe OHD\\textbackslash{}u0092s use of the Basic Formal Ontology and Ontol-\\textbackslash{}nogy of Biomedical Investigations as an upper-level\\textbackslash{}nframework sets the stage for seamlessly extending it to\\textbackslash{}ngeneral medical information. Moreover, the OHD is not\\textbackslash{}na data repository, such as BigMouth {[}8{]}, but a semantic\\textbackslash{}nframework for representing data that may be used in the\\textbackslash{}ndesign of repositories \\textbackslash{}u0096 such as our semantic\\textbackslash{}ntechnology-based repository of information translated\\textbackslash{}nfrom (for now) a single dental practice.\\textbackslash{}nWe considered using SNOMED and its dental subset\\textbackslash{}nSNODENT, but there are problems that make these\\textbackslash{}nstandards, at the moment, unusable for our purposes.\\textbackslash{}nFirst, their licenses restrict modification of substantial\\textbackslash{}nparts of the standard. This prevents us from reorganiz-\\textbackslash{}ning content according to realist principles, adding defini-\\textbackslash{}ntions, or adding or correcting axioms. Not all countries\\textbackslash{}nlicense to use SNOMED, and this would prevent our\\textbackslash{}nwork from being replicable worldwide.\\textbackslash{}nSecond, there are serious quality issues with\\textbackslash{}nSNOMED, and SNODENT in particular {[}9{]}. A major\\textbackslash{}nissue is the question of ontological commitment \\textbackslash{}u0096 what\\textbackslash{}nterms mean. The vast majority of terms in SNODENT\\textbackslash{}nand SNOMED come without textual definitions {[}10{]},\\textbackslash{}nand the question of what SNOMED terms actually rep-\\textbackslash{}nresent is still up for debate.\\textbackslash{}nThird, use of these terminologies typically is within a\\textbackslash{}nlayered framework that brings unnecessarily complica-\\textbackslash{}ntion {[}11{]}. In common usage these resources are bound\\textbackslash{}nto data models of medical records {[}12{]}. That means that\\textbackslash{}none needs to separately understand the data models and\\textbackslash{}nthe ontology. By contrast, in our approach the ingredi-\\textbackslash{}nents for a representation are simple \\textbackslash{}u0096 an OWL ontology\\textbackslash{}nand high-quality SPARQL, OWL and RDF W3C specifi-\\textbackslash{}ncations. Those logic-based specifications are substan-\\textbackslash{}ntially clearer than HL7 specifications.\\textbackslash{}nThe Open Biological and Biomedical Ontology (OBO)\\textbackslash{}nFoundry approach is to have, for any given class, a single\\textbackslash{}nidentifier, if necessary coordinating with developers of\\textbackslash{}nother ontologies. The realism-based approach empha-\\textbackslash{}nsizes that classes are collections of instances, that the in-\\textbackslash{}nstances are things in the world, and that documentation\\textbackslash{}nshould make clear what those instances are. The OHD\\textbackslash{}nand the semantic technologies used to implement the\\textbackslash{}nontology make it relatively easy to merge data. The data\\textbackslash{}nis just added together, untransformed. It is possible to\\textbackslash{}ndo this, in theory, because all parts of the representation\\textbackslash{}nare clearly understood, the types of entities are shared,\\textbackslash{}nand the choice to represent particulars using the stand-\\textbackslash{}nard methods provided by semantic web standards allow\\textbackslash{}nfor little creativity in how concrete representations are\\textbackslash{}nconstructed. Because our focus is on showing how a uni-\\textbackslash{}nfied representation system works, we consider out of\\textbackslash{}nscope general methods for harmonizing or interchanging\\textbackslash{}ndata with different representations, as is the focus of\\textbackslash{}nHL7.\\textbackslash{}nRecently, authors AR and WD have started participat-\\textbackslash{}ning in the review and development of SNODENT. It is\\textbackslash{}nentirely possible that in the future that SNOMED and\\textbackslash{}nSNODENT might be used in the same manner that we\\textbackslash{}nuse OHD here. The OHD and the source code used for\\textbackslash{}ntranslation and analysis are available in full at https://\\textbackslash{}ngithub.com/oral-health-and-disease-ontologies/ohd-\\textbackslash{}nontology and in part in the Additional file 1.\\textbackslash{}nMethods\\textbackslash{}nOntology development\\textbackslash{}nThe OHD was developed in a collaborative effort be-\\textbackslash{}ntween dental researchers, practicing dentists, statisti-\\textbackslash{}ncians, informatics experts, and ontologists. Our first task\\textbackslash{}nwas to identify which dental entities would be repre-\\textbackslash{}nsented. To guide this process, we developed a set of re-\\textbackslash{}nsearch questions. For example, for the research question,\\textbackslash{}n\\textbackslash{}u0093What is the time from one restoration to its replace-\\textbackslash{}nment on the same tooth?\\textbackslash{}u0094, we determined that we would\\textbackslash{}nneed to represent restoration procedures, the dates of\\textbackslash{}nthe procedures, patients, patients\\textbackslash{}u0092 teeth, surfaces of\\textbackslash{}nteeth, and the restoration materials used to restore teeth.\\textbackslash{}nWe provide the list of driving research questions in\\textbackslash{}nAdditional file 1.\\textbackslash{}nOnce our domain of focus was identified, our next\\textbackslash{}nstep was to catalog the terms1 we would need in the\\textbackslash{}nontology. We imported the Basic Formal Ontology\\textbackslash{}n(BFO) and the Ontology for General Medical Science\\textbackslash{}n(OGMS) as a whole and otherwise extracted terms from\\textbackslash{}nexisting OBO Foundry ontologies that represented en-\\textbackslash{}ntities relevant to our dental health domain using custom\\textbackslash{}n1In this paper, we use the word \\textbackslash{}u0091term\\textbackslash{}u0092 as a unique natural language\\textbackslash{}nexpression for a class, instance, or relation in our ontology.\\textbackslash{}nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 3 of 19\\textbackslash{}nprograms as well as the OntoFox2 web tool {[}13{]}. The\\textbackslash{}nOntoFox tool implements the Minimum Information to\\textbackslash{}nReference an External Ontology Term (MIREOT)\\textbackslash{}nprinciple {[}14{]}. MIREOT is a practice by which one im-\\textbackslash{}nports a selected set of terms from another ontology ra-\\textbackslash{}nther than including the whole ontology, as importing in\\textbackslash{}nOWL would do. Where relevant terms were not present\\textbackslash{}nin an existing ontology, we created new terms. Each new\\textbackslash{}nterm was assigned an Internationalized Resource Identi-\\textbackslash{}nfier (IRI) {[}15{]}, a human-readable label, a definition or\\textbackslash{}ndocumentation, and the name of the term\\textbackslash{}u0092s editor(s).\\textbackslash{}nWhen appropriate, other metadata was included, such as\\textbackslash{}nthe reference source for a definition and comments\\textbackslash{}nabout a term\\textbackslash{}u0092s definition such as its rationale, scope, and\\textbackslash{}nusage. Throughout the ontology development process,\\textbackslash{}nthe definitions were reviewed multiple times by team\\textbackslash{}nmembers. In the following sections, we discuss the\\textbackslash{}nmethods for acquiring the necessary terms.\\textbackslash{}nOntology architecture\\textbackslash{}nThe OHD is constructed in line with a number of OBO\\textbackslash{}nFoundry principles. The OBO Foundry {[}16{]} is a collect-\\textbackslash{}nive of ontology developers who are committed to collab-\\textbackslash{}noration and adherence to shared principles. The mission\\textbackslash{}nof the OBO Foundry is to develop a family of interoper-\\textbackslash{}nable ontologies that are both logically well-formed and\\textbackslash{}nscientifically accurate. OBO Foundry principles include\\textbackslash{}nuse of the Basic Formal Ontology (BFO) {[}17{]}, an upper-\\textbackslash{}nlevel ontology, use of a standard IRI identifier space, re-\\textbackslash{}nuse, where possible, of other Foundry ontologies, and\\textbackslash{}nthe inclusion of a textual and, where feasible, logical def-\\textbackslash{}ninition for each class and relation.\\textbackslash{}nOntology reuse\\textbackslash{}nThe OHD uses BFO as its upper-level ontology. BFO is de-\\textbackslash{}nsigned as a domain-independent ontology based on princi-\\textbackslash{}nples of ontological realism {[}18{]} As an upper-level ontology,\\textbackslash{}nBFO establishes categories such as material entities, pro-\\textbackslash{}ncesses, time, space, and realizable entities (properties), as well\\textbackslash{}nas relations among them, such as the relation between a par-\\textbackslash{}nticipant and a process they participate in.\\textbackslash{}nWe reuse a number of classes and relations from exist-\\textbackslash{}ning OBO Foundry ontologies, such as the Foundational\\textbackslash{}nModel of Anatomy (FMA) {[}19{]} and the Ontology for\\textbackslash{}nBiomedical Investigations (OBI) {[}20{]}.\\textbackslash{}nThis construction methodology serves two purposes.\\textbackslash{}nFirst, it allows us to leverage the experience of the devel-\\textbackslash{}nopers of OBO ontologies. Second, adhering to OBO\\textbackslash{}nstandards and precedents makes the OHD more easily\\textbackslash{}ninteroperable with other OBO ontologies {[}16{]}, and this\\textbackslash{}nallows developers to reuse our classes and provide\\textbackslash{}nfeedback on how to improve the OHD. A summary of\\textbackslash{}nthe reused ontologies is provided in Table 1.\\textbackslash{}nClasses from the ontologies listed in Table 1 are then\\textbackslash{}nextended to encompass entities in the oral health do-\\textbackslash{}nmain. At present, this includes classes for representing\\textbackslash{}nteeth and tooth surfaces, dental procedures, patients,\\textbackslash{}nproviders, restoration materials, dental findings, and bill-\\textbackslash{}ning codes. Each of these classes is discussed in the fol-\\textbackslash{}nlowing sections.\\textbackslash{}nAnatomical structures\\textbackslash{}nWe use the FMA\\textbackslash{}u0092s classes to represent anatomical struc-\\textbackslash{}ntures, such as jaws, teeth, and tooth roots. However, in\\textbackslash{}nour initial construction of the OHD, we found that the\\textbackslash{}nFMA was not adequate for representing surfaces of\\textbackslash{}nteeth. The FMA\\textbackslash{}u0092s class surface of tooth is used to repre-\\textbackslash{}nsent the two-dimensional curved plane that forms the\\textbackslash{}nouter boundary of a tooth. This is not suitable for repre-\\textbackslash{}nsenting the portions of enamel into which restoration\\textbackslash{}nmaterial is placed. Thus, we added the class surface en-\\textbackslash{}namel of tooth to represent the portions of enamel that\\textbackslash{}nconstitute a tooth\\textbackslash{}u0092s anatomical crown. The need for this\\textbackslash{}nclass was reported to the FMA\\textbackslash{}u0092s curators, and the FMA\\textbackslash{}nnow includes the class surface layer of tooth3 to address\\textbackslash{}nthis.\\textbackslash{}nUntil recently, the FMA was authored in a representa-\\textbackslash{}ntion system called Protégé Frames. In order to use it\\textbackslash{}nwithin the OBO framework we needed to translate from\\textbackslash{}nthe native frames version to a version that integrates\\textbackslash{}nwith OBO ontologies. As part of that translation, classes\\textbackslash{}nin FMA were placed as children of the appropriate BFO\\textbackslash{}nor OBO classes. Second, we needed to translate the\\textbackslash{}nframes expressions {[}25{]} to OWL before we could use it\\textbackslash{}nwith the other classes OBO classes.\\textbackslash{}n2http://ontofox.hegroup.org 3http://purl.org/sig/ont/fma/fma290055 (accessed August 2018)\\textbackslash{}nTable 1 Summary of ontology reuse in OHD\\textbackslash{}nOntology Classes/relations reused or specialized\\textbackslash{}nBasic Formal Ontology (BFO) upper-level ontology used to\\textbackslash{}ncoordinate other OBO ontologies\\textbackslash{}nOntology for General Medical\\textbackslash{}nScience (OGMS) {[}21{]}\\textbackslash{}nhealth care entities; e.g., patient role,\\textbackslash{}nvisit, disorder\\textbackslash{}nFoundational Model of\\textbackslash{}nAnatomy (FMA)\\textbackslash{}nanatomical entities; e.g., jaw, tooth,\\textbackslash{}ntooth surface\\textbackslash{}nOntology for Biomedical\\textbackslash{}nInvestigations (OBI)\\textbackslash{}nrelations between processes to\\textbackslash{}nentities; e.g., restoration procedure has\\textbackslash{}nspecified input some tooth\\textbackslash{}nInformation Artifact Ontology\\textbackslash{}n(IAO) {[}22{]}\\textbackslash{}ninformation entities in the dental\\textbackslash{}nhealth care domain; e.g., billing codes,\\textbackslash{}ngoals of dental procedures\\textbackslash{}nOntology of Medically Related\\textbackslash{}nSocial Entities (OMRSE) {[}23{]}\\textbackslash{}ngender of patient\\textbackslash{}nCommon Anatomy Reference\\textbackslash{}nOntology (CARO) {[}24{]}\\textbackslash{}nmale and female organism\\textbackslash{}nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 4 of 19\\textbackslash{}nPatients and health care providers\\textbackslash{}nA given dental procedure (such as an oral evaluation or\\textbackslash{}nrestoration procedure) will minimally involve a patient\\textbackslash{}nand the dental health care provider. We define for the\\textbackslash{}npatient and health care provider roles that characterize\\textbackslash{}nthe way in which patients and providers participate in\\textbackslash{}ndental procedures.\\textbackslash{}nIn BFO, roles are realizable entities which are in\\textbackslash{}nturn dependent entities. A dependent entity is one\\textbackslash{}nthat cannot exist unless the entity bearing the role\\textbackslash{}nexists. For example, a particular patient role cannot\\textbackslash{}nexist unless the organism that bears the role (i.e., the\\textbackslash{}npatient) exists. A role is optional in the sense that an\\textbackslash{}nentity may gain or lose a role without its physical\\textbackslash{}nmakeup being changed. For instance, a person may\\textbackslash{}ncease to be patient at some practice due to the prac-\\textbackslash{}ntice going out of business. The practice\\textbackslash{}u0092s going out of\\textbackslash{}nbusiness is an event that is external to the person,\\textbackslash{}nand, thus, does not necessitate that the person is\\textbackslash{}nsomehow physically changed. Roles are realizable in\\textbackslash{}nthe sense that their existence can be manifested in a\\textbackslash{}ncorrelated process. For instance a dental hygienist role\\textbackslash{}nis realized when the hygienist engages in processes\\textbackslash{}nrelated to their profession, such as plaque removal\\textbackslash{}nand application of fluoride treatment. Roles and other\\textbackslash{}ndependent continuants inhere, or are borne by, mater-\\textbackslash{}nial entities.\\textbackslash{}nEmploying this distinction between roles and their\\textbackslash{}nbearers, we define the types dental health care provider\\textbackslash{}nand human dental patient by first defining the appropri-\\textbackslash{}nate roles for each kind of entity, and then defining pro-\\textbackslash{}nviders and patients as being bearers of the roles4:\\textbackslash{}nA dental health care provider role is a role that\\textbackslash{}ninheres in a person who is licensed to provide den-\\textbackslash{}ntal health care and is realized in a health care\\textbackslash{}nprocess.\\textbackslash{}nA dental health care provider is a human being who\\textbackslash{}nbears a dental health care provider role.\\textbackslash{}nA patient role is a role that inheres in a person and\\textbackslash{}nis realized by the process of being under the care of\\textbackslash{}na physician or health care provider. (OGMS)\\textbackslash{}nA dental patient role is a patient role that is realized\\textbackslash{}nby the process of being under the care of a dental\\textbackslash{}nhealth care provider.\\textbackslash{}nA human dental patient is a human being who\\textbackslash{}nbears a dental patient role.\\textbackslash{}nIn order to define the patient\\textbackslash{}u0092s gender, we use the gen-\\textbackslash{}nder role types from the Ontology of Medically Related\\textbackslash{}nSocial Entities (OMRSE). The OMRSE is a realist repre-\\textbackslash{}nsentation of medically related social entities developed\\textbackslash{}nto cover demographics data and common roles of people\\textbackslash{}nin healthcare encounters for reuse in the context of the\\textbackslash{}nOBO Foundry. The gender role types are defined as\\textbackslash{}nfollows:\\textbackslash{}nA gender role is a human social role borne by a hu-\\textbackslash{}nman being that is realized in behavior which is con-\\textbackslash{}nsidered socially appropriate for individuals of a\\textbackslash{}nspecific sex in the context of a specific culture.\\textbackslash{}n(OMRSE)\\textbackslash{}nA female gender role is a gender role borne by a hu-\\textbackslash{}nman being that is realized in behavior which is con-\\textbackslash{}nsidered socially appropriate for individuals of the\\textbackslash{}nfemale sex in the context of the culture in question.\\textbackslash{}n(OMRSE)\\textbackslash{}nA male gender role is a gender role borne by a hu-\\textbackslash{}nman being that is realized in behavior which is con-\\textbackslash{}nsidered socially appropriate for individuals of the\\textbackslash{}nmale sex in the context of the culture in question.\\textbackslash{}n(OMRSE)\\textbackslash{}nFemale and male dental patients are then simply de-\\textbackslash{}nfined by relating the patient to the female and male gen-\\textbackslash{}nder roles:\\textbackslash{}nA female dental patient is a human dental patient\\textbackslash{}nwho bears a female gender role.\\textbackslash{}nA male dental patient is a human dental patient\\textbackslash{}nwho bears a male gender role.\\textbackslash{}nUsing roles to define patients and dental health care\\textbackslash{}nproviders has two advantages. First, because roles are\\textbackslash{}nformally defined, they represent the semantics for how\\textbackslash{}nan entity participates in a procedure. That is, for a given\\textbackslash{}ndental procedure, the patient participant is the entity\\textbackslash{}nwhose participation realizes the dental patient role, and\\textbackslash{}nthe provider participant is the entity whose participation\\textbackslash{}nrealizes the dental health care provider role. In contrast,\\textbackslash{}nfield names and values in relational databases are purely\\textbackslash{}nsyntactic.\\textbackslash{}nSecond, by using gender roles instead of anatomical\\textbackslash{}nsex to represent male and female dental patients, we\\textbackslash{}nallow for the possibility that the gender a patient assigns\\textbackslash{}nto himself or herself may differ from the patient\\textbackslash{}u0092s ana-\\textbackslash{}ntomical sex (at birth), matching the common practice of\\textbackslash{}nrecording patient-reported gender in clinical systems. In\\textbackslash{}nthose cases in which biological sex needs to be4Classes/relations are defined in the OHD unless indicated otherwise.\\textbackslash{}nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 5 of 19\\textbackslash{}nrepresented, the OHD includes CARO\\textbackslash{}u0092s types female or-\\textbackslash{}nganism and male organism:\\textbackslash{}nA female organism is a gonochoristic organism that\\textbackslash{}ncan produce female gametes. (CARO)\\textbackslash{}nA male organism is a gonochoristic organism that\\textbackslash{}ncan produce male gametes. (CARO)\\textbackslash{}nUsing these classes, a patient\\textbackslash{}u0092s biological sex can then\\textbackslash{}nbe defined according to biological criteria rather than\\textbackslash{}ngender selection.\\textbackslash{}nDental procedures\\textbackslash{}nWe define the class dental procedure as a subclass of\\textbackslash{}nOGMS\\textbackslash{}u0092 health care encounter class:\\textbackslash{}nA health care encounter is a temporally-connected\\textbackslash{}nhealth care process that has as participants an\\textbackslash{}norganization or person realizing the health care pro-\\textbackslash{}nvider role and a person realizing the patient role.\\textbackslash{}nThe health care provider role and patient role are\\textbackslash{}nrealized during the health care encounter. (OGMS)\\textbackslash{}nA dental procedure is a health care encounter that\\textbackslash{}nrealizes a dental patient role in which the patient\\textbackslash{}nundergoes a diagnostic or therapeutic process.\\textbackslash{}nAs illustrated in Fig. 1, specific dental procedures are\\textbackslash{}nthen defined by specializing the dental procedure class.\\textbackslash{}nFor instance, endodontic procedure, surgical dental pro-\\textbackslash{}ncedure, and tooth restoration procedure are defined as\\textbackslash{}nfollows:\\textbackslash{}nAn endodontic procedure is a dental procedure that\\textbackslash{}nis performed on the pulp chamber and/or root canal\\textbackslash{}nof a tooth, or a part thereof.\\textbackslash{}nA surgical dental procedure is a dental procedure in\\textbackslash{}nwhich there is structural alteration of soft tissue or\\textbackslash{}nbone in or around the oral cavity by incision or\\textbackslash{}ndestruction of tissues or by manipulation with in-\\textbackslash{}nstruments causing localized alteration or transporta-\\textbackslash{}ntion of tissue, including lasers, ultrasound, ionizing\\textbackslash{}nradiation, scalpels, probes, and needles.\\textbackslash{}nA tooth restoration procedure is dental procedure in\\textbackslash{}nwhich either a whole tooth or a part of a tooth is re-\\textbackslash{}nplaced by dental restoration material in order to re-\\textbackslash{}nestablish the tooth\\textbackslash{}'s anatomical and functional form\\textbackslash{}nand function.\\textbackslash{}nMore specific surgical and restoration procedures are\\textbackslash{}nthen defined as subclasses of these terms. For example, a\\textbackslash{}nnon-exhaustive set of surgical and restorative procedures\\textbackslash{}ndefined in the OHD include:\\textbackslash{}nA tooth extraction procedure is a surgical dental\\textbackslash{}nprocedure that removes a tooth from the oral cavity.\\textbackslash{}nA crown restoration procedure is a tooth restoration\\textbackslash{}nprocedure whereby an artificial crown replaces all or\\textbackslash{}npart of the natural dental crown.\\textbackslash{}nA direct restoration procedure is a tooth restoration\\textbackslash{}nprocedure in which the dental restoration material is\\textbackslash{}nplaced in the tooth via some direct dental material\\textbackslash{}ninsertion process.\\textbackslash{}nAn indirect restoration procedure is a tooth restor-\\textbackslash{}nation procedure in which the dental restoration ma-\\textbackslash{}nterial is placed in the tooth via some dental material\\textbackslash{}ntooth attachment process.\\textbackslash{}nAn intracoronal restoration procedure is a tooth res-\\textbackslash{}ntoration procedure in which a dental restoration ma-\\textbackslash{}nterial is placed into a site that is located in the\\textbackslash{}ncrown of the tooth.\\textbackslash{}nA veneer restoration procedure is a tooth restoration\\textbackslash{}nprocedure in which a thin layer of material (i.e., a\\textbackslash{}nveneer) is placed over one or more surfaces of the\\textbackslash{}nFig. 1 A portion of the hierarchy of health care encounters in OHD. Numbers represent the number of direct subclasses for a class, some not\\textbackslash{}nshown for reasons of space\\textbackslash{}nDuncan et al. Journal of Biomedical Semantics            (2020) 11:8 Page 6 of 19\\textbackslash{}ntooth for purposes such as improving the aesthetics\\textbackslash{}nof the tooth or protecting the tooth\\textbackslash{}'s surface from\\textbackslash{}ndamage.\\textbackslash{}nPatients and providers are related to dental procedures\\textbackslash{}nusing BFO\\textbackslash{}u0092s has participant and realizes relations. The\\textbackslash{}nhas participant relation is a general way of relating pro-\\textbackslash{}ncesses to the entities involved in them. For example, an\\textbackslash{}noral evaluation (minimally) has as participants the pa-\\textbackslash{}ntient undergoing the evaluation and the provider doing\\textbackslash{}nthe evaluation. The realizes relation holds between a\\textbackslash{}nprocess and a realizable entity such as a role. A role is\\textbackslash{}ndefined in terms of what bears it, what process realizes\\textbackslash{}nit, and the manner in which the bearer participates in\\textbackslash{}nthe process. As an example, consider the aforementioned\\textbackslash{}ndental patient role and dental health care provider role.\\textbackslash{}nWhen a dental procedure is performed, the procedure\\textbackslash{}nrealizes the roles of the patient and provider. The person\\textbackslash{}nupon whom the procedure is performed acts (or be-\\textbackslash{}nhaves) as the dental patient and the person doing the\\textbackslash{}nprocedure acts (or behaves) as the provider. In this way,\\textbackslash{}nthe dental procedure realizes the dental patient role of\\textbackslash{}nthe patient and the dental health care provider role of\\textbackslash{}nthe provider.\\textbackslash{}nBFO defines temporal regions and a relation occupies\\textbackslash{}ntemporal region that defines the temporal span of a\\textbackslash{}nprocess. However, we don\\textbackslash{}u0092t use this representation for\\textbackslash{}ntwo reasons. First, there isn\\textbackslash{}u0092t yet an established OBO\\textbackslash{}npractice for specifying concrete dates. Second, the time\\textbackslash{}nof a procedure is recorded only to the granularity of a\\textbackslash{}nday. Pending development of representations that ac-\\textbackslash{}ncommodate these issues, we defined a date property, oc-\\textbackslash{}ncurrence date, that relates a process to an xsd:dateTime\\textbackslash{}nsome time during which the process occurred. Another\\textbackslash{}ndata property birth date was defined to relate a patient\\textbackslash{}nto their date of birth.\\textbackslash{}nTo characterize the way in which a tooth participates\\textbackslash{}nin a specific dental procedure, we define roles that are\\textbackslash{}nborne by the tooth and realized in the appropriate corre-\\textbackslash{}nsponding procedure. For example, in order to represent\\textbackslash{}nthat a tooth undergoes a root canal treatment, we specify\\textbackslash{}nthat a tooth bears a particular tooth to undergo endodon-\\textbackslash{}ntic procedure role and this role is then realized in a par-\\textbackslash{}nticular endodontic procedure.\\textbackslash{}nFor procedures that involve restorative materials, we de-\\textbackslash{}nfine a dental restoration material role that is borne by (i.e.\\textbackslash{}npossessed by) the restoration material. The role helps de-\\textbackslash{}nfine the material in a domain-neutral way. All gold is\\textbackslash{}nmetal, but not all gold is used in dental restorations, just\\textbackslash{}nthose that bear the dental restoration material role.\\textbackslash{}nThis role is then realized by the corresponding restor-\\textbackslash{}nation procedure. For instance, an intracoronal restor-\\textbackslash{}nation procedure (see above) realizes the dental\\textbackslash{}nrestoration material role of the material that is placed\\textbackslash{}ninside the crown of the tooth. In procedures that involve\\textbackslash{}na specific kind of material, we use OBI\\textbackslash{}u0092s has\\_specified\\_\\textbackslash{}ninput relation to express that a procedure uses that ma-\\textbackslash{}nterial. For example, an amalgam filling restoration is de-\\textbackslash{}nfined as follows:\\textbackslash{}nAn amalgam filling restoration is an intracoronal\\textbackslash{}nrestoration procedure that uses amalgam to restore\\textbackslash{}nthe tooth.\\textbackslash{}nAs part of the logical framework of the OHD, we then\\textbackslash{}ninclude the axiom that an amalgam filling restoration\\textbackslash{}nhas\\_specified\\_input some portion of amalgam restor-\\textbackslash{}nation material.\\textbackslash{}nRestoration materials, restored teeth, and prosthetics\\textbackslash{}nFor dental procedures that involve the use of restoration\\textbackslash{}nmaterials (e.g., amalgam), we define the restoration ma-\\textbackslash{}nterials in terms of the role the material has in replacing\\textbackslash{}nportions of the tooth. In general, dental restoration ma-\\textbackslash{}nterial has the role of serving as a prosthetic, that is, the\\textbackslash{}nmaterial has the role of replacing a missing body part.\\textbackslash{}nHowever, not all prosthetics replace the function of the\\textbackslash{}nmissing body part, for example, a prosthetic eye cannot\\textbackslash{}nsee, although it still functions to maintain the shape of\\textbackslash{}nthe skull near the eyes. To address this, we define the\\textbackslash{}nterm functional prosthetic role to represent a prosthetic\\textbackslash{}nthat performs the function of the replaced body part.\\textbackslash{}nSince dental restoration materials perform the function\\textbackslash{}nof parts of the tooth they replace, we define dental res-\\textbackslash{}ntoration material role as a subtype of functional pros-\\textbackslash{}nthetic role:\\textbackslash{}nA functional prosthetic role is a prosthetic role that\\textbackslash{}nis realized by activities in which the material entity\\textbackslash{}n(bearing the role) is used a manner that is similar to\\textbackslash{}nhow the body part that the prosthesis replaces\\textbackslash{}nwould be used.\\textbackslash{}nA dental restoration material role is a functional\\textbackslash{}nprosthetic role that is borne by a portion of dental\\textbackslash{}nrestoration material and is realized in a tooth restor-\\textbackslash{}nation procedure in which the restoration material\\textbackslash{}nbecomes part of a restored tooth.\\textbackslash{}nFunctional prosthetic role is not a term that is sp'\n\\item[text9] 'RESEARCH Open Access\\textbackslash{}nIdentifying disease trajectories with\\textbackslash{}npredicate information from a knowledge\\textbackslash{}ngraph\\textbackslash{}nWytze J. Vlietstra1* , Rein Vos1,2, Marjan van den Akker3,4, Erik M. van Mulligen1 and Jan A. Kors1\\textbackslash{}nAbstract\\textbackslash{}nBackground: Knowledge graphs can represent the contents of biomedical literature and databases as subject-\\textbackslash{}npredicate-object triples, thereby enabling comprehensive analyses that identify e.g. relationships between diseases.\\textbackslash{}nSome diseases are often diagnosed in patients in specific temporal sequences, which are referred to as disease\\textbackslash{}ntrajectories. Here, we determine whether a sequence of two diseases forms a trajectory by leveraging the predicate\\textbackslash{}ninformation from paths between (disease) proteins in a knowledge graph. Furthermore, we determine the added\\textbackslash{}nvalue of directional information of predicates for this task. To do so, we create four feature sets, based on two\\textbackslash{}nmethods for representing indirect paths, and both with and without directional information of predicates (i.e.,\\textbackslash{}nwhich protein is considered subject and which object). The added value of the directional information of predicates\\textbackslash{}nis quantified by comparing the classification performance of the feature sets that include or exclude it.\\textbackslash{}nResults: Our method achieved a maximum area under the ROC curve of 89.8\\% and 74.5\\% when evaluated with\\textbackslash{}ntwo different reference sets. Use of directional information of predicates significantly improved performance by 6.5\\textbackslash{}nand 2.0 percentage points respectively.\\textbackslash{}nConclusions: Our work demonstrates that predicates between proteins can be used to identify disease trajectories.\\textbackslash{}nUsing the directional information of predicates significantly improved performance over not using this information.\\textbackslash{}nKeywords: Knowledge graph, Disease trajectories, Predicates, Temporal relationships, Directionality of predicates,\\textbackslash{}nProtein-protein interactions\\textbackslash{}nBackground\\textbackslash{}nKnowledge graphs can be used to represent the biomed-\\textbackslash{}nical knowledge published in literature and databases {[}1{]}.\\textbackslash{}nKnowledge is formalized as subject-predicate-object tri-\\textbackslash{}nples, where pairs of entities are related to each other by\\textbackslash{}npredicates {[}2{]}. By integrating triples from a variety of\\textbackslash{}nsources, knowledge graphs can be used to perform com-\\textbackslash{}nputational analyses on the comprehensive body of bio-\\textbackslash{}nmedical knowledge {[}3{]}. Previous work has used such\\textbackslash{}nanalyses to identify new relationships between pairs of\\textbackslash{}nentities, e.g., between drugs and diseases {[}4, 5{]}, genes\\textbackslash{}nand phenotypes {[}6, 7{]}, or between diseases {[}8, 9{]}.\\textbackslash{}nMuch research has been performed with knowledge\\textbackslash{}ngraphs that only consist of proteins, commonly referred\\textbackslash{}nto as protein-protein interaction networks. Through the\\textbackslash{}ninvolvement of proteins in metabolic, signaling, immune,\\textbackslash{}nand gene-regulatory networks, protein-protein inter-\\textbackslash{}naction networks can help to mechanistically explain dis-\\textbackslash{}nease and physiological processes {[}10\\textbackslash{}u009612{]}. Even though\\textbackslash{}npredicates further specify the types of interactions be-\\textbackslash{}ntween proteins, thereby providing additional information\\textbackslash{}nthat can be analyzed, protein-protein interaction net-\\textbackslash{}nworks usually do not use them. Instead, most methods\\textbackslash{}nanalyze the network topology of proteins {[}12{]}. However,\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\textbackslash{}nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\textbackslash{}nchanges were made. The images or other third party material in this article are included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\textbackslash{}npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\textbackslash{}nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\textbackslash{}ndata made available in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}n* Correspondence: w.vlietstra@erasmusmc.nl\\textbackslash{}n1Department of Medical Informatics, Erasmus University Medical Center, Dr.\\textbackslash{}nMolewaterplein 50, 3015 GE Rotterdam, the Netherlands\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00228-8\\textbackslash{}nwe have recently shown that analyses that are performed\\textbackslash{}non protein knowledge graphs benefit from predicate in-\\textbackslash{}nformation {[}13{]}.\\textbackslash{}nBy using the predicates that specify the mechanisms\\textbackslash{}nby which proteins interact, temporal pathobiological re-\\textbackslash{}nlationships may also be identified, although this has not\\textbackslash{}nbeen demonstrated yet. A key application for such tem-\\textbackslash{}nporal analyses is the identification of disease trajectories,\\textbackslash{}nwhich are commonly occurring temporal sequences of\\textbackslash{}ndiseases diagnosed in patients {[}14, 15{]}. An example of a\\textbackslash{}ndisease trajectory found in a study by Jensen et al. {[}14{]} is\\textbackslash{}nrheumatoid arthritis-precedes-heart failure, where pre-\\textbackslash{}ncedes is defined as \\textbackslash{}u0093occurs earlier in time. {[}\\textbackslash{}u0085{]}\\textbackslash{}u0094 {[}16{]}. The\\textbackslash{}noccurrence of the reverse, heart failure-precedes-\\textbackslash{}nrheumatoid arthritis, was found to occur significantly\\textbackslash{}nless frequently in the same study, and therefore was not\\textbackslash{}nclassified as a trajectory.\\textbackslash{}nIdentifying relationships between diseases is an im-\\textbackslash{}nportant and popular research topic for protein-protein\\textbackslash{}ninteraction networks (see Related work section). In such\\textbackslash{}nanalyses diseases are represented by so-called disease\\textbackslash{}nproteins, which are proteins encoded by genes that are\\textbackslash{}nassociated with a disease {[}17, 18{]}. Often cited benefits\\textbackslash{}ninclude an improved understanding of the biological\\textbackslash{}nmechanisms underlying disease interactions {[}8, 19, 20{]},\\textbackslash{}nand the ability to anticipate the next disease, thereby\\textbackslash{}nproviding the knowledge necessary to improve treatment\\textbackslash{}nplans and interventions {[}14, 21{]}. However, the temporal\\textbackslash{}naspects of relationships between diseases still require\\textbackslash{}nfurther investigation. We therefore aim to automatically\\textbackslash{}ndetermine whether a given sequence of two diseases\\textbackslash{}nforms a trajectory. We do so by leveraging the predicate\\textbackslash{}ninformation from paths between (disease) proteins in a\\textbackslash{}nknowledge graph. We also determine whether there is\\textbackslash{}nadded value in using directional information of predi-\\textbackslash{}ncates for this task.\\textbackslash{}nRelated work\\textbackslash{}nPrevious authors have mostly focused on identifying un-\\textbackslash{}ndirected relationships between diseases with protein net-\\textbackslash{}nworks {[}19\\textbackslash{}u009623{]}. For example, Kontou et al. created a\\textbackslash{}ndisease-disease graph, where an edge between diseases\\textbackslash{}nindicated that they shared at least one disease gene {[}23{]}.\\textbackslash{}nSun et al. calculated the similarity between diseases\\textbackslash{}nbased on their shared disease proteins, shared physio-\\textbackslash{}nlogical processes associated with these proteins, or the\\textbackslash{}ngraph structures between the proteins {[}20{]}. Li and Agar-\\textbackslash{}nwal identified which biological pathways were associated\\textbackslash{}nwith diseases via their disease proteins, and identified re-\\textbackslash{}nlationships between diseases based on the number of\\textbackslash{}nshared pathways {[}19{]}. Menche et al. identified so-called\\textbackslash{}ndisease modules, which are clusters of closely interre-\\textbackslash{}nlated disease proteins {[}22{]}. They found that short dis-\\textbackslash{}ntances between the modules of diseases were predictive\\textbackslash{}nfor pathobiological relationships. Contrary to Kontou\\textbackslash{}net al., they demonstrated that sharing disease proteins is\\textbackslash{}nnot a requirement for diseases to be related to each\\textbackslash{}nother.\\textbackslash{}nTo our knowledge, Bang et al. were the only ones to\\textbackslash{}nuse a directed protein-protein interaction network to\\textbackslash{}nidentify disease trajectories {[}21{]}. The disease proteins of\\textbackslash{}npairs of diseases were used to identify shared biomolecu-\\textbackslash{}nlar pathways, after which the locations of the disease\\textbackslash{}nproteins within these pathways were determined. The\\textbackslash{}ndisease with most upstream disease proteins was classi-\\textbackslash{}nfied as the first within the sequence of diseases. Add-\\textbackslash{}nitionally, 13 million Medicare records were used to\\textbackslash{}ncalculate two relative risk scores for each pair of dis-\\textbackslash{}neases, corresponding with the two possible temporal se-\\textbackslash{}nquences of the disease pair. If the sequence determined\\textbackslash{}nwith the protein pathways concurred with the sequence\\textbackslash{}nthat generated the largest relative risk, that sequence\\textbackslash{}nwas identified as a trajectory. Between a total of 2604\\textbackslash{}ndiseases, their method suggested 61 trajectories. These\\textbackslash{}nwere evaluated with the biomedical literature, where fur-\\textbackslash{}nther leads were found for 16 of them. Because the au-\\textbackslash{}nthors only evaluated the trajectories that were suggested\\textbackslash{}nby their method, it is unclear how many trajectories the\\textbackslash{}nmethod failed to identify.\\textbackslash{}nMaterials \\& methods\\textbackslash{}nReference sets\\textbackslash{}nThe ability of our method to identify disease trajectories\\textbackslash{}nwas evaluated with two reference sets, which have iden-\\textbackslash{}ntified disease trajectories by different means. The first\\textbackslash{}nreference set consisted of statistically-derived disease tra-\\textbackslash{}njectories from a large retrospective study of Danish hos-\\textbackslash{}npital data, while the second set consisted of literature-\\textbackslash{}nvalidated disease trajectories that were based on a small\\textbackslash{}nprospective study of Dutch general-practitioner data.\\textbackslash{}nJensen reference set\\textbackslash{}nThe first reference set was based on a study of Jensen\\textbackslash{}net al. {[}14{]}. They retrospectively identified 4014 disease\\textbackslash{}ntrajectories from 6.2 million electronic patient records of\\textbackslash{}nDanish hospitals based on diagnoses assigned over 14.9\\textbackslash{}nyears. All diagnoses in these patient records were repre-\\textbackslash{}nsented as International Statistical Classification of Dis-\\textbackslash{}neases and Related Health Problems 10th Revision (ICD-\\textbackslash{}n10) codes. Jensen used the hierarchy within the ICD-10\\textbackslash{}nto aggregate all diagnoses to a high abstraction level,\\textbackslash{}nresulting in 681 two-digit codes, such as \\textbackslash{}u0093Malignant neo-\\textbackslash{}nplasm of breast\\textbackslash{}u0094 (C50) or \\textbackslash{}u0093Type 2 diabetes mellitus\\textbackslash{}u0094\\textbackslash{}n(E11).\\textbackslash{}nJensen derived the disease trajectories from the Danish\\textbackslash{}nhospital data in a two-step process. First, they identified\\textbackslash{}nsequences of two diseases that were diagnosed within 5\\textbackslash{}nyears from each other in at least 10 patients, and which\\textbackslash{}nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 2 of 11\\textbackslash{}nhad a relative risk higher than 1. Subsequently, the direc-\\textbackslash{}ntion of each sequence had to be corroborated by a bino-\\textbackslash{}nmial test that compared the frequency of the sequence to\\textbackslash{}nthe frequency of its reversed sequence. Sequences that ful-\\textbackslash{}nfilled both criteria were classified as disease trajectories.\\textbackslash{}nTo represent the diseases in the Jensen set on the pro-\\textbackslash{}ntein level, we used the expert-annotated associations be-\\textbackslash{}ntween proteins and diseases from the manually curated\\textbackslash{}nsubset of DisGeNet {[}18{]}. The Unified Medical Language\\textbackslash{}nSystem (UMLS) MRCONSO table was used to map the\\textbackslash{}nICD-10 codes of the Jensen trajectories to the UMLS\\textbackslash{}nidentifiers that are used in DisGeNet. Two diseases, \\textbackslash{}u0093Ac-\\textbackslash{}ncidental poisoning by and exposure to other gases and\\textbackslash{}nvapours\\textbackslash{}u0094 (E47) and \\textbackslash{}u0093Influenza due to identified zoonotic\\textbackslash{}nor pandemic influenza virus\\textbackslash{}u0094 (J09), were lost because\\textbackslash{}ntheir ICD-10 codes could not be mapped to a UMLS\\textbackslash{}nidentifier. Because only 25\\% of the high-level diseases in\\textbackslash{}nthe Jensen set were represented within DisGeNet, we\\textbackslash{}nused the \\textbackslash{}u0093narrower\\textbackslash{}u0094 and \\textbackslash{}u0093child\\textbackslash{}u0094 relationships from the\\textbackslash{}nUMLS MRREL table to identify subclasses of all diseases.\\textbackslash{}nBy expanding the diseases with their subclasses, the per-\\textbackslash{}ncentage of diseases to which disease proteins could be\\textbackslash{}nassigned was increased to 68\\% (465 of 679 diseases).\\textbackslash{}nFrom the 4014 disease trajectories in the Jensen set,\\textbackslash{}nthere were 2530 trajectories where disease proteins\\textbackslash{}ncould be assigned to both diseases (63\\%). These 2530\\textbackslash{}ntrajectories, which were used as positive cases in this ref-\\textbackslash{}nerence set, contained 453 of the 465 diseases to which\\textbackslash{}ndisease proteins could be assigned (97\\%). On average,\\textbackslash{}ndiseases had 90 disease proteins assigned to them (me-\\textbackslash{}ndian: 29, interquartile range: 7\\textbackslash{}u009694). Disease proteins\\textbackslash{}nwere on average assigned to 6.2 diseases (median: 3,\\textbackslash{}ninterquartile range: 2\\textbackslash{}u00968).\\textbackslash{}nA set of 168,870 non-trajectories was constructed by\\textbackslash{}ncreating all possible sequences of the 453 included dis-\\textbackslash{}neases, minus the trajectories that were described by Jen-\\textbackslash{}nsen. The set of non-trajectories thereby included\\textbackslash{}nrandom pairs of diseases, the reversed temporal se-\\textbackslash{}nquences of these random pairs, as well as the reversed\\textbackslash{}ntemporal sequences of the trajectories. In the following,\\textbackslash{}nwe will refer to the trajectories and non-trajectories as\\textbackslash{}npositive and negative cases to align with common ter-\\textbackslash{}nminology in the machine learning field.\\textbackslash{}nVan den Akker reference set\\textbackslash{}nThe second reference set was based on a prospective co-\\textbackslash{}nhort study on disease susceptibility by Van den Akker\\textbackslash{}net al. {[}24{]}. They followed a Dutch cohort of 3460\\textbackslash{}npatients over 2 years, during which their general practi-\\textbackslash{}ntioner notes were examined for sequences of Inter-\\textbackslash{}nnational Classification of Primary Care (ICPC) codes\\textbackslash{}nthat represent chronic, permanent, and recurrent dis-\\textbackslash{}neases. In the Netherlands, each citizen is registered with\\textbackslash{}na general practitioner, who acts like a gatekeeper for\\textbackslash{}nsecondary and tertiary medical care, and is responsible\\textbackslash{}nfor maintaining a complete medical history of the\\textbackslash{}npatient.\\textbackslash{}nA total of 473 unique sequences of diseases were\\textbackslash{}nfound in this cohort, containing 122 distinct diseases.\\textbackslash{}nEach sequence was manually evaluated using the pub-\\textbackslash{}nlished biomedical literature and medical handbooks.\\textbackslash{}nThere were 65 sequences of diseases where the literature\\textbackslash{}nstated that the first disease increased the susceptibility of\\textbackslash{}nacquiring the second disease, and 408 sequences where\\textbackslash{}nno evidence of increased susceptibility was found. To\\textbackslash{}nmaintain consistent terminology, we will refer to se-\\textbackslash{}nquences with increased susceptibility as trajectories or\\textbackslash{}npositives and to sequences without increased susceptibil-\\textbackslash{}nity as non-trajectories or negatives.\\textbackslash{}nTo assign disease proteins to these 122 diseases we\\textbackslash{}nfollowed the same procedure as for the Jensen set by\\textbackslash{}nusing the MRCONSO table to map the ICPC codes to\\textbackslash{}nUMLS identifiers, after which the MRREL table was used\\textbackslash{}nto group them with their subclasses. Disease proteins\\textbackslash{}ncould be assigned to 97 diseases, which formed 55 tra-\\textbackslash{}njectories and 316 non-trajectories. On average, diseases\\textbackslash{}nhad 137 disease proteins assigned to them (median: 49,\\textbackslash{}ninterquartile range: 17\\textbackslash{}u0096167). Disease proteins were on\\textbackslash{}naverage assigned to 3 diseases (median: 2, interquartile\\textbackslash{}nrange: 1\\textbackslash{}u00964).\\textbackslash{}nTo determine whether our method could also identify\\textbackslash{}nthe correct temporal sequence of the trajectories, 54\\textbackslash{}nadditional non-trajectories were created by reversing the\\textbackslash{}nsequence of the diseases in the literature-supported tra-\\textbackslash{}njectories (the reverse sequence of one trajectory was\\textbackslash{}nalready included as a non-trajectory in the data from the\\textbackslash{}ngeneral practitioners).\\textbackslash{}nKnowledge graph\\textbackslash{}nThe predicates between proteins were extracted from\\textbackslash{}nthe Euretos Knowledge Platform (EKP), a commercially\\textbackslash{}navailable knowledge graph (http://www.euretos.com). In\\textbackslash{}nthe EKP, information from more than 200 knowledge\\textbackslash{}nsources from a wide variety of domains in the life sci-\\textbackslash{}nences is represented as triples. The biomedical entities\\textbackslash{}nsuch as proteins, drugs, or diseases that form the sub-\\textbackslash{}njects and objects of these triples are represented in the\\textbackslash{}nknowledge graph as vertices, each of which has one or\\textbackslash{}nmore identifiers associated with it from external data-\\textbackslash{}nbases. Mappings between the entities described in the\\textbackslash{}ndifferent knowledge sources underlying the knowledge\\textbackslash{}ngraph were made by matching their identifiers. The\\textbackslash{}npredicate and provenance of each triple are specified as\\textbackslash{}npart of an edge between the two vertices that represent\\textbackslash{}nthe subject and object. The direction of the predicate\\textbackslash{}ngoes from subject to object. The predicates in the under-\\textbackslash{}nlying knowledge sources were matched to a standardized\\textbackslash{}nset of 203 predicates. If an exact match was not\\textbackslash{}nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 3 of 11\\textbackslash{}navailable, a predicate was manually mapped. If there\\textbackslash{}nwere no explicit predicates in a database that was used\\textbackslash{}nas a knowledge source, the predicates were manually de-\\textbackslash{}nrived from the database schema. A path between two\\textbackslash{}nvertices is defined as a sequence of triples, or possibly a\\textbackslash{}nsingle triple, connecting the vertices.\\textbackslash{}nThe contents of the EKP are represented as documents\\textbackslash{}nin a NoSQL database, which allows them to be flexibly\\textbackslash{}nmodelled and indexed. The EKP can be run on a\\textbackslash{}nreasonably-powered server, requiring an 8-core proces-\\textbackslash{}nsor and 60GB of memory as a minimum. It has previ-\\textbackslash{}nously been used in pre-clinical research for drug efficacy\\textbackslash{}nscreening {[}13{]}, prioritizing existing drugs as repurposing\\textbackslash{}ncandidates for autosomal dominant polycystic kidney\\textbackslash{}ndisease {[}25{]}, and pathway enrichment {[}26{]}.\\textbackslash{}nFeature sets \\& machine learning\\textbackslash{}nThe paths between the disease proteins were extracted\\textbackslash{}nfrom the EKP. To keep our graph comprehensible, we\\textbackslash{}nonly extracted paths that consisted of one or two triples,\\textbackslash{}ni.e., paths where two disease proteins are connected by\\textbackslash{}nat most one intermediate protein. Within this range,\\textbackslash{}nthree scenarios for the paths between the disease proteins\\textbackslash{}nof two diseases A and B were distinguished (Fig. 1.):\\textbackslash{}n1) Overlap, where A and B share a disease protein,\\textbackslash{}noptionally with a path to itself, e.g. a disease protein\\textbackslash{}nof which two copies bind with each other\\textbackslash{}n(homodimerization).\\textbackslash{}n2) Direct path, where a disease protein of A and a\\textbackslash{}ndisease protein of B are part of one triple.\\textbackslash{}n3) Indirect path, where one intermediate protein\\textbackslash{}nconnects the disease proteins of A and B, requiring\\textbackslash{}na sequence of two triples.\\textbackslash{}nTwo different methods to represent indirect paths be-\\textbackslash{}ntween disease proteins were compared. The first method\\textbackslash{}nconstructed so-called metapaths {[}5{]}, where the sequence\\textbackslash{}nof predicates in an indirect path was used as single feature.\\textbackslash{}nThe second method, which we refer to as split paths, con-\\textbackslash{}nsidered each predicate in the indirect paths as a separate\\textbackslash{}nfeature {[}13{]}. Each method was tested both with and with-\\textbackslash{}nout directional information of predicates. Predicates were\\textbackslash{}neither considered to all be undirected, or predicates were\\textbackslash{}ncategorized as being directed or undirected based on ex-\\textbackslash{}npert assessment (described in the Assessment of predicate\\textbackslash{}ndirectionality section below), which we refer to as the\\textbackslash{}nMixed variation. In the overlap scenario, where the subject\\textbackslash{}nand the object were the same protein, predicates were al-\\textbackslash{}nways considered to be undirected.\\textbackslash{}nAll features were binary. Figure 2 shows the four\\textbackslash{}nfeature sets that are derived from the example\\textbackslash{}nshown in Fig. 1. We also experimented with feature\\textbackslash{}nsets where all predicates were directed as indicated\\textbackslash{}nby the subject and object of the triple in the EKP.\\textbackslash{}nHowever, because some predicates are explicitly de-\\textbackslash{}nfined as being undirected, using any directional in-\\textbackslash{}nformation from triples with these predicates would\\textbackslash{}ncontradict these definitions. Nonetheless, for the\\textbackslash{}nsake of completeness we have chosen to present\\textbackslash{}nthese results in Additional file 1.\\textbackslash{}nRandom forests were trained to classify the sequences of\\textbackslash{}ndiseases as positive or negative. Classification performance\\textbackslash{}nFig. 1 Schematic overview of the overlap, direct, and indirect scenarios that were extracted from the knowledge graph. Both diseases A and\\textbackslash{}ndisease B have three disease proteins (DP) associated with them according to the manually curated subset of DisGeNet. DisGeNet describes that\\textbackslash{}nDP1 is known to be associated with both diseases, while the knowledge graph describes that it has a \\textbackslash{}u0093binds with\\textbackslash{}u0094 relationship to itself. DP2 and\\textbackslash{}nDP4 have a direct \\textbackslash{}u0093inhibits\\textbackslash{}u0094 relationship, and DP3 and DP5 are connected through an indirect path, by an intermediate protein (IP). The arrows\\textbackslash{}nbetween the proteins indicate which protein is the subject of the \\textbackslash{}u0093inhibits\\textbackslash{}u0094 predicate, and which one its object. The \\textbackslash{}u0093binds with\\textbackslash{}u0094 predicate was\\textbackslash{}nconsidered to be undirected by the experts, and therefore does not have a direction. Based on the paths in the knowledge graph, four feature\\textbackslash{}nsets are created, based on two methods to represent indirect paths, and both with and without the directional information of predicates\\textbackslash{}nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 4 of 11\\textbackslash{}nwas measured with the area under the receiver operator\\textbackslash{}ncharacteristic curve (AUC) of a 10-fold cross-validation\\textbackslash{}nexperiment {[}27, 28{]}. We report the mean and standard de-\\textbackslash{}nviation of the AUCs of 10 repeated cross-validation exper-\\textbackslash{}niments. The same folds that were used in the experiments\\textbackslash{}nwith undirected predicates were also used in the experi-\\textbackslash{}nments with directed predicates, after which the differences\\textbackslash{}nbetween the test folds were tested for significance with a\\textbackslash{}ntwo-sided, paired t-test.\\textbackslash{}nTo control for the differences in prevalence and num-\\textbackslash{}nber of cases between the two reference sets, we also re-\\textbackslash{}nport the classification performance after undersampling\\textbackslash{}nthe number of positive and negative cases in the Jensen\\textbackslash{}nset to match those in the Van den Akker set.\\textbackslash{}nFor the best performing classifiers we also report sensi-\\textbackslash{}ntivity and specificity at the probability cutoff for which the\\textbackslash{}nYouden index (sensitivity + specificity \\textbackslash{}u0096 1) is largest {[}29{]}.\\textbackslash{}nMachine learning and evaluation of results were per-\\textbackslash{}nformed in R {[}30{]} with the packages caret {[}31{]}, ranger\\textbackslash{}n{[}32{]}, and pROC {[}33{]}.\\textbackslash{}nAssessment of predicate directionality\\textbackslash{}nThree experts with a strong biomedical background and\\textbackslash{}nfamiliarity with knowledge graphs assessed the direction-\\textbackslash{}nality of 47 distinct predicates that were found in the ex-\\textbackslash{}ntracted paths. They were provided with definitions of\\textbackslash{}nthese predicates which were obtained from the Pathway\\textbackslash{}nCommons resource {[}34{]}. If not available, definitions\\textbackslash{}nwere sought through the National Library of Medicine\\textbackslash{}n{[}35{]}, or the OBO foundry {[}36{]}. The assessors could\\textbackslash{}ncategorize each predicate as \\textbackslash{}u0093directed\\textbackslash{}u0094, \\textbackslash{}u0093undirected\\textbackslash{}u0094, or\\textbackslash{}n\\textbackslash{}u0093don\\textbackslash{}u0092t know\\textbackslash{}u0094. To establish directionality, a predicate had\\textbackslash{}nto be categorized as directed or undirected by a majority\\textbackslash{}n(i.e., two or three) of the assessors. Predicates that con-\\textbackslash{}ntain a negation (e.g., \\textbackslash{}u0093does not interact with\\textbackslash{}u0094) were auto-\\textbackslash{}nmatically categorized the same as the corresponding\\textbackslash{}npredicate without negation (\\textbackslash{}u0093interacts with\\textbackslash{}u0094), and there-\\textbackslash{}nfore not presented to the assessors. For some predicates\\textbackslash{}nthe categorization was straightforward. For example,\\textbackslash{}nPathway Commons defines the predicate \\textbackslash{}u0093interacts with\\textbackslash{}u0094\\textbackslash{}nas \\textbackslash{}u0093This is an undirected relation between participant\\textbackslash{}nFig. 2 The four feature sets that were derived from the paths between the disease proteins in Fig. 1. All features are binary: Black fields indicate a\\textbackslash{}n\\textbackslash{}u0093True\\textbackslash{}u0094 value, while empty fields indicate a \\textbackslash{}u0093False\\textbackslash{}u0094 value. For the \\textbackslash{}u0093Mixed\\textbackslash{}u0094 feature sets, the \\textbackslash{}u0093Binds with\\textbackslash{}u0094 predicate is assessed to be undirected by\\textbackslash{}nexperts, while the \\textbackslash{}u0093Inhibits\\textbackslash{}u0094 predicate is assessed to be directed\\textbackslash{}nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 5 of 11\\textbackslash{}nproteins of a molecular interaction. {[}\\textbackslash{}u0085{]}\\textbackslash{}u0094 , and the predi-\\textbackslash{}ncate \\textbackslash{}u0093catalysis precedes\\textbackslash{}u0094 as \\textbackslash{}u0093This relation defines di-\\textbackslash{}nrected interactions between proteins. {[}\\textbackslash{}u0085{]}\\textbackslash{}u0094 {[}34{]}. Six\\textbackslash{}npredicates did not reach a majority in the first round\\textbackslash{}nand were anonymously commented upon by the asses-\\textbackslash{}nsors to motivate their categorization. These comments\\textbackslash{}nwere shared between the assessors, after which they\\textbackslash{}ncould update their initial choice. Each predicate was\\textbackslash{}nthen categorized with a majority.\\textbackslash{}nTable 1 shows the 12 predicates that were categorized\\textbackslash{}nas undirected by the three experts. The other 35 predi-\\textbackslash{}ncates were categorized as directed. A complete overview\\textbackslash{}nof the predicates can be found in Additional file 2.\\textbackslash{}nResults\\textbackslash{}nExtracted paths\\textbackslash{}nIn total, 6859 distinct disease proteins were assigned to\\textbackslash{}nthe diseases in both reference sets, three of which could\\textbackslash{}nnot be mapped to the EKP. Another 430 (6.3\\%) of the\\textbackslash{}ndisease proteins were not found in any of the extracted\\textbackslash{}npaths. From these disease proteins, 314 had no relation-\\textbackslash{}nship to any other protein in the EKP.\\textbackslash{}nThe remaining 6426 disease proteins were involved in\\textbackslash{}n1,338,310 direct paths and 833,546,575 indirect paths,\\textbackslash{}nwhile 2581 disease proteins had 7354 paths to them-\\textbackslash{}nselves. All paths were based on 2,015,738 distinct triples,\\textbackslash{}nwhich contained 17,132 different proteins and 47 differ-\\textbackslash{}nent predicates.\\textbackslash{}nThe overlap scenario, where the two diseases in the\\textbackslash{}ntrajectory share at least one disease protein (scenario 1,\\textbackslash{}nFeature sets \\& Machine learning section), occurred in\\textbackslash{}n58\\% of the positive cases of the Jensen set, and 95\\% of\\textbackslash{}nthe positive cases of the Van den Akker set. No indirect\\textbackslash{}npaths (scenario 3, Feature sets \\& Machine learning sec-\\textbackslash{}ntion) were found between the disease proteins of 119\\textbackslash{}npositive cases (4.7\\%), and 18,217 negative cases of the\\textbackslash{}nJensen set (10.8\\%), and one positive case (1.8\\%) and 15\\textbackslash{}nnegative cases (4.1\\%) of the Van den Akker set.\\textbackslash{}nClassification results\\textbackslash{}nThe classification performance for both reference sets is\\textbackslash{}nshown in Table 2. Mixed metapaths performed best,\\textbackslash{}nachieving mean AUCs of 89.8\\% for the Jensen set and\\textbackslash{}n74.5\\% for the Van den Akker set. Overall, classification\\textbackslash{}nperformance on the Van den Akker set was 9.9 to 15.3\\textbackslash{}npercentage points lower than on the Jensen set, while\\textbackslash{}nstandard deviations were 9.6 to 11.3 percentage points\\textbackslash{}nhigher. Metapaths performed 4.1 to 7.0 percentage\\textbackslash{}npoints better than split paths. The performance of the\\textbackslash{}nmixed feature sets was 1.9 to 6.5 percentage points\\textbackslash{}nhigher than the undirected feature sets. All differences\\textbackslash{}nbetween mixed and undirected feature sets were signifi-\\textbackslash{}ncant (p-values for Jensen metapaths and split paths: <\\textbackslash{}n0.001; Van den Akker metapaths: 0.02, split paths 0.001).\\textbackslash{}nTo quantify how much of the difference in AUC be-\\textbackslash{}ntween the two reference sets could be attributed to their\\textbackslash{}ndifference in size, the Jensen set was undersampled to\\textbackslash{}nthe same number of positive and negative cases as the\\textbackslash{}nVan den Akker set. With the exception of the mixed\\textbackslash{}nmetapaths, performance dropped below the performance\\textbackslash{}nthat was achieved with the Van den Akker set. The\\textbackslash{}nstandard deviations also increased from 0.9\\textbackslash{}u00961.7\\% to 8.4\\textbackslash{}u0096\\textbackslash{}n12.3\\%. The latter values are comparable to the standard\\textbackslash{}ndeviations on the Van den Akker set.\\textbackslash{}nFigure 3 shows the receiver operating characteristic\\textbackslash{}n(ROC) curves of the mixed metapath classifiers that per-\\textbackslash{}nformed best. For the Jensen set, sensitivity and specificity\\textbackslash{}nat the maximum Youden index were 79.2\\% and 82.4\\%,\\textbackslash{}nrespectively, while for the Van den Akker set these were\\textbackslash{}n73.6\\% and 64.3\\%.\\textbackslash{}nError analysis\\textbackslash{}nFor our best classifier (mixed metapath features, trained\\textbackslash{}non the Jensen set), we analyzed the top-15 false-positive\\textbackslash{}nand the top-15 false-negative cases, searching the litera-\\textbackslash{}nture for information that might explain the errors. The\\textbackslash{}nresults of our analysis of the false positives are shown in\\textbackslash{}nTable 3. Overall, the first 10 out of the top 15 false posi-\\textbackslash{}ntives appear to be omissions from the Jensen set rather\\textbackslash{}nthan misclassifications. For two false-positive cases, po-\\textbackslash{}ntential mechanisms have been suggested, but the current\\textbackslash{}nevidence is inconclusive on whether those mechanisms\\textbackslash{}nare valid. For the remaining three false-positive cases no\\textbackslash{}nliterature could be found, which may therefore be inter-\\textbackslash{}nesting leads for further investigation.\\textbackslash{}nTable 4 shows the results for the top-15 false nega-\\textbackslash{}ntives. For six false negatives, the second disease was\\textbackslash{}nlikely to be caused by the treatment of the first disease.\\textbackslash{}nFor example, the radiation that is used to treat the ma-\\textbackslash{}nlignant neoplasm of the larynx may compromise the\\textbackslash{}nTable 1 Predicates categorized as undirected as a result of the\\textbackslash{}nassessment process\\textbackslash{}nUndirected Predicates\\textbackslash{}nbinds with\\textbackslash{}ncoexists with\\textbackslash{}ndoes not coexist with\\textbackslash{}nforms protein complex with\\textbackslash{}ninteracts with\\textbackslash{}ndoes not interact with\\textbackslash{}nis associated with\\textbackslash{}nis compared with\\textbackslash{}nis functionally related to\\textbackslash{}nis spatially related to\\textbackslash{}nis the same as\\textbackslash{}northolog is associated with\\textbackslash{}nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 6 of 11\\textbackslash{}nimmune system around the throat and mouth, thereby\\textbackslash{}nincreasing susceptibility to oropharyngeal candidiasis\\textbackslash{}n{[}54{]}. Two false-negative trajectories are likely to have\\textbackslash{}nmechanical causes, rather than molecular pathways. The\\textbackslash{}ntrajectory from malignant neoplasms of the ovary to nu-\\textbackslash{}ntrient deficiency can be explained by the blocking of the\\textbackslash{}nintestines by the ovarian tumor, thereby blocking the en-\\textbackslash{}ntire digestive system {[}53{]}. For four of the false-negative\\textbackslash{}ntrajectories, no description could be found in the litera-\\textbackslash{}nture, making their assessment impossible. Assessment of\\textbackslash{}nthe three remaining false negatives is speculative. For ex-\\textbackslash{}nample, the trajectory from transient ischemic attacks\\textbackslash{}n(TIA) to vitamin B12 deficiencies may be an artifact of\\textbackslash{}nthe medical record keeping. Vitamin B12 is known to\\textbackslash{}nprotect against TIAs {[}52{]}, so what may often happen is\\textbackslash{}nthat a vitamin B12 deficiency is only diagnosed after the\\textbackslash{}nmore acute TIA has been treated in an emergency room.\\textbackslash{}nDiscussion\\textbackslash{}nOur work demonstrates that disease trajectories can be\\textbackslash{}nidentified with the predicates between proteins in a know-\\textbackslash{}nledge graph. To do so, our machine-learning based meth-\\textbackslash{}nodology needed to successfully identify both the correct\\textbackslash{}npairs of diseases, as well as their correct temporal se-\\textbackslash{}nquences. Overall, representing indirect paths as metapaths\\textbackslash{}nperformed superior as compared to representing them as\\textbackslash{}nsplit paths. Using the directional information of predicates\\textbackslash{}nsignificantly improved performance over not using this\\textbackslash{}ninformation. Undersampling the Jensen set to the same\\textbackslash{}nnumber of positive and negative cases as the Van den\\textbackslash{}nAkker set showed that its lower performance and higher\\textbackslash{}nstandard deviations could partially be explained by its small\\textbackslash{}nsize.\\textbackslash{}nIn previous work, Bang et al. {[}21{]} identified disease trajec-\\textbackslash{}ntories by calculating the relative risk between two diseases\\textbackslash{}nand combining this with the relative position of disease pro-\\textbackslash{}nteins in biomolecular pathways. Their method is fully\\textbackslash{}ndependent on shared disease proteins between the two dis-\\textbackslash{}neases, whereas our method also works when there are only\\textbackslash{}n(in) direct paths between disease proteins. In the Jensen set,\\textbackslash{}nthis holds for 42\\% of the trajectories. Performance compari-\\textbackslash{}nson of the methods is difficult because Bang et al. only vali-\\textbackslash{}ndated the disease trajectories that were suggested by their\\textbackslash{}nmethod, but did not validate the non-trajectories. Thus,\\textbackslash{}nonly the precision of their method can be calculated but no\\textbackslash{}ninsight is provided in the number of false-negative trajec-\\textbackslash{}ntories. A final complication for the comparison between the\\textbackslash{}ntwo methods is the claim of Bang et al. to discover causal\\textbackslash{}nrelationships between diseases, rather than only temporal\\textbackslash{}nones. Unfortunately, they refer to an example to define\\textbackslash{}ncausal relationships between diseases, making it difficult to\\textbackslash{}npinpoint how these differ from disease trajectories.\\textbackslash{}nAlthough we do not foresee direct clinical application\\textbackslash{}nof our work, our high performance may persuade ex-\\textbackslash{}nperts to further examine the protein paths underlying\\textbackslash{}nsome positively classified trajectories, either known or\\textbackslash{}nTable 2 Classification results for the four feature sets for both reference sets\\textbackslash{}nJensen set Jensen set - undersampled Van den Akker set\\textbackslash{}nMetapaths Split paths Metapaths Split paths Metapaths Split paths\\textbackslash{}nUndirected 83.3 (1.7) 78.3 (1.7) 64.2 (12.1) 61.9 (12.3) 72.5 (11.8) 68.4 (13.0)\\textbackslash{}nMixed 89.8 (0.9) 82.8 (1.2) 82.3 (8.4) 69.6 (13.1) 74.5 (10.5) 70.3 (11.4)\\textbackslash{}nThe values in the columns indicate the mean AUC and its standard deviation in \\% of 10 cross-validation experiments\\textbackslash{}nFig. 3 ROC curves of the mixed metapaths classifiers for the Jensen set and the Van den Akker set\\textbackslash{}nVlietstra et al. Journal of Biomedical Semantics            (2020) 11:9 Page 7 of 11\\textbackslash{}nnewly suggested. Interpreting these protein paths might\\textbackslash{}nprovide additional clues about the mechanism through\\textbackslash{}nwhich the first disease leads to the second. Identifying\\textbackslash{}nand understanding these mechanisms is likely to im-\\textbackslash{}nprove prevention, prediction of disease progression,\\textbackslash{}nintervention, and drug development, thereby indirectly\\textbackslash{}nsupporting clinical practice and health-care policy. For\\textbackslash{}nnow, such detailed examinations of the protein paths\\textbackslash{}nwere beyond the scope of this project.\\textbackslash{}nA downside of working on the protein level was that not\\textbackslash{}nall disease trajectories could be studied. More than a third\\textbackslash{}nof the trajectories of the Jensen set, and a fifth of the Van\\textbackslash{}nden Akker set was lost because disease proteins could not\\textbackslash{}nbe assigned to one or both of the diseases in a trajectory.\\textbackslash{}nEven when disease proteins could be assigned to both dis-\\textbackslash{}neases, alternative explanations were sometimes more\\textbackslash{}nplausible. For example, our analysis of the false-negative\\textbackslash{}ncases suggested that some trajectories could be explained\\textbackslash{}nmechanically, or were likely due to a side effect of the\\textbackslash{}ntreatment for the first disease. To determine the true per-\\textbackslash{}nformance of our method, a validated set of trajectories that\\textbackslash{}nare caused by biomolecular mechanisms would be needed.\\textbackslash{}nAlternatively, the range of trajectories that can be analyzed\\textbackslash{}nmay be broadened by linking diseases to other types of\\textbackslash{}ndisease information available in the EKP, e.g., information\\textbackslash{}nabout drugs or physiological processes.\\textbackslash{}nThe two reference sets that were used in this research\\textbackslash{}nwere both based on patient'\n\\item[text10] 'Moen et al. Journal of Biomedical Semantics           (2020) 11:10 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00229-7\\textbackslash{}nRESEARCH Open Access\\textbackslash{}nAssisting nurses in care documentation:\\textbackslash{}nfrom automated sentence classification to\\textbackslash{}ncoherent document structures with subject\\textbackslash{}nheadings\\textbackslash{}nHans Moen1*\\textbackslash{}u0086 , Kai Hakala1,2\\textbackslash{}u0086, Laura-Maria Peltonen3, Hanna-Maria Matinolli3, Henry Suhonen3,4,\\textbackslash{}nKirsi Terho3,4, Riitta Danielsson-Ojala3,4, Maija Valta4, Filip Ginter1, Tapio Salakoski1 and Sanna Salanterä3,4\\textbackslash{}nAbstract\\textbackslash{}nBackground: Up to 35\\% of nurses\\textbackslash{}u0092 working time is spent on care documentation. We describe the evaluation of a\\textbackslash{}nsystem aimed at assisting nurses in documenting patient care and potentially reducing the documentation workload.\\textbackslash{}nOur goal is to enable nurses to write or dictate nursing notes in a narrative manner without having to manually\\textbackslash{}nstructure their text under subject headings. In the current care classification standard used in the targeted hospital,\\textbackslash{}nthere aremore than 500 subject headings to choose from, making it challenging and time consuming for nurses to use.\\textbackslash{}nMethods: The task of the presented system is to automatically group sentences into paragraphs and assign subject\\textbackslash{}nheadings. For classification the system relies on a neural network-based text classification model. The nursing notes\\textbackslash{}nare initially classified on sentence level. Subsequently coherent paragraphs are constructed from related sentences.\\textbackslash{}nResults: Based on a manual evaluation conducted by a group of three domain experts, we find that in about 69\\% of\\textbackslash{}nthe paragraphs formed by the system the topics of the sentences are coherent and the assigned paragraph headings\\textbackslash{}ncorrectly describe the topics. We also show that the use of a paragraph merging step reduces the number of\\textbackslash{}nparagraphs produced by 23\\% without affecting the performance of the system.\\textbackslash{}nConclusions: The study shows that the presented system produces a coherent and logical structure for freely written\\textbackslash{}nnursing narratives and has the potential to reduce the time and effort nurses are currently spending on documenting\\textbackslash{}ncare in hospitals.\\textbackslash{}nKeywords: Patient care documentation, Nursing documentation, Electronic health records, Text classification,\\textbackslash{}nNatural language processing, Neural networks, Model interpretation\\textbackslash{}n*Correspondence: hnsmoen@gmail.com\\textbackslash{}n\\textbackslash{}u0086Hans Moen and Kai Hakala contributed equally to this work.\\textbackslash{}n1Department of Future Technologies, University of Turku, Vesilinnantie 5,\\textbackslash{}n20500 Turku, Finland\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\textbackslash{}ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were\\textbackslash{}nmade. The images or other third party material in this article are included in the article\\textbackslash{}u0092s Creative Commons licence, unless\\textbackslash{}nindicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}u0092s Creative Commons licence and your\\textbackslash{}nintended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly\\textbackslash{}nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative\\textbackslash{}nCommons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made\\textbackslash{}navailable in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 2 of 12\\textbackslash{}nBackground\\textbackslash{}nCare documentation is important for supporting the con-\\textbackslash{}ntinuity of care in hospitals. According to literature, nurses\\textbackslash{}nspend up to 35\\% (with an average of 19\\%) of their working\\textbackslash{}ntime on documentation {[}1{]}. Naturally, if we can reduce the\\textbackslash{}ntime that nurses spend on documentation, more time will\\textbackslash{}nbe available for direct patient care.\\textbackslash{}nTo support tasks such as navigation, planning and sta-\\textbackslash{}ntistical analysis, nurses in many countries are required\\textbackslash{}nto perform structuring of the information they write {[}2{]}.\\textbackslash{}nSuch structuring approaches include the use of documen-\\textbackslash{}ntation standards, classifications and standardized termi-\\textbackslash{}nnologies {[}3{]}. However, this usually adds certain restric-\\textbackslash{}ntions and requirements to the documentation process\\textbackslash{}ncompared to writing the information in an unstruc-\\textbackslash{}ntured narrative manner. In Finland, nurses are nowa-\\textbackslash{}ndays expected to structure the information they write\\textbackslash{}nby using subject headings from the Finnish Care Clas-\\textbackslash{}nsification (FinCC) standard {[}4{]}. This includes selecting\\textbackslash{}nthe correct subject headings and writing the associated\\textbackslash{}ninformation underneath. In this way, each subject head-\\textbackslash{}ning forms a paragraph in the nursing note. As an example,\\textbackslash{}nif a nurse wants to write something about administrated\\textbackslash{}nwound care, he/she will first have to select an appropri-\\textbackslash{}nate heading, e.g. \\textbackslash{}u0093Wound\\textbackslash{}u0094. FinCC consists primarily of two\\textbackslash{}ntaxonomy resources, the Finnish Classification of Nurs-\\textbackslash{}ning Diagnoses (FiCND) and the Finnish Classification of\\textbackslash{}nNursing Interventions (FiCNI), and both of these have a\\textbackslash{}nthree-level hierarchy. For example, one branch in FiCND\\textbackslash{}nis: \\textbackslash{}u0093Tissue integrity\\textbackslash{}u0094 (level 1), \\textbackslash{}u0093Chronic wound\\textbackslash{}u0094 (level 2)\\textbackslash{}nand \\textbackslash{}u0093Infected wound\\textbackslash{}u0094 (level 3). Another example, a branch\\textbackslash{}nfrom FiCNI is: \\textbackslash{}u0093Medication\\textbackslash{}u0094 (level 1), \\textbackslash{}u0093Pharmacotherapy\\textbackslash{}u0094\\textbackslash{}n(level 2) and \\textbackslash{}u0093Pharmaceutical treatment, oral instructions\\textbackslash{}u0094\\textbackslash{}n(level 3). However, FinCC consists of more than 500 sub-\\textbackslash{}nject headings, covering both interventions and diagnoses.\\textbackslash{}nThis makes it potentially challenging and time consuming\\textbackslash{}nfor nurses to use since they are required to memorize, use\\textbackslash{}nand structure the information they write according to a\\textbackslash{}nlarge number of subject headings {[}5{]}.\\textbackslash{}nWhat we are aiming for is to develop a system that can\\textbackslash{}nassist nurses in selecting suitable subject headings and in\\textbackslash{}nstructuring the text accordingly.We hypothesize that such\\textbackslash{}na system has the potential to save time and effort required\\textbackslash{}nfor documentation and ultimately free up more time for\\textbackslash{}nother tasks. We see two use-cases for such a system: One\\textbackslash{}nis where the system assists nurses in selecting appropri-\\textbackslash{}nate headings when they write, in a suggestive manner, e.g.,\\textbackslash{}nper sentence or paragraph; A second use-case is where\\textbackslash{}nnurses are allowed to write or dictate (by voice to text)\\textbackslash{}nin a fully unstructured narrative manner, without having\\textbackslash{}nto take into consideration the structure or the use of sub-\\textbackslash{}nject headings. Instead the system assigns subject headings\\textbackslash{}nafterwards and restructures the text into paragraphs. In\\textbackslash{}nthis study we focus on the second use-case.\\textbackslash{}nThis is the continuation of a previously reported study\\textbackslash{}nthat focused on assessing how an earlier version of the\\textbackslash{}nsystem performs on the level of sentences {[}6{]}. The main\\textbackslash{}nconclusion of that study is that a sentence classification\\textbackslash{}nmodel trained on semi-structured nursing notes can be\\textbackslash{}napplied on unstructured free nursing narratives without a\\textbackslash{}nsubstantial decline in accuracy.\\textbackslash{}nThis time we focus on paragraph-level assessment,\\textbackslash{}nwhere we also explore a post-processing step aimed at\\textbackslash{}nreducing the number of paragraphs initially generated\\textbackslash{}nby the system. To evaluate our system, a team of three\\textbackslash{}ndomain experts (aka evaluators) conduct a manual evalu-\\textbackslash{}nation to assess both the grouping of sentences into para-\\textbackslash{}ngraphs and the correctness of the assigned headings. In\\textbackslash{}naddition we analyze the classification model in an attempt\\textbackslash{}nto identify conflicts between the actual use of the sub-\\textbackslash{}nject headings and the intended use according to the FinCC\\textbackslash{}ntaxonomy.\\textbackslash{}nAt the core of our system is a text classification model\\textbackslash{}nbased on a bidirectional long short-termmemory (LSTM)\\textbackslash{}nrecurrent neural network architecture {[}7, 8{]}. As train-\\textbackslash{}ning data we use a large collection of nursing notes from\\textbackslash{}na Finnish hospital which contain subject headings and\\textbackslash{}nwhich are structured accordingly. Further, to acquire the\\textbackslash{}ntype of narrative text that we would like to use as input\\textbackslash{}nto the system, without a bias towards a particular struc-\\textbackslash{}nture and subject headings, we made a set of nursing notes\\textbackslash{}nbased on artificial patients that we use for testing.\\textbackslash{}nRelated work\\textbackslash{}nAs we focus on classifying individual sentences, the work\\textbackslash{}nis closely related to other short text classification studies.\\textbackslash{}nHowever, most of the prior work focuses on texts col-\\textbackslash{}nlected from social media or other online sources {[}9\\textbackslash{}u009611{]}.\\textbackslash{}nInterestingly, Zhang et al. {[}12{]} conclude that the optimal\\textbackslash{}ntext classification method is strongly dependent on the\\textbackslash{}nselected task, warranting domain specific research on this\\textbackslash{}ntopic.\\textbackslash{}nIn the clinical domain, a common objective for text\\textbackslash{}nclassification has been the automated assignment of ICD\\textbackslash{}ncodes to the target documents {[}13\\textbackslash{}u009615{]}. For instance Xie et\\textbackslash{}nal. {[}16{]} use a neural model for mapping diagnosis descrip-\\textbackslash{}ntions extracted from discharge notes to the corresponding\\textbackslash{}nICD codes. Similarly Koopman et al. {[}17{]} assign ICD-10\\textbackslash{}ncodes to death certificates, but limit the scope to various\\textbackslash{}ncancer types.\\textbackslash{}nFor cases where available training data is scarce, Wang\\textbackslash{}net al. {[}18{]} propose a system for producing weakly labeled\\textbackslash{}ntraining data, where simple rules are initially used to\\textbackslash{}nlabel a large set of unlabeled clinical documents and\\textbackslash{}nthese labels are subsequently used as training targets for\\textbackslash{}nmachine learning based classifiers. The approach is eval-\\textbackslash{}nuated on smoking status and hip fracture classification,\\textbackslash{}nbut shows mixed results, with a rule-based baseline being\\textbackslash{}nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 3 of 12\\textbackslash{}nthe strongest model in some cases. As our training dataset\\textbackslash{}ninherently contains the used classification labels, we have\\textbackslash{}nnot considered such weak supervision in our research.\\textbackslash{}nTo our knowledge the most recent systematic review\\textbackslash{}non clinical text classification was conducted by Mujtaba\\textbackslash{}net al. {[}19{]}. In addition to comparing the classification\\textbackslash{}napproaches utilized in different studies, the review focuses\\textbackslash{}non the differences in the selected datasets. Their study\\textbackslash{}nindicates that along with the medical literature, clinical\\textbackslash{}ntext classification research mostly focuses on pathology,\\textbackslash{}nradiology and autopsy reports, whereas other clinical doc-\\textbackslash{}numents such as nursing care records are far less stud-\\textbackslash{}nied. Moreover, the vast majority of the reviewed studies\\textbackslash{}nonly evaluate their methods on English data, leading to\\textbackslash{}nMujtaba et al. suggesting wider range of languages to be\\textbackslash{}nincluded in these studies.\\textbackslash{}nAs an additional note, Mujtaba et al. also conclude that\\textbackslash{}ndeep learning methods are still relatively poorly studied in\\textbackslash{}nthis domain. However, lately neural approaches have been\\textbackslash{}nsuggested for a wide range of medical text classification\\textbackslash{}npurposes {[}20\\textbackslash{}u009622{]}.\\textbackslash{}nMore related to our research are prior studies on clinical\\textbackslash{}nnote segmentation. Denny et al. {[}23{]} present an approach\\textbackslash{}nfor detecting section headers in clinical notes based on the\\textbackslash{}nfree text. More precisely, they focus on history and phys-\\textbackslash{}nical examination documents where the goal is to identify\\textbackslash{}nand normalize section headers as well as to detect section\\textbackslash{}nboundaries. Li et al. {[}24{]} present a system that catego-\\textbackslash{}nrizes sections in clinical notes into one of 15 pre-defined\\textbackslash{}nsection labels for notes already split into sections. Their\\textbackslash{}napproach relies on modelling the dependencies of consec-\\textbackslash{}nutive section labels with Hidden Markov Models. In {[}25{]}\\textbackslash{}ncoarse topics are assigned to the sections found in clin-\\textbackslash{}nical notes. These topics are here seen as separate from\\textbackslash{}nthe section headings used by the clinicians when writing,\\textbackslash{}nthus the section headings are considered as input to the\\textbackslash{}nclassifier along with the free text.\\textbackslash{}nA distinction between our study and the prior work\\textbackslash{}nis that we operate with an order of magnitude larger\\textbackslash{}nset of section labels. Additionally, we rely on semi-\\textbackslash{}nstructured nursing notes as training data with the devel-\\textbackslash{}noped method subsequently being applied on unstruc-\\textbackslash{}ntured notes. Thus, we do not utilize any prior knowledge\\textbackslash{}nabout paragraphs/sections. Grouping the text into sensi-\\textbackslash{}nble paragraphs is instead a task for the presented system \\textbackslash{}u0096\\textbackslash{}ntogether with assigning subject headings.\\textbackslash{}nMethods\\textbackslash{}nOur ultimate goal is to develop a system that is able\\textbackslash{}nto automatically identify and classify, on sentence level,\\textbackslash{}ninterventions and diagnoses mentioned in nursing narra-\\textbackslash{}ntives, and further capable of grouping the text into sensible\\textbackslash{}nparagraphs with subject headings reflecting their topics.\\textbackslash{}nIn other words, we are aiming for a system that can let\\textbackslash{}nnurses simply write or dictate in a narrative manner with-\\textbackslash{}nout having to plan and structure the text with respect to\\textbackslash{}nparagraphs and subject headings. In pursuing this goal\\textbackslash{}nwe have implemented a prototype system with a neural\\textbackslash{}nnetwork-based text classification model at its core. In this\\textbackslash{}nsection we describe the data and methods used in the\\textbackslash{}nimplementation and evaluation.\\textbackslash{}nData\\textbackslash{}nThe data set used for training is a collection of approxi-\\textbackslash{}nmately 0.5 million patients\\textbackslash{}u0092 nursing notes extracted from a\\textbackslash{}nuniversity hospital in Finland. The selection criteria were\\textbackslash{}npatients with any type of heart-related problem in the\\textbackslash{}nperiod 2005 to 2009 and nursing notes from all units vis-\\textbackslash{}nited during their hospital stay are included. The data is\\textbackslash{}ncollected during a transition period between two classi-\\textbackslash{}nfication standards, the latter being the mentioned FinCC\\textbackslash{}nstandard. This means that our training data contains a\\textbackslash{}nmixture of headings from these two. We only use sen-\\textbackslash{}ntences occurring in paragraphs with subject headings,\\textbackslash{}nwhich amounts to approximately 5.5 million sentences,\\textbackslash{}n133,890 unique tokens and approximately 38.5 million\\textbackslash{}ntokens in total. We exclude all subject headings used less\\textbackslash{}nthan 100 times, resulting in 676 unique subject headings,\\textbackslash{}nwhere their frequency count range from 100 to 222,984,\\textbackslash{}nwith an average of 4,896. The individual sentences are\\textbackslash{}nused as a training example with the corresponding subject\\textbackslash{}nheading as the target class to be predicted. The average\\textbackslash{}nsentence length is 7 tokens1 and the average number of\\textbackslash{}nsentences per paragraph is 2.1. The data set is split into\\textbackslash{}ntraining (60\\%), development (20\\%) and test (20\\%) sets and\\textbackslash{}nfurther used to train and optimize the text classification\\textbackslash{}nmodel.\\textbackslash{}nText classification model\\textbackslash{}nThe classification task is approached as a sentence-level\\textbackslash{}nmulticlass classification task, where each sentence is\\textbackslash{}nassumed to have one correct subject heading (label). Our\\textbackslash{}ntext classification model is based on a bidirectional short-\\textbackslash{}nterm memory (LSTM) recurrent neural network archi-\\textbackslash{}ntecture {[}7, 8{]}. The model receives a sequence of words\\textbackslash{}nas its input and encodes them into latent feature vectors\\textbackslash{}n(dimensionality 300). These vectors are subsequently used\\textbackslash{}nas the input for a bidirectional LSTM layer (dimensional-\\textbackslash{}nity 600 per direction). As the final layer a fully connected\\textbackslash{}nlayer with the dimensionality corresponding to the num-\\textbackslash{}nber of target subject headings is used. The word embed-\\textbackslash{}ndings are pretrained with Word2vec {[}26{]}. The model is\\textbackslash{}noptimized for categorical cross-entropy with Adam opti-\\textbackslash{}nmizer {[}27{]}, stopping early based on the development set\\textbackslash{}nperformance. As machine learning tools we primarily use\\textbackslash{}n1Space separated units.\\textbackslash{}nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 4 of 12\\textbackslash{}nthe Keras deep learning library {[}28{]}, with TensorFlow\\textbackslash{}nlibrary {[}29{]} as backend.\\textbackslash{}nWe want to emphasize that the focus of this paper is not\\textbackslash{}nto find the optimal text classificationmethod and parame-\\textbackslash{}nter settings for this task. This has instead been the focus of\\textbackslash{}nanother study {[}30{]}, where a range of different state-of-the-\\textbackslash{}nart and baseline text classification methods are tested and\\textbackslash{}ncompared. Results from the mentioned study indicate that\\textbackslash{}na bidirectional version of LSTM networks performs best\\textbackslash{}nwhen compared to other classification methods/models,\\textbackslash{}nincluding convolutional neural networks, support vector\\textbackslash{}nmachines and random forests {[}31\\textbackslash{}u009633{]}.\\textbackslash{}nOn the test set, when the classifier is allowed to suggest\\textbackslash{}none subject heading per sentence, it suggests the correct\\textbackslash{}nheading for 54.35\\% of the sentences according to auto-\\textbackslash{}nmated evaluation. When allowed to suggest 10 headings\\textbackslash{}nper sentence, the correct one is among these 89.54\\% of the\\textbackslash{}ntime (see {[}30{]} for more details).\\textbackslash{}nSubject heading prediction and grouping into paragraphs\\textbackslash{}nSince our prototype system relies primarily on a sentence-\\textbackslash{}nlevel classification model, it starts by classifying each sen-\\textbackslash{}ntence individually before grouping them into paragraphs.\\textbackslash{}nHowever, this might arguably be the opposite order of how\\textbackslash{}na human would approach this task. The system\\textbackslash{}u0092s opera-\\textbackslash{}ntion can be described as a four-step process. Step 1: First\\textbackslash{}nthe text is split into sentences. For this we rely on a com-\\textbackslash{}nbination of the NLTK tokenizers for Finnish {[}34{]} and a set\\textbackslash{}nof regular expressions tailored for the clinical text. Step 2:\\textbackslash{}nNext the classification model is used to classify each sen-\\textbackslash{}ntence individually and assign the top predicted heading\\textbackslash{}n(the one with the highest confidence value). Step 3: As\\textbackslash{}na third step the sentences with the same assigned sub-\\textbackslash{}nject heading are grouped into paragraphs. Step 4: The\\textbackslash{}nfourth step focuses on merging paragraphs whose content\\textbackslash{}nand assigned headings are close to each other in terms of\\textbackslash{}nmeaning. This fourth step is included to potentially reduce\\textbackslash{}nthe number of paragraphs to more closely simulate how\\textbackslash{}nnurses document. Below we explain in more detail how\\textbackslash{}nthis fourth step is done.\\textbackslash{}nParagraphmerging explained: In the previous study {[}6{]},\\textbackslash{}nthe evaluators reported that the system showed a ten-\\textbackslash{}ndency to assign subject headings with a high level of\\textbackslash{}nspecificity, and sometimes even too specific to be prac-\\textbackslash{}ntical. For example, for two or more sentences describing\\textbackslash{}ndifferent aspects of pain management in the same nurs-\\textbackslash{}ning note, such as treatment and medication, the system\\textbackslash{}nwould in some cases assign these to different subject\\textbackslash{}nheadings, possibly headings of different level of speci-\\textbackslash{}nficity/abstraction. This meant that, in some cases, unnec-\\textbackslash{}nessarily many unique headings, thus paragraphs, were\\textbackslash{}nassigned to each nursing note.\\textbackslash{}nIn an attempt to reduce the number of paragraphs cre-\\textbackslash{}nated, to more closely simulate how nurses document, we\\textbackslash{}nhave implemented an experimental post-processing step\\textbackslash{}nthat enables the system to merge paragraphs (within a\\textbackslash{}nnursing note) that it finds to have similar subject head-\\textbackslash{}nings. For this we primarily rely on the confidence values\\textbackslash{}nof the classification model, as well as extracted vec-\\textbackslash{}ntor representations of each subject heading. The LSTM\\textbackslash{}nlayer outputs 600 dimensional sentence encodings for\\textbackslash{}nboth directions of the input sequence, resulting in 1200\\textbackslash{}ndimensional vectors representations for the subject head-\\textbackslash{}nings. These we use to calculate heading similarity by\\textbackslash{}napplying the cosine distance metric. See the \\textbackslash{}u0093Data anal-\\textbackslash{}nysis\\textbackslash{}u0094 section for further description of these heading\\textbackslash{}nvectors.\\textbackslash{}nFirst a paragraph-to-paragraph similarity matrix is\\textbackslash{}nformed reflecting how each paragraph would consider the\\textbackslash{}nsubject headings from the other paragraphs (from step\\textbackslash{}n3) as a likely candidate heading. To this end we define\\textbackslash{}na simple asymmetric similarity function which measures\\textbackslash{}nhow inclined a paragraph (source) is towards the head-\\textbackslash{}ning of another paragraph (target) in the same nursing\\textbackslash{}nnote. For each sentence in a given source paragraph we\\textbackslash{}ntake the classifier\\textbackslash{}u0092s confidence of the sentence belong-\\textbackslash{}ning to the target heading and subtract the difference in\\textbackslash{}nthe confidence between predicting the source heading\\textbackslash{}nand the target heading. The individual sentence scores\\textbackslash{}nare averaged and further summed with the cosine dis-\\textbackslash{}ntance between the source and target headings and the\\textbackslash{}nrelative size of the target paragraph (compared against\\textbackslash{}nthe whole nursing note). The first component, relying\\textbackslash{}non the confidence values of the classifier, describes how\\textbackslash{}nwell the sentences fit in the target paragraph. The sec-\\textbackslash{}nond component measures how semantically similar the\\textbackslash{}ncompared paragraph headings are, more similar headings\\textbackslash{}nbeing more likely to be merged. The third component\\textbackslash{}nincreases preference towards retaining the headings of\\textbackslash{}nthe larger paragraphs. This scoring function produces\\textbackslash{}nvalues in the range 3 to minus 2. Note that it is not\\textbackslash{}nsymmetrical.\\textbackslash{}nTo determine if two paragraphs are to be merged,\\textbackslash{}nwe require that the similarity between these two para-\\textbackslash{}ngraphs, in both directions, is above a given threshold.\\textbackslash{}nIf the threshold is exceeded, the two most similar para-\\textbackslash{}ngraphs are merged, keeping the heading of the para-\\textbackslash{}ngraph with the lowest score out of the two. Subsequently\\textbackslash{}nthe similarity matrix is recalculated, and the process is\\textbackslash{}nrepeated until no paragraph pairs can bemerged.We opti-\\textbackslash{}nmize this threshold on a sample of nursing notes from\\textbackslash{}nthe test data where paragraph information and head-\\textbackslash{}nings are removed. A threshold is found that enables the\\textbackslash{}nsystem to generate approximately the same number of\\textbackslash{}nparagraphs as in the original versions of these nursing\\textbackslash{}nnotes.\\textbackslash{}nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 5 of 12\\textbackslash{}nSystem evaluation\\textbackslash{}nIn this experiment the focus is on evaluating how the\\textbackslash{}nsystem performs at the intended task. Two versions of\\textbackslash{}nthe system are manually evaluated, NoMerging and\\textbackslash{}nWithMerging, where the difference is that NoMerging\\textbackslash{}nonly performs steps 1\\textbackslash{}u00963, while WithMerging also per-\\textbackslash{}nforms step 4. This comparison is done to see if the para-\\textbackslash{}ngraph merging (step 4) can be done without reducing\\textbackslash{}nsystem performance according to the evaluators\\textbackslash{}u0092 assess-\\textbackslash{}nments. To perform the evaluation two domain experts\\textbackslash{}nwith nursing background first evaluated the paragraphs\\textbackslash{}nindividually. Then we consulted a third domain expert\\textbackslash{}nwho provided a third opinion for the instances where the\\textbackslash{}ntwo disagreed. Finally the three of them agreed on the final\\textbackslash{}nconsensus version which we report here.\\textbackslash{}nThe evaluation focuses on two aspects of the struc-\\textbackslash{}ntured notes produced by the system: 1) The correctness of\\textbackslash{}nthe assigned subject headings at paragraph level. Table 1\\textbackslash{}nshows the classes used by the evaluators; 2) The quality of\\textbackslash{}nthe formed paragraphs, i.e. sentence grouping. The classes\\textbackslash{}nused in this assessment are shown in Table 2.\\textbackslash{}nThe nursing notes from the training data have been\\textbackslash{}nplanned, structured and written with subject headings in\\textbackslash{}nmind. One could argue that by simply removing headings\\textbackslash{}nand paragraph information, automated evaluation could\\textbackslash{}nbe implemented. However, we found that the sentences\\textbackslash{}nhere, which are structured under subject headings, have a\\textbackslash{}ntendency to be biased towards the topic of their headings,\\textbackslash{}nand sometimes their meaning can only be interpreted in\\textbackslash{}nthe context of their headings. Also, this structuring forces\\textbackslash{}nthe nurses to write very short and concise things, whereas\\textbackslash{}nwhen given the freedom to write in a narrative manner,\\textbackslash{}nmore complex sentence structures are present. Thus, to\\textbackslash{}nobtain relevant nursing notes for evaluation of our use\\textbackslash{}ncase \\textbackslash{}u0096 notes written in a free narrative style without plan-\\textbackslash{}nning for or considering the use of paragraphs and subject\\textbackslash{}nheadings \\textbackslash{}u0096 we asked five domain experts with nursing\\textbackslash{}nbackground to write notes based on made up artificial\\textbackslash{}npatients. In total, 40 nursing notes, each note representing\\textbackslash{}none day of provided care for a patient, were generated. The\\textbackslash{}ntop part of Fig. 1 shows an example of one such nursing\\textbackslash{}nnote.\\textbackslash{}nTable 1 Classes used by the evaluators when assessing the\\textbackslash{}nheadings assigned by the system\\textbackslash{}nClass Description\\textbackslash{}n1 Correct: the subject heading suits the text in this paragraph.\\textbackslash{}n2 Partly correct: the subject heading only suits some of the text,\\textbackslash{}nnot all.\\textbackslash{}n3 Incorrect: the subject heading does not seem to suit any of\\textbackslash{}nthe text.\\textbackslash{}n4 Unable to assess: unable to asses whether or not this subject\\textbackslash{}nheading is suitable.\\textbackslash{}nTable 2 Classes used by the evaluators when assessing the\\textbackslash{}nparagraphs formed by the system\\textbackslash{}nClass Description\\textbackslash{}na Sensible grouping: it makes sense to have these sentences\\textbackslash{}ngrouped together as a separate paragraph based on their\\textbackslash{}ntopic(s) (even if the subject heading may not fit).\\textbackslash{}nb Inconsistent/problematic grouping \\textbackslash{}u0096 alt1: one or more\\textbackslash{}nsentences in this paragraph would fit better in other para-\\textbackslash{}ngraph(s) in this note.\\textbackslash{}nc Inconsistent/problematic grouping \\textbackslash{}u0096 alt2: one or more\\textbackslash{}nsentences in this paragraph do not belong in this or any of the\\textbackslash{}nother paragraphs in this note.\\textbackslash{}nd Unable to assess: unable to evaluate this paragraph.\\textbackslash{}nThese 40 nursing notes were fed to the two versions of\\textbackslash{}nthe system, NoMerging and WithMerging. For eval-\\textbackslash{}nuation purposes the output was stored as spreadsheets,\\textbackslash{}none for each system, each containing both the origi-\\textbackslash{}nnal and the generated/structured version of each nursing\\textbackslash{}nnote.\\textbackslash{}nStatistical analyzes were performed to investigate differ-\\textbackslash{}nences in themanual evaluations of the two system versions\\textbackslash{}n(Pearson\\textbackslash{}u0092s chi-squared test), as well as to see if there is\\textbackslash{}na possible correlation between manual evaluations and\\textbackslash{}nthe classification model\\textbackslash{}u0092s confidence values (Spearman\\textbackslash{}u0092s\\textbackslash{}nrho).\\textbackslash{}nTo gain some qualitative feedback on the system, we also\\textbackslash{}nasked the evaluators to answer the following open-ended\\textbackslash{}nquestions:\\textbackslash{}nQ1: Can you mention the main strengths that you found\\textbackslash{}nwith the system(s)?\\textbackslash{}nQ2: Can you mention the main weaknesses that you\\textbackslash{}nfound with the system(s)?\\textbackslash{}nQ3: Do you, or do you not, think that this kind of a\\textbackslash{}nsystem would be helpful when it comes to nursing\\textbackslash{}ndocumentation, and why?\\textbackslash{}nData analysis\\textbackslash{}nWe hypothesize that the large amount of subject headings\\textbackslash{}nin the FinCC classification standard may cause confusion\\textbackslash{}namong the nurses in terms of what headings should be\\textbackslash{}nused in documenting the various aspects of the admin-\\textbackslash{}nistrated care. Thus, to obtain a deeper understanding of\\textbackslash{}nthe evaluated sentence classification model and the care\\textbackslash{}ndocumentation conventions of the nurses, we analyze\\textbackslash{}nthe heading representations learned by the classification\\textbackslash{}nmodel \\textbackslash{}u0096 reflecting how they have been used \\textbackslash{}u0096 and how\\textbackslash{}nthis may differ from their description and intended use\\textbackslash{}nbased on FinCC.\\textbackslash{}nThe weights of the fully connected output layer of the\\textbackslash{}ntrained classifier can be seen as semantic representations\\textbackslash{}nof the subject headings since the weights corresponding\\textbackslash{}nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 6 of 12\\textbackslash{}nFig. 1 Nursing note example. Top: Without any particular structure or assigned subject headings. Input to the system. Bottom: Grouped into\\textbackslash{}nparagraphs with assigned headings. Output from the system. This has been translated from Finnish to English\\textbackslash{}nto a given heading define how strongly the heading is\\textbackslash{}nactivated for a given input sentence, compared against\\textbackslash{}nother possible headings. Thus, two headings with similar\\textbackslash{}nweights will have similar probabilities of being assigned\\textbackslash{}nto a given input sentence. Inversely, under the assump-\\textbackslash{}ntion that the model has learned the classification task well,\\textbackslash{}nit can be hypothesized that if two headings have similar\\textbackslash{}nweights, the sentences assigned under these headings in\\textbackslash{}nthe training data are also similar. Note that these represen-\\textbackslash{}ntations are not based on the names of the subject headings,\\textbackslash{}nbut instead on the actual sentences written under the\\textbackslash{}nheadings.\\textbackslash{}nOur main goal in this analysis is to verify whether we are\\textbackslash{}nable to find subject headings which are semantically sim-\\textbackslash{}nilar according to our classification model, but far apart in\\textbackslash{}nthe used FinCC taxonomy, or vice versa. This allows us to\\textbackslash{}nidentify conflicts between the actual use of headings and\\textbackslash{}ntheir intended use according to the taxonomy. To mea-\\textbackslash{}nsure the distances of the subject heading representations\\textbackslash{}nwe simply calculate the cosine distance across all heading\\textbackslash{}npairs.\\textbackslash{}nThe used FinCC classification standard is comprised of\\textbackslash{}n3 top level categories: nursing diagnoses, nursing inter-\\textbackslash{}nventions and nursing outcomes, however the nursing out-\\textbackslash{}nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 7 of 12\\textbackslash{}ncome headings are not present in the used data. Both\\textbackslash{}nnursing diagnoses and interventions use a hierarchical\\textbackslash{}nstructure with maximum depth of 3. To form a single tree,\\textbackslash{}nwe connect the diagnoses and interventions categories\\textbackslash{}nwith an artificial root node. This combined tree has amax-\\textbackslash{}nimum depth of 4. To measure the distances of headings in\\textbackslash{}nFinCC we calculate the shortest path between the head-\\textbackslash{}ning nodes in the tree. Although simple, this approach has\\textbackslash{}nshown strong performance in measuring concept similar-\\textbackslash{}nities in other biomedical ontologies {[}35{]}.\\textbackslash{}nOnce we have the two distances calculated for all sub-\\textbackslash{}nject heading pairs \\textbackslash{}u0096 cosine distance and distance in the\\textbackslash{}ntree \\textbackslash{}u0096 we rank each pair based on these two, resulting\\textbackslash{}nin two distinct rankings. The conflicting pairs that we\\textbackslash{}nselect for further analysis are the ones being furthest apart\\textbackslash{}naccording to these two rankings.\\textbackslash{}nSince the nursing notes include the used subject head-\\textbackslash{}nings as plain text, without containing the actual FinCC\\textbackslash{}nidentifiers, we use strict string matching to map the head-\\textbackslash{}nings to the corresponding FinCC concepts. This leaves us\\textbackslash{}nwith 263 headings for this analysis out of the total 676\\textbackslash{}nheadings in our data set. The excluded headings either\\textbackslash{}noriginate from the older classification standard or contain\\textbackslash{}nspelling variations.\\textbackslash{}nResults\\textbackslash{}nIn this section we first present the results from the sys-\\textbackslash{}ntem evaluation. Next we highlight some of the observa-\\textbackslash{}ntions from the analysis of subject heading representations\\textbackslash{}naccording to the classification model and the underlying\\textbackslash{}nclassification standard.\\textbackslash{}nSystem evaluation results\\textbackslash{}nThis experiment provided insight into how the system\\textbackslash{}nperforms at the intended task of assigning applicable sub-\\textbackslash{}nject headings and grouping sentences into paragraphs.\\textbackslash{}nTable 3 shows how well the assigned subject headings fit\\textbackslash{}nthe text in the paragraphs. Table 4 reflects what the eval-\\textbackslash{}nuators think about the integrity of the paragraphs formed\\textbackslash{}nby the system.\\textbackslash{}nSee Fig. 1 for an example showing a input note to the\\textbackslash{}nsystem (top) and the output (bottom) where the text is\\textbackslash{}ngrouped into paragraphs with assigned subject headings.\\textbackslash{}nTable 3 Subject headings evaluation results. See Table 1 for an\\textbackslash{}nexplanation of the classes\\textbackslash{}nClass NoMerging n(tot=396) WithMerging n(tot=305)\\textbackslash{}n1 70.45\\% 279 71.15\\% 217\\textbackslash{}n2 14.65\\% 58 16.72\\% 51\\textbackslash{}n3 14.14\\% 56 11.80\\% 36\\textbackslash{}n4 0.76\\% 3 0.33\\% 1\\textbackslash{}n1+2 85.10\\% 337 87.87\\% 268\\textbackslash{}nTable 4 Paragraph (sentence grouping) evaluation results. See\\textbackslash{}nTable 2 for an explanation of the classes\\textbackslash{}nClass NoMerging n(tot=396) WithMerging n(tot=305)\\textbackslash{}na 79.55\\% 315 79.02\\% 241\\textbackslash{}nb 15.66\\% 62 12.13\\% 37\\textbackslash{}nc 3.79\\% 15 8.52\\% 26\\textbackslash{}nd 1.01\\% 4 0.33\\% 1\\textbackslash{}nOverall these results show that the system is able to\\textbackslash{}nprovide suitable subject headings for about 71\\% of the\\textbackslash{}nparagraphs (class \\textbackslash{}u00911\\textbackslash{}u0092). They also indicate that about 79\\%\\textbackslash{}nof the paragraphs formed are sensible (class \\textbackslash{}u0091a\\textbackslash{}u0092). By sensi-\\textbackslash{}nble paragraphs we mean that all the sentences within are\\textbackslash{}nrelated to the same topic and that none of them would fit\\textbackslash{}nbetter elsewhere in the corresponding nursing note.\\textbackslash{}nWhen using NoMerging the number of paragraphs\\textbackslash{}nformed is 396, with an average of 9.9 per note (min=3,\\textbackslash{}nmax=19). When using WithMerging, which also per-\\textbackslash{}nforms the paragraph merging step, the number of para-\\textbackslash{}ngraphs is reduced by 23\\%, down to 305, with the average\\textbackslash{}nper note being 7.6 (min=2, max=17).\\textbackslash{}nWe also calculated how many of the formed para-\\textbackslash{}ngraphs were consistent (class \\textbackslash{}u0091a\\textbackslash{}u0092) while also having a\\textbackslash{}nsuitable subject heading (class \\textbackslash{}u00911\\textbackslash{}u0092). The result is seen in\\textbackslash{}nTable 5 and shows that 66.67\\% (NoMerging) and 68.85\\%\\textbackslash{}n(WithMerging) of the paragraphs are both sensible and\\textbackslash{}nhave a correctly describing subject heading assigned to\\textbackslash{}nthem. These results show that the merging step results in\\textbackslash{}nbasically no loss in performance.\\textbackslash{}nPearson\\textbackslash{}u0092s chi-squared tests were performed to see\\textbackslash{}nwhether there are statistically significant differences\\textbackslash{}nbetween the evaluation results of NoMerging and\\textbackslash{}nWithMerging based on 1) the subject heading correct-\\textbackslash{}nness evaluations, and 2) the paragraph (sentence merging)\\textbackslash{}nquality evaluation results2. The evaluation of 1) does not\\textbackslash{}nseem to be dependent on what system version was used\\textbackslash{}n(X2 (2, N = 697) = 1.20, p = 0.55). However, there is a\\textbackslash{}nstatistically significant difference between the two when\\textbackslash{}nlooking at 2) (X2 (2, N = 696) = 8.12, p = 0.02).\\textbackslash{}nIt is possible that the confidence values of the classifier\\textbackslash{}nmay provide some indication of paragraph correctness in\\textbackslash{}nthat there is a correlation between the classifier\\textbackslash{}u0092s confi-\\textbackslash{}ndence value for an assigned heading and the paragraph\\textbackslash{}nbeing correct according to the manual evaluation results.\\textbackslash{}nUsing Spearman\\textbackslash{}u0092s rho to compare the manual evaluation\\textbackslash{}nresults of WithMerging with the classifiers confidence\\textbackslash{}nvalues for each paragraph\\textbackslash{}u0092s assigned heading (average\\textbackslash{}nacross sentences), we found there to be a negative correla-\\textbackslash{}ntion between classifier confidence values and the heading\\textbackslash{}nassignment ratings (Spearman\\textbackslash{}u0092s rho = -0.42, \\textbackslash{}u0093moderate\\textbackslash{}u0094);\\textbackslash{}n2Here we excluded classes \\textbackslash{}u00914\\textbackslash{}u0092 and \\textbackslash{}u0091d\\textbackslash{}u0092 due to their low frequency (n<5).\\textbackslash{}nMoen et al. Journal of Biomedical Semantics           (2020) 11:10 Page 8 of 12\\textbackslash{}nTable 5 Results showing the percentage of sensible paragraphs\\textbackslash{}n(i.e. sentence groupings) with correct headings assigned\\textbackslash{}nClass NoMerging n(tot=396) WithMerging n(tot=305)\\textbackslash{}n1 and a 66.67\\% 264 68.85\\% 210\\textbackslash{}nas well as between classifier confidence values and the rat-\\textbackslash{}nings of th'\n\\item[text11] 'RESEARCH Open Access\\textbackslash{}nDe-identifying free text of Japanese\\textbackslash{}nelectronic health records\\textbackslash{}nKohei Kajiyama1, Hiromasa Horiguchi2, Takashi Okumura3, Mizuki Morita4 and Yoshinobu Kano1*\\textbackslash{}nAbstract\\textbackslash{}nBackground: Recently, more electronic data sources are becoming available in the healthcare domain. Electronic\\textbackslash{}nhealth records (EHRs), with their vast amounts of potentially available data, can greatly improve healthcare.\\textbackslash{}nAlthough EHR de-identification is necessary to protect personal information, automatic de-identification of Japanese\\textbackslash{}nlanguage EHRs has not been studied sufficiently. This study was conducted to raise de-identification performance\\textbackslash{}nfor Japanese EHRs through classic machine learning, deep learning, and rule-based methods, depending on the\\textbackslash{}ndataset.\\textbackslash{}nResults: Using three datasets, we implemented de-identification systems for Japanese EHRs and compared the de-\\textbackslash{}nidentification performances found for rule-based, Conditional Random Fields (CRF), and Long-Short Term Memory\\textbackslash{}n(LSTM)-based methods. Gold standard tags for de-identification are annotated manually for age, hospital, person, sex,\\textbackslash{}nand time. We used different combinations of our datasets to train and evaluate our three methods. Our best F1-\\textbackslash{}nscores were 84.23, 68.19, and 81.67 points, respectively, for evaluations of the MedNLP dataset, a dummy EHR\\textbackslash{}ndataset that was virtually written by a medical doctor, and a Pathology Report dataset. Our LSTM-based method\\textbackslash{}nwas the best performing, except for the MedNLP dataset. The rule-based method was best for the MedNLP dataset.\\textbackslash{}nThe LSTM-based method achieved a good score of 83.07 points for this MedNLP dataset, which differs by 1.16\\textbackslash{}npoints from the best score obtained using the rule-based method. Results suggest that LSTM adapted well to\\textbackslash{}ndifferent characteristics of our datasets. Our LSTM-based method performed better than our CRF-based method,\\textbackslash{}nyielding a 7.41 point F1-score, when applied to our Pathology Report dataset. This report is the first of study\\textbackslash{}napplying this LSTM-based method to any de-identification task of a Japanese EHR.\\textbackslash{}nConclusions: Our LSTM-based machine learning method was able to extract named entities to be de-identified\\textbackslash{}nwith better performance, in general, than that of our rule-based methods. However, machine learning methods are\\textbackslash{}ninadequate for processing expressions with low occurrence. Our future work will specifically examine the\\textbackslash{}ncombination of LSTM and rule-based methods to achieve better performance.\\textbackslash{}nOur currently achieved level of performance is sufficiently higher than that of publicly available Japanese de-\\textbackslash{}nidentification tools. Therefore, our system will be applied to actual de-identification tasks in hospitals.\\textbackslash{}nKeywords: De-identification, Electronic health records, Japanese language\\textbackslash{}nBackground\\textbackslash{}nRecently, more electronic data sources are becoming\\textbackslash{}navailable in the healthcare domain. Utilization of\\textbackslash{}nelectronic health records (EHRs), with their vast\\textbackslash{}namounts of potentially useful data, is an important task\\textbackslash{}nin the healthcare domain. New legislation in Japan has\\textbackslash{}naddressed the treatment of medical data. The \\textbackslash{}u0093Act on\\textbackslash{}nthe Protection of Personal Information {[}1{]}\\textbackslash{}u0094 was revised\\textbackslash{}nin 2017 to stipulate that developers de-identify \\textbackslash{}u0093special\\textbackslash{}ncare-required personal information.\\textbackslash{}u0094 This legislation\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\textbackslash{}nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\textbackslash{}nchanges were made. The images or other third party material in this article are included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\textbackslash{}npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\textbackslash{}nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\textbackslash{}ndata made available in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}n* Correspondence: kano@inf.shizuoka.ac.jp\\textbackslash{}n1Faculty of Informatics, Shizuoka University, Johoku 3-5-1, Naka-ku,\\textbackslash{}nHamamatsu, Shizuoka 432-8011, Japan\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00227-9\\textbackslash{}nfurther restricts the use of personal identification codes\\textbackslash{}nincluding individual numbers (e.g. health insurance card\\textbackslash{}nnumbers, driver\\textbackslash{}u0092s license card numbers, and governmen-\\textbackslash{}ntal personnel numbers), biometric information (e.g. fin-\\textbackslash{}ngerprints, DNA, voice, and appearances), and\\textbackslash{}ninformation related to disability. This legislation can be\\textbackslash{}ncompared with the \\textbackslash{}u0093Health Insurance Portability and\\textbackslash{}nAccountability Act (HIPAA) {[}2{]}\\textbackslash{}u0094 of the United States, in\\textbackslash{}nthat the Japanese Act in 2017 includes additional codes,\\textbackslash{}nwith abstract specifications such as \\textbackslash{}u0093you should strive\\textbackslash{}nnot to discriminate or impose improper burdens,\\textbackslash{}u0094 and\\textbackslash{}nwith exclusion of birth dates and criminal histories, as\\textbackslash{}nstipulated by HIPAA. Another related act of Japanese le-\\textbackslash{}ngislation, the \\textbackslash{}u0093Act on Anonymously Processed Medical\\textbackslash{}nInformation to Contribute to Medical Research and De-\\textbackslash{}nvelopment {[}3{]}\\textbackslash{}u0094 was established in 2018. This legislation\\textbackslash{}nallows specific third-party institutes to handle EHRs,\\textbackslash{}nthereby promoting wider utilization of medical data.\\textbackslash{}nDe-identification of structured data in EHRs is easier\\textbackslash{}nthan that of unstructured data because it is straightfor-\\textbackslash{}nward to apply de-identification methods to structured\\textbackslash{}ndata such as numerical tables. Although de-identification\\textbackslash{}nof unstructured data in EHRs is necessary, it is virtually\\textbackslash{}nimpossible to de-identify the huge number of documents\\textbackslash{}nmanually.\\textbackslash{}nSeveral earlier works have examined EHR de-\\textbackslash{}nidentification. The Informatics for Integrating Biology \\&\\textbackslash{}nthe Bedside (i2b2) task {[}4{]} in 2006 was intended for\\textbackslash{}nautomatic de-identification of clinical records to satisfy\\textbackslash{}nHIPAA requirements {[}2{]}. An earlier study prepared 889\\textbackslash{}nEHRs, comprising 669 EHRs for training and 220 EHRs\\textbackslash{}nfor testing. Their annotations included 929 patient tags,\\textbackslash{}n3751 doctor tags, 263 location tags, 2400 hospital tags,\\textbackslash{}n7098 date tags, 4809 id tags, 232 phone\\_number tags,\\textbackslash{}nand 16 age tags. The best performing method of i2b2 in-\\textbackslash{}ncorporated diverse features such as a lexicon, part-of-\\textbackslash{}nspeech identification, word frequencies, and dictionaries\\textbackslash{}nfor learning using an ID3 tree learning algorithm.\\textbackslash{}nGrouin and Zweigenbaum {[}5{]} prepared 312 cardiovas-\\textbackslash{}ncular EHRs in French, with 3142 tags annotated by two\\textbackslash{}nannotators (kappa = 0.87). Their tags include 238 date\\textbackslash{}ntags, 205 last\\_name tags, 109 first\\_name tags, 43 hospital\\textbackslash{}ntags, 22 town tags, 8 zip\\_code tags, 8 address tags, 8\\textbackslash{}nphone tags, 8 med\\_device tags, 3 serial\\_number tags. Of\\textbackslash{}nthe person tags, 75\\% were replaced with other French\\textbackslash{}nperson names. The other 25\\% were replaced with inter-\\textbackslash{}nnational names. They also collected 10 photopathology\\textbackslash{}ndocuments, for which a single annotator assigned 29\\textbackslash{}ndate tags, 68 last\\_name tags, 53 first\\_name tags, 17 hos-\\textbackslash{}npital tags, 17 town tags, 13 zip\\_code tags, 14 address\\textbackslash{}ntags, 1 phone tag, 1 med\\_device tag, and 7 serial\\_number\\textbackslash{}ntags. They performed de-identification experiments\\textbackslash{}nusing 250 documents as their training data and 62 docu-\\textbackslash{}nments as their test data for the cardiology corpus. They\\textbackslash{}nobtained better F1-scores (exact match, 0.883; overlap\\textbackslash{}nmatch, 0.887) using conditional random fields (CRF)\\textbackslash{}nthan they obtained using their rule-based method (exact\\textbackslash{}nmatch, 0.843; overlap match, 0.847). However, their\\textbackslash{}nrule-based method was better for the photopathology\\textbackslash{}ncorpus (exact match, 0.681; overlap match, 0.693) than\\textbackslash{}ntheir CRF-based method (exact match, 0.638; overlap\\textbackslash{}nmatch, 0.638) because the data were fewer than those of\\textbackslash{}nthe cardiology corpus.\\textbackslash{}nGrouin and Névéol {[}6{]} discussed annotation guidelines\\textbackslash{}nfor French clinical records. After collecting 170,000 doc-\\textbackslash{}numents of 1000 patient records from five hospitals, they\\textbackslash{}nfirst prepared a rule-based system and their CRF-based\\textbackslash{}nsystem from their earlier study {[}5{]}, which we described\\textbackslash{}nearlier. Their rule-based system relies on 80 patterns\\textbackslash{}nspecifically designed to process the training corpus, and\\textbackslash{}nlists which they gathered from existing resources from\\textbackslash{}nthe internet. They randomly selected 100 documents\\textbackslash{}n(Set 1) from their dataset and applied both systems. For\\textbackslash{}neach document, they randomly showed one output of\\textbackslash{}nthe two systems to the annotators for revision. They ap-\\textbackslash{}nplied their rule-based system to another set of 100 docu-\\textbackslash{}nments (Set 2), which were further reviewed and revised\\textbackslash{}nby a human annotator. They re-trained their CRF-based\\textbackslash{}nsystem using the revised Set 2 annotations, which is fur-\\textbackslash{}nther applied to the other set of 100 documents (Set 3).\\textbackslash{}nAnnotators reviewed these annotations in subsets for\\textbackslash{}ndifferent agreement analyses. The study also compared\\textbackslash{}nhuman revision times among different annotation sets,\\textbackslash{}nwhich was a main objective of their study. They anno-\\textbackslash{}ntated 99 address tags, 101 zip\\_code tags, 462 date tags,\\textbackslash{}n47 e-mail tags, 224 hospital tags, 59 identifier tags, 871\\textbackslash{}nlast\\_name tags, 750 first\\_name tags, 383 telephone tags,\\textbackslash{}n218 city tags, in Set 1. They reported their rule-based\\textbackslash{}nmethod as better (0.813) in terms of the F1-score than\\textbackslash{}ntheir CRF-based method (0.519) when evaluated with 50\\textbackslash{}ndocuments in Set 1. When trained with Set 2, the corpus\\textbackslash{}nof the same domain, their CRF-based system performed\\textbackslash{}nbetter, yielding 0.953 for Set 3 and 0.888 for Set 1 in\\textbackslash{}ntheir F1-scores.\\textbackslash{}nFrom the Stockholm EPR {[}7{]}, a Swedish database of\\textbackslash{}nmore than one million patient records from two thou-\\textbackslash{}nsand clinics, Dalianis and Velupillai {[}8{]} extracted 100 pa-\\textbackslash{}ntient records to create gold standard for automatic de-\\textbackslash{}nidentifications based on HIPAA. They annotated 4423\\textbackslash{}ntags, including 56 age tags, 710 date\\_part tags, 500 full\\_\\textbackslash{}ndate tags, 923 last\\_name tags, 1021 health\\_care\\_unit\\textbackslash{}ntags, 148 location tags, and 136 phone\\_number tags.\\textbackslash{}nThey pointed out that Swedish morphology is more\\textbackslash{}ncomplex than that of English. It includes more inflec-\\textbackslash{}ntions, making the de-identification task in Swedish more\\textbackslash{}ndifficult.\\textbackslash{}nJian et al. {[}9{]} compiled a dataset of 3000 documents in\\textbackslash{}nChinese. It comprises 1500 hospitalization records, 1000\\textbackslash{}nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 2 of 12\\textbackslash{}nsummaries, 250 consulting records, and 250 death re-\\textbackslash{}ncords. They extracted 300 documents from this dataset\\textbackslash{}nrandomly, discussed a mode of de-identification with\\textbackslash{}nlower annotation cost. They annotated their tags to\\textbackslash{}nthese 300 documents (kappa = 0.76 between two annota-\\textbackslash{}ntors for their 100 document subset). Then they applied\\textbackslash{}ntheir pattern-matching module to these 300 documents,\\textbackslash{}nyielding a dense set of 201 sentences that include PHI\\textbackslash{}n(Protected Health Information). These 201 sentences in-\\textbackslash{}ncluded 141 name tags, 51 address tags, and 22 hospital\\textbackslash{}ntags.\\textbackslash{}nDu et al. {[}10{]} conducted de-identification experiments\\textbackslash{}nusing 14,719 discharge summaries in Chinese: two stu-\\textbackslash{}ndents annotated 25,403 tags. This dataset includes 6403\\textbackslash{}ninstitution tags, 11,301 date tags, 33 age tags, 2078 pa-\\textbackslash{}ntient\\_name tags, 3912 doctor\\_name tags, 326 province\\textbackslash{}ntags, 310 city tags, 774 country tags, 917 street tags, 277\\textbackslash{}nadmission\\_num tags, 21 pathological\\_num tags, 23 x-\\textbackslash{}nray\\_num tags, 263 phone tags, 420 doctor\\_num tags, and\\textbackslash{}n13 ultrasonic\\_num tags (inter-annotator agreement was\\textbackslash{}n96\\%, kappa = 0.826). Their experiments demonstrated\\textbackslash{}nthat their method of combining rules and CRF per-\\textbackslash{}nformed best, yielding a 98.78 F1-score. The Chinese lan-\\textbackslash{}nguage shares some issues with the Japanese language:\\textbackslash{}nthey both require tokenization because no spaces exist\\textbackslash{}nbetween words. This issue makes de-identification tasks\\textbackslash{}nmore difficult than they are in other languages.\\textbackslash{}nThe reports described above present a range of differ-\\textbackslash{}nent evaluation scores. However they adopted different\\textbackslash{}nannotation criteria, which make direct comparison diffi-\\textbackslash{}ncult. For instance, Grouin and Névéol used more de-\\textbackslash{}ntailed annotations than those used by Jian et al., as\\textbackslash{}nfollows. Jian et al. introduced Doctor and Patient tags,\\textbackslash{}nbut evaluated both simply as Name. Grouin and Névéol\\textbackslash{}nintroduced ZipCode, Identifier, Telephone, and City tags,\\textbackslash{}nnone of which is annotated in the work of Jian et al.\\textbackslash{}nAdditionally, they assigned Last Name and First Name\\textbackslash{}ntags, where performance of First Name was better than\\textbackslash{}nLast Name by around 10 points. However, both are\\textbackslash{}nworse than the results reported by Jian et al., probably\\textbackslash{}nbecause Jian et al. applied their pattern-matching algo-\\textbackslash{}nrithm to filter their training data. Regarding Address\\textbackslash{}ntags, Jian et al. obtained a 94.2 point F-score, whereas\\textbackslash{}nthe Grouin and Névéol CRF method obtained scores of\\textbackslash{}nfewer than 10 points. As Grouin and Névéol suggested,\\textbackslash{}neliminating City tags in street names can greatly improve\\textbackslash{}ntheir results: their rule-based method yielded an 86 point\\textbackslash{}nF-score.\\textbackslash{}nUnfortunately, automatic de-identification of EHRs\\textbackslash{}nhas not been studied sufficiently for Japanese language.\\textbackslash{}nDe-identification shared tasks for Japanese EHRs were\\textbackslash{}nheld as tasks in MedNLP-1 {[}11{]}. Then named entity ex-\\textbackslash{}ntraction was attempted in MedNLP-2 {[}12{]} tasks using\\textbackslash{}ndatasets similar to MedNLP-1. We designate MedNLP-1\\textbackslash{}nsimply as MedNLP hereinafter because we specifically\\textbackslash{}nexamine de-identification tasks but not other tasks held\\textbackslash{}nin the MedNLP shared task series.\\textbackslash{}nRegarding machine learning methods, Support Vector\\textbackslash{}nMachine (SVM) {[}13{]} and CRF {[}14{]} were used often in\\textbackslash{}nearlier Named Entity Recognition (NER) tasks in\\textbackslash{}naddition to rule-based methods. Recent deep learning\\textbackslash{}nmethods include Long-Short Term Memory (LSTM)\\textbackslash{}n{[}15{]} with character-embedding and word-embedding\\textbackslash{}n{[}16{]}, which performed best for the CoNLL 2002 {[}17{]}\\textbackslash{}n(Spanish and Dutch) and CoNLL 2003 {[}18{]} (English and\\textbackslash{}nGerman) NER shared task data: these tasks require de-\\textbackslash{}ntection of \\textbackslash{}u0093personal\\textbackslash{}u0094, \\textbackslash{}u0093location\\textbackslash{}u0094, \\textbackslash{}u0093organization\\textbackslash{}u0094, and\\textbackslash{}n\\textbackslash{}u0093other\\textbackslash{}u0094 tag types. Another LSTM model, which is similar\\textbackslash{}nto earlier work {[}16{]}, was also applied to a task of NER\\textbackslash{}nfrom Japanese newspapers {[}19{]}. Although deep neural\\textbackslash{}nnetwork models have been showing better results re-\\textbackslash{}ncently, rule-based methods are still often better than ma-\\textbackslash{}nchine learning methods, especially when insufficient\\textbackslash{}nannotated data are available.\\textbackslash{}nTo evaluate the effectiveness of such different methods\\textbackslash{}nfor the Japanese language, we implemented two EHR de-\\textbackslash{}nidentification systems for the Japanese language in our\\textbackslash{}nearlier work {[}20{]}. We used the MedNLP shared task\\textbackslash{}ndataset and our own dummy EHR dataset, which was\\textbackslash{}nwritten as a virtual database by medical professionals\\textbackslash{}nwho hold medical doctor certification. Based on this\\textbackslash{}nearlier work, we added a new dataset of pathology re-\\textbackslash{}nports to this study, for which we annotated the following\\textbackslash{}ntags. De-identification tags of age, hospital, sex, time,\\textbackslash{}nand person are annotated manually in all these datasets,\\textbackslash{}nfollowing the annotation standard of the MedNLP\\textbackslash{}nshared task to facilitate comparison with earlier studies.\\textbackslash{}nWe assume these annotations as our gold standard for\\textbackslash{}nour de-identification task. To these three datasets, we\\textbackslash{}napplied a rule-based method, a CRF-based method, and\\textbackslash{}nan LSTM-based method. Additionally, we have anno-\\textbackslash{}ntated our own tags to these three datasets by three anno-\\textbackslash{}ntators to calculate inter-annotator agreement. We have\\textbackslash{}nobserved the coherency of the original annotations of\\textbackslash{}nthe datasets. Overall, this study differs from our earlier\\textbackslash{}nwork {[}20{]} in that we added a new pathology dataset and\\textbackslash{}nits annotations, trained and evaluated our machine\\textbackslash{}nlearning models using the new dataset, and evaluated\\textbackslash{}nthe results using newly created annotations by three an-\\textbackslash{}nnotators to observe characteristics of the original and\\textbackslash{}nour own annotations.\\textbackslash{}nDatasets\\textbackslash{}nOur datasets were derived from three sources: MedNLP,\\textbackslash{}ndummy EHRs, and pathology reports. Irrespective of the\\textbackslash{}ndataset source, de-identification tags of five types are an-\\textbackslash{}nnotated manually: age (numerical expressions of sub-\\textbackslash{}nject\\textbackslash{}u0092s ages including its numerical classifiers), hospital\\textbackslash{}nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 3 of 12\\textbackslash{}n(hospital names), sex (male or female), time (subject re-\\textbackslash{}nlated time expressions with its numerical classifiers), and\\textbackslash{}nperson (person names). Characteristics of these datasets\\textbackslash{}nare presented in Table 1. It is noteworthy that texts of\\textbackslash{}nthe MedNLP and dummy EHRs are not actual texts, but\\textbackslash{}nthey were written by medical professionals, each of\\textbackslash{}nwhom holds medical doctor certification. However, char-\\textbackslash{}nacteristics of the descriptions differ between these two\\textbackslash{}nsources, probably because of differences of the writers.\\textbackslash{}nThe number of annotators is not described for the\\textbackslash{}nMedNLP dataset, but a single annotator created the an-\\textbackslash{}nnotations of the dummy EHR dataset and the Pathology\\textbackslash{}nReport dataset, individually.\\textbackslash{}nMedNLP shared task dataset\\textbackslash{}nWe used the MedNLP de-identification task dataset for\\textbackslash{}ncomparison with earlier studies that have used the same\\textbackslash{}ndataset. This dataset includes the dummy EHRs\\textbackslash{}n(discharge summaries) of 50 patients. Although the\\textbackslash{}ntraining dataset and test dataset were provided from the\\textbackslash{}nshared task organizers, the test dataset of the formal run\\textbackslash{}nis not publicly available now. It is not possible to\\textbackslash{}ncompare results directly with earlier works in the\\textbackslash{}nMedNLP shared task formal run (Tables 2 and 3 show\\textbackslash{}nthe formal run results). However, both training and test\\textbackslash{}ndatasets were originally parts of a single dataset. There-\\textbackslash{}nfore, we can discuss their characteristics in comparison\\textbackslash{}nwith those found in earlier works conducted using the\\textbackslash{}ntraining dataset only. We calculated inter-annotator\\textbackslash{}nagreement by three annotators for the training dataset.\\textbackslash{}nThe average F1-score of three pairs among these three\\textbackslash{}nannotators was 86.1, in 500 sentences of this dataset.\\textbackslash{}nDummy EHRs\\textbackslash{}nAnother source is our original dummy EHRs. We\\textbackslash{}nbuilt our own dummy EHRs of 32 patients, assuming\\textbackslash{}nthat the patients were hospitalized. Documents of our\\textbackslash{}ndummy EHRs were written by medical professionals\\textbackslash{}n(doctors). We added manual annotations for de-\\textbackslash{}nidentification following the guidelines of the MedNLP\\textbackslash{}nshared task. These annotations were originally\\textbackslash{}nassigned by a single annotator. Additionally, we added\\textbackslash{}nTable 1 Dataset characteristics\\textbackslash{}nDataset name MedNLP Dummy-EHRs Pathology Reports\\textbackslash{}n\\# of documents 50 reports 32 pairs of records and summaries 1000 reports\\textbackslash{}n\\# of sentences 2244 8183 3012\\textbackslash{}n\\# of tokens 42,621 154,132 194,449\\textbackslash{}n\\# of all tags 490 3017 295\\textbackslash{}n\\# of age tags 56 39 0\\textbackslash{}n\\# of hospital tags 75 170 31\\textbackslash{}n\\# of person tags 0 135 224\\textbackslash{}n\\# of sex tags 4 16 0\\textbackslash{}n\\# of time tags 355 2657 40\\textbackslash{}nExample in\\textbackslash{}noriginal Japanese\\textbackslash{}ntext\\textbackslash{}n????????<a > 64\\textbackslash{}n?</a >? < x >??</\\textbackslash{}nx >?\\textbackslash{}n???????????<a > 86?</a > <x >?\\textbackslash{}n?</x >????\\textbackslash{}n<<???? <h >?????????\\textbackslash{}n?</h >? < p >???</p>\\textbackslash{}nExample\\textbackslash{}ntranslated into\\textbackslash{}nEnglish\\textbackslash{}nA < a > 64-year-old</a > <x >\\textbackslash{}nman</x > works in a factory\\textbackslash{}nAn <a > 86-year-old</a > <x > woman</x >\\textbackslash{}nbedridden in a nursing home. Total assistance\\textbackslash{}nrequired\\textbackslash{}n<<Ex-hospital sample < h > Shizudai\\textbackslash{}nDermatology Clinic</h > , < p > Satoshi\\textbackslash{}nKuwata</p>\\textbackslash{}nTable 2 Overall results\\textbackslash{}nP R F A\\textbackslash{}nC3 89.59 91.67 90.62 99.58\\textbackslash{}nB3 91.67 86.57 89.05 99.54\\textbackslash{}nB1 90.05 87.96 88.99 99.49\\textbackslash{}nB2 90.82 87.04 88.89 99.52\\textbackslash{}nC1 92.42 84.72 88.41 99.49\\textbackslash{}nA1 91.50 84.72 87.98 99.47\\textbackslash{}nC2 91.50 84.72 87.98 99.46\\textbackslash{}nA2 90.15 84.72 87.35 99.41\\textbackslash{}nD1 86.10 74.54 79.90 99.36\\textbackslash{}nG1 82.09 76.39 79.14 99.38\\textbackslash{}nD3 85.87 73.15 79.00 99.35\\textbackslash{}nD2 80.81 74.07 77.29 99.24\\textbackslash{}nH2 76.17 75.46 75.81 99.28\\textbackslash{}nH1 75.81 75.46 75.64 99.27\\textbackslash{}nH3 74.88 74.54 74.71 99.26\\textbackslash{}nP, R and F were calculated at the phrase level: P, precision; R, recall; F, F1-measure;\\textbackslash{}nand A, accuracy. A was calculated in the word level (the agreement ratio of B-*, I-*\\textbackslash{}nand O).\\textbackslash{}nThe first column stands for participants\\textbackslash{}u0092 team names, where the first letter stands\\textbackslash{}nfor a team ID and the second numerical value stands for a submission run ID\\textbackslash{}nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 4 of 12\\textbackslash{}nnew annotations by three annotators to a part of this\\textbackslash{}ndataset and calculated inter-annotator agreement. The\\textbackslash{}naverage F1-score of three pairs among these three\\textbackslash{}nannotators was 76.1 for 730 sentences of the Dummy\\textbackslash{}nEHR dataset.\\textbackslash{}nPathology reports\\textbackslash{}nThe other source is a dataset of 1000 short pathology\\textbackslash{}nreports, that differ greatly from the EHRs above. Pathology\\textbackslash{}nreports describe pathological findings by which personal\\textbackslash{}ninformation (names of patients, doctors, hospitals, and\\textbackslash{}ntime expressions) frequently appears, but for which tags of\\textbackslash{}nsex and age rarely appear. Personal names, hospital names,\\textbackslash{}nand dates were manually de-identified beforehand by the\\textbackslash{}ndataset provider, and replaced with special characters. For\\textbackslash{}nmachine learning methods to support realistic training\\textbackslash{}nand evaluation, we replaced these special characters with\\textbackslash{}nrandomly assigned real entity names as follows. For the\\textbackslash{}nhospital names, we collected 96,167 hospital names which\\textbackslash{}ncover most of the Japanese hospital names, published by\\textbackslash{}nthe Japanese government. For the person names, we\\textbackslash{}nmanually created 20 dummy-family names and 20\\textbackslash{}ndummy-first names using one of the last names only, or\\textbackslash{}ncombining one of the last names and one of the first\\textbackslash{}nnames. Additionally, we calculated the inter-annotator\\textbackslash{}nagreement by three annotators. The average F1-score of\\textbackslash{}nthree pairs among these three annotators was 80.2 for 500\\textbackslash{}nsentences of this dataset. This Pathology Report dataset is\\textbackslash{}nthe only real (not dummy) dataset among our three\\textbackslash{}ndatasets. Because we received manually de-identified\\textbackslash{}nversion of the original real pathology reports, no ethical\\textbackslash{}nreview was necessary.\\textbackslash{}nMethods\\textbackslash{}nWe used a Japanese morphological analyzer, Kuromoji,1\\textbackslash{}nfor tokenization and part-of-speech (POS) tagging. We\\textbackslash{}nregistered our customized dictionary, derived from\\textbackslash{}nWikipedia entry names and entries of the Japanese\\textbackslash{}nStandard Disease-code master {[}21{]}, to this morphological\\textbackslash{}nanalyzer in addition to the analyzer\\textbackslash{}u0092s default dictionary.\\textbackslash{}nWe implemented rule-based, CRF-based, and LSTM-\\textbackslash{}nbased methods.\\textbackslash{}nRule-based method\\textbackslash{}nUnfortunately, the implementation of the best system for\\textbackslash{}nthe MedNLP-1 de-identification task {[}22{]} is not publicly\\textbackslash{}navailable. We implemented our own rule-based program\\textbackslash{}nbased on the descriptions in their paper, to replicate the\\textbackslash{}nsame system to the greatest extent possible. We present\\textbackslash{}ntheir rules below for a target word x for each tag type.\\textbackslash{}nAge\\textbackslash{}nIf x\\textbackslash{}u0092s detailed POS is \\textbackslash{}u0093numeral\\textbackslash{}u0094, then apply the rules in\\textbackslash{}nTable 4.\\textbackslash{}nHospital\\textbackslash{}nIf one of following keywords appeared in x, then mark it\\textbackslash{}nas hospital: ?? (a near clinic or hospital), ?? (this\\textbackslash{}nclinic or hospital), or ?? (same clinic or hospital).\\textbackslash{}nIf x\\textbackslash{}u0092s POS is \\textbackslash{}u0093noun\\textbackslash{}u0094 and if detailed POS is not \\textbackslash{}u0093non-au-\\textbackslash{}ntonomous word\\textbackslash{}u0094, or if x is either \\textbackslash{}u0093?\\textbackslash{}u0094, \\textbackslash{}u0093?\\textbackslash{}u0094, \\textbackslash{}u0093?\\textbackslash{}u0094 or \\textbackslash{}u0093?\\textbackslash{}u0094 (these\\textbackslash{}nsymbols are used for manual de-identification because the\\textbackslash{}ndatasets are dummy EHRs), and if suffix of x is one of the\\textbackslash{}nTable 3 Detailed results for each privacy type in MedNLP-1 (De-identification task)\\textbackslash{}n<a > age <x > sex <t > time <h > hospital name\\textbackslash{}nP R F P R F P R F P R F\\textbackslash{}nC3 90.32 87.5 88.89 100 100 100 87.16 91.49 89.27 97.30 94.74 96.00\\textbackslash{}nB3 90.00 84.38 87.10 100 50.00 66.67 91.30 89.36 90.32 97.06 86.84 91.67\\textbackslash{}nB1 93.33 87.5 90.32 100 100 100 90.65 89.36 90.00 89.47 89.47 89.47\\textbackslash{}nB2 90.00 84.38 87.10 100 100 100 91.24 88.65 89.93 91.89 89.47 90.67\\textbackslash{}nC1 96.67 90.62 93.55 100 50.00 66.67 91.18 87.94 89.53 93.55 76.32 84.06\\textbackslash{}nA1 92.86 81.25 86.67 100 50.00 66.67 91.04 86.52 88.73 91.89 89.47 90.67\\textbackslash{}nC2 96.67 90.62 93.55 100 50.00 66.67 89.13 87.23 88.17 96.77 78.95 86.96\\textbackslash{}nA2 92.86 81.25 86.67 100 50.00 66.67 89.05 86.52 87.77 91.89 89.47 90.67\\textbackslash{}nD1 92.31 75.00 82.76 100 50.00 66.67 82.84 78.72 80.73 96.15 65.79 78.12\\textbackslash{}nG1 80.65 78.12 79.37 100 50.00 66.67 84.56 81.56 83.03 72.73 63.16 67.61\\textbackslash{}nD3 88.89 75.00 81.36 100 50.00 66.67 83.08 76.60 79.70 96.15 65.79 78.12\\textbackslash{}nD2 92.31 75.00 82.76 100 50.00 66.67 75.86 78.01 76.92 96.15 65.79 78.12\\textbackslash{}nH2 83.87 81.25 82.54 100 100 100 73.79 75.89 74.83 77.78 73.68 75.68\\textbackslash{}nH1 80.65 78.12 79.37 100 100 100 75.86 78.01 76.92 70.27 68.42 69.33\\textbackslash{}nH3 83.87 81.25 82.54 100 100 100 73.79 75.89 74.83 70.27 68.42 69.33\\textbackslash{}nP, R and F were calculated at the phrase level: P, precision; R, recall; F, F1-measure; and A, accuracy. A was calculated in the word level (the agreement ratio of B-*, I-* and O).\\textbackslash{}nThe first column stands for participants\\textbackslash{}u0092 team names, where the first letter stands for a team ID and the second numerical value stands for a submission run ID\\textbackslash{}n1https://www.atilika.com/ja/kuromoji/\\textbackslash{}nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 5 of 12\\textbackslash{}nfollowing keywords, then mark it as hospital:?? (hospital\\textbackslash{}nor clinic),????? (clinic), or?? (clinic).\\textbackslash{}nSex\\textbackslash{}nIf x is either ?? (man), ?? (woman), men, women,\\textbackslash{}nman, woman (in English), then mark it as sex.\\textbackslash{}nTime\\textbackslash{}nIf x\\textbackslash{}u0092s detailed POS is \\textbackslash{}u0093numeral\\textbackslash{}u0094 and if x consists\\textbackslash{}nof four-digit-numbers+slash+two-or-one-digit-numbers\\textbackslash{}n(corresponds to \\textbackslash{}u0093yyyy/mm\\textbackslash{}u0094) or two-or-one-digit-\\textbackslash{}nnumbers+slash+two-or-one-digit-numbers (corresponds to\\textbackslash{}n\\textbackslash{}u0093mm/dd\\textbackslash{}u0094), then mark it as time.\\textbackslash{}nTable 4 Rules used for our rule-based method, original Japanese with English translations\\textbackslash{}nOption 1 main rule Option 2\\textbackslash{}n?\\textbackslash{}n(next)\\textbackslash{}n??? two years ago ?? (from)\\textbackslash{}n?\\textbackslash{}n(before)\\textbackslash{}n?? last year ?? (until)\\textbackslash{}n???\\textbackslash{}n(before hospitalization)\\textbackslash{}n?? last month ? (\\textbackslash{}u0091s)\\textbackslash{}n???\\textbackslash{}n(after hospitalization)\\textbackslash{}n?? last week ?? (early)\\textbackslash{}n????\\textbackslash{}n(after visit)\\textbackslash{}n?? yesterday ?? (last)\\textbackslash{}n??\\textbackslash{}n(a.m.)\\textbackslash{}n?? this year -- (from)\\textbackslash{}n??\\textbackslash{}n(p.m.)\\textbackslash{}n?? this month -- (from)\\textbackslash{}n????\\textbackslash{}n(after onset)\\textbackslash{}n?? this week ?? (over)\\textbackslash{}n??????\\textbackslash{}n(after onset)\\textbackslash{}n?? today ?? (under)\\textbackslash{}n??????\\textbackslash{}n(after care)\\textbackslash{}n?? today ?? (from)\\textbackslash{}n?? next year ? (when)\\textbackslash{}n?? next month ? (about)\\textbackslash{}n?? next week ?? (about)\\textbackslash{}n?? tomorrow ?? (about)\\textbackslash{}n??? the week after next ?? (early)\\textbackslash{}n??? day after tomorrow ?? (mid)\\textbackslash{}n?? same year ?? (late)\\textbackslash{}n?? same month ? (spring)\\textbackslash{}n?? same day ? (summer)\\textbackslash{}n?? following year ? (fall)\\textbackslash{}n?? the next day ? (winter)\\textbackslash{}n?? the next morning ? (morning)\\textbackslash{}n?? the previous day ? (noon)\\textbackslash{}n?? early morning ? (evening)\\textbackslash{}n??? after that ? (night)\\textbackslash{}nxx? xx (year) ?? (early morning)\\textbackslash{}nxx? xx (month) ?? (early morning)\\textbackslash{}nxx?? xx (week) ?? (before)\\textbackslash{}nxx? xx (day) ?? (after)\\textbackslash{}nxx? xx (o\\textbackslash{}u0092clock) ?? (evening)\\textbackslash{}nxx? xx (minutes) ?? (about)\\textbackslash{}nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 6 of 12\\textbackslash{}nIf x\\textbackslash{}u0092s detailed POS is \\textbackslash{}u0093numeral\\textbackslash{}u0094 and followed by either\\textbackslash{}nof ? (old), ? (old), or? (\\textbackslash{}u0091s), then mark it as time.\\textbackslash{}nIf x is followed further by either of \\textbackslash{}u0093??\\textbackslash{}u0094, \\textbackslash{}u0093??\\textbackslash{}u0094, \\textbackslash{}u0093?\\textbackslash{}n?\\textbackslash{}u0094, \\textbackslash{}u0093??\\textbackslash{}u0094, \\textbackslash{}u0093??\\textbackslash{}u0094, \\textbackslash{}u0093??\\textbackslash{}u0094, \\textbackslash{}u0093?\\textbackslash{}u0094, \\textbackslash{}u0093?\\textbackslash{}u0094, \\textbackslash{}u0093??\\textbackslash{}u0094, \\textbackslash{}u0093??\\textbackslash{}u0094,\\textbackslash{}n\\textbackslash{}u0093??\\textbackslash{}u0094, \\textbackslash{}u0093????\\textbackslash{}u0094, \\textbackslash{}u0093????\\textbackslash{}u0094, \\textbackslash{}u0093???\\textbackslash{}u0094, \\textbackslash{}u0093????\\textbackslash{}u0094,\\textbackslash{}nor \\textbackslash{}u0093????\\textbackslash{}u0094, then include these words in the span of\\textbackslash{}nthe marked time tag.\\textbackslash{}nCRF-based method\\textbackslash{}nWe implemented a CRF-based system because many\\textbackslash{}nparticipants used CRFs in the MedNLP-1 de-identification\\textbackslash{}ntask, including the second-best team and the baseline\\textbackslash{}nsystem. The best participant used a rule-based system, as\\textbackslash{}ndescribed previously. We used the MALLET2 library for\\textbackslash{}nCRF implementation. We defined five training features for\\textbackslash{}neach token3: part-of-speech (POS), detailed POS, character\\textbackslash{}ntype (Hiragana, Katakana, Kanji, or Number), a binary\\textbackslash{}nfeature whether a token is included in our user dictionary\\textbackslash{}nor not, and another binary feature whether a token is\\textbackslash{}nbeginning of its sentence or not.\\textbackslash{}nLSTM-based method\\textbackslash{}nOur LSTM-based method combines bidirectional LSTM\\textbackslash{}n(bi-LSTM) and CRF, using character-based and word-\\textbackslash{}nbased embeddings (Fig. 1) following earlier work that\\textbackslash{}nhad been reported as successful for other languages {[}16{]}.\\textbackslash{}nFor word-based embedding, we used the existing\\textbackslash{}nWord2Vec {[}23{]} model, which was trained using Japanese\\textbackslash{}nWikipedia.4 We used bi-LSTM to embed characters;\\textbackslash{}nthen we concatenated these two embeddings. This\\textbackslash{}nconcatenated output was fed to another bi-LSTM and\\textbackslash{}nthen sent to a CRF to output IOB tags.\\textbackslash{}nOur implementation has been made publicly available\\textbackslash{}nin GitHub.5 Table 5 presents the parameter settings.\\textbackslash{}nResults\\textbackslash{}nExperiment settings and evaluation metrics\\textbackslash{}nWe followed the evaluation metrics of the MedNLP-1\\textbackslash{}nshared task using IOB2 tagging {[}24{]}. We used four-fold\\textbackslash{}ncross validation, whereas the rule-based method requires\\textbackslash{}nno training data. We prepared five datasets: MedNLP\\textbackslash{}n(MedNLP), dummy EHRs (dummy), pathology reports\\textbackslash{}n(pathology), and MedNLP + dummy EHRs (MedNLP +\\textbackslash{}ndummy). We also prepared a dataset that comprises\\textbackslash{}nthese three datasets (all). For each dataset, we applied\\textbackslash{}ncross validation. The CRF and LSTM are trained with\\textbackslash{}nthree patterns of training data: the target dataset only,\\textbackslash{}none of other datasets only, MedNLP + dummy, and all.\\textbackslash{}nOur evaluation uses a strict match of named entity\\textbackslash{}nspans, calculating F1-scores, precisions, and recalls.\\textbackslash{}nTable 6 presents the evaluation results.\\textbackslash{}nResults obtained using the MedNLP dataset\\textbackslash{}nIn this MedNLP dataset, the total number of sex is very\\textbackslash{}nsmall; that of person is zero. The rule-based system per-\\textbackslash{}nformed best in terms of the F1-score because its rules\\textbackslash{}nwere tuned originally to the very MedNLP dataset.\\textbackslash{}nLSTM performed best for age and time, probably be-\\textbackslash{}ncause these tags exhibit typical patterns of less variation.\\textbackslash{}nLSTM is superior to Rule, except for sex and hospital.\\textbackslash{}nRegarding sex, we observe better performance when\\textbackslash{}nLSTM uses more training data. Therefore, the data size\\textbackslash{}nis expected to have been the reason why LSTM was not\\textbackslash{}ngood in sex.\\textbackslash{}nResults obtained using the dummy EHR dataset\\textbackslash{}nLSTM (M + d) performed best in terms of the F1-score.\\textbackslash{}nCRF performed better when trained by M+ d dataset\\textbackslash{}nthan with the target dataset only. This performance in-\\textbackslash{}ncrease consists of decrease of age and increase of all\\textbackslash{}nother tags, suggesting that these two datasets differ in\\textbackslash{}ntheir age tag annotation scheme.\\textbackslash{}nThe overall performance of this dummy EHR dataset\\textbackslash{}nis worse than the MedNLP dataset, suggesting that the\\textbackslash{}ndummy EHR dataset is more difficult to de-identify.\\textbackslash{}nResults obtained using the pathology report dataset\\textbackslash{}nThe LSTM-based method was better (81.67) than the\\textbackslash{}nCRF-based method (74.26), as shown by the 7.41 point\\textbackslash{}nF1-score when applied to our Pathology Report dataset.\\textbackslash{}nOur rule-based system achieved very high recall, but\\textbackslash{}nvery low precision scores for time, exhibiting a difference\\textbackslash{}nby 38 points. The pathology reports include many clin-\\textbackslash{}nical inspection values written in an \\textbackslash{}u0093xx/yy\\textbackslash{}u0094 format, which\\textbackslash{}nmight engender confusion with dates expressed in an\\textbackslash{}n\\textbackslash{}u0093mm/dd\\textbackslash{}u0094 format. We applied a workaround to limit\\textbackslash{}n{[}1 < = mm < = 12{]} and {[}1 < = dd < = 31{]}, but it was insuf-\\textbackslash{}nficient: we need contextual information, not just rules.\\textbackslash{}nIn addition, hospital is better than time, with less differ-\\textbackslash{}nence (15 points) of precision and recall.\\textbackslash{}nWhen trained with the Pathology Report dataset only,\\textbackslash{}nits performance is better than our rule-based system.\\textbackslash{}nWhen trained with the M+ d dataset, which does not\\textbackslash{}ncontain the pathology dataset, neither CRF nor LSTM\\textbackslash{}nworks fine because the pathology reports differ greatly in\\textbackslash{}nterms of their styles of description and named entities.\\textbackslash{}nDiscussion\\textbackslash{}nThese results suggest that our datasets have quite differ-\\textbackslash{}nent characteristics in what context and in what form\\textbackslash{}ntheir named entities appear, but LSTM adapted to these\\textbackslash{}ndifferences well. Adding the Pathological Report dataset\\textbackslash{}n2http://mallet.cs.umass.edu/\\textbackslash{}n3Hereinafter, \\textbackslash{}u0093token\\textbackslash{}u0094 means a \\textbackslash{}u0093morpheme\\textbackslash{}u0094 of the Japanese language,\\textbackslash{}nwhich does not have any space between tokens. A \\textbackslash{}u0093morpheme\\textbackslash{}u0094 is the\\textbackslash{}nsmallest meaningful unit in a language.\\textbackslash{}n4http://www.cl.ecei.tohoku.ac.jp/\\textasciitilde{}m\\textasciitilde{}suzuki/jawiki\\_vector/\\textbackslash{}n5https://github.com/johokugsk\\textbackslash{}nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 7 of 12\\textbackslash{}nto the training data seems to degrade the system per-\\textbackslash{}nformance for other target test datasets because of the\\textbackslash{}ndifferent dataset characteristics (examples presented in\\textbackslash{}nTable 1). For example, when trained with the Patho-\\textbackslash{}nlogical Report dataset, the hospital tags of the MedNLP\\textbackslash{}ndataset show lower performance because of the different\\textbackslash{}ndescriptions of hospital names among these two data-\\textbackslash{}nsets. The Pathological Report dataset has full hospital\\textbackslash{}nnames such as \\textbackslash{}u0093Shizudai Dermatology Clinic,\\textbackslash{}u0094 but the\\textbackslash{}nother two datasets have more casual descriptions such as\\textbackslash{}nFig. 1 Conceptual figure of our LSTM-based model, showing embedding and NER in separate figures. + means concatenation. The first figure\\textbackslash{}nshows the embedding part, where Wx is an x\\textbackslash{}nth input word, Lx,i is an i\\textbackslash{}nth letter of the word Wx, r denotes right to left (forward) LSTM, l denotes left\\textbackslash{}nto right (backward) LSTM, Vx is an intermediate node which corresponds to Wx. The second figure shows the NER part, where fl denotes forward\\textbackslash{}nLSTM, bl denotes backward LSTM, c denotes concatenated vector, finally a CRF layer is shown with an example predicted named entities in the\\textbackslash{}nBIO annotation style\\textbackslash{}nTable 5 LSTM parameter settings\\textbackslash{}nWord embedding size 200\\textbackslash{}nCharacter embedding size 100\\textbackslash{}nHidden layer of character 100\\textbackslash{}nHidden layer of LSTM 300\\textbackslash{}nLearning rate 0.001\\textbackslash{}nKajiyama et al. Journal of Biomedical Semantics           (2020) 11:11 Page 8 of 12\\textbackslash{}nTable 6 Evaluation results for each tag and in total, for different methods (rule, CRF, LSTM) and different evaluation datasets\\textbackslash{}n(MedNLP, dummy EHR, and pathology reports). M, d, and P respectively denote tr'\n\\item[text12] 'Wolff et al. Journal of Biomedical Semantics           (2020) 11:12 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00226-w\\textbackslash{}nRESEARCH Open Access\\textbackslash{}nMethodologically grounded semantic\\textbackslash{}nanalysis of large volume of chilean medical\\textbackslash{}nliterature data applied to the analysis of\\textbackslash{}nmedical research funding efficiency in Chile\\textbackslash{}nPatricio Wolff1, Sebastián Ríos1, David Clavijo1, Manuel Graña2* and Miguel Carrasco3\\textbackslash{}nAbstract\\textbackslash{}nBackground: Medical knowledge is accumulated in scientific research papers along time. In order to exploit this\\textbackslash{}nknowledge by automated systems, there is a growing interest in developing text mining methodologies to extract,\\textbackslash{}nstructure, and analyze in the shortest time possible the knowledge encoded in the large volume of medical literature.\\textbackslash{}nIn this paper, we use the Latent Dirichlet Allocation approach to analyze the correlation between funding efforts and\\textbackslash{}nactually published research results in order to provide the policy makers with a systematic and rigorous tool to assess\\textbackslash{}nthe efficiency of funding programs in the medical area.\\textbackslash{}nResults: We have tested our methodology in the Revista Médica de Chile, years 2012-2015. 50 relevant semantic\\textbackslash{}ntopics were identified within 643 medical scientific research papers. Relationships between the identified semantic\\textbackslash{}ntopics were uncovered using visualization methods. We have also been able to analyze the funding patterns of\\textbackslash{}nscientific research underlying these publications. We found that only 29\\% of the publications declare funding sources,\\textbackslash{}nand we identified five topic clusters that concentrate 86\\% of the declared funds.\\textbackslash{}nConclusions: Our methodology allows analyzing and interpreting the current state of medical research at a national\\textbackslash{}nlevel. The funding source analysis may be useful at the policy making level in order to assess the impact of actual\\textbackslash{}nfunding policies, and to design new policies.\\textbackslash{}nKeywords: Data science, Machine learning, Latent Dirichlet allocation, Healthcare management, Strategy\\textbackslash{}nBackground\\textbackslash{}nDue to the speed of innovation and change of research\\textbackslash{}ntrends in the medical community, research topic tax-\\textbackslash{}nonomies published by governmental agencies for funding\\textbackslash{}ncalls often diverge from the reality of the research practice.\\textbackslash{}nOur working hypothesis is that semantic topic analysis\\textbackslash{}nprovides an unbiased and accurate portrait of the actual\\textbackslash{}nresearch topics that are generating published results. In\\textbackslash{}nthis paper we exploit the information from a national\\textbackslash{}n*Correspondence: manuel.grana@ehu.es\\textbackslash{}n2Computational Intelligence Group, University of Basque Country, P. Manuel\\textbackslash{}nLardizabal 1, 20018 San Sebastián, Spain\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nmedical publication, described below, to identify the areas\\textbackslash{}nof active research, correlating them with the acknowl-\\textbackslash{}nedged funding sources, and non-funded personal effort\\textbackslash{}nbacking these scientific results. This analysis provides the\\textbackslash{}npolicymaker with a systematic, unbiased, and automated\\textbackslash{}ntool for the evaluation of the results of funding programs,\\textbackslash{}nallowing to assess the coherence of the national research\\textbackslash{}nfunding policies with the actual research outcomes.\\textbackslash{}nMethodology background'\n\\item[text13] 'RESEARCH Open Access\\textbackslash{}nAn integrative knowledge graph for rare\\textbackslash{}ndiseases, derived from the Genetic and\\textbackslash{}nRare Diseases Information Center (GARD)\\textbackslash{}nQian Zhu1*\\textbackslash{}u0086 , Dac-Trung Nguyen1\\textbackslash{}u0086, Ivan Grishagin1, Noel Southall1, Eric Sid2 and Anne Pariser2\\textbackslash{}nAbstract\\textbackslash{}nBackground: The Genetic and Rare Diseases (GARD) Information Center was established by the National Institutes\\textbackslash{}nof Health (NIH) to provide freely accessible consumer health information on over 6500 genetic and rare diseases. As\\textbackslash{}nthe cumulative scientific understanding and underlying evidence for these diseases have expanded over time,\\textbackslash{}nexisting practices to generate knowledge from these publications and resources have not been able to keep pace.\\textbackslash{}nThrough determining the applicability of computational approaches to enhance or replace manual curation tasks,\\textbackslash{}nwe aim to both improve the sustainability and relevance of consumer health information, but also to develop a\\textbackslash{}nfoundational database, from which translational science researchers may start to unravel disease characteristics that\\textbackslash{}nare vital to the research process.\\textbackslash{}nResults: We developed a meta-ontology based integrative knowledge graph for rare diseases in Neo4j. This\\textbackslash{}nintegrative knowledge graph includes a total of 3,819,623 nodes and 84,223,681 relations from 34 different\\textbackslash{}nbiomedical data resources, including curated drug and rare disease associations. Semi-automatic mappings were\\textbackslash{}ngenerated for 2154 unique FDA orphan designations to 776 unique GARD diseases, and 3322 unique FDA\\textbackslash{}ndesignated drugs to UNII, as well as 180,363 associations between drug and indication from Inxight Drugs, which\\textbackslash{}nwere integrated into the knowledge graph. We conducted four case studies to demonstrate the capabilities of this\\textbackslash{}nintegrative knowledge graph in accelerating the curation of scientific understanding on rare diseases through the\\textbackslash{}ngeneration of disease mappings/profiles and pathogenesis associations.\\textbackslash{}nConclusions: By integrating well-established database resources, we developed an integrative knowledge graph\\textbackslash{}ncontaining a large volume of biomedical and research data. Demonstration of several immediate use cases and\\textbackslash{}nlimitations of this process reveal both the potential feasibility and barriers of utilizing graph-based resources and\\textbackslash{}napproaches to support their use by providers of consumer health information, such as GARD, that may struggle\\textbackslash{}nwith the needs of maintaining knowledge reliant on an evolving and growing evidence-base. Finally, the successful\\textbackslash{}nintegration of these datasets into a freely accessible knowledge graph highlights an opportunity to take a\\textbackslash{}ntranslational science view on the field of rare diseases by enabling researchers to identify disease characteristics,\\textbackslash{}nwhich may play a role in the translation of discover across different research domains.\\textbackslash{}nKeywords: GARD, Rare diseases, Ontology, Data integration, Knowledge graph\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\textbackslash{}nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\textbackslash{}nchanges were made. The images or other third party material in this article are included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\textbackslash{}npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\textbackslash{}nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\textbackslash{}ndata made available in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}n* Correspondence: qian.zhu@nih.gov\\textbackslash{}n\\textbackslash{}u0086Qian Zhu and Dac-Trung Nguyen contributed equally to this work.\\textbackslash{}n1Division of Pre-Clinical Innovation, National Center for Advancing\\textbackslash{}nTranslational Sciences (NCATS), National Institutes of Health (NIH), Rockville,\\textbackslash{}nMD 20850, USA\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00232-y\\textbackslash{}nIntroduction\\textbackslash{}nAn estimated 30 million people in the United States are\\textbackslash{}naffected by a rare disease, which is defined as a disease\\textbackslash{}nthat affects fewer than 200,000 individuals in the United\\textbackslash{}nStates {[}1{]}. The majority of rare disease are thought to\\textbackslash{}nhave a genetic etiology {[}2{]} with studies reporting them\\textbackslash{}nresponsible for almost 10\\% of adult and 30\\% of pediatric\\textbackslash{}nhospitalizations {[}3{]}. Despite the great heterogeneity of\\textbackslash{}ndiseases included in this definition, many patients and\\textbackslash{}ntheir families share in common struggles, such as with\\textbackslash{}ndiagnostic delay leading to \\textbackslash{}u0093an average of 7.6 years\\textbackslash{}u0094 from\\textbackslash{}ninitial onset of symptoms to receiving a diagnosis and\\textbackslash{}nrequiring the involvement of 7.3 physicians on average\\textbackslash{}n{[}4{]}. These shared challenges faced in the broader rare\\textbackslash{}ndisease patient community are often due to a lack of\\textbackslash{}neither up-to-date information or awareness amongst\\textbackslash{}nproviders and the public at large. Efforts to tackle these\\textbackslash{}nissues led to the passage of the Rare Disease Act of 2002\\textbackslash{}nand the establishment of several programs by the\\textbackslash{}nNational Institutes of Health (NIH) to improve research\\textbackslash{}nactivities and public access to information on rare dis-\\textbackslash{}neases. In particular, the Genetic and Rare Diseases\\textbackslash{}n(GARD) information center was charged with providing\\textbackslash{}nfreely accessible consumer health information in plain\\textbackslash{}nlanguage, and it has been investigating the challenge of\\textbackslash{}nshifting from an entirely manual process to leveraging\\textbackslash{}ncomputational approaches to curate the accumulated\\textbackslash{}nbiomedical and clinical research knowledge of over 6500\\textbackslash{}nrare diseases, and more rapidly make information ac-\\textbackslash{}ncessible 1) to educate patients, families, and health care\\textbackslash{}nproviders with more accurate and real-time knowledge\\textbackslash{}nabout a rare disease, and 2) to support novel scientific\\textbackslash{}nresearch efforts and apply disease-agnostic translational\\textbackslash{}nscience approaches to the field of rare diseases as a\\textbackslash{}nwhole {[}5{]}.\\textbackslash{}nGiven the pace of ongoing scientific discovery, parsing\\textbackslash{}nthrough the accumulated research publications and\\textbackslash{}nconveying this knowledge in a plain language format ac-\\textbackslash{}ncessible to low-health literacy audiences presents a sig-\\textbackslash{}nnificant task for a single disease, let alone for over 6500\\textbackslash{}nrare diseases. Thus, a huge amount of effort to accumu-\\textbackslash{}nlate and curate data for rare diseases has been made glo-\\textbackslash{}nbally. For instance, the GARD Information Center\\textbackslash{}nprovides interpretable profiles in plain language for each\\textbackslash{}nrare disease {[}5{]}; Orphanet focuses on expert manual cur-\\textbackslash{}nation of a disease\\textbackslash{}u0092s clinical presentation {[}6{]}; and Online\\textbackslash{}nMendelian Inheritance in Man® (OMIM®) conducts a\\textbackslash{}nsimilar expert-driven focus on defining genotype and\\textbackslash{}nphenotype relationships {[}7{]}. The discreteness of such\\textbackslash{}nheterogeneous data, however, impedes their direct use\\textbackslash{}nfor consumer audiences. To overcome this barrier, in\\textbackslash{}nthis study, we integrated these well-known resources in\\textbackslash{}none knowledge graph to semantically interconnect all\\textbackslash{}ndata together by means of the data points as nodes and\\textbackslash{}ntheir relationships as edges, as a first step in bridging the\\textbackslash{}nuse of these resources in consumer-facing health\\textbackslash{}ninformation.\\textbackslash{}nBiomedical data integration is an important and tech-\\textbackslash{}nnical approach to tackling biomedical problems. Current\\textbackslash{}nprogress in computational technology allows vast data\\textbackslash{}nstorages and powerful computational processes to be\\textbackslash{}nmore affordable and accessible. As a result, biomedical\\textbackslash{}nscientists have gradually gained an awareness of the im-\\textbackslash{}nportance of pooling diverse types of data pertaining to a\\textbackslash{}nspecific medical entity to enhance their research under-\\textbackslash{}nstanding {[}8{]}. Representing integrative data in the form of\\textbackslash{}na graph has attracted many interests, particularly in the\\textbackslash{}nbiomedical domain. Karczewski K, et al. have reviewed\\textbackslash{}nand discussed the potential and usage and challenges of\\textbackslash{}nintegrating diverse types of omics data for human health\\textbackslash{}nand disease {[}9{]}. Biomedical Informatics Research Net-\\textbackslash{}nwork (BIRN) is an integrative resource by semantically\\textbackslash{}nintegrating data produced by multiple institutions for\\textbackslash{}ndata analysis on Neurosciences {[}10{]}. Similar efforts have\\textbackslash{}nalso begun to emerge with applications directed at the\\textbackslash{}nfield in rare disease, such as the semantic Diseasecard,\\textbackslash{}nwhich integrates rare disease data from distinct sources\\textbackslash{}nin a semantic web environment {[}11{]}. A similar EU plat-\\textbackslash{}nform, RD-Connect connects databases, registries, bio-\\textbackslash{}nbanks and clinical bioinformatics to support research in\\textbackslash{}ndiscovering new genes, biomarkers, and therapeutic\\textbackslash{}ntargets more quickly and efficiently {[}12{]}. The Monarch\\textbackslash{}nInitiative as another analytic platform, semantically inte-\\textbackslash{}ngrates genotype and phenotype data across differing spe-\\textbackslash{}ncies and sources {[}13{]}, and has led to the establishment\\textbackslash{}nof MONDO (Monarch Merged Disease Ontology) {[}14{]}\\textbackslash{}nas a cohesive ontology for connecting many of the dis-\\textbackslash{}nease databases and resources. The integrative knowledge\\textbackslash{}ngraph we introduce in this study applies well-established\\textbackslash{}nrare disease data drawn from GARD, Orphanet, OMIM\\textbackslash{}nand MONDO as a backbone, and then expands to a\\textbackslash{}nwide spectrum of additional biomedical data, including\\textbackslash{}nphenotypes, genes and curated FDA approved drugs and\\textbackslash{}nFDA orphan drug designations.\\textbackslash{}nThere are demonstrated merits and successes in using\\textbackslash{}ngraph database to support the management of large bio-\\textbackslash{}nmedical datasets. While relational databases excel at man-\\textbackslash{}naging relationships between data, graph databases provide\\textbackslash{}nunique abilities to manage n-th degree relationships\\textbackslash{}namong complex types of biomedical data. Furthermore,\\textbackslash{}ngraph databases are particularly apt at representing hier-\\textbackslash{}narchical data, such as disease categories and complex se-\\textbackslash{}nmantic relationships among different types of data. Neo4j\\textbackslash{}nas a graph database management system {[}15{]}, has been\\textbackslash{}nwidely applied in such use cases within the biomedical do-\\textbackslash{}nmain. Such as, Gratzl S, et al. demonstrated the utility of\\textbackslash{}nNeo4j in developing integrated visual analysis platform for\\textbackslash{}nbiomedical data {[}16{]}; Himmelstein D, et al. constructed\\textbackslash{}nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 Page 2 of 13\\textbackslash{}nHetionet, an integrative Neo4j network that encodes\\textbackslash{}nknowledge from millions of biomedical studies to\\textbackslash{}nprioritize drugs for repurposing {[}17{]}. In this paper, we\\textbackslash{}nintroduce this rare diseases integrative knowledge graph,\\textbackslash{}nbuilt in Neo4j as a backend graph database ingesting a\\textbackslash{}nlarge variety of biomedical datasets. We detail data prepar-\\textbackslash{}nation and entity resolution methodologies in generating\\textbackslash{}ninitial insights and results, and the potential benefits for\\textbackslash{}nutilizing a knowledge graph-based approach to interpret\\textbackslash{}nbiomedical research at a scale and pace that would be un-\\textbackslash{}nsustainable when limited to the manual curation efforts\\textbackslash{}nthat define current processes used in curating consumer\\textbackslash{}nhealth information.\\textbackslash{}nMaterials\\textbackslash{}nAt the time of writing, the knowledge graph integrates\\textbackslash{}n34 different biomedical datasets including GARD. We\\textbackslash{}nbriefly describe several primary resources as below.\\textbackslash{}nRare disease related data resources\\textbackslash{}nBesides GARD data retrieved from our internal database,\\textbackslash{}nall other datasets were downloaded from NCBO Biopor-\\textbackslash{}ntal {[}18{]}.\\textbackslash{}nGARD is currently managed by the Office of Rare\\textbackslash{}nDiseases Research (ORDR) within the National Center\\textbackslash{}nfor Advancing Translational Sciences (NCATS), has\\textbackslash{}nremained an important portal for patients, health-care\\textbackslash{}nprofessionals, and researchers seeking to understand the\\textbackslash{}ncurrent state of genetic and rare diseases. GARD in-\\textbackslash{}ncludes curated disease information comprised of 15 dif-\\textbackslash{}nferent sections, such as summary, diagnosis, inheritance,\\textbackslash{}netc. Notably, not all of GARD diseases have a complete\\textbackslash{}nlist of these 15 information sections, due to data unavail-\\textbackslash{}nability at the time of curation and update. In this study,\\textbackslash{}nwe extracted and applied disease specific information\\textbackslash{}nsections, if applicable from our internal GARD database.\\textbackslash{}nOther sections, such as Resources, Organizations will be\\textbackslash{}nexplored in the future {[}5{]}.\\textbackslash{}nOrphanet is a unique resource, gathering and improv-\\textbackslash{}ning knowledge on rare diseases so as to improve the\\textbackslash{}ndiagnosis, care and treatment of patients with rare dis-\\textbackslash{}neases {[}6{]}.\\textbackslash{}nMonarch Disease Ontology (MONDO) is a semi-\\textbackslash{}nautomatically constructed ontology that merges multiple\\textbackslash{}ndisease resources to yield a coherent merged ontology\\textbackslash{}n{[}14{]}.\\textbackslash{}nOnline Mendelian Inheritance in Man® (OMIM®) is\\textbackslash{}na comprehensive, authoritative compendium of human\\textbackslash{}ngenes and genetic phenotypes. The full-text, referenced\\textbackslash{}noverviews in OMIM contain information on all known\\textbackslash{}nmendelian disorders and over 15,000 genes {[}7{]}.\\textbackslash{}nHuman Phenotype Ontology (HPO) provides a stan-\\textbackslash{}ndardized vocabulary of phenotypic abnormalities en-\\textbackslash{}ncountered in human disease {[}19{]}.\\textbackslash{}nFDA orphan drugs\\textbackslash{}nFDA orphan drug designation provides orphan desig-\\textbackslash{}nnations to drugs and biologics, which are defined as\\textbackslash{}nthose intended for the safe and effective treatment, diag-\\textbackslash{}nnosis or prevention of rare diseases/disorders {[}20{]}. In\\textbackslash{}nthis study, we employed orphan drug designation data\\textbackslash{}nfrom the FDA {[}21{]}, several examples of FDA orphan\\textbackslash{}ndrug designations retrieved from the FDA are shown in\\textbackslash{}nTable 1. Specifically we utilized the associations between\\textbackslash{}nFDA designated drugs (the column of \\textbackslash{}u0093Generic Name\\textbackslash{}u0094\\textbackslash{}nin Table 1) and their designations (the column of \\textbackslash{}u0093Or-\\textbackslash{}nphan Designation\\textbackslash{}u0094 in Table 1). Although the data is pre-\\textbackslash{}nsented in a structured form, orphan designation is\\textbackslash{}ncaptured in free text, such as examples shown in Table\\textbackslash{}n1. In that manner, additional curation was conducted in\\textbackslash{}nthis study to be able to map orphan designations to\\textbackslash{}nGARD diseases and designated drugs to UNII (Unique\\textbackslash{}nIngredient Identifier).\\textbackslash{}nInxight Drugs is a drug resource developed by NCAT\\textbackslash{}nS. Inxight Drugs {[}22{]} incorporates the most comprehen-\\textbackslash{}nsive subset of substances and related biological mecha-\\textbackslash{}nnisms pertaining to translational research and connects\\textbackslash{}nthem to the appropriate disease indications. As part of\\textbackslash{}nInxight Drugs, explicit connections between drugs and\\textbackslash{}nconditions were manually identified from scientific arti-\\textbackslash{}ncles, press releases, FDA labels, and large-scale databases\\textbackslash{}n(e.g. AdisInsight {[}23{]}). For those identified associations,\\textbackslash{}nthe curators manually matched conditions to MeSH,\\textbackslash{}nDisease Ontology (DO), and drugs to UNII (Unique In-\\textbackslash{}ngredient Identifier). For example, one association pre-\\textbackslash{}nsenting in Inxight Drugs is as \\textbackslash{}u0093CYROMAZINE\\textbackslash{}u0094 (with\\textbackslash{}nUNII: CA49Y29RA9) has indication of \\textbackslash{}u0093MYIASIS,\\textbackslash{}nCUTANEOUS MYIASIS OF SHEEP\\textbackslash{}u0094. In this study, we\\textbackslash{}nextracted associations between FDA approved drugs and\\textbackslash{}ndiseases, and integrated them into our integrative know-\\textbackslash{}nledge graph.\\textbackslash{}nMethods\\textbackslash{}nIn this paper, we detail the process of developing the in-\\textbackslash{}ntegrative knowledge graph for rare diseases with inclu-\\textbackslash{}nsion of multiple well-known biomedical datasets\\textbackslash{}nincluding GARD. We also demonstrate the use of this\\textbackslash{}nintegrative graph to support biomedical research for rare\\textbackslash{}ndiseases. More details about this process is described as\\textbackslash{}nbelow.\\textbackslash{}nData collection\\textbackslash{}nGARD data is curated in two folds, manual curation by\\textbackslash{}ninformation specialists from GARD, and programmatic\\textbackslash{}nextraction from Orphanet. The curated data is stored in\\textbackslash{}na relational database, from where we extracted GARD\\textbackslash{}ndata for this study. GARD provides comprehensive in-\\textbackslash{}nformation about rare diseases from different aspects,\\textbackslash{}nincluding summary, sign and symptoms, treatment,\\textbackslash{}nZhu et al. Journal of Biomedical Semantics           (2020) 11:13 Page 3 of 13'\n\\item[text14] 'REVIEW Open Access\\textbackslash{}nNatural language processing algorithms\\textbackslash{}nfor mapping clinical text fragments onto\\textbackslash{}nontology concepts: a systematic review\\textbackslash{}nand recommendations for future studies\\textbackslash{}nMartijn G. Kersloot1,2* , Florentien J. P. van Putten1, Ameen Abu-Hanna1, Ronald Cornet1 and Derk L. Arts1,2\\textbackslash{}nAbstract\\textbackslash{}nBackground: Free-text descriptions in electronic health records (EHRs) can be of interest for clinical research\\textbackslash{}nand care optimization. However, free text cannot be readily interpreted by a computer and, therefore, has\\textbackslash{}nlimited value. Natural Language Processing (NLP) algorithms can make free text machine-interpretable by\\textbackslash{}nattaching ontology concepts to it. However, implementations of NLP algorithms are not evaluated\\textbackslash{}nconsistently. Therefore, the objective of this study was to review the current methods used for developing\\textbackslash{}nand evaluating NLP algorithms that map clinical text fragments onto ontology concepts. To standardize the\\textbackslash{}nevaluation of algorithms and reduce heterogeneity between studies, we propose a list of recommendations.\\textbackslash{}nMethods: Two reviewers examined publications indexed by Scopus, IEEE, MEDLINE, EMBASE, the ACM Digital\\textbackslash{}nLibrary, and the ACL Anthology. Publications reporting on NLP for mapping clinical text from EHRs to\\textbackslash{}nontology concepts were included. Year, country, setting, objective, evaluation and validation methods, NLP\\textbackslash{}nalgorithms, terminology systems, dataset size and language, performance measures, reference standard,\\textbackslash{}ngeneralizability, operational use, and source code availability were extracted. The studies\\textbackslash{}u0092 objectives were\\textbackslash{}ncategorized by way of induction. These results were used to define recommendations.\\textbackslash{}nResults: Two thousand three hundred fifty five unique studies were identified. Two hundred fifty six studies\\textbackslash{}nreported on the development of NLP algorithms for mapping free text to ontology concepts. Seventy-seven\\textbackslash{}ndescribed development and evaluation. Twenty-two studies did not perform a validation on unseen data and\\textbackslash{}n68 studies did not perform external validation. Of 23 studies that claimed that their algorithm was\\textbackslash{}ngeneralizable, 5 tested this by external validation. A list of sixteen recommendations regarding the usage of\\textbackslash{}nNLP systems and algorithms, usage of data, evaluation and validation, presentation of results, and\\textbackslash{}ngeneralizability of results was developed.\\textbackslash{}n(Continued on next page)\\textbackslash{}n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License,\\textbackslash{}nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\\textbackslash{}nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if\\textbackslash{}nchanges were made. The images or other third party material in this article are included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}'s Creative Commons\\textbackslash{}nlicence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain\\textbackslash{}npermission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\textbackslash{}nThe Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the\\textbackslash{}ndata made available in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}n* Correspondence: m.g.kersloot@amsterdamumc.nl\\textbackslash{}n1Amsterdam UMC, University of Amsterdam, Department of Medical\\textbackslash{}nInformatics, Amsterdam Public Health Research Institute Castor EDC, Room\\textbackslash{}nJ1B-109, PO Box 22700, 1100 DE Amsterdam, The Netherlands\\textbackslash{}n2Castor EDC, Amsterdam, The Netherlands\\textbackslash{}nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00231-z\\textbackslash{}n(Continued from previous page)\\textbackslash{}nConclusion: We found many heterogeneous approaches to the reporting on the development and evaluation of NLP\\textbackslash{}nalgorithms that map clinical text to ontology concepts. Over one-fourth of the identified publications did not perform\\textbackslash{}nan evaluation. In addition, over one-fourth of the included studies did not perform a validation, and 88\\% did not\\textbackslash{}nperform external validation. We believe that our recommendations, alongside an existing reporting standard, will\\textbackslash{}nincrease the reproducibility and reusability of future studies and NLP algorithms in medicine.\\textbackslash{}nKeywords: Ontologies, Entity linking, Annotation, Concept mapping, Named-entity recognition, Natural language\\textbackslash{}nprocessing, Evaluation studies, Recommendations for future studies\\textbackslash{}nBackground\\textbackslash{}nOne of the main activities of clinicians, besides providing\\textbackslash{}ndirect patient care, is documenting care in the electronic\\textbackslash{}nhealth record (EHR). Currently, clinicians document clin-\\textbackslash{}nical findings and symptoms primarily as free-text descrip-\\textbackslash{}ntions within clinical notes in the EHR since they are not\\textbackslash{}nable to fully express complex clinical findings and nuances\\textbackslash{}nof every patient in a structured format {[}1, 2{]}. These free-\\textbackslash{}ntext descriptions are, amongst other purposes, of interest\\textbackslash{}nfor clinical research {[}3, 4{]}, as they cover more information\\textbackslash{}nabout patients than structured EHR data {[}5{]}. However,\\textbackslash{}nfree-text descriptions cannot be readily processed by a\\textbackslash{}ncomputer and, therefore, have limited value in research\\textbackslash{}nand care optimization.\\textbackslash{}nOne method to make free text machine-processable is\\textbackslash{}nentity linking, also known as annotation, i.e., mapping\\textbackslash{}nfree-text phrases to ontology concepts that express the\\textbackslash{}nphrases\\textbackslash{}u0092 meaning. Ontologies are explicit formal specifica-\\textbackslash{}ntions of the concepts in a domain and relations among\\textbackslash{}nthem {[}6{]}. In the medical domain, SNOMED CT {[}7{]} and\\textbackslash{}nthe Human Phenotype Ontology (HPO) {[}8{]} are examples\\textbackslash{}nof widely used ontologies to annotate clinical data. After\\textbackslash{}nthe data has been annotated, it can be reused by clinicians\\textbackslash{}nto query EHRs {[}9, 10{]}, to classify patients into different\\textbackslash{}nrisk groups {[}11, 12{]}, to detect a patient\\textbackslash{}u0092s eligibility for clin-\\textbackslash{}nical trials {[}13{]}, and for clinical research {[}14{]}.\\textbackslash{}nNatural Language Processing (NLP) can be used to\\textbackslash{}n(semi-)automatically process free text. The literature indi-\\textbackslash{}ncates that NLP algorithms have been broadly adopted and\\textbackslash{}nimplemented in the field of medicine {[}15, 16{]}, including\\textbackslash{}nalgorithms that map clinical text to ontology concepts\\textbackslash{}n{[}17{]}. Unfortunately, implementations of these algorithms\\textbackslash{}nare not being evaluated consistently or according to a pre-\\textbackslash{}ndefined framework and limited availability of data sets and\\textbackslash{}ntools hampers external validation {[}18{]}.\\textbackslash{}nTo improve and standardize the development and evalu-\\textbackslash{}nation of NLP algorithms, a good practice guideline for\\textbackslash{}nevaluating NLP implementations is desirable {[}19, 20{]}.\\textbackslash{}nSuch a guideline would enable researchers to reduce the\\textbackslash{}nheterogeneity between the evaluation methodology and\\textbackslash{}nreporting of their studies. Generic reporting guidelines\\textbackslash{}nsuch as TRIPOD {[}21{]} for prediction models, STROBE\\textbackslash{}n{[}22{]} for observational studies, RECORD {[}23{]} for studies\\textbackslash{}nconducted using routinely-collected health data, and\\textbackslash{}nSTARD {[}24{]} for diagnostic accuracy studies, are available,\\textbackslash{}nbut are often not used in NLP research. This is presum-\\textbackslash{}nably because some guideline elements do not apply to\\textbackslash{}nNLP and some NLP-related elements are missing or un-\\textbackslash{}nclear. We, therefore, believe that a list of recommenda-\\textbackslash{}ntions for the evaluation methods of and reporting on\\textbackslash{}nNLP studies, complementary to the generic reporting\\textbackslash{}nguidelines, will help to improve the quality of future\\textbackslash{}nstudies.\\textbackslash{}nIn this study, we will systematically review the\\textbackslash{}ncurrent state of the development and evaluation of\\textbackslash{}nNLP algorithms that map clinical text onto ontology\\textbackslash{}nconcepts, in order to quantify the heterogeneity of\\textbackslash{}nmethodologies used. We will propose a structured list\\textbackslash{}nof recommendations, which is harmonized from exist-\\textbackslash{}ning standards and based on the outcomes of the re-\\textbackslash{}nview, to support the systematic evaluation of the\\textbackslash{}nalgorithms in future studies.\\textbackslash{}nMethods\\textbackslash{}nThis study consists of two phases: a systematic review of\\textbackslash{}nthe literature and the formation of recommendations\\textbackslash{}nbased on the findings of the review.\\textbackslash{}nLiterature review\\textbackslash{}nA systematic review of the literature was performed\\textbackslash{}nusing the Preferred Reporting Items for Systematic re-\\textbackslash{}nviews and Meta-Analyses (PRISMA) statement {[}25{]}.\\textbackslash{}nSearch strategy and study selection\\textbackslash{}nWe searched Scopus, IEEE, MEDLINE, EMBASE, the As-\\textbackslash{}nsociation for Computing Machinery (ACM) Digital Library,\\textbackslash{}nand the Association for Computational Linguistics (ACL)\\textbackslash{}nAnthology for the following keywords: Natural Language\\textbackslash{}nProcessing, Medical Language Processing, Electronic Health\\textbackslash{}nRecord, reports, charts, clinical notes, clinical text, medical\\textbackslash{}nnotes, ontolog*, concept*, encod*, annotat*, code, and cod-\\textbackslash{}ning. We excluded the words \\textbackslash{}u0091reports\\textbackslash{}u0092 and \\textbackslash{}u0091charts\\textbackslash{}u0092 in the\\textbackslash{}nACL and ACM databases since these databases also contain\\textbackslash{}npublications on non-medical subjects. The detailed search\\textbackslash{}nstrategies for each database can be found in Additional file\\textbackslash{}n2. We searched until December 19, 2019 and applied the\\textbackslash{}nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 2 of 21\\textbackslash{}nfilters \\textbackslash{}u0093English\\textbackslash{}u0094 and \\textbackslash{}u0093has abstract\\textbackslash{}u0094 for all databases. More-\\textbackslash{}nover, we applied the filters \\textbackslash{}u0093Medicine, Health Professions,\\textbackslash{}nand Nursing\\textbackslash{}u0094 for Scopus, the filters \\textbackslash{}u0093Conferences\\textbackslash{}u0094, \\textbackslash{}u0093Jour-\\textbackslash{}nnals\\textbackslash{}u0094, and \\textbackslash{}u0093Early Access Articles\\textbackslash{}u0094 for IEEE, and the filter\\textbackslash{}n\\textbackslash{}u0093Article\\textbackslash{}u0094 for Scopus and EMBASE. EndNote X9 {[}26{]} and\\textbackslash{}nRayyan {[}27{]} were used to review and delete duplicates.\\textbackslash{}nThe selection process consisted of three phases. In the\\textbackslash{}nfirst phase, two independent reviewers with a Medical\\textbackslash{}nInformatics background (MK, FP) individually assessed\\textbackslash{}nthe resulting titles and abstracts and selected publica-\\textbackslash{}ntions that fitted the criteria described below.\\textbackslash{}nInclusion criteria were:\\textbackslash{}n\\textbackslash{}001 Medical language processing as the main topic of\\textbackslash{}nthe publication\\textbackslash{}n\\textbackslash{}001 Use of EHR data, clinical reports, or clinical notes\\textbackslash{}n\\textbackslash{}001 Algorithm performs annotation\\textbackslash{}n\\textbackslash{}001 Publication is written in English\\textbackslash{}nFig. 1 PRISMA flow diagram\\textbackslash{}nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 3 of 21\\textbackslash{}nSome studies do not describe the application of NLP in\\textbackslash{}ntheir study by only listing NLP as the used method, instead\\textbackslash{}nof describing its specific implementation. Additionally,\\textbackslash{}nsome studies create their own ontology to perform NLP\\textbackslash{}ntasks, instead of using an established, domain-accepted\\textbackslash{}nontology. Both approaches limit the generalizability of the\\textbackslash{}nstudy\\textbackslash{}u0092s methods. Therefore, we defined the following exclu-\\textbackslash{}nsion criteria:\\textbackslash{}n\\textbackslash{}001 Implementation was not described\\textbackslash{}n\\textbackslash{}001 Implementation does not use an existing established\\textbackslash{}nontology for encoding\\textbackslash{}nTable 1 Induced objective tasks with their definition and an example\\textbackslash{}nInduced NLP task(s) Description Example\\textbackslash{}nConcept detection 1 Assign ontology concepts to phrases in free\\textbackslash{}ntext (i.e., entity linking or annotation)\\textbackslash{}n\\textbackslash{}u0093Systolic blood pressure\\textbackslash{}u0094 can be represented as SNOMED-CT\\textbackslash{}nconcept 271649006 \\textbar{} Systolic blood pressure (observable entity) \\textbar{}\\textbackslash{}nEvent detection Detect events in free text \\textbackslash{}u0093Patient visited the outpatient clinic in January 2020\\textbackslash{}u0094 is an\\textbackslash{}nevent of type Visit.\\textbackslash{}nRelationship detection Detect semantic relationships between\\textbackslash{}nconcepts in free text\\textbackslash{}nThe concept Lung cancer in \\textbackslash{}u0093This patient was diagnosed with\\textbackslash{}nrecurrent lung cancer\\textbackslash{}u0094 is related to the concept Recurrence.\\textbackslash{}nText normalization Transform free text into a single canonical\\textbackslash{}nform\\textbackslash{}n\\textbackslash{}u0093This patient was diagnosed with influenza last year.\\textbackslash{}u0094 becomes\\textbackslash{}n\\textbackslash{}u0093This patient be diagnose with influenza last year.\\textbackslash{}u0094\\textbackslash{}nText summarization Create a short summary of free text and\\textbackslash{}npossible restructure the text based on this\\textbackslash{}nsummary\\textbackslash{}n\\textbackslash{}u0093Last year, this patient visited the clinic and was diagnosed with\\textbackslash{}ndiabetes mellitus type 2, and in addition to his diabetes, the\\textbackslash{}npatient was also diagnosed with hypertension\\textbackslash{}u0094 becomes\\textbackslash{}n\\textbackslash{}u0093Last year, this patient was diagnosed with diabetes mellitus\\textbackslash{}ntype 2 and hypertension\\textbackslash{}u0094.\\textbackslash{}nClassification Assign categories to free text A report containing the text \\textbackslash{}u0093This patient is not diagnosed\\textbackslash{}nyet\\textbackslash{}u0094 will be assigned to the category Undiagnosed.\\textbackslash{}nPrediction Create a predictive model based on free text Predict the outcome of the APACHE score based on the\\textbackslash{}n(free-text) content in a patient chart.\\textbackslash{}nIdentification Identify documents (e.g., reports or patient\\textbackslash{}ncharts) that match a specific condition\\textbackslash{}nbased on the contents of the document\\textbackslash{}nFind all patient charts that describe patients with hypertension\\textbackslash{}nand a BMI above 30.\\textbackslash{}nSoftware development Develop new or build upon existing NLP\\textbackslash{}nsoftware\\textbackslash{}nA new algorithm was developed to map ontology concepts\\textbackslash{}nto free text in clinical reports.\\textbackslash{}nSoftware evaluation Evaluate the effectiveness of NLP software The mapping algorithm has an F-score of 0.874.\\textbackslash{}n1.Also known as Medical Entity Linking and Medical Concept Normalization\\textbackslash{}nTable 2 Induced objective categories with their definition and associated NLP task(s)\\textbackslash{}nInduced category Induced NLP task(s) Definition\\textbackslash{}nComputer-assisted coding Concept detection Perform semi-automated annotation (i.e., with a human in the loop)\\textbackslash{}nInformation comparison Concept detection\\textbackslash{}nEvent detection\\textbackslash{}nRelationship detection\\textbackslash{}nCompare extracted structured information to information available in free-text form\\textbackslash{}nInformation enrichment Concept detection\\textbackslash{}nEvent detection\\textbackslash{}nRelationship detection\\textbackslash{}nText normalization\\textbackslash{}nText summarization\\textbackslash{}nExtract structured information from free text and attach this new information to the source\\textbackslash{}nInformation extraction Concept detection\\textbackslash{}nEvent detection\\textbackslash{}nRelationship detection\\textbackslash{}nExtract structured information from free text\\textbackslash{}nPrediction Classification\\textbackslash{}nPrediction\\textbackslash{}nIdentification\\textbackslash{}nUse structured information to classify free-text reports, predict outcomes, or identify cases\\textbackslash{}nSoftware development\\textbackslash{}nand evaluation\\textbackslash{}nSoftware development\\textbackslash{}nSoftware evaluation\\textbackslash{}nDevelop new NLP software or evaluate new or existing NLP software\\textbackslash{}nText processing Text normalization\\textbackslash{}nText summarization\\textbackslash{}nTransform free text into a new, more comprehensible form\\textbackslash{}nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 4 of 21\\textbackslash{}nTa\\textbackslash{}nb\\textbackslash{}nle\\textbackslash{}n3\\textbackslash{}nIn\\textbackslash{}ncl\\textbackslash{}nud\\textbackslash{}ned\\textbackslash{}npu\\textbackslash{}nbl\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nth\\textbackslash{}nei\\textbackslash{}nr\\textbackslash{}nfir\\textbackslash{}nst\\textbackslash{}nau\\textbackslash{}nth\\textbackslash{}nor\\textbackslash{}n,y\\textbackslash{}nea\\textbackslash{}nr,\\textbackslash{}ntit\\textbackslash{}nle\\textbackslash{}n,a\\textbackslash{}nnd\\textbackslash{}nco\\textbackslash{}nun\\textbackslash{}ntr\\textbackslash{}ny\\textbackslash{}nA\\textbackslash{}nut\\textbackslash{}nho\\textbackslash{}nr\\textbackslash{}nY\\textbackslash{}nea\\textbackslash{}nr\\textbackslash{}nC\\textbackslash{}nou\\textbackslash{}nnt\\textbackslash{}nry\\textbackslash{}nC\\textbackslash{}nha\\textbackslash{}nlle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nIn\\textbackslash{}nd\\textbackslash{}nuc\\textbackslash{}ned\\textbackslash{}nob\\textbackslash{}nje\\textbackslash{}nct\\textbackslash{}niv\\textbackslash{}ne\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nor\\textbackslash{}nig\\textbackslash{}nin\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nla\\textbackslash{}nng\\textbackslash{}nua\\textbackslash{}ng\\textbackslash{}ne\\textbackslash{}nU\\textbackslash{}nse\\textbackslash{}nd\\textbackslash{}nsy\\textbackslash{}nst\\textbackslash{}nem\\textbackslash{}nTe\\textbackslash{}nrm\\textbackslash{}n.S\\textbackslash{}nys\\textbackslash{}n.\\textbackslash{}nIn\\textbackslash{}nus\\textbackslash{}ne\\textbackslash{}nSo\\textbackslash{}nur\\textbackslash{}nce\\textbackslash{}nco\\textbackslash{}nd\\textbackslash{}ne\\textbackslash{}nRe\\textbackslash{}nf\\textbackslash{}nA\\textbackslash{}nfs\\textbackslash{}nha\\textbackslash{}nr\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nW\\textbackslash{}nar\\textbackslash{}neh\\textbackslash{}nou\\textbackslash{}nse\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n(C\\textbackslash{}nPT\\textbackslash{}n,H\\textbackslash{}nC\\textbackslash{}nPC\\textbackslash{}nS,\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-\\textbackslash{}n10\\textbackslash{}n,I\\textbackslash{}nC\\textbackslash{}nD\\textbackslash{}n10\\textbackslash{}nC\\textbackslash{}nM\\textbackslash{}n/\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n9C\\textbackslash{}nM\\textbackslash{}n,\\textbackslash{}nLO\\textbackslash{}nIN\\textbackslash{}nC\\textbackslash{}n,M\\textbackslash{}neS\\textbackslash{}nH\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}n-\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nRx\\textbackslash{}nN\\textbackslash{}nor\\textbackslash{}nm\\textbackslash{}n)\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}no,\\textbackslash{}non\\textbackslash{}nly\\textbackslash{}nlin\\textbackslash{}nks\\textbackslash{}nto\\textbackslash{}ncT\\textbackslash{}nA\\textbackslash{}nKE\\textbackslash{}nS\\textbackslash{}nso\\textbackslash{}nur\\textbackslash{}nce\\textbackslash{}nco\\textbackslash{}nde\\textbackslash{}n{[}2\\textbackslash{}n9{]}\\textbackslash{}nA\\textbackslash{}nln\\textbackslash{}naz\\textbackslash{}nza\\textbackslash{}nw\\textbackslash{}ni\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nU\\textbackslash{}nK\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nPh\\textbackslash{}nen\\textbackslash{}noC\\textbackslash{}nH\\textbackslash{}nF\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n1\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}3\\textbackslash{}n0{]}\\textbackslash{}nA\\textbackslash{}ntu\\textbackslash{}ntx\\textbackslash{}na\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nSp\\textbackslash{}nai\\textbackslash{}nn\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nEH\\textbackslash{}nR\\textbackslash{}ndo\\textbackslash{}ncu\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nts\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nSp\\textbackslash{}nan\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n(S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}n-C\\textbackslash{}nT\\textbackslash{}nfo\\textbackslash{}nr\\textbackslash{}nno\\textbackslash{}nrm\\textbackslash{}nal\\textbackslash{}niz\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn)\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt,\\textbackslash{}nai\\textbackslash{}nm\\textbackslash{}nto\\textbackslash{}nem\\textbackslash{}nbe\\textbackslash{}nd\\textbackslash{}nit\\textbackslash{}nin\\textbackslash{}nhu\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}n-s\\textbackslash{}nup\\textbackslash{}ner\\textbackslash{}nvi\\textbackslash{}nse\\textbackslash{}nd\\textbackslash{}nlo\\textbackslash{}nop\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}3\\textbackslash{}n1{]}\\textbackslash{}nBa\\textbackslash{}nrr\\textbackslash{}net\\textbackslash{}nt\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nPa\\textbackslash{}nlli\\textbackslash{}nat\\textbackslash{}niv\\textbackslash{}ne\\textbackslash{}nca\\textbackslash{}nre\\textbackslash{}nco\\textbackslash{}nns\\textbackslash{}nul\\textbackslash{}nt\\textbackslash{}nle\\textbackslash{}ntt\\textbackslash{}ner\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}no,\\textbackslash{}nbu\\textbackslash{}nt\\textbackslash{}npl\\textbackslash{}nan\\textbackslash{}nne\\textbackslash{}nd\\textbackslash{}n{[}3\\textbackslash{}n2{]}\\textbackslash{}nBe\\textbackslash{}nck\\textbackslash{}ner\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nG\\textbackslash{}ner\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}ny\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nSh\\textbackslash{}nA\\textbackslash{}nRe\\textbackslash{}n/C\\textbackslash{}nLE\\textbackslash{}nF\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n3)\\textbackslash{}n2\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nG\\textbackslash{}ner\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}n(E\\textbackslash{}nng\\textbackslash{}nlis\\textbackslash{}nh)\\textbackslash{}n,\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n(G\\textbackslash{}ner\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}n)\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt,\\textbackslash{}nst\\textbackslash{}nill\\textbackslash{}nun\\textbackslash{}nde\\textbackslash{}nr\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}3\\textbackslash{}n3{]}\\textbackslash{}nBe\\textbackslash{}nck\\textbackslash{}ner\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nG\\textbackslash{}ner\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}ny\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nof\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nkn\\textbackslash{}now\\textbackslash{}nn\\textbackslash{}nco\\textbackslash{}nlo\\textbackslash{}nre\\textbackslash{}nct\\textbackslash{}nal\\textbackslash{}nca\\textbackslash{}nnc\\textbackslash{}ner\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nG\\textbackslash{}ner\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nle\\textbackslash{}nd\\textbackslash{}nto\\textbackslash{}nim\\textbackslash{}npr\\textbackslash{}nov\\textbackslash{}ned\\textbackslash{}nqu\\textbackslash{}nal\\textbackslash{}nity\\textbackslash{}nof\\textbackslash{}nca\\textbackslash{}nre\\textbackslash{}nfo\\textbackslash{}nr\\textbackslash{}nco\\textbackslash{}nlo\\textbackslash{}nre\\textbackslash{}nct\\textbackslash{}nal\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}3\\textbackslash{}n4{]}\\textbackslash{}nBe\\textbackslash{}nja\\textbackslash{}nn\\textbackslash{}n20\\textbackslash{}n15\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nD\\textbackslash{}nis\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nge\\textbackslash{}nsu\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nar\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nse\\textbackslash{}nt\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}n+\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}3\\textbackslash{}n5{]}\\textbackslash{}nC\\textbackslash{}nas\\textbackslash{}ntr\\textbackslash{}no\\textbackslash{}n20\\textbackslash{}n10\\textbackslash{}nSp\\textbackslash{}nai\\textbackslash{}nn\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}n\\textbackslash{}u0091m\\textbackslash{}nos\\textbackslash{}nt\\textbackslash{}nre\\textbackslash{}nle\\textbackslash{}nva\\textbackslash{}nnt\\textbackslash{}nin\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}u0092\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nSp\\textbackslash{}nan\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}3\\textbackslash{}n6{]}\\textbackslash{}nC\\textbackslash{}nat\\textbackslash{}nlin\\textbackslash{}ng\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nU\\textbackslash{}nK\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nSo\\textbackslash{}nft\\textbackslash{}nw\\textbackslash{}nar\\textbackslash{}ne\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nev\\textbackslash{}nal\\textbackslash{}nua\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nM\\textbackslash{}nIM\\textbackslash{}nIC\\textbackslash{}n-II\\textbackslash{}nId\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}n4\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-9\\textbackslash{}n-C\\textbackslash{}nM\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}3\\textbackslash{}n7{]}\\textbackslash{}nC\\textbackslash{}nha\\textbackslash{}npm\\textbackslash{}nan\\textbackslash{}n20\\textbackslash{}n04\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nEm\\textbackslash{}ner\\textbackslash{}nge\\textbackslash{}nnc\\textbackslash{}ny\\textbackslash{}nde\\textbackslash{}npa\\textbackslash{}nrt\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}3\\textbackslash{}n8{]}\\textbackslash{}nC\\textbackslash{}nhe\\textbackslash{}nn\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nD\\textbackslash{}nis\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nge\\textbackslash{}nsu\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nar\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}npr\\textbackslash{}nog\\textbackslash{}nre\\textbackslash{}nss\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}3\\textbackslash{}n9{]}\\textbackslash{}nC\\textbackslash{}nhi\\textbackslash{}nar\\textbackslash{}nam\\textbackslash{}nel\\textbackslash{}nlo\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nIta\\textbackslash{}nly\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}n(c\\textbackslash{}nar\\textbackslash{}ndi\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny,\\textbackslash{}ndi\\textbackslash{}nab\\textbackslash{}net\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny,\\textbackslash{}nhe\\textbackslash{}npa\\textbackslash{}nto\\textbackslash{}nlo\\textbackslash{}ngy\\textbackslash{}n,n\\textbackslash{}nep\\textbackslash{}nhr\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny,\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}non\\textbackslash{}nco\\textbackslash{}nlo\\textbackslash{}ngy\\textbackslash{}n)\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nIta\\textbackslash{}nlia\\textbackslash{}nn\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}4\\textbackslash{}n0{]}\\textbackslash{}nC\\textbackslash{}nho\\textbackslash{}nde\\textbackslash{}ny\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nSe\\textbackslash{}nm\\textbackslash{}nEv\\textbackslash{}nal\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n4)\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nIC\\textbackslash{}nU\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na:\\textbackslash{}nD\\textbackslash{}nis\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nge\\textbackslash{}nsu\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nar\\textbackslash{}nie\\textbackslash{}ns,\\textbackslash{}nEC\\textbackslash{}nG\\textbackslash{}n,\\textbackslash{}nec\\textbackslash{}nho\\textbackslash{}n,a\\textbackslash{}nnd\\textbackslash{}nra\\textbackslash{}ndi\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}4\\textbackslash{}n1{]}\\textbackslash{}nC\\textbackslash{}nhu\\textbackslash{}nng\\textbackslash{}n20\\textbackslash{}n05\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nEc\\textbackslash{}nho\\textbackslash{}nca\\textbackslash{}nrd\\textbackslash{}nio\\textbackslash{}ngr\\textbackslash{}nam\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt,\\textbackslash{}nit\\textbackslash{}nw\\textbackslash{}nill\\textbackslash{}nbe\\textbackslash{}nus\\textbackslash{}ned\\textbackslash{}nto\\textbackslash{}npo\\textbackslash{}npu\\textbackslash{}nla\\textbackslash{}nte\\textbackslash{}na\\textbackslash{}nre\\textbackslash{}ngi\\textbackslash{}nst\\textbackslash{}nry\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}4\\textbackslash{}n2{]}\\textbackslash{}nC\\textbackslash{}nom\\textbackslash{}nbi\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nIta\\textbackslash{}nly\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nVi\\textbackslash{}ngi\\textbackslash{}nSe\\textbackslash{}ngn\\textbackslash{}n(a\\textbackslash{}ndv\\textbackslash{}ner\\textbackslash{}nse\\textbackslash{}ndr\\textbackslash{}nug\\textbackslash{}nre\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nns\\textbackslash{}n)\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nIta\\textbackslash{}nlia\\textbackslash{}nn\\textbackslash{}n+\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nM\\textbackslash{}ned\\textbackslash{}nD\\textbackslash{}nRA\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nim\\textbackslash{}npl\\textbackslash{}nem\\textbackslash{}nen\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nin\\textbackslash{}nVi\\textbackslash{}ngi\\textbackslash{}nFa\\textbackslash{}nrm\\textbackslash{}nac\\textbackslash{}no\\textbackslash{}nPs\\textbackslash{}neu\\textbackslash{}ndo\\textbackslash{}nco\\textbackslash{}nde\\textbackslash{}n{[}4\\textbackslash{}n3{]}\\textbackslash{}nD\\textbackslash{}ne\\textbackslash{}nBr\\textbackslash{}nui\\textbackslash{}njn\\textbackslash{}n20\\textbackslash{}n11\\textbackslash{}nC\\textbackslash{}nan\\textbackslash{}nad\\textbackslash{}na\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nH\\textbackslash{}nos\\textbackslash{}npi\\textbackslash{}nta\\textbackslash{}nld\\textbackslash{}nis\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nge\\textbackslash{}nsu\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nar\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}npr\\textbackslash{}nog\\textbackslash{}nre\\textbackslash{}nss\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}4\\textbackslash{}n4{]}\\textbackslash{}nD\\textbackslash{}nei\\textbackslash{}nss\\textbackslash{}ner\\textbackslash{}not\\textbackslash{}nh\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nSi\\textbackslash{}nx\\textbackslash{}nse\\textbackslash{}nts\\textbackslash{}nof\\textbackslash{}nre\\textbackslash{}nal\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nfo\\textbackslash{}nur\\textbackslash{}ndi\\textbackslash{}nffe\\textbackslash{}nre\\textbackslash{}nnt\\textbackslash{}nm\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nce\\textbackslash{}nnt\\textbackslash{}ner\\textbackslash{}ns.\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nH\\textbackslash{}nPO\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}4\\textbackslash{}n5{]}\\textbackslash{}nD\\textbackslash{}nem\\textbackslash{}nne\\textbackslash{}nr-\\textbackslash{}nFu\\textbackslash{}nsh\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}n20\\textbackslash{}n17\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nSo\\textbackslash{}nft\\textbackslash{}nw\\textbackslash{}nar\\textbackslash{}ne\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nev\\textbackslash{}nal\\textbackslash{}nua\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nBi\\textbackslash{}noS\\textbackslash{}nco\\textbackslash{}npe\\textbackslash{}n5 ,\\textbackslash{}nN\\textbackslash{}nC\\textbackslash{}nBI\\textbackslash{}ndi\\textbackslash{}nse\\textbackslash{}nas\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n6 ,\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/\\textbackslash{}nVA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3 ,\\textbackslash{}nSh\\textbackslash{}nA\\textbackslash{}nRe\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n7 ,\\textbackslash{}nLH\\textbackslash{}nC\\textbackslash{}nte\\textbackslash{}nst\\textbackslash{}nco\\textbackslash{}nlle\\textbackslash{}nct\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}n(b\\textbackslash{}nio\\textbackslash{}nlo\\textbackslash{}ngi\\textbackslash{}nca\\textbackslash{}nl/\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}njo\\textbackslash{}nur\\textbackslash{}nna\\textbackslash{}nla\\textbackslash{}nbs\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}nts\\textbackslash{}n)\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nus\\textbackslash{}ned\\textbackslash{}nin\\textbackslash{}not\\textbackslash{}nhe\\textbackslash{}nr\\textbackslash{}npa\\textbackslash{}npe\\textbackslash{}nrs\\textbackslash{}nid\\textbackslash{}nen\\textbackslash{}ntif\\textbackslash{}nie\\textbackslash{}nd\\textbackslash{}nin\\textbackslash{}nlit\\textbackslash{}ner\\textbackslash{}nat\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nse\\textbackslash{}nar\\textbackslash{}nch\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}4\\textbackslash{}n6{]}\\textbackslash{}nD\\textbackslash{}niv\\textbackslash{}nita\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nPa\\textbackslash{}nrt\\textbackslash{}ns:\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nSo\\textbackslash{}nft\\textbackslash{}nw\\textbackslash{}nar\\textbackslash{}ne\\textbackslash{}nRa\\textbackslash{}nnd\\textbackslash{}nom\\textbackslash{}nly\\textbackslash{}nse\\textbackslash{}nle\\textbackslash{}nct\\textbackslash{}ned\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nre\\textbackslash{}nco\\textbackslash{}nrd\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n(le\\textbackslash{}nve\\textbackslash{}nl0\\textbackslash{}n+\\textbackslash{}n9)\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nus\\textbackslash{}ned\\textbackslash{}nby\\textbackslash{}nVA\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}4\\textbackslash{}n7{]}\\textbackslash{}nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 5 of 21\\textbackslash{}nTa\\textbackslash{}nb\\textbackslash{}nle\\textbackslash{}n3\\textbackslash{}nIn\\textbackslash{}ncl\\textbackslash{}nud\\textbackslash{}ned\\textbackslash{}npu\\textbackslash{}nbl\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nth\\textbackslash{}nei\\textbackslash{}nr\\textbackslash{}nfir\\textbackslash{}nst\\textbackslash{}nau\\textbackslash{}nth\\textbackslash{}nor\\textbackslash{}n,y\\textbackslash{}nea\\textbackslash{}nr,\\textbackslash{}ntit\\textbackslash{}nle\\textbackslash{}n,a\\textbackslash{}nnd\\textbackslash{}nco\\textbackslash{}nun\\textbackslash{}ntr\\textbackslash{}ny\\textbackslash{}n(C\\textbackslash{}non\\textbackslash{}ntin\\textbackslash{}nue\\textbackslash{}nd)\\textbackslash{}nA\\textbackslash{}nut\\textbackslash{}nho\\textbackslash{}nr\\textbackslash{}nY\\textbackslash{}nea\\textbackslash{}nr\\textbackslash{}nC\\textbackslash{}nou\\textbackslash{}nnt\\textbackslash{}nry\\textbackslash{}nC\\textbackslash{}nha\\textbackslash{}nlle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nIn\\textbackslash{}nd\\textbackslash{}nuc\\textbackslash{}ned\\textbackslash{}nob\\textbackslash{}nje\\textbackslash{}nct\\textbackslash{}niv\\textbackslash{}ne\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nor\\textbackslash{}nig\\textbackslash{}nin\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nla\\textbackslash{}nng\\textbackslash{}nua\\textbackslash{}ng\\textbackslash{}ne\\textbackslash{}nU\\textbackslash{}nse\\textbackslash{}nd\\textbackslash{}nsy\\textbackslash{}nst\\textbackslash{}nem\\textbackslash{}nTe\\textbackslash{}nrm\\textbackslash{}n.S\\textbackslash{}nys\\textbackslash{}n.\\textbackslash{}nIn\\textbackslash{}nus\\textbackslash{}ne\\textbackslash{}nSo\\textbackslash{}nur\\textbackslash{}nce\\textbackslash{}nco\\textbackslash{}nd\\textbackslash{}ne\\textbackslash{}nRe\\textbackslash{}nf\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nev\\textbackslash{}nal\\textbackslash{}nua\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nm\\textbackslash{}nos\\textbackslash{}nt\\textbackslash{}nfre\\textbackslash{}nqu\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}ndo\\textbackslash{}ncu\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nty\\textbackslash{}npe\\textbackslash{}ns\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nic\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nC\\textbackslash{}nom\\textbackslash{}npu\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nIn\\textbackslash{}nfra\\textbackslash{}nst\\textbackslash{}nru\\textbackslash{}nct\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nD\\textbackslash{}nua\\textbackslash{}nrt\\textbackslash{}ne\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nPo\\textbackslash{}nrt\\textbackslash{}nug\\textbackslash{}nal\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nD\\textbackslash{}nea\\textbackslash{}nth\\textbackslash{}nce\\textbackslash{}nrt\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}nte\\textbackslash{}ns,\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nbu\\textbackslash{}nlle\\textbackslash{}ntin\\textbackslash{}ns,\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nau\\textbackslash{}nto\\textbackslash{}nps\\textbackslash{}ny\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nPo\\textbackslash{}nrt\\textbackslash{}nug\\textbackslash{}nue\\textbackslash{}nse\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-1\\textbackslash{}n0\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nus\\textbackslash{}ned\\textbackslash{}nby\\textbackslash{}nPo\\textbackslash{}nrt\\textbackslash{}nug\\textbackslash{}nes\\textbackslash{}ne\\textbackslash{}nM\\textbackslash{}nin\\textbackslash{}nis\\textbackslash{}ntr\\textbackslash{}ny\\textbackslash{}nof\\textbackslash{}nH\\textbackslash{}nea\\textbackslash{}nlth\\textbackslash{}nfo\\textbackslash{}nr\\textbackslash{}nne\\textbackslash{}nar\\textbackslash{}nre\\textbackslash{}nal\\textbackslash{}n-t\\textbackslash{}nim\\textbackslash{}ne\\textbackslash{}nde\\textbackslash{}nat\\textbackslash{}nh\\textbackslash{}nca\\textbackslash{}nus\\textbackslash{}ne\\textbackslash{}nsu\\textbackslash{}nrv\\textbackslash{}nei\\textbackslash{}nlla\\textbackslash{}nnc\\textbackslash{}ne\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}4\\textbackslash{}n8{]}\\textbackslash{}nFa\\textbackslash{}nlis\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nK\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nM\\textbackslash{}nIM\\textbackslash{}nIC\\textbackslash{}n-II\\textbackslash{}nId\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}n4\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-9\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}4\\textbackslash{}n9{]}\\textbackslash{}nFe\\textbackslash{}nrr\\textbackslash{}não\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nPo\\textbackslash{}nrt\\textbackslash{}nug\\textbackslash{}nal\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nIn\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}nad\\textbackslash{}nul\\textbackslash{}nt\\textbackslash{}nep\\textbackslash{}nis\\textbackslash{}nod\\textbackslash{}nes\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nEH\\textbackslash{}nR\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nPo\\textbackslash{}nrt\\textbackslash{}nug\\textbackslash{}nue\\textbackslash{}nse\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-9\\textbackslash{}n-C\\textbackslash{}nM\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}5\\textbackslash{}n0{]}\\textbackslash{}nG\\textbackslash{}ner\\textbackslash{}nbi\\textbackslash{}ner\\textbackslash{}n20\\textbackslash{}n11\\textbackslash{}nFr\\textbackslash{}nan\\textbackslash{}nce\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nom\\textbackslash{}npu\\textbackslash{}nte\\textbackslash{}nriz\\textbackslash{}ned\\textbackslash{}nem\\textbackslash{}ner\\textbackslash{}nge\\textbackslash{}nnc\\textbackslash{}ny\\textbackslash{}nde\\textbackslash{}npa\\textbackslash{}nrt\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nm\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nre\\textbackslash{}nco\\textbackslash{}nrd\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nFr\\textbackslash{}nen\\textbackslash{}nch\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-1\\textbackslash{}n0,\\textbackslash{}nC\\textbackslash{}nC\\textbackslash{}nA\\textbackslash{}nM\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nA\\textbackslash{}nTC\\textbackslash{}n,M\\textbackslash{}neS\\textbackslash{}nH\\textbackslash{}n,I\\textbackslash{}nC\\textbackslash{}nPC\\textbackslash{}n-2\\textbackslash{}n,\\textbackslash{}nD\\textbackslash{}nC\\textbackslash{}nR\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt,\\textbackslash{}nw\\textbackslash{}nill\\textbackslash{}nbe\\textbackslash{}nin\\textbackslash{}nte\\textbackslash{}ngr\\textbackslash{}nat\\textbackslash{}ned\\textbackslash{}nin\\textbackslash{}nto\\textbackslash{}na\\textbackslash{}nC\\textbackslash{}nD\\textbackslash{}nSS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}5\\textbackslash{}n1{]}\\textbackslash{}nG\\textbackslash{}noi\\textbackslash{}nco\\textbackslash{}nec\\textbackslash{}nhe\\textbackslash{}na\\textbackslash{}nSa\\textbackslash{}nla\\textbackslash{}nza\\textbackslash{}nr\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nSp\\textbackslash{}nai\\textbackslash{}nn\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nD\\textbackslash{}nia\\textbackslash{}ngn\\textbackslash{}nos\\textbackslash{}ntic\\textbackslash{}nte\\textbackslash{}nxt\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}nre\\textbackslash{}nco\\textbackslash{}nrd\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nSp\\textbackslash{}nan\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-9\\textbackslash{}n-C\\textbackslash{}nM\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}5\\textbackslash{}n2{]}\\textbackslash{}nH\\textbackslash{}nam\\textbackslash{}nid\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nla\\textbackslash{}nss\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nes\\textbackslash{}nof\\textbackslash{}nIra\\textbackslash{}nq\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nA\\textbackslash{}nfg\\textbackslash{}nha\\textbackslash{}nni\\textbackslash{}nst\\textbackslash{}nan\\textbackslash{}nve\\textbackslash{}nte\\textbackslash{}nra\\textbackslash{}nns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nVA\\textbackslash{}nna\\textbackslash{}ntio\\textbackslash{}nna\\textbackslash{}nlc\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nba\\textbackslash{}nse\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}5\\textbackslash{}n3{]}\\textbackslash{}nH\\textbackslash{}nas\\textbackslash{}nsa\\textbackslash{}nnz\\textbackslash{}nad\\textbackslash{}neh\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nA\\textbackslash{}nus\\textbackslash{}ntr\\textbackslash{}nal\\textbackslash{}nia\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nSh\\textbackslash{}nA\\textbackslash{}nRe\\textbackslash{}n/C\\textbackslash{}nLE\\textbackslash{}nF\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n3)\\textbackslash{}n2\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}5\\textbackslash{}n4{]}\\textbackslash{}nH\\textbackslash{}nel\\textbackslash{}nw\\textbackslash{}ne\\textbackslash{}n20\\textbackslash{}n17\\textbackslash{}nLe\\textbackslash{}nba\\textbackslash{}nno\\textbackslash{}nn\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nom\\textbackslash{}npu\\textbackslash{}nte\\textbackslash{}nr-\\textbackslash{}nas\\textbackslash{}nsi\\textbackslash{}nst\\textbackslash{}ned\\textbackslash{}nco\\textbackslash{}ndi\\textbackslash{}nng\\textbackslash{}nM\\textbackslash{}nIM\\textbackslash{}nIC\\textbackslash{}n-II\\textbackslash{}nId\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,I\\textbackslash{}nC\\textbackslash{}nD\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}5\\textbackslash{}n5{]}\\textbackslash{}nH\\textbackslash{}ner\\textbackslash{}nsh\\textbackslash{}n20\\textbackslash{}n01\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nRa\\textbackslash{}ndi\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}nim\\textbackslash{}nag\\textbackslash{}ne\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}no,\\textbackslash{}nst\\textbackslash{}nill\\textbackslash{}nin\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt/\\textbackslash{}nte\\textbackslash{}nst\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nPs\\textbackslash{}neu\\textbackslash{}ndo\\textbackslash{}nco\\textbackslash{}nde\\textbackslash{}n{[}5\\textbackslash{}n6{]}\\textbackslash{}nH\\textbackslash{}noo\\textbackslash{}nge\\textbackslash{}nnd\\textbackslash{}noo\\textbackslash{}nrn\\textbackslash{}n20\\textbackslash{}n15\\textbackslash{}nN\\textbackslash{}net\\textbackslash{}nhe\\textbackslash{}nrla\\textbackslash{}nnd\\textbackslash{}ns\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nPr\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}non\\textbackslash{}nsu\\textbackslash{}nlta\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nof\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nin\\textbackslash{}na\\textbackslash{}npr\\textbackslash{}nim\\textbackslash{}nar\\textbackslash{}ny\\textbackslash{}nca\\textbackslash{}nre\\textbackslash{}nse\\textbackslash{}ntt\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nD\\textbackslash{}nut\\textbackslash{}nch\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}n-C\\textbackslash{}nT,\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,I\\textbackslash{}nC\\textbackslash{}nPC\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}5\\textbackslash{}n7{]}\\textbackslash{}nJin\\textbackslash{}nda\\textbackslash{}nl\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n2)\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n2)\\textbackslash{}n8\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nM\\textbackslash{}neS\\textbackslash{}nH\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}5\\textbackslash{}n8{]}\\textbackslash{}nKa\\textbackslash{}nng\\textbackslash{}n20\\textbackslash{}n09\\textbackslash{}nKo\\textbackslash{}nre\\textbackslash{}na\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nD\\textbackslash{}nis\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nge\\textbackslash{}nsu\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nar\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nKo\\textbackslash{}nre\\textbackslash{}nan\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nKO\\textbackslash{}nM\\textbackslash{}nET\\textbackslash{}n,U\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}5\\textbackslash{}n9{]}\\textbackslash{}nKe\\textbackslash{}nrs\\textbackslash{}nlo\\textbackslash{}not\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nN\\textbackslash{}net\\textbackslash{}nhe\\textbackslash{}nrla\\textbackslash{}nnd\\textbackslash{}ns\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}n(N\\textbackslash{}non\\textbackslash{}n-s\\textbackslash{}nm\\textbackslash{}nal\\textbackslash{}nlc\\textbackslash{}nel\\textbackslash{}nl)\\textbackslash{}nLu\\textbackslash{}nng\\textbackslash{}nca\\textbackslash{}nnc\\textbackslash{}ner\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nts\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}6\\textbackslash{}n0{]}\\textbackslash{}nKö\\textbackslash{}nni\\textbackslash{}ng\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nG\\textbackslash{}ner\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}ny\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nSo\\textbackslash{}nft\\textbackslash{}nw\\textbackslash{}nar\\textbackslash{}ne\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nev\\textbackslash{}nal\\textbackslash{}nua\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nD\\textbackslash{}nis\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nge\\textbackslash{}nle\\textbackslash{}ntt\\textbackslash{}ner\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nBA\\textbackslash{}nSE\\textbackslash{}n-II\\textbackslash{}nst\\textbackslash{}nud\\textbackslash{}ny\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nG\\textbackslash{}ner\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nW\\textbackslash{}nin\\textbackslash{}nge\\textbackslash{}nrt\\textbackslash{}n-N\\textbackslash{}nom\\textbackslash{}nen\\textbackslash{}ncl\\textbackslash{}nat\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nN\\textbackslash{}no,\\textbackslash{}nst\\textbackslash{}nill\\textbackslash{}nha\\textbackslash{}ns\\textbackslash{}nto\\textbackslash{}npr\\textbackslash{}nov\\textbackslash{}ne\\textbackslash{}nits\\textbackslash{}nva\\textbackslash{}nlu\\textbackslash{}ne\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}6\\textbackslash{}n1{]}\\textbackslash{}nLi\\textbackslash{}n20\\textbackslash{}n15\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nco\\textbackslash{}nm\\textbackslash{}npa\\textbackslash{}nris\\textbackslash{}non\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}ndi\\textbackslash{}nsc\\textbackslash{}nha\\textbackslash{}nrg\\textbackslash{}ne\\textbackslash{}npr\\textbackslash{}nes\\textbackslash{}ncr\\textbackslash{}nip\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nlis\\textbackslash{}nts\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nRx\\textbackslash{}nN\\textbackslash{}nor\\textbackslash{}nm\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt,\\textbackslash{}npl\\textbackslash{}nan\\textbackslash{}ns\\textbackslash{}nto\\textbackslash{}nm\\textbackslash{}nov\\textbackslash{}ne\\textbackslash{}nto\\textbackslash{}npr\\textbackslash{}nod\\textbackslash{}nuc\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nPs\\textbackslash{}neu\\textbackslash{}ndo\\textbackslash{}nco\\textbackslash{}nde\\textbackslash{}n{[}6\\textbackslash{}n2{]}\\textbackslash{}nLi\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nEH\\textbackslash{}nR\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nM\\textbackslash{}ned\\textbackslash{}nD\\textbackslash{}nRA\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}6\\textbackslash{}n3{]}\\textbackslash{}nLi\\textbackslash{}nng\\textbackslash{}nre\\textbackslash{}nn\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nla\\textbackslash{}nss\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nSt\\textbackslash{}nru\\textbackslash{}nct\\textbackslash{}nur\\textbackslash{}ned\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nun\\textbackslash{}nst\\textbackslash{}nru\\textbackslash{}nct\\textbackslash{}nur\\textbackslash{}ned\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}ntw\\textbackslash{}no\\textbackslash{}nEH\\textbackslash{}nR\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nba\\textbackslash{}nse\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,I\\textbackslash{}nC\\textbackslash{}nD\\textbackslash{}n-9\\textbackslash{}n,R\\textbackslash{}nxN\\textbackslash{}nor\\textbackslash{}nm\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}1\\textbackslash{}n2{]}\\textbackslash{}nLi\\textbackslash{}nu\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}ndi\\textbackslash{}nffe\\textbackslash{}nre\\textbackslash{}nnt\\textbackslash{}nin\\textbackslash{}nst\\textbackslash{}nitu\\textbackslash{}ntio\\textbackslash{}nns\\textbackslash{}n+\\textbackslash{}nPu\\textbackslash{}nbM\\textbackslash{}ned\\textbackslash{}nC\\textbackslash{}nas\\textbackslash{}ne\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}nab\\textbackslash{}nst\\textbackslash{}nra\\textbackslash{}nct\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}n+\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nH\\textbackslash{}nPO\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}6\\textbackslash{}n4{]}\\textbackslash{}nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 6 of 21\\textbackslash{}nTa\\textbackslash{}nb\\textbackslash{}nle\\textbackslash{}n3\\textbackslash{}nIn\\textbackslash{}ncl\\textbackslash{}nud\\textbackslash{}ned\\textbackslash{}npu\\textbackslash{}nbl\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nth\\textbackslash{}nei\\textbackslash{}nr\\textbackslash{}nfir\\textbackslash{}nst\\textbackslash{}nau\\textbackslash{}nth\\textbackslash{}nor\\textbackslash{}n,y\\textbackslash{}nea\\textbackslash{}nr,\\textbackslash{}ntit\\textbackslash{}nle\\textbackslash{}n,a\\textbackslash{}nnd\\textbackslash{}nco\\textbackslash{}nun\\textbackslash{}ntr\\textbackslash{}ny\\textbackslash{}n(C\\textbackslash{}non\\textbackslash{}ntin\\textbackslash{}nue\\textbackslash{}nd)\\textbackslash{}nA\\textbackslash{}nut\\textbackslash{}nho\\textbackslash{}nr\\textbackslash{}nY\\textbackslash{}nea\\textbackslash{}nr\\textbackslash{}nC\\textbackslash{}nou\\textbackslash{}nnt\\textbackslash{}nry\\textbackslash{}nC\\textbackslash{}nha\\textbackslash{}nlle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nIn\\textbackslash{}nd\\textbackslash{}nuc\\textbackslash{}ned\\textbackslash{}nob\\textbackslash{}nje\\textbackslash{}nct\\textbackslash{}niv\\textbackslash{}ne\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nor\\textbackslash{}nig\\textbackslash{}nin\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nla\\textbackslash{}nng\\textbackslash{}nua\\textbackslash{}ng\\textbackslash{}ne\\textbackslash{}nU\\textbackslash{}nse\\textbackslash{}nd\\textbackslash{}nsy\\textbackslash{}nst\\textbackslash{}nem\\textbackslash{}nTe\\textbackslash{}nrm\\textbackslash{}n.S\\textbackslash{}nys\\textbackslash{}n.\\textbackslash{}nIn\\textbackslash{}nus\\textbackslash{}ne\\textbackslash{}nSo\\textbackslash{}nur\\textbackslash{}nce\\textbackslash{}nco\\textbackslash{}nd\\textbackslash{}ne\\textbackslash{}nRe\\textbackslash{}nf\\textbackslash{}nLo\\textbackslash{}nw\\textbackslash{}ne\\textbackslash{}n20\\textbackslash{}n09\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nSi\\textbackslash{}nng\\textbackslash{}nle\\textbackslash{}n-s\\textbackslash{}npe\\textbackslash{}nci\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}npa\\textbackslash{}nth\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}6\\textbackslash{}n5{]}\\textbackslash{}nLu\\textbackslash{}no\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nPa\\textbackslash{}nth\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}ncu\\textbackslash{}nrr\\textbackslash{}nen\\textbackslash{}ntly\\textbackslash{}nw\\textbackslash{}nor\\textbackslash{}nki\\textbackslash{}nng\\textbackslash{}non\\textbackslash{}npr\\textbackslash{}noj\\textbackslash{}nec\\textbackslash{}nt\\textbackslash{}nin\\textbackslash{}nm\\textbackslash{}nul\\textbackslash{}ntip\\textbackslash{}nle\\textbackslash{}nho\\textbackslash{}nsp\\textbackslash{}nita\\textbackslash{}nls\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}6\\textbackslash{}n6{]}\\textbackslash{}nM\\textbackslash{}ney\\textbackslash{}nst\\textbackslash{}nre\\textbackslash{}n20\\textbackslash{}n06\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}ndo\\textbackslash{}ncu\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nts\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nad\\textbackslash{}nul\\textbackslash{}nt\\textbackslash{}nin\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nin\\textbackslash{}na\\textbackslash{}nca\\textbackslash{}nrd\\textbackslash{}nio\\textbackslash{}nva\\textbackslash{}nsc\\textbackslash{}nul\\textbackslash{}nar\\textbackslash{}nun\\textbackslash{}nit\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n(le\\textbackslash{}nve\\textbackslash{}nl0\\textbackslash{}n),\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt,\\textbackslash{}nte\\textbackslash{}nst\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nin\\textbackslash{}npr\\textbackslash{}nac\\textbackslash{}ntic\\textbackslash{}ne\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}6\\textbackslash{}n7{]}\\textbackslash{}nM\\textbackslash{}ney\\textbackslash{}nst\\textbackslash{}nre\\textbackslash{}n20\\textbackslash{}n10\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n(2\\textbackslash{}n00\\textbackslash{}n9)\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nse\\textbackslash{}nt\\textbackslash{}n(2\\textbackslash{}n00\\textbackslash{}n9)\\textbackslash{}n9\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt,\\textbackslash{}npo\\textbackslash{}nss\\textbackslash{}nib\\textbackslash{}nle\\textbackslash{}nin\\textbackslash{}nte\\textbackslash{}ngr\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nin\\textbackslash{}nre\\textbackslash{}nse\\textbackslash{}nar\\textbackslash{}nch\\textbackslash{}nin\\textbackslash{}nfra\\textbackslash{}nst\\textbackslash{}nru\\textbackslash{}nct\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}6\\textbackslash{}n8{]}\\textbackslash{}nM\\textbackslash{}nin\\textbackslash{}nar\\textbackslash{}nd\\textbackslash{}n20\\textbackslash{}n11\\textbackslash{}nFr\\textbackslash{}nan\\textbackslash{}nce\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}6\\textbackslash{}n9{]}\\textbackslash{}nM\\textbackslash{}nis\\textbackslash{}nhr\\textbackslash{}na\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nN\\textbackslash{}nIH\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nC\\textbackslash{}nen\\textbackslash{}nte\\textbackslash{}nr\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nw\\textbackslash{}nar\\textbackslash{}neh\\textbackslash{}nou\\textbackslash{}nse\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,H\\textbackslash{}nPO\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}7\\textbackslash{}n0{]}\\textbackslash{}nN\\textbackslash{}ngu\\textbackslash{}nye\\textbackslash{}nn\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nA\\textbackslash{}nus\\textbackslash{}ntr\\textbackslash{}nal\\textbackslash{}nia\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nom\\textbackslash{}npu\\textbackslash{}nte\\textbackslash{}nr-\\textbackslash{}nas\\textbackslash{}nsi\\textbackslash{}nst\\textbackslash{}ned\\textbackslash{}nco\\textbackslash{}ndi\\textbackslash{}nng\\textbackslash{}nH\\textbackslash{}nos\\textbackslash{}npi\\textbackslash{}nta\\textbackslash{}nlp\\textbackslash{}nro\\textbackslash{}ngr\\textbackslash{}nes\\textbackslash{}ns\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-1\\textbackslash{}n0-\\textbackslash{}nA\\textbackslash{}nM\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}7\\textbackslash{}n1{]}\\textbackslash{}nO\\textbackslash{}nel\\textbackslash{}nlri\\textbackslash{}nch\\textbackslash{}n20\\textbackslash{}n15\\textbackslash{}nU\\textbackslash{}nK\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nPu\\textbackslash{}nbM\\textbackslash{}ned\\textbackslash{}nab\\textbackslash{}nst\\textbackslash{}nra\\textbackslash{}nct\\textbackslash{}ns,\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}ntr\\textbackslash{}nia\\textbackslash{}nl\\textbackslash{}nin\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn,\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3 ,\\textbackslash{}nSH\\textbackslash{}nA\\textbackslash{}nRE\\textbackslash{}n/C\\textbackslash{}nLE\\textbackslash{}nF\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n3)\\textbackslash{}n2\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}7\\textbackslash{}n2{]}\\textbackslash{}nPa\\textbackslash{}ntr\\textbackslash{}nic\\textbackslash{}nk\\textbackslash{}n20\\textbackslash{}n11\\textbackslash{}nA\\textbackslash{}nus\\textbackslash{}ntr\\textbackslash{}nal\\textbackslash{}nia\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}7\\textbackslash{}n3{]}\\textbackslash{}nPé\\textbackslash{}nre\\textbackslash{}nz\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nSp\\textbackslash{}nai\\textbackslash{}nn\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nTe\\textbackslash{}nxt\\textbackslash{}npr\\textbackslash{}noc\\textbackslash{}nes\\textbackslash{}nsi\\textbackslash{}nng\\textbackslash{}nSp\\textbackslash{}non\\textbackslash{}nta\\textbackslash{}nne\\textbackslash{}nou\\textbackslash{}ns\\textbackslash{}nD\\textbackslash{}nTs\\textbackslash{}nra\\textbackslash{}nnd\\textbackslash{}nom\\textbackslash{}nly\\textbackslash{}nse\\textbackslash{}nle\\textbackslash{}nct\\textbackslash{}ned\\textbackslash{}nen\\textbackslash{}ntr\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nSp\\textbackslash{}nan\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}7\\textbackslash{}n4{]}\\textbackslash{}nRe\\textbackslash{}nát\\textbackslash{}neg\\textbackslash{}nui\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nC\\textbackslash{}nan\\textbackslash{}nad\\textbackslash{}na\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n00\\textbackslash{}n8)\\textbackslash{}n10\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nRx\\textbackslash{}nN\\textbackslash{}nor\\textbackslash{}nm\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}7\\textbackslash{}n5{]}\\textbackslash{}nRo\\textbackslash{}nbe\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}n20\\textbackslash{}n11\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,I\\textbackslash{}nC\\textbackslash{}nD\\textbackslash{}n-9\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}7\\textbackslash{}n6{]}\\textbackslash{}nRo\\textbackslash{}nus\\textbackslash{}nse\\textbackslash{}nau\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nco\\textbackslash{}nm\\textbackslash{}npa\\textbackslash{}nris\\textbackslash{}non\\textbackslash{}nED\\textbackslash{}nen\\textbackslash{}nco\\textbackslash{}nun\\textbackslash{}nte\\textbackslash{}nrs\\textbackslash{}nfo\\textbackslash{}nr\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nhe\\textbackslash{}nad\\textbackslash{}nac\\textbackslash{}nhe\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nho\\textbackslash{}nre\\textbackslash{}nce\\textbackslash{}niv\\textbackslash{}ned\\textbackslash{}nhe\\textbackslash{}nad\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n:S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nRa\\textbackslash{}ndL\\textbackslash{}nex\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}7\\textbackslash{}n7{]}\\textbackslash{}nSa\\textbackslash{}nvo\\textbackslash{}nva\\textbackslash{}n20\\textbackslash{}n10\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n(2\\textbackslash{}n00\\textbackslash{}n6,\\textbackslash{}n20\\textbackslash{}n08\\textbackslash{}n)\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nSu\\textbackslash{}nbs\\textbackslash{}net\\textbackslash{}nof\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nEM\\textbackslash{}nR\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nRx\\textbackslash{}nN\\textbackslash{}nor\\textbackslash{}nm\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nus\\textbackslash{}ned\\textbackslash{}nin\\textbackslash{}not\\textbackslash{}nhe\\textbackslash{}nr\\textbackslash{}npa\\textbackslash{}npe\\textbackslash{}nrs\\textbackslash{}nid\\textbackslash{}nen\\textbackslash{}ntif\\textbackslash{}nie\\textbackslash{}nd\\textbackslash{}nin\\textbackslash{}nlit\\textbackslash{}ner\\textbackslash{}nat\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nse\\textbackslash{}nar\\textbackslash{}nch\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}7\\textbackslash{}n8{]}\\textbackslash{}nSh\\textbackslash{}niv\\textbackslash{}nad\\textbackslash{}ne\\textbackslash{}n20\\textbackslash{}n15\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/U\\textbackslash{}nTH\\textbackslash{}nea\\textbackslash{}nlth\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n4)\\textbackslash{}nC\\textbackslash{}nla\\textbackslash{}nss\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n4)\\textbackslash{}n11\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}1\\textbackslash{}n1{]}\\textbackslash{}nSh\\textbackslash{}noe\\textbackslash{}nnb\\textbackslash{}nill\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nEH\\textbackslash{}nR\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nhy\\textbackslash{}npe\\textbackslash{}nrt\\textbackslash{}nen\\textbackslash{}nsi\\textbackslash{}non\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,S\\textbackslash{}nN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}7\\textbackslash{}n9{]}\\textbackslash{}nSo\\textbackslash{}nhn\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nlin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nm\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}ntio\\textbackslash{}nns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nRx\\textbackslash{}nN\\textbackslash{}nor\\textbackslash{}nm\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}8\\textbackslash{}n0{]}\\textbackslash{}nSo\\textbackslash{}nlti\\textbackslash{}n20\\textbackslash{}n08\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nen\\textbackslash{}nric\\textbackslash{}nhm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nC\\textbackslash{}nar\\textbackslash{}ndi\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}nam\\textbackslash{}nbu\\textbackslash{}nla\\textbackslash{}nto\\textbackslash{}nry\\textbackslash{}npr\\textbackslash{}nog\\textbackslash{}nre\\textbackslash{}nss\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}8\\textbackslash{}n1{]}\\textbackslash{}nSo\\textbackslash{}nria\\textbackslash{}nno\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nSp\\textbackslash{}nai\\textbackslash{}nn\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nem\\textbackslash{}ner\\textbackslash{}nge\\textbackslash{}nnc\\textbackslash{}ny\\textbackslash{}ndi\\textbackslash{}nsc\\textbackslash{}nha\\textbackslash{}nrg\\textbackslash{}ne\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nSp\\textbackslash{}nan\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}8\\textbackslash{}n2{]}\\textbackslash{}nSo\\textbackslash{}nys\\textbackslash{}nal\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nPa\\textbackslash{}nrt\\textbackslash{}ns:\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}nSo\\textbackslash{}nft\\textbackslash{}nw\\textbackslash{}nar\\textbackslash{}ne\\textbackslash{}nD\\textbackslash{}nis\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nge\\textbackslash{}nsu\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nar\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}n+\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nus\\textbackslash{}ned\\textbackslash{}nby\\textbackslash{}nva\\textbackslash{}nrio\\textbackslash{}nus\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}8\\textbackslash{}n3{]}\\textbackslash{}nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 7 of 21\\textbackslash{}nTa\\textbackslash{}nb\\textbackslash{}nle\\textbackslash{}n3\\textbackslash{}nIn\\textbackslash{}ncl\\textbackslash{}nud\\textbackslash{}ned\\textbackslash{}npu\\textbackslash{}nbl\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nth\\textbackslash{}nei\\textbackslash{}nr\\textbackslash{}nfir\\textbackslash{}nst\\textbackslash{}nau\\textbackslash{}nth\\textbackslash{}nor\\textbackslash{}n,y\\textbackslash{}nea\\textbackslash{}nr,\\textbackslash{}ntit\\textbackslash{}nle\\textbackslash{}n,a\\textbackslash{}nnd\\textbackslash{}nco\\textbackslash{}nun\\textbackslash{}ntr\\textbackslash{}ny\\textbackslash{}n(C\\textbackslash{}non\\textbackslash{}ntin\\textbackslash{}nue\\textbackslash{}nd)\\textbackslash{}nA\\textbackslash{}nut\\textbackslash{}nho\\textbackslash{}nr\\textbackslash{}nY\\textbackslash{}nea\\textbackslash{}nr\\textbackslash{}nC\\textbackslash{}nou\\textbackslash{}nnt\\textbackslash{}nry\\textbackslash{}nC\\textbackslash{}nha\\textbackslash{}nlle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nIn\\textbackslash{}nd\\textbackslash{}nuc\\textbackslash{}ned\\textbackslash{}nob\\textbackslash{}nje\\textbackslash{}nct\\textbackslash{}niv\\textbackslash{}ne\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nor\\textbackslash{}nig\\textbackslash{}nin\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nla\\textbackslash{}nng\\textbackslash{}nua\\textbackslash{}ng\\textbackslash{}ne\\textbackslash{}nU\\textbackslash{}nse\\textbackslash{}nd\\textbackslash{}nsy\\textbackslash{}nst\\textbackslash{}nem\\textbackslash{}nTe\\textbackslash{}nrm\\textbackslash{}n.S\\textbackslash{}nys\\textbackslash{}n.\\textbackslash{}nIn\\textbackslash{}nus\\textbackslash{}ne\\textbackslash{}nSo\\textbackslash{}nur\\textbackslash{}nce\\textbackslash{}nco\\textbackslash{}nd\\textbackslash{}ne\\textbackslash{}nRe\\textbackslash{}nf\\textbackslash{}n(2\\textbackslash{}n00\\textbackslash{}n9\\textbackslash{}n+\\textbackslash{}n20\\textbackslash{}n10\\textbackslash{}n),\\textbackslash{}nSh\\textbackslash{}nA\\textbackslash{}nRe\\textbackslash{}n/C\\textbackslash{}nLE\\textbackslash{}nF\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n3)\\textbackslash{}n,S\\textbackslash{}nem\\textbackslash{}n-E\\textbackslash{}nVA\\textbackslash{}nL\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n4)\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nev\\textbackslash{}nal\\textbackslash{}nua\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3 ,\\textbackslash{}nou\\textbackslash{}ntp\\textbackslash{}nat\\textbackslash{}nie\\textbackslash{}nnt\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nvi\\textbackslash{}nsi\\textbackslash{}nt\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns,\\textbackslash{}nm\\textbackslash{}noc\\textbackslash{}nk\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}ndo\\textbackslash{}ncu\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nts\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nin\\textbackslash{}nst\\textbackslash{}nitu\\textbackslash{}ntio\\textbackslash{}nns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nin\\textbackslash{}ndu\\textbackslash{}nst\\textbackslash{}nria\\textbackslash{}nl\\textbackslash{}nen\\textbackslash{}ntit\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nSp\\textbackslash{}nas\\textbackslash{}ni?\\textbackslash{}n20\\textbackslash{}n15\\textbackslash{}nU\\textbackslash{}nK\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nM\\textbackslash{}nRI\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nof\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nTR\\textbackslash{}nA\\textbackslash{}nK,\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,M\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nIN\\textbackslash{}n,\\textbackslash{}nRa\\textbackslash{}ndL\\textbackslash{}nex\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}8\\textbackslash{}n4{]}\\textbackslash{}nSt\\textbackslash{}nra\\textbackslash{}nus\\textbackslash{}ns\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nPa\\textbackslash{}nth\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nof\\textbackslash{}nbr\\textbackslash{}nea\\textbackslash{}nst\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}npr\\textbackslash{}nos\\textbackslash{}nta\\textbackslash{}nte\\textbackslash{}nca\\textbackslash{}nnc\\textbackslash{}ner\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}8\\textbackslash{}n5{]}\\textbackslash{}nSu\\textbackslash{}nng\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nTa\\textbackslash{}niw\\textbackslash{}nan\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nas\\textbackslash{}nes\\textbackslash{}nof\\textbackslash{}nad\\textbackslash{}nul\\textbackslash{}nt\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nA\\textbackslash{}nIS\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}8\\textbackslash{}n6{]}\\textbackslash{}nTc\\textbackslash{}nhe\\textbackslash{}nch\\textbackslash{}nm\\textbackslash{}ned\\textbackslash{}njie\\textbackslash{}nv\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nFr\\textbackslash{}nan\\textbackslash{}nce\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nQ\\textbackslash{}nua\\textbackslash{}ner\\textbackslash{}no\\textbackslash{}n(F\\textbackslash{}nre\\textbackslash{}nnc\\textbackslash{}nh\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nLI\\textbackslash{}nN\\textbackslash{}nE\\textbackslash{}nab\\textbackslash{}nst\\textbackslash{}nra\\textbackslash{}nct\\textbackslash{}ntit\\textbackslash{}nle\\textbackslash{}ns\\textbackslash{}n+\\textbackslash{}nEM\\textbackslash{}nEA\\textbackslash{}ndr\\textbackslash{}nug\\textbackslash{}nla\\textbackslash{}nbe\\textbackslash{}nls\\textbackslash{}n)+\\textbackslash{}nC\\textbackslash{}nép\\textbackslash{}niD\\textbackslash{}nC\\textbackslash{}n(IC\\textbackslash{}nD\\textbackslash{}n-1\\textbackslash{}n0\\textbackslash{}nco\\textbackslash{}ndi\\textbackslash{}nng\\textbackslash{}nof\\textbackslash{}nde\\textbackslash{}nat\\textbackslash{}nh\\textbackslash{}nce\\textbackslash{}nrt\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}nte\\textbackslash{}ns)\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nFr\\textbackslash{}nen\\textbackslash{}nch\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nte\\textbackslash{}nrm\\textbackslash{}nin\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}n(IC\\textbackslash{}nD\\textbackslash{}n-1\\textbackslash{}n0)\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nav\\textbackslash{}nai\\textbackslash{}nla\\textbackslash{}nbl\\textbackslash{}ne\\textbackslash{}nin\\textbackslash{}nSI\\textbackslash{}nFR\\textbackslash{}nBi\\textbackslash{}noP\\textbackslash{}nor\\textbackslash{}nta\\textbackslash{}nl\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}8\\textbackslash{}n7{]}\\textbackslash{}nTe\\textbackslash{}nrn\\textbackslash{}noi\\textbackslash{}ns\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nFr\\textbackslash{}nan\\textbackslash{}nce\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nla\\textbackslash{}nss\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ndo\\textbackslash{}nsc\\textbackslash{}nop\\textbackslash{}ny\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nrit\\textbackslash{}nte\\textbackslash{}nn\\textbackslash{}nbe\\textbackslash{}ntw\\textbackslash{}nee\\textbackslash{}nn\\textbackslash{}n20\\textbackslash{}n15\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}n20\\textbackslash{}n16\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nFr\\textbackslash{}nen\\textbackslash{}nch\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nC\\textbackslash{}nC\\textbackslash{}nA\\textbackslash{}nM\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}8\\textbackslash{}n8{]}\\textbackslash{}nTr\\textbackslash{}nav\\textbackslash{}ner\\textbackslash{}ns\\textbackslash{}n20\\textbackslash{}n04\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nhi\\textbackslash{}nef\\textbackslash{}nco\\textbackslash{}nm\\textbackslash{}npl\\textbackslash{}nai\\textbackslash{}nnt\\textbackslash{}nte\\textbackslash{}nxt\\textbackslash{}nen\\textbackslash{}ntr\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nfo\\textbackslash{}nr\\textbackslash{}nal\\textbackslash{}nl\\textbackslash{}nem\\textbackslash{}ner\\textbackslash{}nge\\textbackslash{}nnc\\textbackslash{}ny\\textbackslash{}nde\\textbackslash{}npa\\textbackslash{}nrt\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nvi\\textbackslash{}nsi\\textbackslash{}nts\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}8\\textbackslash{}n9{]}\\textbackslash{}nTu\\textbackslash{}nlk\\textbackslash{}nen\\textbackslash{}ns\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nBe\\textbackslash{}nlg\\textbackslash{}niu\\textbackslash{}nm\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nYe\\textbackslash{}ns\\textbackslash{}n{[}9\\textbackslash{}n0{]}\\textbackslash{}nU\\textbackslash{}nsu\\textbackslash{}ni\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nJa\\textbackslash{}npa\\textbackslash{}nn\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nPr\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nEl\\textbackslash{}nec\\textbackslash{}ntr\\textbackslash{}non\\textbackslash{}nic\\textbackslash{}nm\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nhi\\textbackslash{}nst\\textbackslash{}nor\\textbackslash{}ny\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nph\\textbackslash{}nar\\textbackslash{}nm\\textbackslash{}nac\\textbackslash{}ny\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nJa\\textbackslash{}npa\\textbackslash{}nne\\textbackslash{}nse\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-1\\textbackslash{}n0\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nye\\textbackslash{}nt,\\textbackslash{}nex\\textbackslash{}npe\\textbackslash{}nct\\textbackslash{}nto\\textbackslash{}nus\\textbackslash{}ne\\textbackslash{}nit\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}9\\textbackslash{}n1{]}\\textbackslash{}nVa\\textbackslash{}nltc\\textbackslash{}nhi\\textbackslash{}nno\\textbackslash{}nv\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nla\\textbackslash{}nss\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nRa\\textbackslash{}ndi\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns,\\textbackslash{}nem\\textbackslash{}ner\\textbackslash{}nge\\textbackslash{}nnc\\textbackslash{}ny\\textbackslash{}nde\\textbackslash{}npa\\textbackslash{}nrt\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}n+\\textbackslash{}not\\textbackslash{}nhe\\textbackslash{}nr\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nRa\\textbackslash{}ndL\\textbackslash{}nex\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}9\\textbackslash{}n2{]}\\textbackslash{}nW\\textbackslash{}nad\\textbackslash{}nia\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nla\\textbackslash{}nss\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nhe\\textbackslash{}nst\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}9\\textbackslash{}n3{]}\\textbackslash{}nW\\textbackslash{}nal\\textbackslash{}nke\\textbackslash{}nr\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nTr\\textbackslash{}nea\\textbackslash{}ntm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nsi\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nEM\\textbackslash{}nR\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}9\\textbackslash{}n4{]}\\textbackslash{}nXi\\textbackslash{}ne\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nC\\textbackslash{}nhi\\textbackslash{}nna\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nM\\textbackslash{}nIM\\textbackslash{}nIC\\textbackslash{}n-II\\textbackslash{}nId\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}n4\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nIC\\textbackslash{}nD\\textbackslash{}n-9\\textbackslash{}n-C\\textbackslash{}nM\\textbackslash{}n,I\\textbackslash{}nC\\textbackslash{}nD\\textbackslash{}n-1\\textbackslash{}n0\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}9\\textbackslash{}n5{]}\\textbackslash{}nXu\\textbackslash{}n20\\textbackslash{}n11\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nla\\textbackslash{}nss\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nC\\textbackslash{}nRC\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}nca\\textbackslash{}nse\\textbackslash{}ns\\textbackslash{}nfro\\textbackslash{}nm\\textbackslash{}nth\\textbackslash{}ne\\textbackslash{}nSy\\textbackslash{}nnt\\textbackslash{}nhe\\textbackslash{}ntic\\textbackslash{}nD\\textbackslash{}ner\\textbackslash{}niv\\textbackslash{}nat\\textbackslash{}niv\\textbackslash{}ne\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nba\\textbackslash{}nse\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}no,\\textbackslash{}nst\\textbackslash{}nill\\textbackslash{}nun\\textbackslash{}nde\\textbackslash{}nr\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}9\\textbackslash{}n6{]}\\textbackslash{}nYa\\textbackslash{}nda\\textbackslash{}nv\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nPr\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nEm\\textbackslash{}ner\\textbackslash{}nge\\textbackslash{}nnc\\textbackslash{}ny\\textbackslash{}nde\\textbackslash{}npa\\textbackslash{}nrt\\textbackslash{}nm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nC\\textbackslash{}nT\\textbackslash{}nim\\textbackslash{}nag\\textbackslash{}nin\\textbackslash{}ng\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nco\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nlin\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}n{[}9\\textbackslash{}n7{]}\\textbackslash{}nYa\\textbackslash{}no\\textbackslash{}n20\\textbackslash{}n19\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nPr\\textbackslash{}ned\\textbackslash{}nic\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n00\\textbackslash{}n8)\\textbackslash{}n10\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nPa\\textbackslash{}nrt\\textbackslash{}n(S\\textbackslash{}nor\\textbackslash{}nl)\\textbackslash{}n{[}9\\textbackslash{}n8{]}\\textbackslash{}nZe\\textbackslash{}nng\\textbackslash{}n20\\textbackslash{}n18\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nC\\textbackslash{}nla\\textbackslash{}nss\\textbackslash{}nifi\\textbackslash{}nca\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nPr\\textbackslash{}nog\\textbackslash{}nre\\textbackslash{}nss\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nbr\\textbackslash{}nea\\textbackslash{}nst\\textbackslash{}nca\\textbackslash{}nnc\\textbackslash{}ner\\textbackslash{}nsu\\textbackslash{}nrg\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}npa\\textbackslash{}nth\\textbackslash{}nol\\textbackslash{}nog\\textbackslash{}ny\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}n(+\\textbackslash{}nex\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng)\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}9\\textbackslash{}n9{]}\\textbackslash{}nZh\\textbackslash{}nan\\textbackslash{}ng\\textbackslash{}n20\\textbackslash{}n13\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n3\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nG\\textbackslash{}nEN\\textbackslash{}nIA\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(M\\textbackslash{}nED\\textbackslash{}nLI\\textbackslash{}nN\\textbackslash{}nE\\textbackslash{}nab\\textbackslash{}nst\\textbackslash{}nra\\textbackslash{}nct\\textbackslash{}ns)\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}1\\textbackslash{}n00\\textbackslash{}n{]}\\textbackslash{}nZh\\textbackslash{}nou\\textbackslash{}n20\\textbackslash{}n06\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nRe\\textbackslash{}nco\\textbackslash{}nrd\\textbackslash{}ns\\textbackslash{}nof\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nw\\textbackslash{}nith\\textbackslash{}nbr\\textbackslash{}nea\\textbackslash{}nst\\textbackslash{}nco\\textbackslash{}nm\\textbackslash{}npl\\textbackslash{}nai\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}nN\\textbackslash{}no,\\textbackslash{}nst\\textbackslash{}nill\\textbackslash{}nun\\textbackslash{}nde\\textbackslash{}nr\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}1\\textbackslash{}n01\\textbackslash{}n{]}\\textbackslash{}nZh\\textbackslash{}nou\\textbackslash{}n20\\textbackslash{}n11\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nSo\\textbackslash{}nft\\textbackslash{}nw\\textbackslash{}nar\\textbackslash{}ne\\textbackslash{}nC\\textbackslash{}nO\\textbackslash{}nPD\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nC\\textbackslash{}nA\\textbackslash{}nD\\textbackslash{}npa\\textbackslash{}ntie\\textbackslash{}nnt\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nN\\textbackslash{}new\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nRx\\textbackslash{}nN\\textbackslash{}nor\\textbackslash{}nm\\textbackslash{}n,\\textbackslash{}nYe\\textbackslash{}ns,\\textbackslash{}nde\\textbackslash{}nsc\\textbackslash{}nrib\\textbackslash{}ned\\textbackslash{}nin\\textbackslash{}not\\textbackslash{}nhe\\textbackslash{}nr\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}n{[}1\\textbackslash{}n02\\textbackslash{}n{]}\\textbackslash{}nKersloot et al. Journal of Biomedical Semantics           (2020) 11:14 Page 8 of 21\\textbackslash{}nTa\\textbackslash{}nb\\textbackslash{}nle\\textbackslash{}n3\\textbackslash{}nIn\\textbackslash{}ncl\\textbackslash{}nud\\textbackslash{}ned\\textbackslash{}npu\\textbackslash{}nbl\\textbackslash{}nic\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nth\\textbackslash{}nei\\textbackslash{}nr\\textbackslash{}nfir\\textbackslash{}nst\\textbackslash{}nau\\textbackslash{}nth\\textbackslash{}nor\\textbackslash{}n,y\\textbackslash{}nea\\textbackslash{}nr,\\textbackslash{}ntit\\textbackslash{}nle\\textbackslash{}n,a\\textbackslash{}nnd\\textbackslash{}nco\\textbackslash{}nun\\textbackslash{}ntr\\textbackslash{}ny\\textbackslash{}n(C\\textbackslash{}non\\textbackslash{}ntin\\textbackslash{}nue\\textbackslash{}nd)\\textbackslash{}nA\\textbackslash{}nut\\textbackslash{}nho\\textbackslash{}nr\\textbackslash{}nY\\textbackslash{}nea\\textbackslash{}nr\\textbackslash{}nC\\textbackslash{}nou\\textbackslash{}nnt\\textbackslash{}nry\\textbackslash{}nC\\textbackslash{}nha\\textbackslash{}nlle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nIn\\textbackslash{}nd\\textbackslash{}nuc\\textbackslash{}ned\\textbackslash{}nob\\textbackslash{}nje\\textbackslash{}nct\\textbackslash{}niv\\textbackslash{}ne\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nor\\textbackslash{}nig\\textbackslash{}nin\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}nas\\textbackslash{}net\\textbackslash{}nD\\textbackslash{}nat\\textbackslash{}na\\textbackslash{}nla\\textbackslash{}nng\\textbackslash{}nua\\textbackslash{}ng\\textbackslash{}ne\\textbackslash{}nU\\textbackslash{}nse\\textbackslash{}nd\\textbackslash{}nsy\\textbackslash{}nst\\textbackslash{}nem\\textbackslash{}nTe\\textbackslash{}nrm\\textbackslash{}n.S\\textbackslash{}nys\\textbackslash{}n.\\textbackslash{}nIn\\textbackslash{}nus\\textbackslash{}ne\\textbackslash{}nSo\\textbackslash{}nur\\textbackslash{}nce\\textbackslash{}nco\\textbackslash{}nd\\textbackslash{}ne\\textbackslash{}nRe\\textbackslash{}nf\\textbackslash{}nde\\textbackslash{}nve\\textbackslash{}nlo\\textbackslash{}npm\\textbackslash{}nen\\textbackslash{}nt\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nev\\textbackslash{}nal\\textbackslash{}nua\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nU\\textbackslash{}nM\\textbackslash{}nLS\\textbackslash{}n,P\\textbackslash{}nPL\\textbackslash{}n,M\\textbackslash{}nD\\textbackslash{}nD\\textbackslash{}n,H\\textbackslash{}nL7\\textbackslash{}nva\\textbackslash{}nlu\\textbackslash{}ne\\textbackslash{}nse\\textbackslash{}nts\\textbackslash{}npa\\textbackslash{}npe\\textbackslash{}nr\\textbackslash{}n(1\\textbackslash{}n03\\textbackslash{}n{]})\\textbackslash{}nZh\\textbackslash{}nou\\textbackslash{}n20\\textbackslash{}n14\\textbackslash{}nU\\textbackslash{}nSA\\textbackslash{}nN\\textbackslash{}no\\textbackslash{}nIn\\textbackslash{}nfo\\textbackslash{}nrm\\textbackslash{}nat\\textbackslash{}nio\\textbackslash{}nn\\textbackslash{}nex\\textbackslash{}ntr\\textbackslash{}nac\\textbackslash{}ntio\\textbackslash{}nn\\textbackslash{}nA\\textbackslash{}ndm\\textbackslash{}nis\\textbackslash{}nsi\\textbackslash{}non\\textbackslash{}nno\\textbackslash{}nte\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}ndi\\textbackslash{}nsc\\textbackslash{}nha\\textbackslash{}nrg\\textbackslash{}ne\\textbackslash{}nsu\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nar\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nO\\textbackslash{}nw\\textbackslash{}nn\\textbackslash{}nEn\\textbackslash{}ngl\\textbackslash{}nis\\textbackslash{}nh\\textbackslash{}nEx\\textbackslash{}nis\\textbackslash{}ntin\\textbackslash{}ng\\textbackslash{}nSN\\textbackslash{}nO\\textbackslash{}nM\\textbackslash{}nED\\textbackslash{}nC\\textbackslash{}nT,\\textbackslash{}nH\\textbackslash{}nL7\\textbackslash{}nRo\\textbackslash{}nle\\textbackslash{}nC\\textbackslash{}nod\\textbackslash{}nes\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nlis\\textbackslash{}nte\\textbackslash{}nd\\textbackslash{}nN\\textbackslash{}not\\textbackslash{}nap\\textbackslash{}npl\\textbackslash{}nic\\textbackslash{}nab\\textbackslash{}nle\\textbackslash{}n{[}1\\textbackslash{}n03\\textbackslash{}n{]}\\textbackslash{}n1.\\textbackslash{}nPh\\textbackslash{}nen\\textbackslash{}noC\\textbackslash{}nH\\textbackslash{}nF\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n:n\\textbackslash{}nar\\textbackslash{}nra\\textbackslash{}ntiv\\textbackslash{}ne\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}nfr\\textbackslash{}nom\\textbackslash{}nel\\textbackslash{}nec\\textbackslash{}ntr\\textbackslash{}non\\textbackslash{}nic\\textbackslash{}nhe\\textbackslash{}nal\\textbackslash{}nth\\textbackslash{}nre\\textbackslash{}nco\\textbackslash{}nrd\\textbackslash{}ns\\textbackslash{}n(E\\textbackslash{}nH\\textbackslash{}nRs\\textbackslash{}n)\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}nlit\\textbackslash{}ner\\textbackslash{}nat\\textbackslash{}nur\\textbackslash{}ne\\textbackslash{}nar\\textbackslash{}ntic\\textbackslash{}nle\\textbackslash{}ns\\textbackslash{}n2.\\textbackslash{}nSh\\textbackslash{}nA\\textbackslash{}nRe\\textbackslash{}n/C\\textbackslash{}nLE\\textbackslash{}nF\\textbackslash{}nco\\textbackslash{}nrp\\textbackslash{}nus\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n3)\\textbackslash{}n:n\\textbackslash{}nar\\textbackslash{}nra\\textbackslash{}ntiv\\textbackslash{}ne\\textbackslash{}ncl\\textbackslash{}nin\\textbackslash{}nic\\textbackslash{}nal\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}n3.\\textbackslash{}ni2\\textbackslash{}nb2\\textbackslash{}n/V\\textbackslash{}nA\\textbackslash{}nch\\textbackslash{}nal\\textbackslash{}nle\\textbackslash{}nng\\textbackslash{}ne\\textbackslash{}nda\\textbackslash{}nta\\textbackslash{}nse\\textbackslash{}nt\\textbackslash{}n(2\\textbackslash{}n01\\textbackslash{}n0)\\textbackslash{}n:d\\textbackslash{}nis\\textbackslash{}nch\\textbackslash{}nar\\textbackslash{}nge\\textbackslash{}nsu\\textbackslash{}nm\\textbackslash{}nm\\textbackslash{}nar\\textbackslash{}nie\\textbackslash{}ns\\textbackslash{}nan\\textbackslash{}nd\\textbackslash{}npr\\textbackslash{}nog\\textbackslash{}nre\\textbackslash{}nss\\textbackslash{}nre\\textbackslash{}npo\\textbackslash{}nrt\\textbackslash{}ns\\textbackslash{}n4.\\textbackslash{}nM\\textbackslash{}nIM\\textbackslash{}nIC\\textbackslash{}n-II'\n\\item[text15] 'RESEARCH Open Access\\textbackslash{}nOntological representation, classification\\textbackslash{}nand data-driven computing of phenotypes\\textbackslash{}nAlexandr Uciteli1,2* , Christoph Beger1,3, Toralf Kirsten2,4,5, Frank A. Meineke1,2 and Heinrich Herre1,2*\\textbackslash{}nAbstract\\textbackslash{}nBackground: The successful determination and analysis of phenotypes plays a key role in the diagnostic process,\\textbackslash{}nthe evaluation of risk factors and the recruitment of participants for clinical and epidemiological studies. The\\textbackslash{}ndevelopment of computable phenotype algorithms to solve these tasks is a challenging problem, caused by various\\textbackslash{}nreasons. Firstly, the term \\textbackslash{}u0091phenotype\\textbackslash{}u0092 has no generally agreed definition and its meaning depends on context.\\textbackslash{}nSecondly, the phenotypes are most commonly specified as non-computable descriptive documents. Recent\\textbackslash{}nattempts have shown that ontologies are a suitable way to handle phenotypes and that they can support clinical\\textbackslash{}nresearch and decision making.\\textbackslash{}nThe SMITH Consortium is dedicated to rapidly establish an integrative medical informatics framework to provide\\textbackslash{}nphysicians with the best available data and knowledge and enable innovative use of healthcare data for research\\textbackslash{}nand treatment optimisation. In the context of a methodological use case \\textbackslash{}u0091phenotype pipeline\\textbackslash{}u0092 (PheP), a technology\\textbackslash{}nto automatically generate phenotype classifications and annotations based on electronic health records (EHR) is\\textbackslash{}ndeveloped. A large series of phenotype algorithms will be implemented. This implies that for each algorithm a\\textbackslash{}nclassification scheme and its input variables have to be defined. Furthermore, a phenotype engine is required to\\textbackslash{}nevaluate and execute developed algorithms.\\textbackslash{}nResults: In this article, we present a Core Ontology of Phenotypes (COP) and the software Phenotype Manager\\textbackslash{}n(PhenoMan), which implements a novel ontology-based method to model, classify and compute phenotypes from\\textbackslash{}nalready available data. Our solution includes an enhanced iterative reasoning process combining classification tasks\\textbackslash{}nwith mathematical calculations at runtime. The ontology as well as the reasoning method were successfully\\textbackslash{}nevaluated with selected phenotypes including SOFA score, socio-economic status, body surface area and WHO BMI\\textbackslash{}nclassification based on available medical data.\\textbackslash{}nConclusions: We developed a novel ontology-based method to model phenotypes of living beings with the aim\\textbackslash{}nof automated phenotype reasoning based on available data. This new approach can be used in clinical context,\\textbackslash{}ne.g., for supporting the diagnostic process, evaluating risk factors, and recruiting appropriate participants for clinical\\textbackslash{}nand epidemiological studies.\\textbackslash{}nKeywords: Phenotype definition, Phenotype classification, Phenotype calculation, Phenotype ontology, Phenotype\\textbackslash{}nreasoning\\textbackslash{}n© The Author(s). 2020, corrected publication 2020. Open Access This article is licensed under a Creative Commons Attribution\\textbackslash{}n4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as\\textbackslash{}nlong as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\\textbackslash{}nand indicate if changes were made. The images or other third party material in this article are included in the article\\textbackslash{}'s Creative\\textbackslash{}nCommons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\\textbackslash{}'s Creative\\textbackslash{}nCommons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need\\textbackslash{}nto obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\\textbackslash{}nlicenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.\\textbackslash{}n0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\\textbackslash{}n* Correspondence: auciteli@imise.uni-leipzig.de; heinrich.herre@imise.uni-\\textbackslash{}nleipzig.de\\textbackslash{}n1Institute for Medical Informatics, Statistics and Epidemiology (IMISE),\\textbackslash{}nUniversity of Leipzig, Leipzig, Germany\\textbackslash{}nFull list of author information is available at the end of the article\\textbackslash{}nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 \\textbackslash{}nhttps://doi.org/10.1186/s13326-020-00230-0\\textbackslash{}nBackground\\textbackslash{}nDespite its long ago introduction in 1909 by Wilhelm\\textbackslash{}nJohannsen, the term \\textbackslash{}u0091phenotype\\textbackslash{}u0092 still has no generally\\textbackslash{}nagreed definition {[}1{]}. Usually, a phenotype is consid-\\textbackslash{}nered as an observable characteristic or trait of an or-\\textbackslash{}nganism, such as its morphology, function, behaviour\\textbackslash{}nor its biochemical and physiological properties {[}1\\textbackslash{}u00963{]}.\\textbackslash{}nScheuermann et al. define a phenotype as a (combin-\\textbackslash{}nation of) bodily feature(s) (physical components, bod-\\textbackslash{}nily qualities or bodily processes) of an organism\\textbackslash{}ndetermined by the interaction of its genetic make-up\\textbackslash{}nand environment {[}4{]}. From the medical perspective,\\textbackslash{}nclinical (clinically abnormal) and disease phenotypes\\textbackslash{}n(clinical phenotype characterising a single disease) are\\textbackslash{}nconsidered. According to Scheuermann et al., a dis-\\textbackslash{}nease phenotype can exist without being observed. Ob-\\textbackslash{}nserved bodily features that could be of clinical\\textbackslash{}nrelevance are called \\textbackslash{}u0091Sign\\textbackslash{}u0092( \\textbackslash{}u0093\\textbackslash{}u0085 observed in a physical\\textbackslash{}nexamination and is deemed by the clinician to be of\\textbackslash{}nclinical significance\\textbackslash{}u0094) or \\textbackslash{}u0091Symptom\\textbackslash{}u0092 ( \\textbackslash{}u0093\\textbackslash{}u0085 observed by\\textbackslash{}nthe patient and is hypothesized by the patient to be a\\textbackslash{}nrealization of a disease\\textbackslash{}u0094) {[}4{]}.\\textbackslash{}nCorrect determination of phenotypes plays a key\\textbackslash{}nrole for diagnosis of diseases, evaluation of risk fac-\\textbackslash{}ntors and recruitment of patients for clinical and epi-\\textbackslash{}ndemiological studies {[}5, 6{]}. One challenge is to\\textbackslash{}ntranslate phenotype algorithms, which \\textbackslash{}u0093are most com-\\textbackslash{}nmonly represented as non-computable descriptive\\textbackslash{}ndocuments and knowledge artifacts\\textbackslash{}u0094 {[}7{]}, into\\textbackslash{}nmachine-readable form. This paper focuses on devel-\\textbackslash{}noping a general phenotype representation model that\\textbackslash{}ncan be used for data-driven phenotype computing,\\textbackslash{}ni.e., software-supported determination of phenotypes\\textbackslash{}nbased on the data of an organism. The model to be\\textbackslash{}ndeveloped must support both the biological and the\\textbackslash{}nmedical views of the phenotype notion. Recent at-\\textbackslash{}ntempts have shown that ontologies are suitable to\\textbackslash{}nhandle phenotypes and that they can support clinical\\textbackslash{}nresearch and decision making {[}8\\textbackslash{}u009610{]}.\\textbackslash{}nThere is a large ongoing initiative in Germany, the so\\textbackslash{}ncalled German Medical Informatics Initiative (MII) {[}11,\\textbackslash{}n12{]} that aims at making clinical data available for re-\\textbackslash{}nsearch. Most German university hospitals participate in\\textbackslash{}none of four funded consortia. Smart Medical Informa-\\textbackslash{}ntion Technology for Healthcare (SMITH) is one of these\\textbackslash{}nconsortia {[}13{]}. Within the ongoing SMITH project, a\\textbackslash{}nphenotyping pipeline (PheP) will be established to sys-\\textbackslash{}ntematically develop, evaluate and execute validated algo-\\textbackslash{}nrithms and models for classifying and annotating patient\\textbackslash{}ncare data. These annotations and derivatives will be pro-\\textbackslash{}nvided for triggering alerts and actions, data sharing and\\textbackslash{}ndeep analyses of patient care and outcomes. The general\\textbackslash{}ndesign and concept of the SMITH phenotyping pipeline\\textbackslash{}nis presented in {[}14{]}.\\textbackslash{}nIn this article, we propose a novel ontology-based\\textbackslash{}nmethod to model and compute phenotypes. Our ap-\\textbackslash{}nproach provides an extended reasoning combining\\textbackslash{}nphenotypic data to derive complex phenotypes based on\\textbackslash{}ncalculations and classifications.\\textbackslash{}nMethods\\textbackslash{}nPhenotypes can be derived from available data that may\\textbackslash{}nhave been measured (quantitative data) or observed and\\textbackslash{}nqualitatively described (categorical data). The data can,\\textbackslash{}nfor example, come from Electronic Health Records\\textbackslash{}n(EHR) (clinical data) or from a research database of a\\textbackslash{}nclinical/epidemiological study (research data). In SMIT\\textbackslash{}nH, the required EHR data will be integrated into a cen-\\textbackslash{}ntral Health Data Storage (HDS) at each site. The inte-\\textbackslash{}ngrated data is homogeneously represented in each HDS\\textbackslash{}nusing HL7 FHIR {[}15{]} and can be queried utilising FHIR\\textbackslash{}nSearch {[}16{]} (Fig. 1). Structured data from different\\textbackslash{}nsource systems in hospitals as well as unstructured doc-\\textbackslash{}numents will be extracted, transformed and loaded into\\textbackslash{}nthe HDS. Natural Language Processing (NLP) techniques\\textbackslash{}nare used to extract and transform relevant data from un-\\textbackslash{}nstructured EHR documents into structured form. In\\textbackslash{}nSMITH and the German Medical Informatics Initiative,\\textbackslash{}nthe software tool ART-DECOR {[}17{]} is used to specify an\\textbackslash{}noverarching global schema, the so-called core data set\\textbackslash{}n{[}18{]}. The core data subsumes the minimal set of data el-\\textbackslash{}nements that each site (i.e., University Hospital) needs to\\textbackslash{}nprovide in a harmonised manner. In this way, data ele-\\textbackslash{}nments are specified based on HL7 templates, their re-\\textbackslash{}nspective value sets, referenced terminologies, exemplified\\textbackslash{}nuse scenarios and data. These specifications are the basis\\textbackslash{}nfor the ontology-based phenotype representation in our\\textbackslash{}napproach.\\textbackslash{}nFig. 1 Integration of the PhenoMan. The Metadata Manager models\\textbackslash{}nbasic data elements using ART-DECOR. The Phenotype Designer\\textbackslash{}nimports the ART-DECOR specification and develops phenotype\\textbackslash{}nmodels (PheSO) utilising the PhenoMan Editor. The PhenoMan\\textbackslash{}nrequests required input data from the FHIR Server, computes\\textbackslash{}nphenotypes and writes the results back to the FHIR Server\\textbackslash{}nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 2 of 17\\textbackslash{}nHL7 FHIR and FHIR Search\\textbackslash{}nHealthcare records are increasingly digitised. The\\textbackslash{}nEHR must be discoverable and understandable. The\\textbackslash{}npatient data must be structured and standardised to\\textbackslash{}nsupport machine-based processing and automated\\textbackslash{}nclinical decision making. The FHIR (Fast Healthcare\\textbackslash{}nInteroperability Resources) specification is a HL7\\textbackslash{}nstandard for modelling and exchanging healthcare in-\\textbackslash{}nformation {[}15{]}. FHIR provides a base set of resource\\textbackslash{}ntypes representing relevant clinical concepts that can\\textbackslash{}nbe used to store and exchange data in order to solve\\textbackslash{}na wide range of healthcare related problems. For\\textbackslash{}neach resource type, the corresponding information\\textbackslash{}ncontents and structure are specified. The resources\\textbackslash{}ncan be used either by themselves or combined to\\textbackslash{}ncomplex documents representing a coherent set of\\textbackslash{}nhealthcare information {[}15{]}. The FHIR resource types\\textbackslash{}ninclude, inter alia, \\textbackslash{}u0091Patient\\textbackslash{}u0092 (\\textbackslash{}u0093Demographics and other\\textbackslash{}nadministrative information about an individual or\\textbackslash{}nanimal receiving care or other health-related ser-\\textbackslash{}nvices.\\textbackslash{}u0094), \\textbackslash{}u0091Observation\\textbackslash{}u0092 (\\textbackslash{}u0093Measurements and simple as-\\textbackslash{}nsertions made about a patient, device or other\\textbackslash{}nsubject.\\textbackslash{}u0094) and \\textbackslash{}u0091Condition\\textbackslash{}u0092 (\\textbackslash{}u0093A clinical condition, prob-\\textbackslash{}nlem, diagnosis, or other event, situation, issue, or clin-\\textbackslash{}nical concept that has risen to a level of concern.\\textbackslash{}u0094)\\textbackslash{}n{[}15{]}.\\textbackslash{}nThe FHIR Search Framework {[}16{]} is part of the\\textbackslash{}nHL7 FHIR standard and provides a range of opera-\\textbackslash{}ntions and parameters (series of name = value pairs) to\\textbackslash{}nsearch for existing FHIR resources in the underlying\\textbackslash{}nrepository. In the simplest case, a search is executed\\textbackslash{}nby performing a GET operation in the RESTful\\textbackslash{}nframework:\\textbackslash{}nGET {[}base-url{]}/{[}resource-type{]}?name = value\\&...\\{\\&\\_\\textbackslash{}nformat = {[}mime-type{]}\\}\\}.\\textbackslash{}ne.g., GET {[}base-url{]}/Patient?gender =male.\\textbackslash{}nFor numeric parameter types (number, date or quan-\\textbackslash{}ntity), a value range can be defined using a prefix to the\\textbackslash{}nparameter value (e.g., gt = greater than, le = less or\\textbackslash{}nequal).\\textbackslash{}nThe \\textbackslash{}u0091\\&\\textbackslash{}u0092 (AND) operator between single search criteria is\\textbackslash{}nused to search for the intersection of resources that match\\textbackslash{}nall criteria specified by each individual search parameter\\textbackslash{}n(e.g., Patient?gender =male\\&birthdate = gt1970). To search\\textbackslash{}nfor resources with one of the specified parameter values\\textbackslash{}n(OR), the values must be separated by a comma (e.g., Obser-\\textbackslash{}nvation?code= http://loinc.org?3141-9, http://snomed.info/\\textbackslash{}nsct?27113001, i.e., weight code from LOINC or SNOMED).\\textbackslash{}nThe following query contains AND combinations of single\\textbackslash{}ncriteria (code AND value-quantity) as well as OR linking of\\textbackslash{}ncode values and can be used to search for weight observa-\\textbackslash{}ntions where the weight is greater than 75 kg:\\textbackslash{}nObservation?code= http://loinc.org?3141-9, http://snome-\\textbackslash{}nd.info/sct?27113001\\&value-quantity=gt75??kg.\\textbackslash{}nART-DECOR\\textbackslash{}nART-DECOR is an open-source tool suite that supports\\textbackslash{}nthe creation and maintenance of HL7 templates, value\\textbackslash{}nsets, scenarios and datasets {[}17{]}. To specify and hier-\\textbackslash{}narchically structure required data elements (items, con-\\textbackslash{}ncepts, variables) we use the Dataset Editor of ART-\\textbackslash{}nDECOR. Data elements can possess several attributes,\\textbackslash{}nsuch as name, description (in different languages) and\\textbackslash{}nvalue domain (including data type, unit and possible\\textbackslash{}nvalue set) (Fig. 2a).\\textbackslash{}nOne of the most important components of a data\\textbackslash{}nelement is its terminology associations. A termin-\\textbackslash{}nology association defines the binding of dataset con-\\textbackslash{}ncepts to relevant terminology {[}17{]}. To associate a\\textbackslash{}ndata element with a terminology concept, the corre-\\textbackslash{}nsponding code (including the URI or ID of the ter-\\textbackslash{}nminology) must be specified. For instance, the\\textbackslash{}nconcept \\textbackslash{}u0091Fasting glucose {[}Mass/volume{]} in Serum or\\textbackslash{}nPlasma\\textbackslash{}u0092 from LOINC (URI: \\textbackslash{}u0091http://loinc.org\\textbackslash{}u0092) has the\\textbackslash{}ncode \\textbackslash{}u00911558\\textbackslash{}u00966\\textbackslash{}u0092 (Fig. 2a).\\textbackslash{}nFurthermore, additional properties of data elements\\textbackslash{}ncan be defined as key-value pairs. We use this function-\\textbackslash{}nality to specify the mapping between the data element\\textbackslash{}nand the corresponding FHIR resource type (e.g., for fast-\\textbackslash{}ning glucose, key: \\textbackslash{}u0091FHIR\\textbackslash{}u0092, value: \\textbackslash{}u0091Observation\\textbackslash{}u0092) required\\textbackslash{}nfor phenotype computing. Depending on the resource\\textbackslash{}ntype, different FHIR Search parameters must be used to\\textbackslash{}nquery the relevant FHIR resources. Moreover, the differ-\\textbackslash{}nent structure of the resulting resources must be consid-\\textbackslash{}nered to extract required data.\\textbackslash{}nThe resulting dataset specification is available in XML\\textbackslash{}nor JSON and can be parsed by our software.\\textbackslash{}nOntological architecture\\textbackslash{}nOur objective was to design the PhenoMan software\\textbackslash{}naccording to the three-ontology method {[}19{]}. This\\textbackslash{}nmethod is based on interactions of three different\\textbackslash{}nkinds of ontologies: a task ontology (TO), a domain\\textbackslash{}nontology (DO) and a top-level ontology (TLO). The\\textbackslash{}nTO serves as the conceptual model for the software,\\textbackslash{}nthe DO provides the domain-specific knowledge,\\textbackslash{}nwhereas the TLO integrates the TO and the DO and\\textbackslash{}nis used as foundation of them.\\textbackslash{}nIn our case, the Core Ontology of Phenotypes (COP,\\textbackslash{}nsee section \\textbackslash{}'Core Ontology of Phenotypes (COP)\\textbackslash{}') func-\\textbackslash{}ntions as a TO. It describes the general structure of valid\\textbackslash{}nphenotype specifications and thus enables the Pheno-\\textbackslash{}nMan to create such specifications and to use them for\\textbackslash{}nphenotype computing. Concrete phenotype specifica-\\textbackslash{}ntions (domain-specific knowledge) are represented in\\textbackslash{}nPhenotype Specification Ontologies (PheSO, see section\\textbackslash{}n\\textbackslash{}'Phenotype Specification Ontologies (PheSO)\\textbackslash{}') playing\\textbackslash{}nthe role of domain ontologies (DO) in our architecture.\\textbackslash{}nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 3 of 17\\textbackslash{}nFor the foundation of the TO we used the General\\textbackslash{}nFormal Ontology (GFO) {[}20{]} as TLO. GFO has already\\textbackslash{}nbeen successfully applied for a foundation of phenotype-\\textbackslash{}nrelated notions. For instance, a novel approach to repre-\\textbackslash{}nsent complex phenotypes in OWL was proposed im-\\textbackslash{}nproving the consistency and expressiveness of formal\\textbackslash{}nphenotype descriptions {[}10{]}. Another pillar of GFO for a\\textbackslash{}ngrounding of phenotypes is the foundational ontology of\\textbackslash{}nproperties, attributives and data (GFO-Data {[}21{]})\\textbackslash{}nproviding an extensive classification of properties (and\\textbackslash{}nattributives). In the current paper, we especially refer-\\textbackslash{}nence the property notion of GFO (including distinction\\textbackslash{}nbetween single and composite properties {[}22{]}) in our\\textbackslash{}nphenotype representation model supporting data-driven\\textbackslash{}nphenotype computing.\\textbackslash{}nOne of the advantages of the three-ontology method is\\textbackslash{}nthat the software only needs to implement the access to\\textbackslash{}nentities (classes, properties) of the TO (COP), whereas\\textbackslash{}nFig. 2 Mapping between ART-DECOR, PheSO, FHIR Subscription and FHIR Observation entities. a Specification of the data element \\textbackslash{}u0091fasting\\textbackslash{}nglucose\\textbackslash{}u0092 in ART-DECOR. b Annotations of the corresponding class Fasting\\_Glucose after importing the ART-DECOR specification into the PheSO. c\\textbackslash{}nSubscription generated for the class Fasting\\_Glucose. The criteria (FHIR Search query) is encoded. The original URL part is Observation?code=\\textbackslash{}nhttp://loinc.org\\textbar{}1558-6. d Observation of fasting glucose provided by FHIR Server. The observation code, value, date and the referenced patient\\textbackslash{}nare specified. (The same colour of the border indicates the mapping between the entities)\\textbackslash{}nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 4 of 17\\textbackslash{}nthe entities of the corresponding DO (PheSO) are proc-\\textbackslash{}nessed dynamically. The PhenoMan uses the COP as an\\textbackslash{}ninterface to access the PheSO entities.\\textbackslash{}nAdditional requirements for the ontological modelling\\textbackslash{}nwere:\\textbackslash{}n\\textbackslash{}u0096 Developing in OWL (using OWL API {[}23{]}, HermiT\\textbackslash{}n{[}24{]} and Openllet {[}25{]})\\textbackslash{}n\\textbackslash{}u0096 Modelling all attributes and relations that are\\textbackslash{}nrelevant for reasoning as object or data properties\\textbackslash{}n\\textbackslash{}u0096 Modelling all attributes and relations that are not\\textbackslash{}nrelevant for reasoning as annotations\\textbackslash{}n\\textbackslash{}u0096 Usage of general class axioms (based on \\textbackslash{}u0091subclass\\textbackslash{}nof\\textbackslash{}u0092) instead of equivalence classes if only one\\textbackslash{}ndirection (\\textbackslash{}u0091is-a\\textbackslash{}u0092 relation) is relevant for reasoning.\\textbackslash{}nSoftware design\\textbackslash{}nWe defined the following main requirements for the\\textbackslash{}noverall system (Fig. 1):\\textbackslash{}n\\textbackslash{}u0096 The system must support a phenotype specification\\textbackslash{}nproviding a GUI tool (see section \\textbackslash{}'Specification of\\textbackslash{}nphenotypes\\textbackslash{}').\\textbackslash{}n\\textbackslash{}u0096 The phenotype specifications must be saved in a\\textbackslash{}nstandardised ontology (see sections \\textbackslash{}'Core Ontology\\textbackslash{}nof Phenotypes (COP)\\textbackslash{}' and \\textbackslash{}'Phenotype Specification\\textbackslash{}nOntologies (PheSO)\\textbackslash{}').\\textbackslash{}n\\textbackslash{}u0096 The system must be able to correctly compute\\textbackslash{}nphenotypes based on a phenotype specification\\textbackslash{}n(ontology) and input data (see section \\textbackslash{}'Classification\\textbackslash{}nand calculation of phenotypes\\textbackslash{}').\\textbackslash{}n\\textbackslash{}u0096 The system must support an additional\\textbackslash{}nimplementation of mapping components for\\textbackslash{}naccessing required data and metadata repositories.\\textbackslash{}nExample components for metadata import from\\textbackslash{}nART-DECOR as well as for interaction with FHIR\\textbackslash{}nservers (e.g., SMITH HDS) must be implemented\\textbackslash{}n(see sections \\textbackslash{}'Data procurement\\textbackslash{}' and \\textbackslash{}'Transmission\\textbackslash{}nof inferred phenotype classes to the FHIR Server\\textbackslash{}').\\textbackslash{}nThe PhenoMan accesses the FHIR Server, extracts\\textbackslash{}nphenotype-specific data, computes the specified pheno-\\textbackslash{}ntypes and writes the results back to the FHIR Server. For\\textbackslash{}nthis purpose, the PhenoMan provides an API and acts as\\textbackslash{}na web service (using Dropwizard {[}26{]}) (Fig. 1). The Phe-\\textbackslash{}nnoMan is implemented in Java using OWL API {[}23{]} and\\textbackslash{}ntwo reasoners, HermiT {[}24{]} and Openllet {[}25{]}. For cal-\\textbackslash{}nculations we utilize the Java Expression Evaluator (Eva-\\textbackslash{}nlEx) {[}27{]}, but the integration of other libraries (e.g., for\\textbackslash{}nexecuting R scripts) or rule systems (e.g., SWIRL or\\textbackslash{}nDrools) is also possible. The EvalEx enables evaluating\\textbackslash{}nmathematical and Boolean (inter alia, Boolean operators\\textbackslash{}nand IF-THEN-ELSE structures) expressions and sup-\\textbackslash{}nports defining custom functions and operators.\\textbackslash{}nThe PhenoMan Editor1 is a desktop app, which is also\\textbackslash{}ndeveloped with Java and bundled with the PhenoMan\\textbackslash{}nAPI. It offers a graphical user interface based on Java\\textbackslash{}nSwing to specify attributes of phenotype classes and cat-\\textbackslash{}negories using appropriate form fields. On saving, form\\textbackslash{}ncontent is transmitted to the PhenoMan API and written\\textbackslash{}ninto the ontology. The editor can be executed on a local\\textbackslash{}nmachine with a Java runtime environment 8 or higher\\textbackslash{}nand was developed with the aim of rapidly defining\\textbackslash{}nphenotype models.\\textbackslash{}nEvaluation\\textbackslash{}nAn evaluation of our approach was designed and con-\\textbackslash{}nducted. The main objectives of the evaluation were to\\textbackslash{}nprove:\\textbackslash{}n1. Correct functioning of all software components\\textbackslash{}n2. Faultless communication of the software with the\\textbackslash{}nFHIR Server\\textbackslash{}n3. Correctness of all provided phenotype specifications\\textbackslash{}n4. Correct functioning of the overall system by\\textbackslash{}ncomparison with a corresponding SPSS\\textbackslash{}nimplementation of selected phenotypes.\\textbackslash{}nWe evaluated the PhenoMan at different levels. Firstly,\\textbackslash{}nwe tested all functionalities of the PhenoMan API (espe-\\textbackslash{}ncially read/write in the ontology and computing pheno-\\textbackslash{}ntypes) and the communication of the PhenoMan Service\\textbackslash{}nwith the FHIR Server by a set of static JUnit tests using\\textbackslash{}nfixtures (i.e., example PheSOs and patient data).\\textbackslash{}nSecondly, each phenotype specification is shipped with\\textbackslash{}na structured representation (spreadsheet) of test data (in-\\textbackslash{}nput and output), such that the respective phenotype al-\\textbackslash{}ngorithm can be automatically tested. The criterion for a\\textbackslash{}nsuccessful execution of the JUnit tests was a match be-\\textbackslash{}ntween the results calculated by PhenoMan based on pro-\\textbackslash{}nvided input data and the corresponding output data.\\textbackslash{}nFinally, we selected some test case algorithms/deriva-\\textbackslash{}ntives (such as socio-economic status {[}28{]}, body mass\\textbackslash{}nindex {[}29{]}, waist circumference and waist-hip ratio {[}30{]})\\textbackslash{}nfrom the LIFE Adult study {[}31{]} running at the LIFE Re-\\textbackslash{}nsearch Centre for Civilization Diseases, University of\\textbackslash{}nLeipzig. There, derivatives are usually implemented by\\textbackslash{}nepidemiologists, statisticians and other researchers using\\textbackslash{}nthe statistics software SPSS {[}32{]} and R or are database\\textbackslash{}n(SQL) queries and functions, which are automatically ex-\\textbackslash{}necuted at night based on daily captured data. The result-\\textbackslash{}ning data are directly stored within the LIFE research\\textbackslash{}ndatabase in tabular form. More details about partici-\\textbackslash{}npants, their invitation and consenting as well as\\textbackslash{}n1Source code and releases of the PhenoMan Editor are available on\\textbackslash{}nGitHub under the GPL-3.0 license: https://github.com/Onto-Med/Phe-\\textbackslash{}nnoMan-Editor\\textbackslash{}nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 5 of 17\\textbackslash{}nexaminations, interviews, questionnaires and taken spe-\\textbackslash{}ncimen can be found in {[}31{]}. We reproduced selected\\textbackslash{}nSPSS derivatives using the PhenoMan and developed\\textbackslash{}nparameterised JUnit tests to comparatively evaluate the\\textbackslash{}naccuracy of the PhenoMan against the corresponding\\textbackslash{}nSPSS implementation at the LIFE Research Centre. The\\textbackslash{}ncriterion for a successful comparison was a match be-\\textbackslash{}ntween the results calculated by PhenoMan and SPSS\\textbackslash{}nsoftware for each dataset. The performance of our ap-\\textbackslash{}nproach is not a critical issue in our use case (the pheno-\\textbackslash{}ntype computing could run overnight).\\textbackslash{}nThis evaluation included data of thousands of LIFE\\textbackslash{}nparticipants.\\textbackslash{}nResults\\textbackslash{}nCore Ontology of Phenotypes (COP)\\textbackslash{}nWe developed the Core Ontology of Phenotypes\\textbackslash{}n(COP, Fig. 3) to model, classify and calculate pheno-\\textbackslash{}ntypes based on instance data sets (e.g., of a patient).\\textbackslash{}nIn this article, we consider a phenotype as a\\textbackslash{}ndependent individual (in the sense of General Formal\\textbackslash{}nOntology, GFO {[}20{]}), for example, the weight of a\\textbackslash{}nspecific person. Hereinafter, abstract instantiable en-\\textbackslash{}ntities that are instantiated by phenotypes are called\\textbackslash{}nphenotype classes. For instance, the abstract property\\textbackslash{}n\\textbackslash{}u0091weight\\textbackslash{}u0092 possesses individual weights as instances. We\\textbackslash{}ndistinguish between single and composite properties,\\textbackslash{}nand correspondingly, between single and composite\\textbackslash{}nphenotypes. A composite property is defined as a\\textbackslash{}nproperty that has single properties as parts {[}22{]}.\\textbackslash{}nBased on the definitions of single and composite\\textbackslash{}nproperties {[}22{]}, we define single phenotypes as single\\textbackslash{}nproperties (e.g., age, weight, height) and composite\\textbackslash{}nphenotypes as composite properties (e.g., height and\\textbackslash{}nweight, BMI, SOFA score {[}33{]}) of an organism or of\\textbackslash{}none of its subsystems. Properties of an organism are\\textbackslash{}nconsidered as all documentable information about it,\\textbackslash{}nwhereby the modeller is left to decide what is rele-\\textbackslash{}nvant to the current situation. These can be, for ex-\\textbackslash{}nample, observable characteristics or traits of an\\textbackslash{}norganism {[}1\\textbackslash{}u00963{]} or possible manifestations of clinical\\textbackslash{}nphenotypes, such as signs, symptoms or dispositions\\textbackslash{}n{[}4{]}. The corresponding data can be modelled using\\textbackslash{}nthe FHIR Observation or Condition resources.\\textbackslash{}nComposite phenotypes are divided into combined\\textbackslash{}nand derived phenotypes. A combined phenotype is\\textbackslash{}nonly a combination of corresponding phenotypes (e.g.,\\textbackslash{}na combination of height and weight), whereas a de-\\textbackslash{}nrived phenotype is an additional property (e.g., BMI)\\textbackslash{}nderived from the corresponding phenotypes (height\\textbackslash{}nand weight). In the framework of GFO we modelled\\textbackslash{}nproperties using the class gfo: Property. In the present\\textbackslash{}narticle, composite phenotype classes are modelled\\textbackslash{}nusing a Boolean expression based on has\\_part relation\\textbackslash{}n(e.g., weight and height: has\\_part some height and\\textbackslash{}nhas\\_part some weight). Derived phenotype classes\\textbackslash{}nadditionally define a calculation rule/mathematical\\textbackslash{}nformula (e.g., BMI = weight {[}kg{]} / height {[}m{]}2). Fur-\\textbackslash{}nthermore, combined phenotype classes can associate\\textbackslash{}ncertain conditions with specific predefined values\\textbackslash{}n(scores), which can be used, e.g., in further formulas.\\textbackslash{}nFor example, if bilirubin value is greater than 12 mg/\\textbackslash{}ndL, then the value 4 is used for the calculation of the\\textbackslash{}nSOFA score {[}33{]}.\\textbackslash{}nAdditionally, we distinguish between restricted and\\textbackslash{}nnon-restricted phenotype classes, depending on whether\\textbackslash{}ntheir extensions (set of instances) are restricted to a cer-\\textbackslash{}ntain range of individual phenotypes by defined condi-\\textbackslash{}ntions or all instances are allowed. For example, the\\textbackslash{}nphenotype class \\textbackslash{}u0091age\\textbackslash{}u0092 is instantiated by the ages of all liv-\\textbackslash{}ning beings (non-restricted), whereas the phenotype class\\textbackslash{}n\\textbackslash{}u0091young age\\textbackslash{}u0092 is instantiated by the ages of the young ones,\\textbackslash{}ne.g., if the age is below 30 years (restricted).\\textbackslash{}nPhenotype Specification Ontologies (PheSO)\\textbackslash{}nWe consider a phenotype algorithm as a sequence of in-\\textbackslash{}nstructions (1) to classify phenotypes (single or compos-\\textbackslash{}nite) in phenotype classes or (2) to derive additional\\textbackslash{}nproperties (derived phenotypes) from the phenotypes of\\textbackslash{}nan organism. Phenotype algorithms can be implemented,\\textbackslash{}nfor example, using a programming language or a statis-\\textbackslash{}ntics software (e.g., SPSS or R). Our approach is to separ-\\textbackslash{}nate the specification of phenotypes (models) from the\\textbackslash{}nimplementation of corresponding algorithms. The COP\\textbackslash{}nprovides a basic model to specify phenotypes in a stan-\\textbackslash{}ndardised way, while the PhenoMan implements the gen-\\textbackslash{}neral approach, common for all COP-based specifications.\\textbackslash{}nIt is not our aim to completely model the EHR. Instead,\\textbackslash{}nour approach can support the modelling and calculation\\textbackslash{}nof selected phenotypes in a user-friendly standardised\\textbackslash{}nmanner.\\textbackslash{}nPhenotypes are modelled in Phenotype Specification\\textbackslash{}nOntologies (PheSO) using the COP. The phenotype clas-\\textbackslash{}nses and axioms (classification and calculation rules) con-\\textbackslash{}ntained in the PheSO are used by PhenoMan to execute\\textbackslash{}nthe corresponding phenotype algorithm. PheSOs are em-\\textbackslash{}nbedded in the COP in such a way that the classes of the\\textbackslash{}nPheSO are subclasses of the COP classes. Every PheSO\\textbackslash{}nsubclass of the COP classes cop: Single\\_Phenotype, cop:\\textbackslash{}nCombined\\_Phenotype or cop: Derived\\_Phenotype is a\\textbackslash{}nphenotype class and is instantiated by phenotypes. The\\textbackslash{}nFig. 3 Core Ontology of Phenotypes (COP)\\textbackslash{}nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 6 of 17\\textbackslash{}ndirect subclasses are non-restricted (e.g., Fasting\\_Glu-\\textbackslash{}ncose, Fig. 4a), while the subclasses of the non-restricted\\textbackslash{}nphenotype classes are restricted (e.g., Fasting\\_Glucose\\_\\textbackslash{}nABNORMAL, i.e., fasting glucose is greater equal 125\\textbackslash{}nmg/dL, Fig. 4a3).\\textbackslash{}nPhenotype classes possess various common attributes\\textbackslash{}n(e.g., labels, descriptions and codes of external concepts).\\textbackslash{}nOther attributes vary depending on the type of the\\textbackslash{}nphenotype class. The following are examples of such\\textbackslash{}nattributes:\\textbackslash{}n\\textbackslash{}u0096 Non-restricted single phenotype (NSiP) class: unit of\\textbackslash{}nmeasure and optional aggregate function.\\textbackslash{}n\\textbackslash{}u0096 Restricted single (RSiP) and derived phenotype\\textbackslash{}n(RDeP) class: restriction.\\textbackslash{}n\\textbackslash{}u0096 Restricted combined phenotype (RCoP) class:\\textbackslash{}nBoolean expression (based on RSiP, RCoP and RDeP\\textbackslash{}nclasses) and optional score value.\\textbackslash{}n\\textbackslash{}u0096 Non-restricted derived phenotype (NDeP) class:\\textbackslash{}nmathematical formula and Boolean expression\\textbackslash{}nconsisting of AND-linked variables used in the for-\\textbackslash{}nmula (NSiP and non-restricted combined phenotype\\textbackslash{}n(NCoP) classes). If a NCoP class is used as a vari-\\textbackslash{}nable, the RCoP classes (subclasses) of the NCoP class\\textbackslash{}nmust have score values that should be used in the\\textbackslash{}nformula.\\textbackslash{}nSimple attributes of the phenotype classes are defined\\textbackslash{}nas annotations. The logical relations between phenotype\\textbackslash{}nclasses as well as range restrictions are represented in\\textbackslash{}nOWL by anonymous equivalent classes or general class\\textbackslash{}naxioms based on property restrictions.\\textbackslash{}nPhenotype Manager (PhenoMan)\\textbackslash{}nWe developed the software Phenotype Manager (Pheno-\\textbackslash{}nMan), which implements a multistage reasoning ap-\\textbackslash{}nproach combining standard reasoners (e.g., Pellet or\\textbackslash{}nHermiT) and mathematical calculations. This section\\textbackslash{}nbriefly outlines the main functionality of our solution.\\textbackslash{}nSpecification of phenotypes\\textbackslash{}nThe PhenoMan Editor is an interactive user interface for\\textbackslash{}nmanaging and developing PheSOs. The user is able to\\textbackslash{}ncreate a new PheSO or to load an existing ontology. The\\textbackslash{}nFig. 4 Parts of the T2DM PheSO in Protégé. Middle: Phenotype classes (a Single, b Derived, c Combined). Left: Example annotations of the\\textbackslash{}nphenotype classes. Right: Anonymous equivalent classes and general class axioms\\textbackslash{}nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 7 of 17\\textbackslash{}nPhenoMan Editor provides appropriate forms to browse,\\textbackslash{}ncreate and edit categories and phenotype classes of the\\textbackslash{}nontology. Value range restrictions, for example, are de-\\textbackslash{}nfined by selecting a comparison operator and entering\\textbackslash{}nthe corresponding values (Fig. 5). Boolean expressions\\textbackslash{}nare built by drag-and-dropping the phenotype classes\\textbackslash{}nfrom the left side into the expression form field and en-\\textbackslash{}ntering relevant operators (Fig. 6). After submission, the\\textbackslash{}nform data is transmitted to the PhenoMan API and is\\textbackslash{}nstored in the actual PheSO.\\textbackslash{}nFurthermore, an ART-DECOR specification (XML)\\textbackslash{}nof relevant data elements can be imported in the\\textbackslash{}nPheSO. For each data element, a NSiP class is gener-\\textbackslash{}nated. All relevant attributes (name, codes, FHIR re-\\textbackslash{}nsource type, data type, unit, etc.) specified in ART-\\textbackslash{}nDECOR are defined as annotations of corresponding\\textbackslash{}nclasses (Fig. 2a, b).\\textbackslash{}nData procurement\\textbackslash{}nAfter starting the PhenoMan Service, FHIR subscriptions\\textbackslash{}n(rest-hooks) {[}34{]} are generated and transmitted to the\\textbackslash{}nFHIR Server. The structure of the subscription resource\\textbackslash{}nis very simple. The main parts of the resource are the\\textbackslash{}ncriteria and the channel. The FHIR Server uses the cri-\\textbackslash{}nteria (FHIR Search query) to determine resources for\\textbackslash{}nwhich notifications have to be generated. When re-\\textbackslash{}nsources are identified (after creating or updating) meet-\\textbackslash{}ning the criteria, a notification is sent to the address\\textbackslash{}n(\\textbackslash{}u0091endpoint\\textbackslash{}u0092) specified in the section \\textbackslash{}u0091channel\\textbackslash{}u0092.\\textbackslash{}nFig. 5 Specification of the class Fasting\\_Glucose\\_ABNORMAL with the PhenoMan Editor form. We left out some of the metadata fields for\\textbackslash{}nbetter visibility\\textbackslash{}nFig. 6 Specification of the class T2DM\\_Case\\_3 with the PhenoMan Editor form\\textbackslash{}nUciteli et al. Journal of Biomedical Semantics           (2020) 11:15 Page 8 of 17\\textbackslash{}nIn the configuration file of the PhenoMan, a directory\\textbackslash{}ncontaining all available phenotype specifications (Phe-\\textbackslash{}nSOs) as well as the address (URL) of the PhenoMan ser-\\textbackslash{}nvice (including the PheSO name) are defined. For each\\textbackslash{}nNSiP class of each available PheSO (in the defined direc-\\textbackslash{}ntory) a subscription is created. To generate the subscrip-\\textbackslash{}ntion criteria (FHIR Search query), the PhenoMan uses\\textbackslash{}nthe resource type and codes specified in the correspond-\\textbackslash{}ning NSiP class as annotations (Fig. 2b, c). The \\textbackslash{}u0091endpoint\\textbackslash{}u0092\\textbackslash{}nattribute is automatically filled with the URL of the Phe-\\textbackslash{}nnoMan service defined in the configuration file. The\\textbackslash{}nremaining parts of the subscription resource (\\textbackslash{}u0091status\\textbackslash{}u0092,\\textbackslash{}n\\textbackslash{}u0091type\\textbackslash{}u0092 and \\textbackslash{}u0091payload\\textbackslash{}u0092) take default values (\\textbackslash{}u0091active\\textbackslash{}u0092, \\textbackslash{}u0091rest-\\textbackslash{}nhook\\textbackslash{}u0092 and \\textbackslash{}u0091application/json\\textbackslash{}u0092) (Fig. 2c).\\textbackslash{}nAfter receiving a notification (including the\\textbackslash{}ncomplete resource, Fig. 2d), the PhenoMan Service re-\\textbackslash{}nquests further resources (for all other NSiP classes of\\textbackslash{}nthe corresponding PheSO) using FHIR Search. The\\textbackslash{}ngenerated FHIR Search queries are primarily based\\textbackslash{}nupon the codes specified for the NSiP classes (simi-\\textbackslash{}nlarly to subscription criteria), contain a reference to\\textbackslash{}nthe patient and can additionally support possible ag-\\textbackslash{}ngregate functions.\\textbackslash{}nClassification and calculation of phenotypes\\textbackslash{}nAfter receiving required resources, the PhenoMan starts\\textbackslash{}ninferring phenotypes.\\textbackslash{}nFirst, the relevant information is extracted from re-\\textbackslash{}nceived resources and inserted into the ontology. On the\\textbackslash{}none hand, the individual properties (single phenotypes)\\textbackslash{}nare inserted as instances of the direct subclasses of cop:\\textbackslash{}nSingle\\_Phenotype and the values are modelled as prop-\\textbackslash{}nerty assertions based on the has\\_value relation. On the\\textbackslash{}nother hand, a composite phenotype is defined as an in-\\textbackslash{}nstance of the class cop: Composite\\_Phenotype, which\\textbackslash{}ncombines all the single phenotype instances using prop-\\textbackslash{}nerty assertions based on has\\_part relation. Then, our\\textbackslash{}nmultistage reasoning algorithm is executed. The algo-\\textbackslash{}nrithm consists of the following steps:\\textbackslash{}n1. Classification step. A standard reasoner classifies the\\textbackslash{}nexisting instances (assignment to classes).\\textbackslash{}na. Single phenotype instances are classified in RSiP\\textbackslash{}nclasses based on property restrictions.\\textbackslash{}nb. The composite phenotype instance is classified\\textbackslash{}nin RCoP classes based on the specified Boolean\\textbackslash{}nexpression and inferred RSiP, RCoP and RDeP\\textbackslash{}nclasses.\\textbackslash{}nc. The composite phenotype instance is classified\\textbackslash{}nin NDeP classes based on the specified Boolean\\textbackslash{}nexpression and corresponding NSiP and NCoP\\textbackslash{}nclasses. In this case, all variable values required\\textbackslash{}nfor calculating formulas are present.\\textbackslash{}nd. Available instances of NDeP classes\\textbackslash{}n(representing calculated values) are classified in\\textbackslash{}nRDeP classes based on proper'\n\\end{description*}\n",
            "text/plain": [
              "Corpus consisting of 15 documents.\n",
              "text1 :\n",
              "\"Althubaiti et al. Journal of Biomedical Semantics           ...\"\n",
              "\n",
              "text2 :\n",
              "\"RESEARCH Open Access Temporal information extraction from me...\"\n",
              "\n",
              "text3 :\n",
              "\"RESEARCH Open Access Disclosing Main authors and Organisatio...\"\n",
              "\n",
              "text4 :\n",
              "\"Keet Journal of Biomedical Semantics            (2020) 11:4 ...\"\n",
              "\n",
              "text5 :\n",
              "\"Nguyen et al. Journal of Biomedical Semantics            (20...\"\n",
              "\n",
              "text6 :\n",
              "\"Gleim et al. Journal of Biomedical Semantics            (202...\"\n",
              "\n",
              "[ reached max_ndoc ... 9 more documents ]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message:\n",
            "“'dfm.corpus()' is deprecated. Use 'tokens()' first.”\n",
            "Warning message:\n",
            "“ngrams argument is not used.”\n",
            "Warning message:\n",
            "“'...' should not be used for tokens() arguments; use 'tokens()' first.”\n",
            "Warning message:\n",
            "“ngrams argument is not used.”\n",
            "Warning message:\n",
            "“ngrams argument is not used.”\n",
            "Warning message:\n",
            "“'remove' is deprecated; use dfm_remove() instead”\n",
            "Warning message:\n",
            "“'stem' is deprecated; use dfm_wordstem() instead”\n"
          ]
        }
      ],
      "source": [
        "#create corpus\n",
        "myCorpus <- corpus(dataset$description)\n",
        "myCorpus\n",
        "#docvars(myCorpus, field = \"Subject\") <- ifelse(dataset$College==\"Computing and Informatics\",\n",
        "                                             #  \"Computing\",\"Social Science\")\n",
        "#docvars(myCorpus, field = \"Year\") <- as.integer(dataset$Year)\n",
        "## Create the dfm (pre-processing)\n",
        "\n",
        "\n",
        "stopWords <- c(\"can\",\"use\",\"uses\",\"used\",\"using\",\"study\",\"studies\",\"first\",\"second\",\"third\",\"also\",\"across\",\"results\",\"result\",\"resulted\",\"may\",\"however\",\"one\",\"two\",\"three\",\"four\",\"five\",\"among\",\"well\",\"within\",\"many\",\"related\",\"i.e\",\"e.g\",\"find\",\"finding\",\"finds\",\"found\",\"increase\",\"increases\",\"increasing\",\"increased\",\"decreased\",\"decrease\",\"decreases\",\"decreasing\",\"propose\",\"proposal\",\"proposals\",\"proposes\",\"proposed\",\"new\",\"old\",\"differ\",\"differs\",\"different\",\"difference\",\"differences\",\"positive\",\"negative\",\"findings\",\"reports\",\"report\",\"reported\",\"state\",\"states\",\"article\",\"articles\",\"examines\",\"examine\",\"suggest\",\"research\",\"researches\",\"researchers\",\"need\",\"needs\",\"show\",\"shows\",\"association\",\"associations\",\"associated\",\"discuss\",\"discusses\",\"discussed\",\"will\",\"likely\",\"unlikely\",\"paper\",\"method\",\"methods\",\"methodology\",\"compared\",\"specifically\",\"approach\",\"impact\",\"impacts\",\"examine\",\"examined\",\"examines\",\"includes\",\"include\",\"included\",\"including\",\"measure\",\"measures\",\"measured\",\"analysis\",\"analyze\",\"analyses\",\"complete\",\"completes\",\"completed\",\"indicate\",\"indicated\",\"indicates\",\"high\",\"higher\",\"low\",\"lower\",\"follow\",\"follows\",\"following\",\"significant\",\"significance\",\"approach\",\"approaches\",\"approached\",\"model\",\"models\",\"demonstrate\",\"demonstrated\",\"demonstrates\",\"yet\",\"best\",\"worst\",\"better\",\"large\",\"small\",\"larger\",\"smaller\",\"several\",\"few\",\"much\",\"less\",\"given\",\"via\",\"long\",\"short\",\"often\",\"years\",\"along\",\"whether\",\"potential\",\"significantly\",\"influence\",\"influenced\",\"influences\",\"develop\",\"develops\",\"developed\",\"good\",\"bad\",\"based\",\"p\",\"group\",\"groups\",\"effect\",\"affect\",\"affects\",\"effects\",\"sample\",\"samples\",\"relationship\",\"relationships\",\"change\",\"changes\",\"m\",\"k\",\"conclusion\",\"conclusions\",\"present\",\"presents\")\n",
        "dfm <- dfm(myCorpus,\n",
        "           remove = c(stopwords(\"english\"), stopWords),\n",
        "           ngrams= 1L,\n",
        "           stem = F,\n",
        "           remove_numbers = TRUE, \n",
        "           remove_punct = TRUE,\n",
        "           remove_symbols = TRUE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2UH6MW3Oas78",
        "outputId": "8e3550ae-8ae5-49f3-b0c4-4c2c7d7ae387"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<dl>\n",
              "\t<dt>$lower.thresh</dt>\n",
              "\t\t<dd><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>1</li><li>11</li><li>21</li><li>31</li><li>41</li><li>51</li><li>61</li><li>71</li><li>81</li><li>91</li></ol>\n",
              "</dd>\n",
              "\t<dt>$ndocs</dt>\n",
              "\t\t<dd><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>0</li><li>0</li><li>15</li><li>15</li><li>15</li><li>15</li><li>15</li><li>15</li><li>15</li><li>15</li></ol>\n",
              "</dd>\n",
              "\t<dt>$nwords</dt>\n",
              "\t\t<dd><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>3115</li><li>4820</li><li>4916</li><li>4916</li><li>4916</li><li>4916</li><li>4916</li><li>4916</li><li>4916</li><li>4916</li></ol>\n",
              "</dd>\n",
              "\t<dt>$ntokens</dt>\n",
              "\t\t<dd><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>6320</li><li>30616</li><li>36535</li><li>36535</li><li>36535</li><li>36535</li><li>36535</li><li>36535</li><li>36535</li><li>36535</li></ol>\n",
              "</dd>\n",
              "</dl>\n"
            ],
            "text/markdown": "$lower.thresh\n:   1. 1\n2. 11\n3. 21\n4. 31\n5. 41\n6. 51\n7. 61\n8. 71\n9. 81\n10. 91\n\n\n\n$ndocs\n:   1. 0\n2. 0\n3. 15\n4. 15\n5. 15\n6. 15\n7. 15\n8. 15\n9. 15\n10. 15\n\n\n\n$nwords\n:   1. 3115\n2. 4820\n3. 4916\n4. 4916\n5. 4916\n6. 4916\n7. 4916\n8. 4916\n9. 4916\n10. 4916\n\n\n\n$ntokens\n:   1. 6320\n2. 30616\n3. 36535\n4. 36535\n5. 36535\n6. 36535\n7. 36535\n8. 36535\n9. 36535\n10. 36535\n\n\n\n\n\n",
            "text/latex": "\\begin{description}\n\\item[\\$lower.thresh] \\begin{enumerate*}\n\\item 1\n\\item 11\n\\item 21\n\\item 31\n\\item 41\n\\item 51\n\\item 61\n\\item 71\n\\item 81\n\\item 91\n\\end{enumerate*}\n\n\\item[\\$ndocs] \\begin{enumerate*}\n\\item 0\n\\item 0\n\\item 15\n\\item 15\n\\item 15\n\\item 15\n\\item 15\n\\item 15\n\\item 15\n\\item 15\n\\end{enumerate*}\n\n\\item[\\$nwords] \\begin{enumerate*}\n\\item 3115\n\\item 4820\n\\item 4916\n\\item 4916\n\\item 4916\n\\item 4916\n\\item 4916\n\\item 4916\n\\item 4916\n\\item 4916\n\\end{enumerate*}\n\n\\item[\\$ntokens] \\begin{enumerate*}\n\\item 6320\n\\item 30616\n\\item 36535\n\\item 36535\n\\item 36535\n\\item 36535\n\\item 36535\n\\item 36535\n\\item 36535\n\\item 36535\n\\end{enumerate*}\n\n\\end{description}\n",
            "text/plain": [
              "$lower.thresh\n",
              " [1]  1 11 21 31 41 51 61 71 81 91\n",
              "\n",
              "$ndocs\n",
              " [1]  0  0 15 15 15 15 15 15 15 15\n",
              "\n",
              "$nwords\n",
              " [1] 3115 4820 4916 4916 4916 4916 4916 4916 4916 4916\n",
              "\n",
              "$ntokens\n",
              " [1]  6320 30616 36535 36535 36535 36535 36535 36535 36535 36535\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing 4499 of 4916 terms (6962 of 10727 tokens) due to frequency \n",
            "Your corpus now has 15 documents, 417 terms and 3765 tokens."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<dl>\n",
              "\t<dt>$documents</dt>\n",
              "\t\t<dd><dl>\n",
              "\t<dt>$text1</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 263 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>8</td><td>10</td><td>12</td><td>13</td><td>⋯</td><td>400</td><td>401</td><td>402</td><td>403</td><td>405</td><td>409</td><td>411</td><td>413</td><td>415</td><td>416</td></tr>\n",
              "\t<tr><td>1</td><td>1</td><td>4</td><td>1</td><td>1</td><td>1</td><td>3</td><td> 8</td><td> 1</td><td>12</td><td>⋯</td><td>  4</td><td>  6</td><td>  6</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td> 32</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text2</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 45 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>11</td><td>24</td><td>35</td><td>36</td><td>39</td><td>57</td><td>79</td><td>86</td><td>⋯</td><td>318</td><td>319</td><td>351</td><td>355</td><td>358</td><td>365</td><td>376</td><td>387</td><td>390</td><td>399</td></tr>\n",
              "\t<tr><td>1</td><td>1</td><td> 1</td><td> 1</td><td> 1</td><td> 1</td><td> 1</td><td> 3</td><td> 2</td><td> 1</td><td>⋯</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text3</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 266 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>4</td><td>6</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>⋯</td><td>402</td><td>403</td><td>407</td><td>408</td><td>409</td><td>411</td><td>412</td><td>413</td><td>415</td><td>416</td></tr>\n",
              "\t<tr><td>3</td><td>2</td><td>5</td><td>3</td><td>5</td><td>1</td><td> 2</td><td> 1</td><td> 2</td><td>15</td><td>⋯</td><td>  1</td><td>  6</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td>  6</td><td>  3</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text4</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 294 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>5</td><td>6</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>⋯</td><td>405</td><td>406</td><td>407</td><td>408</td><td>409</td><td>410</td><td>411</td><td>413</td><td>414</td><td>416</td></tr>\n",
              "\t<tr><td>6</td><td>1</td><td>3</td><td>1</td><td>1</td><td>2</td><td>1</td><td> 1</td><td> 3</td><td> 5</td><td>⋯</td><td>  2</td><td> 13</td><td>  2</td><td>  1</td><td>  1</td><td>  2</td><td>  3</td><td>  1</td><td>  1</td><td>  2</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text5</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 282 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>9</td><td>10</td><td>11</td><td>13</td><td>14</td><td>⋯</td><td>404</td><td>405</td><td>407</td><td>408</td><td>409</td><td>410</td><td>412</td><td>415</td><td>416</td><td>417</td></tr>\n",
              "\t<tr><td>1</td><td>4</td><td>1</td><td>2</td><td>6</td><td>1</td><td> 6</td><td> 2</td><td>11</td><td> 2</td><td>⋯</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td> 15</td><td> 14</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text6</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 304 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td> 3</td><td>4</td><td>7</td><td>9</td><td>10</td><td>12</td><td>13</td><td>14</td><td>15</td><td>⋯</td><td>405</td><td>406</td><td>407</td><td>408</td><td>409</td><td>410</td><td>411</td><td>413</td><td>414</td><td>416</td></tr>\n",
              "\t<tr><td>1</td><td>16</td><td>6</td><td>5</td><td>2</td><td> 9</td><td> 1</td><td>15</td><td> 3</td><td> 7</td><td>⋯</td><td>  2</td><td>  2</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td> 10</td><td>  1</td><td> 13</td><td> 10</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text7</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 245 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>9</td><td>11</td><td>12</td><td>13</td><td>20</td><td>21</td><td>22</td><td>23</td><td>⋯</td><td>405</td><td>406</td><td>407</td><td>408</td><td>409</td><td>412</td><td>413</td><td>415</td><td>416</td><td>417</td></tr>\n",
              "\t<tr><td>1</td><td>8</td><td>1</td><td> 1</td><td> 2</td><td> 3</td><td>10</td><td> 1</td><td> 1</td><td> 8</td><td>⋯</td><td>  6</td><td>  2</td><td>  2</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  8</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text8</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 314 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>⋯</td><td>407</td><td>408</td><td>409</td><td>410</td><td>411</td><td>412</td><td>413</td><td>414</td><td>415</td><td>416</td></tr>\n",
              "\t<tr><td>4</td><td>1</td><td>3</td><td>2</td><td>1</td><td>1</td><td>1</td><td>4</td><td>1</td><td> 2</td><td>⋯</td><td>  1</td><td>  2</td><td>  1</td><td>  7</td><td>  5</td><td>  3</td><td>  1</td><td>  3</td><td>  1</td><td> 10</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text9</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 294 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>4</td><td>8</td><td>9</td><td>10</td><td>12</td><td>13</td><td>15</td><td>16</td><td>⋯</td><td>402</td><td>403</td><td>404</td><td>407</td><td>408</td><td>409</td><td>410</td><td>413</td><td>414</td><td>416</td></tr>\n",
              "\t<tr><td>2</td><td>2</td><td>1</td><td>3</td><td>1</td><td> 5</td><td> 1</td><td>18</td><td> 1</td><td> 1</td><td>⋯</td><td>  4</td><td>  7</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  4</td><td> 11</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text10</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 316 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td> 4</td><td>6</td><td>7</td><td>9</td><td>10</td><td>12</td><td>13</td><td>⋯</td><td>407</td><td>408</td><td>409</td><td>410</td><td>412</td><td>413</td><td>414</td><td>415</td><td>416</td><td>417</td></tr>\n",
              "\t<tr><td>3</td><td>2</td><td>2</td><td>11</td><td>1</td><td>4</td><td>1</td><td> 4</td><td> 4</td><td>17</td><td>⋯</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  9</td><td>  3</td><td>  7</td><td>  4</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text11</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 275 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>13</td><td>14</td><td>⋯</td><td>402</td><td>403</td><td>404</td><td>406</td><td>407</td><td>408</td><td>409</td><td>415</td><td>416</td><td>417</td></tr>\n",
              "\t<tr><td>1</td><td>2</td><td>2</td><td>2</td><td>5</td><td>3</td><td> 8</td><td> 6</td><td>16</td><td> 1</td><td>⋯</td><td>  2</td><td>  3</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td> 10</td><td> 12</td><td>  5</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text12</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 81 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>5</td><td>6</td><td>7</td><td>13</td><td>15</td><td>19</td><td>22</td><td>⋯</td><td>366</td><td>375</td><td>376</td><td>379</td><td>382</td><td>383</td><td>390</td><td>394</td><td>399</td><td>416</td></tr>\n",
              "\t<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>4</td><td> 1</td><td> 2</td><td> 1</td><td> 1</td><td>⋯</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  2</td><td>  6</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text13</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 236 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>5</td><td>6</td><td>9</td><td>10</td><td>12</td><td>13</td><td>15</td><td>⋯</td><td>396</td><td>398</td><td>401</td><td>407</td><td>408</td><td>409</td><td>411</td><td>412</td><td>413</td><td>416</td></tr>\n",
              "\t<tr><td>2</td><td>1</td><td>7</td><td>1</td><td>1</td><td>1</td><td> 2</td><td> 1</td><td> 6</td><td> 1</td><td>⋯</td><td>  1</td><td>  1</td><td>  4</td><td>  2</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  2</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text14</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 235 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>4</td><td>6</td><td> 8</td><td>9</td><td>10</td><td>13</td><td>14</td><td>⋯</td><td>398</td><td>402</td><td>403</td><td>407</td><td>408</td><td>409</td><td>410</td><td>413</td><td>415</td><td>417</td></tr>\n",
              "\t<tr><td>1</td><td>3</td><td>3</td><td>1</td><td>1</td><td>10</td><td>1</td><td> 4</td><td>69</td><td>20</td><td>⋯</td><td>  2</td><td>  8</td><td>  2</td><td>  3</td><td>  4</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text15</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 315 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>4</td><td>6</td><td>7</td><td>9</td><td>10</td><td>11</td><td>12</td><td>⋯</td><td>403</td><td>405</td><td>407</td><td>408</td><td>409</td><td>410</td><td>411</td><td>413</td><td>414</td><td>417</td></tr>\n",
              "\t<tr><td>3</td><td>3</td><td>6</td><td>2</td><td>1</td><td>1</td><td>1</td><td> 8</td><td> 2</td><td> 4</td><td>⋯</td><td> 24</td><td>  2</td><td>  2</td><td>  1</td><td>  1</td><td>  4</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "</dl>\n",
              "</dd>\n",
              "\t<dt>$vocab</dt>\n",
              "\t\t<dd><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'abl'</li><li>'abstract'</li><li>'access'</li><li>'accord'</li><li>'accur'</li><li>'activ'</li><li>'actual'</li><li>'ad'</li><li>'adapt'</li><li>'addit'</li><li>'address'</li><li>'aim'</li><li>'al'</li><li>'algorithm'</li><li>'allow'</li><li>'alreadi'</li><li>'although'</li><li>'amount'</li><li>'analyz'</li><li>'annot'</li><li>'anoth'</li><li>'appli'</li><li>'applic'</li><li>'appropri'</li><li>'area'</li><li>'articl'</li><li>'aspect'</li><li>'assess'</li><li>'assign'</li><li>'assum'</li><li>'ation'</li><li>'attribut'</li><li>'author'</li><li>'autom'</li><li>'automat'</li><li>'avail'</li><li>'averag'</li><li>'b'</li><li>'background'</li><li>'basic'</li><li>'becom'</li><li>'biolog'</li><li>'biomed'</li><li>'built'</li><li>'c'</li><li>'calcul'</li><li>'care'</li><li>'case'</li><li>'categor'</li><li>'categori'</li><li>'caus'</li><li>'challeng'</li><li>'characterist'</li><li>'class'</li><li>'classif'</li><li>'classifi'</li><li>'clinic'</li><li>'close'</li><li>'cluster'</li><li>'code'</li><li>'coher'</li><li>'collect'</li><li>'com-'</li><li>'combin'</li><li>'come'</li><li>'common'</li><li>'communiti'</li><li>'compar'</li><li>'comparison'</li><li>'complex'</li><li>'comput'</li><li>'con-'</li><li>'concept'</li><li>'concern'</li><li>'condit'</li><li>'conduct'</li><li>'consid'</li><li>'consist'</li><li>'construct'</li><li>'contain'</li><li>'content'</li><li>'context'</li><li>'contribut'</li><li>'copi'</li><li>'copyright'</li><li>'core'</li><li>'correct'</li><li>'correspond'</li><li>'countri'</li><li>'cover'</li><li>'creat'</li><li>'creativ'</li><li>'credit'</li><li>'criteria'</li><li>'current'</li><li>'d'</li><li>'data'</li><li>'databas'</li><li>'dataset'</li><li>'de-'</li><li>'dedic'</li><li>'defin'</li><li>'definit'</li><li>'depend'</li><li>'deriv'</li><li>'describ'</li><li>'descript'</li><li>'design'</li><li>'detail'</li><li>'detect'</li><li>'determin'</li><li>'develop'</li><li>'differ-'</li><li>'direct'</li><li>'dis-'</li><li>'discuss'</li><li>'diseas'</li><li>'distinct'</li><li>'distinguish'</li><li>'distribut'</li><li>'document'</li><li>'domain'</li><li>'due'</li><li>'earli'</li><li>'edg'</li><li>'effect'</li><li>'effort'</li><li>'ehr'</li><li>'either'</li><li>'electron'</li><li>'enabl'</li><li>'enc'</li><li>'encod'</li><li>'end'</li><li>'ent'</li><li>'entiti'</li><li>'equal'</li><li>'establish'</li><li>'et'</li><li>'etc'</li><li>'evalu'</li><li>'even'</li><li>'exampl'</li><li>'exceed'</li><li>'exist'</li><li>'expect'</li><li>'experi'</li><li>'expert'</li><li>'explicit'</li><li>'express'</li><li>'extend'</li><li>'extern'</li><li>'extract'</li><li>'featur'</li><li>'field'</li><li>'fig'</li><li>'figur'</li><li>'file'</li><li>'final'</li><li>'focus'</li><li>'form'</li><li>'formal'</li><li>'format'</li><li>'free'</li><li>'full'</li><li>'fulli'</li><li>'function'</li><li>'futur'</li><li>'general'</li><li>'generat'</li><li>'give'</li><li>'health'</li><li>'healthcar'</li><li>'help'</li><li>'histori'</li><li>'holder'</li><li>'hospit'</li><li>'http://creativecommons.org/licenses/by/4.0/.'</li><li>'http://creativecommons.org/publicdomain/zero/1.0/'</li><li>'human'</li><li>'ical'</li><li>'id'</li><li>'identif'</li><li>'identifi'</li><li>'illustr'</li><li>'imag'</li><li>'implement'</li><li>'import'</li><li>'improv'</li><li>'in-'</li><li>'inclus'</li><li>'index'</li><li>'individu'</li><li>'inform'</li><li>'informat'</li><li>'ing'</li><li>'initi'</li><li>'input'</li><li>'instanc'</li><li>'instead'</li><li>⋯</li><li>'languag'</li><li>'lead'</li><li>'learn'</li><li>'level'</li><li>'librari'</li><li>'licenc'</li><li>'licens'</li><li>'limit'</li><li>'line'</li><li>'link'</li><li>'list'</li><li>'literatur'</li><li>'logic'</li><li>'machin'</li><li>'made'</li><li>'main'</li><li>'major'</li><li>'make'</li><li>'manner'</li><li>'manual'</li><li>'map'</li><li>'match'</li><li>'materi'</li><li>'mean'</li><li>'medic'</li><li>'medicin'</li><li>'medium'</li><li>'ment'</li><li>'methodolog'</li><li>'might'</li><li>'model'</li><li>'moreov'</li><li>'multipl'</li><li>'name'</li><li>'nation'</li><li>'natur'</li><li>'network'</li><li>'next'</li><li>'normal'</li><li>'now'</li><li>'number'</li><li>'object'</li><li>'observ'</li><li>'obtain'</li><li>'ontolog'</li><li>'open'</li><li>'oper'</li><li>'optim'</li><li>'option'</li><li>'order'</li><li>'organ'</li><li>'origin'</li><li>'otherwis'</li><li>'outcom'</li><li>'output'</li><li>'overal'</li><li>'page'</li><li>'part'</li><li>'parti'</li><li>'particip'</li><li>'particular'</li><li>'patient'</li><li>'pattern'</li><li>'perform'</li><li>'permiss'</li><li>'permit'</li><li>'person'</li><li>'point'</li><li>'possibl'</li><li>'practic'</li><li>'pre-'</li><li>'precis'</li><li>'predict'</li><li>'present'</li><li>'previous'</li><li>'principl'</li><li>'pro-'</li><li>'problem'</li><li>'process'</li><li>'program'</li><li>'provid'</li><li>'public'</li><li>'publish'</li><li>'purpos'</li><li>'qualiti'</li><li>'queri'</li><li>'r'</li><li>'rang'</li><li>'reason'</li><li>'receiv'</li><li>'recent'</li><li>'record'</li><li>'refer'</li><li>'regul'</li><li>'relat'</li><li>'relev'</li><li>'remain'</li><li>'repres'</li><li>'represent'</li><li>'reproduct'</li><li>'requir'</li><li>'resourc'</li><li>'respect'</li><li>'restrict'</li><li>'result'</li><li>'review'</li><li>'risk'</li><li>'role'</li><li>'s'</li><li>'scienc'</li><li>'scientif'</li><li>'score'</li><li>'search'</li><li>'section'</li><li>'see'</li><li>'select'</li><li>'semant'</li><li>'sens'</li><li>'sent'</li><li>'separ'</li><li>'set'</li><li>'share'</li><li>'show'</li><li>'shown'</li><li>'sign'</li><li>'similar'</li><li>'sinc'</li><li>'singl'</li><li>'size'</li><li>'sourc'</li><li>'specif'</li><li>'specifi'</li><li>'standard'</li><li>'start'</li><li>'state'</li><li>'statist'</li><li>'statutori'</li><li>'step'</li><li>'still'</li><li>'store'</li><li>'structur'</li><li>'subclass'</li><li>'subject'</li><li>'subset'</li><li>'suggest'</li><li>'suitabl'</li><li>'support'</li><li>'symptom'</li><li>'system'</li><li>'t'</li><li>'tabl'</li><li>'take'</li><li>'target'</li><li>'task'</li><li>'technolog'</li><li>'term'</li><li>'terminolog'</li><li>'test'</li><li>'text'</li><li>'therefor'</li><li>'thus'</li><li>'time'</li><li>'tion'</li><li>'tive'</li><li>'tool'</li><li>'topic'</li><li>'tor'</li><li>'total'</li><li>'train'</li><li>'treatment'</li><li>'type'</li><li>'typic'</li><li>'under'</li><li>'understand'</li><li>'uniqu'</li><li>'unit'</li><li>'univers'</li><li>'unless'</li><li>'updat'</li><li>'upon'</li><li>'usag'</li><li>'use'</li><li>'usual'</li><li>'util'</li><li>'valid'</li><li>'valu'</li><li>'variat'</li><li>'various'</li><li>'version'</li><li>'view'</li><li>'visit'</li><li>'waiver'</li><li>'way'</li><li>'web'</li><li>'whole'</li><li>'wide'</li><li>'without'</li><li>'word'</li><li>'work'</li><li>'written'</li></ol>\n",
              "</dd>\n",
              "\t<dt>$meta</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A data.frame: 15 × 0</caption>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th></tr>\n",
              "\t<tr><th scope=row>2</th></tr>\n",
              "\t<tr><th scope=row>3</th></tr>\n",
              "\t<tr><th scope=row>4</th></tr>\n",
              "\t<tr><th scope=row>5</th></tr>\n",
              "\t<tr><th scope=row>6</th></tr>\n",
              "\t<tr><th scope=row>7</th></tr>\n",
              "\t<tr><th scope=row>8</th></tr>\n",
              "\t<tr><th scope=row>9</th></tr>\n",
              "\t<tr><th scope=row>10</th></tr>\n",
              "\t<tr><th scope=row>11</th></tr>\n",
              "\t<tr><th scope=row>12</th></tr>\n",
              "\t<tr><th scope=row>13</th></tr>\n",
              "\t<tr><th scope=row>14</th></tr>\n",
              "\t<tr><th scope=row>15</th></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$words.removed</dt>\n",
              "\t\t<dd><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'-0.42'</li><li>'-1'</li><li>'-2'</li><li>'-9'</li><li>'-b'</li><li>'-base'</li><li>'-c'</li><li>'-e'</li><li>'-fr'</li><li>'-h'</li><li>'-ii'</li><li>'-j'</li><li>'-n'</li><li>'-s'</li><li>'-t'</li><li>'-w'</li><li>'0-'</li><li>'1-3'</li><li>'1-4'</li><li>'1.7m'</li><li>'10-15'</li><li>'10-fold'</li><li>'10th'</li><li>'1558-6'</li><li>'1a'</li><li>'1amsterdam'</li><li>'1cnrs'</li><li>'1comput'</li><li>'1depart'</li><li>'1divis'</li><li>'1faculti'</li><li>'1http://www.w3.org/tr/2003/pr-owl-guide-20031209/win'</li><li>'1https://www.atilika.com/ja/kuromoji/'</li><li>'1https://www.healthboards.com/'</li><li>'1https://www.ncbi.nlm.nih.gov/pubm'</li><li>'1in'</li><li>'1informatik'</li><li>'1institut'</li><li>'1k'</li><li>'1like'</li><li>'1nation'</li><li>'1sourc'</li><li>'1space'</li><li>'1univers'</li><li>'2-3'</li><li>'2012-2015'</li><li>'23955-6900'</li><li>'26-11-2018'</li><li>'2a'</li><li>'2b'</li><li>'2c'</li><li>'2castor'</li><li>'2center'</li><li>'2comput'</li><li>'2d'</li><li>'2here'</li><li>'2http://mallet.cs.umass.edu/'</li><li>'2http://ontofox.hegroup.org'</li><li>'2http://xiphoid.biostr.washington.edu/fma/shirt_ontology/shirt_ontology_1.'</li><li>'2https://hal.archives-ouvertes.fr/'</li><li>'2https://medhelp.org/'</li><li>'2nd'</li><li>'2univ'</li><li>'3-5-1'</li><li>'3d'</li><li>'3hereinaft'</li><li>'3http://purl.org/sig/ont/fma/fma290055'</li><li>'3http://www.workingontologist.org/examples.zip'</li><li>'3https://www.istex.fr/'</li><li>'432-8011'</li><li>'4a'</li><li>'4a3'</li><li>'4http://www.cl.ecei.tohoku.ac.jp/~m~suzuki/jawiki_vector/'</li><li>'4https://www.i2b2.org/nlp/datasets/main.php'</li><li>'4recent'</li><li>'5https://github.com/johokugsk'</li><li>'5https://n2c2.dbmi.hms.harvard.edu/'</li><li>'5in'</li><li>'60gb'</li><li>'64-year-old'</li><li>'6https://sites.google.com/site/shareclefehealth/'</li><li>'7http://www.cepidc.inserm.fr/'</li><li>'8-core'</li><li>'86-year-old'</li><li>'8gb'</li><li>'8https://knowledge-learning.github.io/ehealthkd-2019'</li><li>'9c'</li><li>'a.m'</li><li>'a1'</li><li>'a2'</li><li>'aachen'</li><li>'ab'</li><li>'abbrevi'</li><li>'abdelhakim1,2'</li><li>'abdomin'</li><li>'abdullah'</li><li>'abil'</li><li>'ablat'</li><li>'abli'</li><li>'abnorm'</li><li>'absenc'</li><li>'abu-hanna1'</li><li>'ac'</li><li>'ac-'</li><li>'acad-'</li><li>'academ'</li><li>'academicstaff'</li><li>'acceler'</li><li>'accept'</li><li>'accom-'</li><li>'accommod'</li><li>'accomplish'</li><li>'accordingly.w'</li><li>'account'</li><li>'accumu-'</li><li>'accumul'</li><li>'accuraci'</li><li>'aceruloplasminemia'</li><li>'ach'</li><li>'acharya8'</li><li>'achiev'</li><li>'acknow-'</li><li>'acknowl-'</li><li>'acknowledg'</li><li>'acl'</li><li>'acm'</li><li>'acquir'</li><li>'acquisit'</li><li>'acroparesthesia'</li><li>'act'</li><li>'acterist'</li><li>'action'</li><li>'activ-'</li><li>'actor'</li><li>'acut'</li><li>'ad-'</li><li>'ad-hoc'</li><li>'adam'</li><li>'add'</li><li>'add-'</li><li>'addi-'</li><li>'adequ'</li><li>'adher'</li><li>'adisinsight'</li><li>'adjac'</li><li>'adjacentregion'</li><li>'adjust'</li><li>'admin-'</li><li>'administr'</li><li>'admission_num'</li><li>'admit'</li><li>'adopt'</li><li>'adr'</li><li>'adult'</li><li>'advanc'</li><li>'advantag'</li><li>'advers'</li><li>'adverse-drug'</li><li>'advic'</li><li>'advis'</li><li>'aesthet'</li><li>'af'</li><li>'affect'</li><li>'affili'</li><li>'affili-'</li><li>'affilia-'</li><li>'afford'</li><li>'aforement'</li><li>'africa'</li><li>'african'</li><li>'africanwildlif'</li><li>'africanwildlifeontology.xml'</li><li>'africanwildlifeontology0'</li><li>'africanwildlifeontology1'</li><li>'africanwildlifeontology1a.owl'</li><li>'africanwildlifeontology2'</li><li>'africanwildlifeontology2a.owl'</li><li>'africanwildlifeontology3'</li><li>'africanwildlifeontologyweb.owl'</li><li>'afrikaan'</li><li>'afterward'</li><li>'ag'</li><li>'ag-'</li><li>'agar-'</li><li>'age'</li><li>'agenc'</li><li>'agent'</li><li>'aggreg'</li><li>'aggress'</li><li>'agnost'</li><li>'ago'</li><li>'agre'</li><li>'agreement'</li><li>'ah'</li><li>'ahornstr'</li><li>'ai'</li><li>'aid'</li><li>'airplan'</li><li>'airway'</li><li>'ak'</li><li>⋯</li><li>'vertigo'</li><li>'vesilinnanti'</li><li>'vessel'</li><li>'vhnguyen@u.nus.edu'</li><li>'vi'</li><li>'viani1'</li><li>'viation'</li><li>'vice'</li><li>'vide'</li><li>'video'</li><li>'vider'</li><li>'vidual'</li><li>'view-'</li><li>'viewer'</li><li>'viewpoint'</li><li>'vii'</li><li>'vincent'</li><li>'violat'</li><li>'vious'</li><li>'viral'</li><li>'virtual'</li><li>'virus'</li><li>'vis-'</li><li>'visibl'</li><li>'vision'</li><li>'visual'</li><li>'visual-'</li><li>'visualis'</li><li>'vital'</li><li>'vitamin'</li><li>'vival'</li><li>'vlietstra'</li><li>'vlietstra1'</li><li>'vo'</li><li>'vocabu-'</li><li>'vocabulari'</li><li>'voic'</li><li>'volum'</li><li>'volv'</li><li>'vomit'</li><li>'vos'</li><li>'vos-'</li><li>'vos1,2'</li><li>'vosview'</li><li>'vous'</li><li>'vp'</li><li>'vpi'</li><li>'vpui'</li><li>'vs'</li><li>'vt'</li><li>'vu'</li><li>'vui'</li><li>'vw1'</li><li>'vw2'</li><li>'vwi'</li><li>'vwj'</li><li>'vwn'</li><li>'vx'</li><li>'w'</li><li>'w.vlietstra@erasmusmc.nl'</li><li>'w1'</li><li>'w2'</li><li>'w3c'</li><li>'waist'</li><li>'waist-hip'</li><li>'wait'</li><li>'waj'</li><li>'wajh'</li><li>'wajhbj'</li><li>'wajhkj'</li><li>'wake'</li><li>'wal'</li><li>'walk'</li><li>'wang'</li><li>'want'</li><li>'ward'</li><li>'warrant'</li><li>'wast'</li><li>'way2'</li><li>'wd'</li><li>'wdduncan@gmail.com'</li><li>'weak'</li><li>'web-bas'</li><li>'websit'</li><li>'week'</li><li>'wei'</li><li>'weight'</li><li>'well-defin'</li><li>'well-establish'</li><li>'well-form'</li><li>'well-known'</li><li>'well-suit'</li><li>'well-understood'</li><li>'well-vers'</li><li>'whale'</li><li>'whatev'</li><li>'whatizit'</li><li>'whatsoev'</li><li>'wherea'</li><li>'wherebi'</li><li>'whim'</li><li>'whose'</li><li>'wider'</li><li>'wikipedia'</li><li>'wildlif'</li><li>'wilhelm'</li><li>'william'</li><li>'window'</li><li>'wine'</li><li>'wine-produc'</li><li>'wine.owl'</li><li>'wineri'</li><li>'winston'</li><li>'winter'</li><li>'wit'</li><li>'with-'</li><li>'withdraw'</li><li>'withhold'</li><li>'withmerg'</li><li>'wk'</li><li>'wn'</li><li>'wolff'</li><li>'wolff1'</li><li>'woman'</li><li>'women'</li><li>'word-'</li><li>'word-bas'</li><li>'word-embed'</li><li>'word2vec'</li><li>'workaround'</li><li>'workflow'</li><li>'workload'</li><li>'workshop'</li><li>'world'</li><li>'worldwid'</li><li>'wors'</li><li>'worth'</li><li>'worthi'</li><li>'wound'</li><li>'write'</li><li>'writer'</li><li>'wrong'</li><li>'wu'</li><li>'wui'</li><li>'wx'</li><li>'wytz'</li><li>'x'</li><li>'x-'</li><li>'x2'</li><li>'x9'</li><li>'xanax'</li><li>'xi'</li><li>'xiang'</li><li>'xie'</li><li>'xml'</li><li>'xn'</li><li>'xori'</li><li>'xpi'</li><li>'xsd:datetim'</li><li>'xt'</li><li>'xu'</li><li>'xx'</li><li>'y'</li><li>'ya'</li><li>'ye'</li><li>'year'</li><li>'yellow'</li><li>'yes'</li><li>'yesterday'</li><li>'yield'</li><li>'yin1'</li><li>'yo'</li><li>'yoshinobu'</li><li>'youden'</li><li>'young'</li><li>'yp'</li><li>'ys'</li><li>'ysis'</li><li>'yt'</li><li>'yw'</li><li>'yy'</li><li>'yyyi'</li><li>'z'</li><li>'za'</li><li>'ze'</li><li>'zero'</li><li>'zh'</li><li>'zhang'</li><li>'zhu'</li><li>'zhu1'</li><li>'zimmermann3'</li><li>'zip_cod'</li><li>'zipcod'</li><li>'zollinger-'</li><li>'zoloft'</li><li>'zoo'</li><li>'zooanim'</li><li>'zoonot'</li><li>'zweigenbaum'</li><li>'þ'</li></ol>\n",
              "</dd>\n",
              "\t<dt>$docs.removed</dt>\n",
              "\t\t<dd>NULL</dd>\n",
              "\t<dt>$tokens.removed</dt>\n",
              "\t\t<dd>6962</dd>\n",
              "\t<dt>$wordcounts</dt>\n",
              "\t\t<dd><style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>2</li><li>2</li><li>1</li><li>1</li><li>1</li><li>5</li><li>9</li><li>1</li><li>1</li><li>3</li><li>1</li><li>15</li><li>1</li><li>2</li><li>3</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>15</li><li>1</li><li>2</li><li>1</li><li>9</li><li>1</li><li>2</li><li>1</li><li>3</li><li>6</li><li>4</li><li>1</li><li>1</li><li>1</li><li>4</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>4</li><li>1</li><li>1</li><li>5</li><li>2</li><li>3</li><li>10</li><li>1</li><li>2</li><li>6</li><li>2</li><li>7</li><li>1</li><li>1</li><li>2</li><li>12</li><li>5</li><li>1</li><li>2</li><li>12</li><li>8</li><li>2</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>3</li><li>1</li><li>2</li><li>4</li><li>1</li><li>4</li><li>4</li><li>3</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>4</li><li>1</li><li>1</li><li>1</li><li>1</li><li>3</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>2</li><li>1</li><li>1</li><li>5</li><li>1</li><li>1</li><li>3</li><li>1</li><li>⋯</li><li>1</li><li>12</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>12</li><li>5</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>3</li><li>2</li><li>4</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>4</li><li>1</li><li>2</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>13</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>8</li><li>1</li><li>1</li><li>1</li><li>4</li><li>7</li><li>1</li><li>3</li><li>2</li><li>1</li><li>4</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>4</li><li>2</li><li>1</li><li>6</li><li>4</li><li>11</li><li>3</li><li>2</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>3</li><li>1</li><li>1</li><li>1</li><li>6</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>8</li><li>1</li><li>1</li><li>1</li><li>3</li><li>12</li><li>2</li><li>1</li><li>1</li><li>1</li><li>3</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>5</li><li>1</li><li>6</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>4</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>5</li><li>1</li><li>2</li><li>4</li><li>1</li><li>1</li><li>1</li><li>3</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>2</li><li>1</li><li>2</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li></ol>\n",
              "</dd>\n",
              "</dl>\n"
            ],
            "text/markdown": "$documents\n:   $text1\n:   \nA matrix: 2 × 263 of type int\n\n| 1 | 2 | 3 | 4 | 5 | 6 | 8 | 10 | 12 | 13 | ⋯ | 400 | 401 | 402 | 403 | 405 | 409 | 411 | 413 | 415 | 416 |\n| 1 | 1 | 4 | 1 | 1 | 1 | 3 |  8 |  1 | 12 | ⋯ |   4 |   6 |   6 |   1 |   1 |   1 |   1 |   2 |  32 |   1 |\n\n\n$text2\n:   \nA matrix: 2 × 45 of type int\n\n| 2 | 3 | 11 | 24 | 35 | 36 | 39 | 57 | 79 | 86 | ⋯ | 318 | 319 | 351 | 355 | 358 | 365 | 376 | 387 | 390 | 399 |\n| 1 | 1 |  1 |  1 |  1 |  1 |  1 |  3 |  2 |  1 | ⋯ |   1 |   1 |   1 |   1 |   1 |   2 |   1 |   1 |   1 |   1 |\n\n\n$text3\n:   \nA matrix: 2 × 266 of type int\n\n| 2 | 3 | 4 | 6 | 8 | 9 | 10 | 11 | 12 | 13 | ⋯ | 402 | 403 | 407 | 408 | 409 | 411 | 412 | 413 | 415 | 416 |\n| 3 | 2 | 5 | 3 | 5 | 1 |  2 |  1 |  2 | 15 | ⋯ |   1 |   6 |   1 |   1 |   1 |   1 |   2 |   1 |   6 |   3 |\n\n\n$text4\n:   \nA matrix: 2 × 294 of type int\n\n| 1 | 2 | 3 | 5 | 6 | 8 | 9 | 10 | 11 | 12 | ⋯ | 405 | 406 | 407 | 408 | 409 | 410 | 411 | 413 | 414 | 416 |\n| 6 | 1 | 3 | 1 | 1 | 2 | 1 |  1 |  3 |  5 | ⋯ |   2 |  13 |   2 |   1 |   1 |   2 |   3 |   1 |   1 |   2 |\n\n\n$text5\n:   \nA matrix: 2 × 282 of type int\n\n| 2 | 3 | 4 | 5 | 6 | 9 | 10 | 11 | 13 | 14 | ⋯ | 404 | 405 | 407 | 408 | 409 | 410 | 412 | 415 | 416 | 417 |\n| 1 | 4 | 1 | 2 | 6 | 1 |  6 |  2 | 11 |  2 | ⋯ |   1 |   1 |   2 |   1 |   1 |   2 |   1 |  15 |  14 |   1 |\n\n\n$text6\n:   \nA matrix: 2 × 304 of type int\n\n| 2 |  3 | 4 | 7 | 9 | 10 | 12 | 13 | 14 | 15 | ⋯ | 405 | 406 | 407 | 408 | 409 | 410 | 411 | 413 | 414 | 416 |\n| 1 | 16 | 6 | 5 | 2 |  9 |  1 | 15 |  3 |  7 | ⋯ |   2 |   2 |   1 |   1 |   1 |   1 |  10 |   1 |  13 |  10 |\n\n\n$text7\n:   \nA matrix: 2 × 245 of type int\n\n| 2 | 3 | 9 | 11 | 12 | 13 | 20 | 21 | 22 | 23 | ⋯ | 405 | 406 | 407 | 408 | 409 | 412 | 413 | 415 | 416 | 417 |\n| 1 | 8 | 1 |  1 |  2 |  3 | 10 |  1 |  1 |  8 | ⋯ |   6 |   2 |   2 |   1 |   1 |   1 |   1 |   2 |   8 |   1 |\n\n\n$text8\n:   \nA matrix: 2 × 314 of type int\n\n| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | ⋯ | 407 | 408 | 409 | 410 | 411 | 412 | 413 | 414 | 415 | 416 |\n| 4 | 1 | 3 | 2 | 1 | 1 | 1 | 4 | 1 |  2 | ⋯ |   1 |   2 |   1 |   7 |   5 |   3 |   1 |   3 |   1 |  10 |\n\n\n$text9\n:   \nA matrix: 2 × 294 of type int\n\n| 2 | 3 | 4 | 8 | 9 | 10 | 12 | 13 | 15 | 16 | ⋯ | 402 | 403 | 404 | 407 | 408 | 409 | 410 | 413 | 414 | 416 |\n| 2 | 2 | 1 | 3 | 1 |  5 |  1 | 18 |  1 |  1 | ⋯ |   4 |   7 |   1 |   1 |   1 |   1 |   1 |   1 |   4 |  11 |\n\n\n$text10\n:   \nA matrix: 2 × 316 of type int\n\n| 1 | 2 | 3 |  4 | 6 | 7 | 9 | 10 | 12 | 13 | ⋯ | 407 | 408 | 409 | 410 | 412 | 413 | 414 | 415 | 416 | 417 |\n| 3 | 2 | 2 | 11 | 1 | 4 | 1 |  4 |  4 | 17 | ⋯ |   1 |   1 |   1 |   1 |   1 |   1 |   9 |   3 |   7 |   4 |\n\n\n$text11\n:   \nA matrix: 2 × 275 of type int\n\n| 1 | 2 | 3 | 7 | 8 | 9 | 10 | 11 | 13 | 14 | ⋯ | 402 | 403 | 404 | 406 | 407 | 408 | 409 | 415 | 416 | 417 |\n| 1 | 2 | 2 | 2 | 5 | 3 |  8 |  6 | 16 |  1 | ⋯ |   2 |   3 |   1 |   1 |   1 |   2 |   1 |  10 |  12 |   5 |\n\n\n$text12\n:   \nA matrix: 2 × 81 of type int\n\n| 1 | 2 | 3 | 5 | 6 | 7 | 13 | 15 | 19 | 22 | ⋯ | 366 | 375 | 376 | 379 | 382 | 383 | 390 | 394 | 399 | 416 |\n| 1 | 1 | 1 | 1 | 1 | 4 |  1 |  2 |  1 |  1 | ⋯ |   1 |   1 |   1 |   2 |   2 |   6 |   1 |   1 |   1 |   1 |\n\n\n$text13\n:   \nA matrix: 2 × 236 of type int\n\n| 1 | 2 | 3 | 5 | 6 | 9 | 10 | 12 | 13 | 15 | ⋯ | 396 | 398 | 401 | 407 | 408 | 409 | 411 | 412 | 413 | 416 |\n| 2 | 1 | 7 | 1 | 1 | 1 |  2 |  1 |  6 |  1 | ⋯ |   1 |   1 |   4 |   2 |   1 |   1 |   1 |   1 |   2 |   2 |\n\n\n$text14\n:   \nA matrix: 2 × 235 of type int\n\n| 1 | 2 | 3 | 4 | 6 |  8 | 9 | 10 | 13 | 14 | ⋯ | 398 | 402 | 403 | 407 | 408 | 409 | 410 | 413 | 415 | 417 |\n| 1 | 3 | 3 | 1 | 1 | 10 | 1 |  4 | 69 | 20 | ⋯ |   2 |   8 |   2 |   3 |   4 |   1 |   1 |   1 |   1 |   1 |\n\n\n$text15\n:   \nA matrix: 2 × 315 of type int\n\n| 1 | 2 | 3 | 4 | 6 | 7 | 9 | 10 | 11 | 12 | ⋯ | 403 | 405 | 407 | 408 | 409 | 410 | 411 | 413 | 414 | 417 |\n| 3 | 3 | 6 | 2 | 1 | 1 | 1 |  8 |  2 |  4 | ⋯ |  24 |   2 |   2 |   1 |   1 |   4 |   1 |   1 |   1 |   1 |\n\n\n\n\n\n$vocab\n:   1. 'abl'\n2. 'abstract'\n3. 'access'\n4. 'accord'\n5. 'accur'\n6. 'activ'\n7. 'actual'\n8. 'ad'\n9. 'adapt'\n10. 'addit'\n11. 'address'\n12. 'aim'\n13. 'al'\n14. 'algorithm'\n15. 'allow'\n16. 'alreadi'\n17. 'although'\n18. 'amount'\n19. 'analyz'\n20. 'annot'\n21. 'anoth'\n22. 'appli'\n23. 'applic'\n24. 'appropri'\n25. 'area'\n26. 'articl'\n27. 'aspect'\n28. 'assess'\n29. 'assign'\n30. 'assum'\n31. 'ation'\n32. 'attribut'\n33. 'author'\n34. 'autom'\n35. 'automat'\n36. 'avail'\n37. 'averag'\n38. 'b'\n39. 'background'\n40. 'basic'\n41. 'becom'\n42. 'biolog'\n43. 'biomed'\n44. 'built'\n45. 'c'\n46. 'calcul'\n47. 'care'\n48. 'case'\n49. 'categor'\n50. 'categori'\n51. 'caus'\n52. 'challeng'\n53. 'characterist'\n54. 'class'\n55. 'classif'\n56. 'classifi'\n57. 'clinic'\n58. 'close'\n59. 'cluster'\n60. 'code'\n61. 'coher'\n62. 'collect'\n63. 'com-'\n64. 'combin'\n65. 'come'\n66. 'common'\n67. 'communiti'\n68. 'compar'\n69. 'comparison'\n70. 'complex'\n71. 'comput'\n72. 'con-'\n73. 'concept'\n74. 'concern'\n75. 'condit'\n76. 'conduct'\n77. 'consid'\n78. 'consist'\n79. 'construct'\n80. 'contain'\n81. 'content'\n82. 'context'\n83. 'contribut'\n84. 'copi'\n85. 'copyright'\n86. 'core'\n87. 'correct'\n88. 'correspond'\n89. 'countri'\n90. 'cover'\n91. 'creat'\n92. 'creativ'\n93. 'credit'\n94. 'criteria'\n95. 'current'\n96. 'd'\n97. 'data'\n98. 'databas'\n99. 'dataset'\n100. 'de-'\n101. 'dedic'\n102. 'defin'\n103. 'definit'\n104. 'depend'\n105. 'deriv'\n106. 'describ'\n107. 'descript'\n108. 'design'\n109. 'detail'\n110. 'detect'\n111. 'determin'\n112. 'develop'\n113. 'differ-'\n114. 'direct'\n115. 'dis-'\n116. 'discuss'\n117. 'diseas'\n118. 'distinct'\n119. 'distinguish'\n120. 'distribut'\n121. 'document'\n122. 'domain'\n123. 'due'\n124. 'earli'\n125. 'edg'\n126. 'effect'\n127. 'effort'\n128. 'ehr'\n129. 'either'\n130. 'electron'\n131. 'enabl'\n132. 'enc'\n133. 'encod'\n134. 'end'\n135. 'ent'\n136. 'entiti'\n137. 'equal'\n138. 'establish'\n139. 'et'\n140. 'etc'\n141. 'evalu'\n142. 'even'\n143. 'exampl'\n144. 'exceed'\n145. 'exist'\n146. 'expect'\n147. 'experi'\n148. 'expert'\n149. 'explicit'\n150. 'express'\n151. 'extend'\n152. 'extern'\n153. 'extract'\n154. 'featur'\n155. 'field'\n156. 'fig'\n157. 'figur'\n158. 'file'\n159. 'final'\n160. 'focus'\n161. 'form'\n162. 'formal'\n163. 'format'\n164. 'free'\n165. 'full'\n166. 'fulli'\n167. 'function'\n168. 'futur'\n169. 'general'\n170. 'generat'\n171. 'give'\n172. 'health'\n173. 'healthcar'\n174. 'help'\n175. 'histori'\n176. 'holder'\n177. 'hospit'\n178. 'http://creativecommons.org/licenses/by/4.0/.'\n179. 'http://creativecommons.org/publicdomain/zero/1.0/'\n180. 'human'\n181. 'ical'\n182. 'id'\n183. 'identif'\n184. 'identifi'\n185. 'illustr'\n186. 'imag'\n187. 'implement'\n188. 'import'\n189. 'improv'\n190. 'in-'\n191. 'inclus'\n192. 'index'\n193. 'individu'\n194. 'inform'\n195. 'informat'\n196. 'ing'\n197. 'initi'\n198. 'input'\n199. 'instanc'\n200. 'instead'\n201. ⋯\n202. 'languag'\n203. 'lead'\n204. 'learn'\n205. 'level'\n206. 'librari'\n207. 'licenc'\n208. 'licens'\n209. 'limit'\n210. 'line'\n211. 'link'\n212. 'list'\n213. 'literatur'\n214. 'logic'\n215. 'machin'\n216. 'made'\n217. 'main'\n218. 'major'\n219. 'make'\n220. 'manner'\n221. 'manual'\n222. 'map'\n223. 'match'\n224. 'materi'\n225. 'mean'\n226. 'medic'\n227. 'medicin'\n228. 'medium'\n229. 'ment'\n230. 'methodolog'\n231. 'might'\n232. 'model'\n233. 'moreov'\n234. 'multipl'\n235. 'name'\n236. 'nation'\n237. 'natur'\n238. 'network'\n239. 'next'\n240. 'normal'\n241. 'now'\n242. 'number'\n243. 'object'\n244. 'observ'\n245. 'obtain'\n246. 'ontolog'\n247. 'open'\n248. 'oper'\n249. 'optim'\n250. 'option'\n251. 'order'\n252. 'organ'\n253. 'origin'\n254. 'otherwis'\n255. 'outcom'\n256. 'output'\n257. 'overal'\n258. 'page'\n259. 'part'\n260. 'parti'\n261. 'particip'\n262. 'particular'\n263. 'patient'\n264. 'pattern'\n265. 'perform'\n266. 'permiss'\n267. 'permit'\n268. 'person'\n269. 'point'\n270. 'possibl'\n271. 'practic'\n272. 'pre-'\n273. 'precis'\n274. 'predict'\n275. 'present'\n276. 'previous'\n277. 'principl'\n278. 'pro-'\n279. 'problem'\n280. 'process'\n281. 'program'\n282. 'provid'\n283. 'public'\n284. 'publish'\n285. 'purpos'\n286. 'qualiti'\n287. 'queri'\n288. 'r'\n289. 'rang'\n290. 'reason'\n291. 'receiv'\n292. 'recent'\n293. 'record'\n294. 'refer'\n295. 'regul'\n296. 'relat'\n297. 'relev'\n298. 'remain'\n299. 'repres'\n300. 'represent'\n301. 'reproduct'\n302. 'requir'\n303. 'resourc'\n304. 'respect'\n305. 'restrict'\n306. 'result'\n307. 'review'\n308. 'risk'\n309. 'role'\n310. 's'\n311. 'scienc'\n312. 'scientif'\n313. 'score'\n314. 'search'\n315. 'section'\n316. 'see'\n317. 'select'\n318. 'semant'\n319. 'sens'\n320. 'sent'\n321. 'separ'\n322. 'set'\n323. 'share'\n324. 'show'\n325. 'shown'\n326. 'sign'\n327. 'similar'\n328. 'sinc'\n329. 'singl'\n330. 'size'\n331. 'sourc'\n332. 'specif'\n333. 'specifi'\n334. 'standard'\n335. 'start'\n336. 'state'\n337. 'statist'\n338. 'statutori'\n339. 'step'\n340. 'still'\n341. 'store'\n342. 'structur'\n343. 'subclass'\n344. 'subject'\n345. 'subset'\n346. 'suggest'\n347. 'suitabl'\n348. 'support'\n349. 'symptom'\n350. 'system'\n351. 't'\n352. 'tabl'\n353. 'take'\n354. 'target'\n355. 'task'\n356. 'technolog'\n357. 'term'\n358. 'terminolog'\n359. 'test'\n360. 'text'\n361. 'therefor'\n362. 'thus'\n363. 'time'\n364. 'tion'\n365. 'tive'\n366. 'tool'\n367. 'topic'\n368. 'tor'\n369. 'total'\n370. 'train'\n371. 'treatment'\n372. 'type'\n373. 'typic'\n374. 'under'\n375. 'understand'\n376. 'uniqu'\n377. 'unit'\n378. 'univers'\n379. 'unless'\n380. 'updat'\n381. 'upon'\n382. 'usag'\n383. 'use'\n384. 'usual'\n385. 'util'\n386. 'valid'\n387. 'valu'\n388. 'variat'\n389. 'various'\n390. 'version'\n391. 'view'\n392. 'visit'\n393. 'waiver'\n394. 'way'\n395. 'web'\n396. 'whole'\n397. 'wide'\n398. 'without'\n399. 'word'\n400. 'work'\n401. 'written'\n\n\n\n$meta\n:   \nA data.frame: 15 × 0\n\n| 1 |\n| 2 |\n| 3 |\n| 4 |\n| 5 |\n| 6 |\n| 7 |\n| 8 |\n| 9 |\n| 10 |\n| 11 |\n| 12 |\n| 13 |\n| 14 |\n| 15 |\n\n\n$words.removed\n:   1. '-0.42'\n2. '-1'\n3. '-2'\n4. '-9'\n5. '-b'\n6. '-base'\n7. '-c'\n8. '-e'\n9. '-fr'\n10. '-h'\n11. '-ii'\n12. '-j'\n13. '-n'\n14. '-s'\n15. '-t'\n16. '-w'\n17. '0-'\n18. '1-3'\n19. '1-4'\n20. '1.7m'\n21. '10-15'\n22. '10-fold'\n23. '10th'\n24. '1558-6'\n25. '1a'\n26. '1amsterdam'\n27. '1cnrs'\n28. '1comput'\n29. '1depart'\n30. '1divis'\n31. '1faculti'\n32. '1http://www.w3.org/tr/2003/pr-owl-guide-20031209/win'\n33. '1https://www.atilika.com/ja/kuromoji/'\n34. '1https://www.healthboards.com/'\n35. '1https://www.ncbi.nlm.nih.gov/pubm'\n36. '1in'\n37. '1informatik'\n38. '1institut'\n39. '1k'\n40. '1like'\n41. '1nation'\n42. '1sourc'\n43. '1space'\n44. '1univers'\n45. '2-3'\n46. '2012-2015'\n47. '23955-6900'\n48. '26-11-2018'\n49. '2a'\n50. '2b'\n51. '2c'\n52. '2castor'\n53. '2center'\n54. '2comput'\n55. '2d'\n56. '2here'\n57. '2http://mallet.cs.umass.edu/'\n58. '2http://ontofox.hegroup.org'\n59. '2http://xiphoid.biostr.washington.edu/fma/shirt_ontology/shirt_ontology_1.'\n60. '2https://hal.archives-ouvertes.fr/'\n61. '2https://medhelp.org/'\n62. '2nd'\n63. '2univ'\n64. '3-5-1'\n65. '3d'\n66. '3hereinaft'\n67. '3http://purl.org/sig/ont/fma/fma290055'\n68. '3http://www.workingontologist.org/examples.zip'\n69. '3https://www.istex.fr/'\n70. '432-8011'\n71. '4a'\n72. '4a3'\n73. '4http://www.cl.ecei.tohoku.ac.jp/~m~suzuki/jawiki_vector/'\n74. '4https://www.i2b2.org/nlp/datasets/main.php'\n75. '4recent'\n76. '5https://github.com/johokugsk'\n77. '5https://n2c2.dbmi.hms.harvard.edu/'\n78. '5in'\n79. '60gb'\n80. '64-year-old'\n81. '6https://sites.google.com/site/shareclefehealth/'\n82. '7http://www.cepidc.inserm.fr/'\n83. '8-core'\n84. '86-year-old'\n85. '8gb'\n86. '8https://knowledge-learning.github.io/ehealthkd-2019'\n87. '9c'\n88. 'a.m'\n89. 'a1'\n90. 'a2'\n91. 'aachen'\n92. 'ab'\n93. 'abbrevi'\n94. 'abdelhakim1,2'\n95. 'abdomin'\n96. 'abdullah'\n97. 'abil'\n98. 'ablat'\n99. 'abli'\n100. 'abnorm'\n101. 'absenc'\n102. 'abu-hanna1'\n103. 'ac'\n104. 'ac-'\n105. 'acad-'\n106. 'academ'\n107. 'academicstaff'\n108. 'acceler'\n109. 'accept'\n110. 'accom-'\n111. 'accommod'\n112. 'accomplish'\n113. 'accordingly.w'\n114. 'account'\n115. 'accumu-'\n116. 'accumul'\n117. 'accuraci'\n118. 'aceruloplasminemia'\n119. 'ach'\n120. 'acharya8'\n121. 'achiev'\n122. 'acknow-'\n123. 'acknowl-'\n124. 'acknowledg'\n125. 'acl'\n126. 'acm'\n127. 'acquir'\n128. 'acquisit'\n129. 'acroparesthesia'\n130. 'act'\n131. 'acterist'\n132. 'action'\n133. 'activ-'\n134. 'actor'\n135. 'acut'\n136. 'ad-'\n137. 'ad-hoc'\n138. 'adam'\n139. 'add'\n140. 'add-'\n141. 'addi-'\n142. 'adequ'\n143. 'adher'\n144. 'adisinsight'\n145. 'adjac'\n146. 'adjacentregion'\n147. 'adjust'\n148. 'admin-'\n149. 'administr'\n150. 'admission_num'\n151. 'admit'\n152. 'adopt'\n153. 'adr'\n154. 'adult'\n155. 'advanc'\n156. 'advantag'\n157. 'advers'\n158. 'adverse-drug'\n159. 'advic'\n160. 'advis'\n161. 'aesthet'\n162. 'af'\n163. 'affect'\n164. 'affili'\n165. 'affili-'\n166. 'affilia-'\n167. 'afford'\n168. 'aforement'\n169. 'africa'\n170. 'african'\n171. 'africanwildlif'\n172. 'africanwildlifeontology.xml'\n173. 'africanwildlifeontology0'\n174. 'africanwildlifeontology1'\n175. 'africanwildlifeontology1a.owl'\n176. 'africanwildlifeontology2'\n177. 'africanwildlifeontology2a.owl'\n178. 'africanwildlifeontology3'\n179. 'africanwildlifeontologyweb.owl'\n180. 'afrikaan'\n181. 'afterward'\n182. 'ag'\n183. 'ag-'\n184. 'agar-'\n185. 'age'\n186. 'agenc'\n187. 'agent'\n188. 'aggreg'\n189. 'aggress'\n190. 'agnost'\n191. 'ago'\n192. 'agre'\n193. 'agreement'\n194. 'ah'\n195. 'ahornstr'\n196. 'ai'\n197. 'aid'\n198. 'airplan'\n199. 'airway'\n200. 'ak'\n201. ⋯\n202. 'vertigo'\n203. 'vesilinnanti'\n204. 'vessel'\n205. 'vhnguyen@u.nus.edu'\n206. 'vi'\n207. 'viani1'\n208. 'viation'\n209. 'vice'\n210. 'vide'\n211. 'video'\n212. 'vider'\n213. 'vidual'\n214. 'view-'\n215. 'viewer'\n216. 'viewpoint'\n217. 'vii'\n218. 'vincent'\n219. 'violat'\n220. 'vious'\n221. 'viral'\n222. 'virtual'\n223. 'virus'\n224. 'vis-'\n225. 'visibl'\n226. 'vision'\n227. 'visual'\n228. 'visual-'\n229. 'visualis'\n230. 'vital'\n231. 'vitamin'\n232. 'vival'\n233. 'vlietstra'\n234. 'vlietstra1'\n235. 'vo'\n236. 'vocabu-'\n237. 'vocabulari'\n238. 'voic'\n239. 'volum'\n240. 'volv'\n241. 'vomit'\n242. 'vos'\n243. 'vos-'\n244. 'vos1,2'\n245. 'vosview'\n246. 'vous'\n247. 'vp'\n248. 'vpi'\n249. 'vpui'\n250. 'vs'\n251. 'vt'\n252. 'vu'\n253. 'vui'\n254. 'vw1'\n255. 'vw2'\n256. 'vwi'\n257. 'vwj'\n258. 'vwn'\n259. 'vx'\n260. 'w'\n261. 'w.vlietstra@erasmusmc.nl'\n262. 'w1'\n263. 'w2'\n264. 'w3c'\n265. 'waist'\n266. 'waist-hip'\n267. 'wait'\n268. 'waj'\n269. 'wajh'\n270. 'wajhbj'\n271. 'wajhkj'\n272. 'wake'\n273. 'wal'\n274. 'walk'\n275. 'wang'\n276. 'want'\n277. 'ward'\n278. 'warrant'\n279. 'wast'\n280. 'way2'\n281. 'wd'\n282. 'wdduncan@gmail.com'\n283. 'weak'\n284. 'web-bas'\n285. 'websit'\n286. 'week'\n287. 'wei'\n288. 'weight'\n289. 'well-defin'\n290. 'well-establish'\n291. 'well-form'\n292. 'well-known'\n293. 'well-suit'\n294. 'well-understood'\n295. 'well-vers'\n296. 'whale'\n297. 'whatev'\n298. 'whatizit'\n299. 'whatsoev'\n300. 'wherea'\n301. 'wherebi'\n302. 'whim'\n303. 'whose'\n304. 'wider'\n305. 'wikipedia'\n306. 'wildlif'\n307. 'wilhelm'\n308. 'william'\n309. 'window'\n310. 'wine'\n311. 'wine-produc'\n312. 'wine.owl'\n313. 'wineri'\n314. 'winston'\n315. 'winter'\n316. 'wit'\n317. 'with-'\n318. 'withdraw'\n319. 'withhold'\n320. 'withmerg'\n321. 'wk'\n322. 'wn'\n323. 'wolff'\n324. 'wolff1'\n325. 'woman'\n326. 'women'\n327. 'word-'\n328. 'word-bas'\n329. 'word-embed'\n330. 'word2vec'\n331. 'workaround'\n332. 'workflow'\n333. 'workload'\n334. 'workshop'\n335. 'world'\n336. 'worldwid'\n337. 'wors'\n338. 'worth'\n339. 'worthi'\n340. 'wound'\n341. 'write'\n342. 'writer'\n343. 'wrong'\n344. 'wu'\n345. 'wui'\n346. 'wx'\n347. 'wytz'\n348. 'x'\n349. 'x-'\n350. 'x2'\n351. 'x9'\n352. 'xanax'\n353. 'xi'\n354. 'xiang'\n355. 'xie'\n356. 'xml'\n357. 'xn'\n358. 'xori'\n359. 'xpi'\n360. 'xsd:datetim'\n361. 'xt'\n362. 'xu'\n363. 'xx'\n364. 'y'\n365. 'ya'\n366. 'ye'\n367. 'year'\n368. 'yellow'\n369. 'yes'\n370. 'yesterday'\n371. 'yield'\n372. 'yin1'\n373. 'yo'\n374. 'yoshinobu'\n375. 'youden'\n376. 'young'\n377. 'yp'\n378. 'ys'\n379. 'ysis'\n380. 'yt'\n381. 'yw'\n382. 'yy'\n383. 'yyyi'\n384. 'z'\n385. 'za'\n386. 'ze'\n387. 'zero'\n388. 'zh'\n389. 'zhang'\n390. 'zhu'\n391. 'zhu1'\n392. 'zimmermann3'\n393. 'zip_cod'\n394. 'zipcod'\n395. 'zollinger-'\n396. 'zoloft'\n397. 'zoo'\n398. 'zooanim'\n399. 'zoonot'\n400. 'zweigenbaum'\n401. 'þ'\n\n\n\n$docs.removed\n:   NULL\n$tokens.removed\n:   6962\n$wordcounts\n:   1. 1\n2. 1\n3. 1\n4. 1\n5. 1\n6. 1\n7. 1\n8. 1\n9. 1\n10. 1\n11. 1\n12. 1\n13. 1\n14. 2\n15. 1\n16. 1\n17. 1\n18. 1\n19. 1\n20. 1\n21. 1\n22. 2\n23. 1\n24. 1\n25. 1\n26. 1\n27. 1\n28. 1\n29. 2\n30. 1\n31. 1\n32. 1\n33. 1\n34. 1\n35. 1\n36. 1\n37. 1\n38. 1\n39. 1\n40. 1\n41. 1\n42. 1\n43. 1\n44. 1\n45. 1\n46. 1\n47. 1\n48. 1\n49. 1\n50. 1\n51. 1\n52. 1\n53. 1\n54. 2\n55. 1\n56. 1\n57. 1\n58. 1\n59. 1\n60. 1\n61. 1\n62. 1\n63. 1\n64. 1\n65. 1\n66. 1\n67. 1\n68. 1\n69. 1\n70. 1\n71. 1\n72. 1\n73. 1\n74. 1\n75. 1\n76. 1\n77. 1\n78. 1\n79. 1\n80. 1\n81. 1\n82. 1\n83. 1\n84. 1\n85. 1\n86. 1\n87. 1\n88. 1\n89. 2\n90. 1\n91. 1\n92. 2\n93. 2\n94. 1\n95. 1\n96. 1\n97. 5\n98. 9\n99. 1\n100. 1\n101. 3\n102. 1\n103. 15\n104. 1\n105. 2\n106. 3\n107. 1\n108. 1\n109. 1\n110. 2\n111. 1\n112. 15\n113. 1\n114. 2\n115. 1\n116. 9\n117. 1\n118. 2\n119. 1\n120. 3\n121. 6\n122. 4\n123. 1\n124. 1\n125. 1\n126. 4\n127. 1\n128. 1\n129. 1\n130. 1\n131. 1\n132. 4\n133. 1\n134. 1\n135. 5\n136. 2\n137. 3\n138. 10\n139. 1\n140. 2\n141. 6\n142. 2\n143. 7\n144. 1\n145. 1\n146. 2\n147. 12\n148. 5\n149. 1\n150. 2\n151. 12\n152. 8\n153. 2\n154. 2\n155. 1\n156. 1\n157. 1\n158. 1\n159. 1\n160. 3\n161. 1\n162. 2\n163. 4\n164. 1\n165. 4\n166. 4\n167. 3\n168. 1\n169. 1\n170. 1\n171. 1\n172. 1\n173. 1\n174. 4\n175. 1\n176. 1\n177. 1\n178. 1\n179. 3\n180. 1\n181. 1\n182. 1\n183. 1\n184. 1\n185. 1\n186. 1\n187. 1\n188. 1\n189. 1\n190. 1\n191. 1\n192. 2\n193. 2\n194. 1\n195. 1\n196. 5\n197. 1\n198. 1\n199. 3\n200. 1\n201. ⋯\n202. 1\n203. 12\n204. 1\n205. 1\n206. 1\n207. 1\n208. 1\n209. 1\n210. 1\n211. 1\n212. 1\n213. 1\n214. 1\n215. 1\n216. 1\n217. 12\n218. 5\n219. 1\n220. 1\n221. 2\n222. 1\n223. 1\n224. 1\n225. 1\n226. 1\n227. 1\n228. 3\n229. 2\n230. 4\n231. 1\n232. 1\n233. 1\n234. 1\n235. 1\n236. 1\n237. 1\n238. 1\n239. 1\n240. 1\n241. 1\n242. 1\n243. 2\n244. 1\n245. 1\n246. 1\n247. 1\n248. 1\n249. 1\n250. 1\n251. 4\n252. 1\n253. 2\n254. 1\n255. 2\n256. 1\n257. 1\n258. 1\n259. 13\n260. 1\n261. 1\n262. 1\n263. 1\n264. 1\n265. 1\n266. 1\n267. 1\n268. 1\n269. 2\n270. 1\n271. 1\n272. 8\n273. 1\n274. 1\n275. 1\n276. 4\n277. 7\n278. 1\n279. 3\n280. 2\n281. 1\n282. 4\n283. 1\n284. 1\n285. 1\n286. 2\n287. 1\n288. 1\n289. 1\n290. 1\n291. 1\n292. 1\n293. 1\n294. 4\n295. 2\n296. 1\n297. 6\n298. 4\n299. 11\n300. 3\n301. 2\n302. 1\n303. 1\n304. 1\n305. 2\n306. 1\n307. 1\n308. 1\n309. 1\n310. 1\n311. 1\n312. 1\n313. 3\n314. 1\n315. 1\n316. 1\n317. 6\n318. 1\n319. 1\n320. 1\n321. 1\n322. 1\n323. 2\n324. 8\n325. 1\n326. 1\n327. 1\n328. 3\n329. 12\n330. 2\n331. 1\n332. 1\n333. 1\n334. 3\n335. 1\n336. 2\n337. 1\n338. 1\n339. 1\n340. 5\n341. 1\n342. 6\n343. 1\n344. 1\n345. 1\n346. 1\n347. 1\n348. 4\n349. 1\n350. 1\n351. 1\n352. 1\n353. 1\n354. 1\n355. 1\n356. 1\n357. 1\n358. 1\n359. 1\n360. 1\n361. 2\n362. 1\n363. 1\n364. 5\n365. 1\n366. 2\n367. 4\n368. 1\n369. 1\n370. 1\n371. 3\n372. 1\n373. 1\n374. 1\n375. 1\n376. 2\n377. 1\n378. 1\n379. 1\n380. 1\n381. 1\n382. 1\n383. 1\n384. 2\n385. 1\n386. 2\n387. 1\n388. 1\n389. 1\n390. 1\n391. 1\n392. 1\n393. 1\n394. 1\n395. 1\n396. 1\n397. 1\n398. 1\n399. 1\n400. 1\n401. 1\n\n\n\n\n\n",
            "text/latex": "\\begin{description}\n\\item[\\$documents] \\begin{description}\n\\item[\\$text1] A matrix: 2 × 263 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 4 & 5 & 6 & 8 & 10 & 12 & 13 & ⋯ & 400 & 401 & 402 & 403 & 405 & 409 & 411 & 413 & 415 & 416\\\\\n\t 1 & 1 & 4 & 1 & 1 & 1 & 3 &  8 &  1 & 12 & ⋯ &   4 &   6 &   6 &   1 &   1 &   1 &   1 &   2 &  32 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text2] A matrix: 2 × 45 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 11 & 24 & 35 & 36 & 39 & 57 & 79 & 86 & ⋯ & 318 & 319 & 351 & 355 & 358 & 365 & 376 & 387 & 390 & 399\\\\\n\t 1 & 1 &  1 &  1 &  1 &  1 &  1 &  3 &  2 &  1 & ⋯ &   1 &   1 &   1 &   1 &   1 &   2 &   1 &   1 &   1 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text3] A matrix: 2 × 266 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 4 & 6 & 8 & 9 & 10 & 11 & 12 & 13 & ⋯ & 402 & 403 & 407 & 408 & 409 & 411 & 412 & 413 & 415 & 416\\\\\n\t 3 & 2 & 5 & 3 & 5 & 1 &  2 &  1 &  2 & 15 & ⋯ &   1 &   6 &   1 &   1 &   1 &   1 &   2 &   1 &   6 &   3\\\\\n\\end{tabular}\n\n\\item[\\$text4] A matrix: 2 × 294 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 5 & 6 & 8 & 9 & 10 & 11 & 12 & ⋯ & 405 & 406 & 407 & 408 & 409 & 410 & 411 & 413 & 414 & 416\\\\\n\t 6 & 1 & 3 & 1 & 1 & 2 & 1 &  1 &  3 &  5 & ⋯ &   2 &  13 &   2 &   1 &   1 &   2 &   3 &   1 &   1 &   2\\\\\n\\end{tabular}\n\n\\item[\\$text5] A matrix: 2 × 282 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 4 & 5 & 6 & 9 & 10 & 11 & 13 & 14 & ⋯ & 404 & 405 & 407 & 408 & 409 & 410 & 412 & 415 & 416 & 417\\\\\n\t 1 & 4 & 1 & 2 & 6 & 1 &  6 &  2 & 11 &  2 & ⋯ &   1 &   1 &   2 &   1 &   1 &   2 &   1 &  15 &  14 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text6] A matrix: 2 × 304 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 &  3 & 4 & 7 & 9 & 10 & 12 & 13 & 14 & 15 & ⋯ & 405 & 406 & 407 & 408 & 409 & 410 & 411 & 413 & 414 & 416\\\\\n\t 1 & 16 & 6 & 5 & 2 &  9 &  1 & 15 &  3 &  7 & ⋯ &   2 &   2 &   1 &   1 &   1 &   1 &  10 &   1 &  13 &  10\\\\\n\\end{tabular}\n\n\\item[\\$text7] A matrix: 2 × 245 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 9 & 11 & 12 & 13 & 20 & 21 & 22 & 23 & ⋯ & 405 & 406 & 407 & 408 & 409 & 412 & 413 & 415 & 416 & 417\\\\\n\t 1 & 8 & 1 &  1 &  2 &  3 & 10 &  1 &  1 &  8 & ⋯ &   6 &   2 &   2 &   1 &   1 &   1 &   1 &   2 &   8 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text8] A matrix: 2 × 314 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & ⋯ & 407 & 408 & 409 & 410 & 411 & 412 & 413 & 414 & 415 & 416\\\\\n\t 4 & 1 & 3 & 2 & 1 & 1 & 1 & 4 & 1 &  2 & ⋯ &   1 &   2 &   1 &   7 &   5 &   3 &   1 &   3 &   1 &  10\\\\\n\\end{tabular}\n\n\\item[\\$text9] A matrix: 2 × 294 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 4 & 8 & 9 & 10 & 12 & 13 & 15 & 16 & ⋯ & 402 & 403 & 404 & 407 & 408 & 409 & 410 & 413 & 414 & 416\\\\\n\t 2 & 2 & 1 & 3 & 1 &  5 &  1 & 18 &  1 &  1 & ⋯ &   4 &   7 &   1 &   1 &   1 &   1 &   1 &   1 &   4 &  11\\\\\n\\end{tabular}\n\n\\item[\\$text10] A matrix: 2 × 316 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 &  4 & 6 & 7 & 9 & 10 & 12 & 13 & ⋯ & 407 & 408 & 409 & 410 & 412 & 413 & 414 & 415 & 416 & 417\\\\\n\t 3 & 2 & 2 & 11 & 1 & 4 & 1 &  4 &  4 & 17 & ⋯ &   1 &   1 &   1 &   1 &   1 &   1 &   9 &   3 &   7 &   4\\\\\n\\end{tabular}\n\n\\item[\\$text11] A matrix: 2 × 275 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 7 & 8 & 9 & 10 & 11 & 13 & 14 & ⋯ & 402 & 403 & 404 & 406 & 407 & 408 & 409 & 415 & 416 & 417\\\\\n\t 1 & 2 & 2 & 2 & 5 & 3 &  8 &  6 & 16 &  1 & ⋯ &   2 &   3 &   1 &   1 &   1 &   2 &   1 &  10 &  12 &   5\\\\\n\\end{tabular}\n\n\\item[\\$text12] A matrix: 2 × 81 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 5 & 6 & 7 & 13 & 15 & 19 & 22 & ⋯ & 366 & 375 & 376 & 379 & 382 & 383 & 390 & 394 & 399 & 416\\\\\n\t 1 & 1 & 1 & 1 & 1 & 4 &  1 &  2 &  1 &  1 & ⋯ &   1 &   1 &   1 &   2 &   2 &   6 &   1 &   1 &   1 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text13] A matrix: 2 × 236 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 5 & 6 & 9 & 10 & 12 & 13 & 15 & ⋯ & 396 & 398 & 401 & 407 & 408 & 409 & 411 & 412 & 413 & 416\\\\\n\t 2 & 1 & 7 & 1 & 1 & 1 &  2 &  1 &  6 &  1 & ⋯ &   1 &   1 &   4 &   2 &   1 &   1 &   1 &   1 &   2 &   2\\\\\n\\end{tabular}\n\n\\item[\\$text14] A matrix: 2 × 235 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 4 & 6 &  8 & 9 & 10 & 13 & 14 & ⋯ & 398 & 402 & 403 & 407 & 408 & 409 & 410 & 413 & 415 & 417\\\\\n\t 1 & 3 & 3 & 1 & 1 & 10 & 1 &  4 & 69 & 20 & ⋯ &   2 &   8 &   2 &   3 &   4 &   1 &   1 &   1 &   1 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text15] A matrix: 2 × 315 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 4 & 6 & 7 & 9 & 10 & 11 & 12 & ⋯ & 403 & 405 & 407 & 408 & 409 & 410 & 411 & 413 & 414 & 417\\\\\n\t 3 & 3 & 6 & 2 & 1 & 1 & 1 &  8 &  2 &  4 & ⋯ &  24 &   2 &   2 &   1 &   1 &   4 &   1 &   1 &   1 &   1\\\\\n\\end{tabular}\n\n\\end{description}\n\n\\item[\\$vocab] \\begin{enumerate*}\n\\item 'abl'\n\\item 'abstract'\n\\item 'access'\n\\item 'accord'\n\\item 'accur'\n\\item 'activ'\n\\item 'actual'\n\\item 'ad'\n\\item 'adapt'\n\\item 'addit'\n\\item 'address'\n\\item 'aim'\n\\item 'al'\n\\item 'algorithm'\n\\item 'allow'\n\\item 'alreadi'\n\\item 'although'\n\\item 'amount'\n\\item 'analyz'\n\\item 'annot'\n\\item 'anoth'\n\\item 'appli'\n\\item 'applic'\n\\item 'appropri'\n\\item 'area'\n\\item 'articl'\n\\item 'aspect'\n\\item 'assess'\n\\item 'assign'\n\\item 'assum'\n\\item 'ation'\n\\item 'attribut'\n\\item 'author'\n\\item 'autom'\n\\item 'automat'\n\\item 'avail'\n\\item 'averag'\n\\item 'b'\n\\item 'background'\n\\item 'basic'\n\\item 'becom'\n\\item 'biolog'\n\\item 'biomed'\n\\item 'built'\n\\item 'c'\n\\item 'calcul'\n\\item 'care'\n\\item 'case'\n\\item 'categor'\n\\item 'categori'\n\\item 'caus'\n\\item 'challeng'\n\\item 'characterist'\n\\item 'class'\n\\item 'classif'\n\\item 'classifi'\n\\item 'clinic'\n\\item 'close'\n\\item 'cluster'\n\\item 'code'\n\\item 'coher'\n\\item 'collect'\n\\item 'com-'\n\\item 'combin'\n\\item 'come'\n\\item 'common'\n\\item 'communiti'\n\\item 'compar'\n\\item 'comparison'\n\\item 'complex'\n\\item 'comput'\n\\item 'con-'\n\\item 'concept'\n\\item 'concern'\n\\item 'condit'\n\\item 'conduct'\n\\item 'consid'\n\\item 'consist'\n\\item 'construct'\n\\item 'contain'\n\\item 'content'\n\\item 'context'\n\\item 'contribut'\n\\item 'copi'\n\\item 'copyright'\n\\item 'core'\n\\item 'correct'\n\\item 'correspond'\n\\item 'countri'\n\\item 'cover'\n\\item 'creat'\n\\item 'creativ'\n\\item 'credit'\n\\item 'criteria'\n\\item 'current'\n\\item 'd'\n\\item 'data'\n\\item 'databas'\n\\item 'dataset'\n\\item 'de-'\n\\item 'dedic'\n\\item 'defin'\n\\item 'definit'\n\\item 'depend'\n\\item 'deriv'\n\\item 'describ'\n\\item 'descript'\n\\item 'design'\n\\item 'detail'\n\\item 'detect'\n\\item 'determin'\n\\item 'develop'\n\\item 'differ-'\n\\item 'direct'\n\\item 'dis-'\n\\item 'discuss'\n\\item 'diseas'\n\\item 'distinct'\n\\item 'distinguish'\n\\item 'distribut'\n\\item 'document'\n\\item 'domain'\n\\item 'due'\n\\item 'earli'\n\\item 'edg'\n\\item 'effect'\n\\item 'effort'\n\\item 'ehr'\n\\item 'either'\n\\item 'electron'\n\\item 'enabl'\n\\item 'enc'\n\\item 'encod'\n\\item 'end'\n\\item 'ent'\n\\item 'entiti'\n\\item 'equal'\n\\item 'establish'\n\\item 'et'\n\\item 'etc'\n\\item 'evalu'\n\\item 'even'\n\\item 'exampl'\n\\item 'exceed'\n\\item 'exist'\n\\item 'expect'\n\\item 'experi'\n\\item 'expert'\n\\item 'explicit'\n\\item 'express'\n\\item 'extend'\n\\item 'extern'\n\\item 'extract'\n\\item 'featur'\n\\item 'field'\n\\item 'fig'\n\\item 'figur'\n\\item 'file'\n\\item 'final'\n\\item 'focus'\n\\item 'form'\n\\item 'formal'\n\\item 'format'\n\\item 'free'\n\\item 'full'\n\\item 'fulli'\n\\item 'function'\n\\item 'futur'\n\\item 'general'\n\\item 'generat'\n\\item 'give'\n\\item 'health'\n\\item 'healthcar'\n\\item 'help'\n\\item 'histori'\n\\item 'holder'\n\\item 'hospit'\n\\item 'http://creativecommons.org/licenses/by/4.0/.'\n\\item 'http://creativecommons.org/publicdomain/zero/1.0/'\n\\item 'human'\n\\item 'ical'\n\\item 'id'\n\\item 'identif'\n\\item 'identifi'\n\\item 'illustr'\n\\item 'imag'\n\\item 'implement'\n\\item 'import'\n\\item 'improv'\n\\item 'in-'\n\\item 'inclus'\n\\item 'index'\n\\item 'individu'\n\\item 'inform'\n\\item 'informat'\n\\item 'ing'\n\\item 'initi'\n\\item 'input'\n\\item 'instanc'\n\\item 'instead'\n\\item ⋯\n\\item 'languag'\n\\item 'lead'\n\\item 'learn'\n\\item 'level'\n\\item 'librari'\n\\item 'licenc'\n\\item 'licens'\n\\item 'limit'\n\\item 'line'\n\\item 'link'\n\\item 'list'\n\\item 'literatur'\n\\item 'logic'\n\\item 'machin'\n\\item 'made'\n\\item 'main'\n\\item 'major'\n\\item 'make'\n\\item 'manner'\n\\item 'manual'\n\\item 'map'\n\\item 'match'\n\\item 'materi'\n\\item 'mean'\n\\item 'medic'\n\\item 'medicin'\n\\item 'medium'\n\\item 'ment'\n\\item 'methodolog'\n\\item 'might'\n\\item 'model'\n\\item 'moreov'\n\\item 'multipl'\n\\item 'name'\n\\item 'nation'\n\\item 'natur'\n\\item 'network'\n\\item 'next'\n\\item 'normal'\n\\item 'now'\n\\item 'number'\n\\item 'object'\n\\item 'observ'\n\\item 'obtain'\n\\item 'ontolog'\n\\item 'open'\n\\item 'oper'\n\\item 'optim'\n\\item 'option'\n\\item 'order'\n\\item 'organ'\n\\item 'origin'\n\\item 'otherwis'\n\\item 'outcom'\n\\item 'output'\n\\item 'overal'\n\\item 'page'\n\\item 'part'\n\\item 'parti'\n\\item 'particip'\n\\item 'particular'\n\\item 'patient'\n\\item 'pattern'\n\\item 'perform'\n\\item 'permiss'\n\\item 'permit'\n\\item 'person'\n\\item 'point'\n\\item 'possibl'\n\\item 'practic'\n\\item 'pre-'\n\\item 'precis'\n\\item 'predict'\n\\item 'present'\n\\item 'previous'\n\\item 'principl'\n\\item 'pro-'\n\\item 'problem'\n\\item 'process'\n\\item 'program'\n\\item 'provid'\n\\item 'public'\n\\item 'publish'\n\\item 'purpos'\n\\item 'qualiti'\n\\item 'queri'\n\\item 'r'\n\\item 'rang'\n\\item 'reason'\n\\item 'receiv'\n\\item 'recent'\n\\item 'record'\n\\item 'refer'\n\\item 'regul'\n\\item 'relat'\n\\item 'relev'\n\\item 'remain'\n\\item 'repres'\n\\item 'represent'\n\\item 'reproduct'\n\\item 'requir'\n\\item 'resourc'\n\\item 'respect'\n\\item 'restrict'\n\\item 'result'\n\\item 'review'\n\\item 'risk'\n\\item 'role'\n\\item 's'\n\\item 'scienc'\n\\item 'scientif'\n\\item 'score'\n\\item 'search'\n\\item 'section'\n\\item 'see'\n\\item 'select'\n\\item 'semant'\n\\item 'sens'\n\\item 'sent'\n\\item 'separ'\n\\item 'set'\n\\item 'share'\n\\item 'show'\n\\item 'shown'\n\\item 'sign'\n\\item 'similar'\n\\item 'sinc'\n\\item 'singl'\n\\item 'size'\n\\item 'sourc'\n\\item 'specif'\n\\item 'specifi'\n\\item 'standard'\n\\item 'start'\n\\item 'state'\n\\item 'statist'\n\\item 'statutori'\n\\item 'step'\n\\item 'still'\n\\item 'store'\n\\item 'structur'\n\\item 'subclass'\n\\item 'subject'\n\\item 'subset'\n\\item 'suggest'\n\\item 'suitabl'\n\\item 'support'\n\\item 'symptom'\n\\item 'system'\n\\item 't'\n\\item 'tabl'\n\\item 'take'\n\\item 'target'\n\\item 'task'\n\\item 'technolog'\n\\item 'term'\n\\item 'terminolog'\n\\item 'test'\n\\item 'text'\n\\item 'therefor'\n\\item 'thus'\n\\item 'time'\n\\item 'tion'\n\\item 'tive'\n\\item 'tool'\n\\item 'topic'\n\\item 'tor'\n\\item 'total'\n\\item 'train'\n\\item 'treatment'\n\\item 'type'\n\\item 'typic'\n\\item 'under'\n\\item 'understand'\n\\item 'uniqu'\n\\item 'unit'\n\\item 'univers'\n\\item 'unless'\n\\item 'updat'\n\\item 'upon'\n\\item 'usag'\n\\item 'use'\n\\item 'usual'\n\\item 'util'\n\\item 'valid'\n\\item 'valu'\n\\item 'variat'\n\\item 'various'\n\\item 'version'\n\\item 'view'\n\\item 'visit'\n\\item 'waiver'\n\\item 'way'\n\\item 'web'\n\\item 'whole'\n\\item 'wide'\n\\item 'without'\n\\item 'word'\n\\item 'work'\n\\item 'written'\n\\end{enumerate*}\n\n\\item[\\$meta] A data.frame: 15 × 0\n\\begin{tabular}{r|}\n\t1\\\\\n\t2\\\\\n\t3\\\\\n\t4\\\\\n\t5\\\\\n\t6\\\\\n\t7\\\\\n\t8\\\\\n\t9\\\\\n\t10\\\\\n\t11\\\\\n\t12\\\\\n\t13\\\\\n\t14\\\\\n\t15\\\\\n\\end{tabular}\n\n\\item[\\$words.removed] \\begin{enumerate*}\n\\item '-0.42'\n\\item '-1'\n\\item '-2'\n\\item '-9'\n\\item '-b'\n\\item '-base'\n\\item '-c'\n\\item '-e'\n\\item '-fr'\n\\item '-h'\n\\item '-ii'\n\\item '-j'\n\\item '-n'\n\\item '-s'\n\\item '-t'\n\\item '-w'\n\\item '0-'\n\\item '1-3'\n\\item '1-4'\n\\item '1.7m'\n\\item '10-15'\n\\item '10-fold'\n\\item '10th'\n\\item '1558-6'\n\\item '1a'\n\\item '1amsterdam'\n\\item '1cnrs'\n\\item '1comput'\n\\item '1depart'\n\\item '1divis'\n\\item '1faculti'\n\\item '1http://www.w3.org/tr/2003/pr-owl-guide-20031209/win'\n\\item '1https://www.atilika.com/ja/kuromoji/'\n\\item '1https://www.healthboards.com/'\n\\item '1https://www.ncbi.nlm.nih.gov/pubm'\n\\item '1in'\n\\item '1informatik'\n\\item '1institut'\n\\item '1k'\n\\item '1like'\n\\item '1nation'\n\\item '1sourc'\n\\item '1space'\n\\item '1univers'\n\\item '2-3'\n\\item '2012-2015'\n\\item '23955-6900'\n\\item '26-11-2018'\n\\item '2a'\n\\item '2b'\n\\item '2c'\n\\item '2castor'\n\\item '2center'\n\\item '2comput'\n\\item '2d'\n\\item '2here'\n\\item '2http://mallet.cs.umass.edu/'\n\\item '2http://ontofox.hegroup.org'\n\\item '2http://xiphoid.biostr.washington.edu/fma/shirt\\_ontology/shirt\\_ontology\\_1.'\n\\item '2https://hal.archives-ouvertes.fr/'\n\\item '2https://medhelp.org/'\n\\item '2nd'\n\\item '2univ'\n\\item '3-5-1'\n\\item '3d'\n\\item '3hereinaft'\n\\item '3http://purl.org/sig/ont/fma/fma290055'\n\\item '3http://www.workingontologist.org/examples.zip'\n\\item '3https://www.istex.fr/'\n\\item '432-8011'\n\\item '4a'\n\\item '4a3'\n\\item '4http://www.cl.ecei.tohoku.ac.jp/\\textasciitilde{}m\\textasciitilde{}suzuki/jawiki\\_vector/'\n\\item '4https://www.i2b2.org/nlp/datasets/main.php'\n\\item '4recent'\n\\item '5https://github.com/johokugsk'\n\\item '5https://n2c2.dbmi.hms.harvard.edu/'\n\\item '5in'\n\\item '60gb'\n\\item '64-year-old'\n\\item '6https://sites.google.com/site/shareclefehealth/'\n\\item '7http://www.cepidc.inserm.fr/'\n\\item '8-core'\n\\item '86-year-old'\n\\item '8gb'\n\\item '8https://knowledge-learning.github.io/ehealthkd-2019'\n\\item '9c'\n\\item 'a.m'\n\\item 'a1'\n\\item 'a2'\n\\item 'aachen'\n\\item 'ab'\n\\item 'abbrevi'\n\\item 'abdelhakim1,2'\n\\item 'abdomin'\n\\item 'abdullah'\n\\item 'abil'\n\\item 'ablat'\n\\item 'abli'\n\\item 'abnorm'\n\\item 'absenc'\n\\item 'abu-hanna1'\n\\item 'ac'\n\\item 'ac-'\n\\item 'acad-'\n\\item 'academ'\n\\item 'academicstaff'\n\\item 'acceler'\n\\item 'accept'\n\\item 'accom-'\n\\item 'accommod'\n\\item 'accomplish'\n\\item 'accordingly.w'\n\\item 'account'\n\\item 'accumu-'\n\\item 'accumul'\n\\item 'accuraci'\n\\item 'aceruloplasminemia'\n\\item 'ach'\n\\item 'acharya8'\n\\item 'achiev'\n\\item 'acknow-'\n\\item 'acknowl-'\n\\item 'acknowledg'\n\\item 'acl'\n\\item 'acm'\n\\item 'acquir'\n\\item 'acquisit'\n\\item 'acroparesthesia'\n\\item 'act'\n\\item 'acterist'\n\\item 'action'\n\\item 'activ-'\n\\item 'actor'\n\\item 'acut'\n\\item 'ad-'\n\\item 'ad-hoc'\n\\item 'adam'\n\\item 'add'\n\\item 'add-'\n\\item 'addi-'\n\\item 'adequ'\n\\item 'adher'\n\\item 'adisinsight'\n\\item 'adjac'\n\\item 'adjacentregion'\n\\item 'adjust'\n\\item 'admin-'\n\\item 'administr'\n\\item 'admission\\_num'\n\\item 'admit'\n\\item 'adopt'\n\\item 'adr'\n\\item 'adult'\n\\item 'advanc'\n\\item 'advantag'\n\\item 'advers'\n\\item 'adverse-drug'\n\\item 'advic'\n\\item 'advis'\n\\item 'aesthet'\n\\item 'af'\n\\item 'affect'\n\\item 'affili'\n\\item 'affili-'\n\\item 'affilia-'\n\\item 'afford'\n\\item 'aforement'\n\\item 'africa'\n\\item 'african'\n\\item 'africanwildlif'\n\\item 'africanwildlifeontology.xml'\n\\item 'africanwildlifeontology0'\n\\item 'africanwildlifeontology1'\n\\item 'africanwildlifeontology1a.owl'\n\\item 'africanwildlifeontology2'\n\\item 'africanwildlifeontology2a.owl'\n\\item 'africanwildlifeontology3'\n\\item 'africanwildlifeontologyweb.owl'\n\\item 'afrikaan'\n\\item 'afterward'\n\\item 'ag'\n\\item 'ag-'\n\\item 'agar-'\n\\item 'age'\n\\item 'agenc'\n\\item 'agent'\n\\item 'aggreg'\n\\item 'aggress'\n\\item 'agnost'\n\\item 'ago'\n\\item 'agre'\n\\item 'agreement'\n\\item 'ah'\n\\item 'ahornstr'\n\\item 'ai'\n\\item 'aid'\n\\item 'airplan'\n\\item 'airway'\n\\item 'ak'\n\\item ⋯\n\\item 'vertigo'\n\\item 'vesilinnanti'\n\\item 'vessel'\n\\item 'vhnguyen@u.nus.edu'\n\\item 'vi'\n\\item 'viani1'\n\\item 'viation'\n\\item 'vice'\n\\item 'vide'\n\\item 'video'\n\\item 'vider'\n\\item 'vidual'\n\\item 'view-'\n\\item 'viewer'\n\\item 'viewpoint'\n\\item 'vii'\n\\item 'vincent'\n\\item 'violat'\n\\item 'vious'\n\\item 'viral'\n\\item 'virtual'\n\\item 'virus'\n\\item 'vis-'\n\\item 'visibl'\n\\item 'vision'\n\\item 'visual'\n\\item 'visual-'\n\\item 'visualis'\n\\item 'vital'\n\\item 'vitamin'\n\\item 'vival'\n\\item 'vlietstra'\n\\item 'vlietstra1'\n\\item 'vo'\n\\item 'vocabu-'\n\\item 'vocabulari'\n\\item 'voic'\n\\item 'volum'\n\\item 'volv'\n\\item 'vomit'\n\\item 'vos'\n\\item 'vos-'\n\\item 'vos1,2'\n\\item 'vosview'\n\\item 'vous'\n\\item 'vp'\n\\item 'vpi'\n\\item 'vpui'\n\\item 'vs'\n\\item 'vt'\n\\item 'vu'\n\\item 'vui'\n\\item 'vw1'\n\\item 'vw2'\n\\item 'vwi'\n\\item 'vwj'\n\\item 'vwn'\n\\item 'vx'\n\\item 'w'\n\\item 'w.vlietstra@erasmusmc.nl'\n\\item 'w1'\n\\item 'w2'\n\\item 'w3c'\n\\item 'waist'\n\\item 'waist-hip'\n\\item 'wait'\n\\item 'waj'\n\\item 'wajh'\n\\item 'wajhbj'\n\\item 'wajhkj'\n\\item 'wake'\n\\item 'wal'\n\\item 'walk'\n\\item 'wang'\n\\item 'want'\n\\item 'ward'\n\\item 'warrant'\n\\item 'wast'\n\\item 'way2'\n\\item 'wd'\n\\item 'wdduncan@gmail.com'\n\\item 'weak'\n\\item 'web-bas'\n\\item 'websit'\n\\item 'week'\n\\item 'wei'\n\\item 'weight'\n\\item 'well-defin'\n\\item 'well-establish'\n\\item 'well-form'\n\\item 'well-known'\n\\item 'well-suit'\n\\item 'well-understood'\n\\item 'well-vers'\n\\item 'whale'\n\\item 'whatev'\n\\item 'whatizit'\n\\item 'whatsoev'\n\\item 'wherea'\n\\item 'wherebi'\n\\item 'whim'\n\\item 'whose'\n\\item 'wider'\n\\item 'wikipedia'\n\\item 'wildlif'\n\\item 'wilhelm'\n\\item 'william'\n\\item 'window'\n\\item 'wine'\n\\item 'wine-produc'\n\\item 'wine.owl'\n\\item 'wineri'\n\\item 'winston'\n\\item 'winter'\n\\item 'wit'\n\\item 'with-'\n\\item 'withdraw'\n\\item 'withhold'\n\\item 'withmerg'\n\\item 'wk'\n\\item 'wn'\n\\item 'wolff'\n\\item 'wolff1'\n\\item 'woman'\n\\item 'women'\n\\item 'word-'\n\\item 'word-bas'\n\\item 'word-embed'\n\\item 'word2vec'\n\\item 'workaround'\n\\item 'workflow'\n\\item 'workload'\n\\item 'workshop'\n\\item 'world'\n\\item 'worldwid'\n\\item 'wors'\n\\item 'worth'\n\\item 'worthi'\n\\item 'wound'\n\\item 'write'\n\\item 'writer'\n\\item 'wrong'\n\\item 'wu'\n\\item 'wui'\n\\item 'wx'\n\\item 'wytz'\n\\item 'x'\n\\item 'x-'\n\\item 'x2'\n\\item 'x9'\n\\item 'xanax'\n\\item 'xi'\n\\item 'xiang'\n\\item 'xie'\n\\item 'xml'\n\\item 'xn'\n\\item 'xori'\n\\item 'xpi'\n\\item 'xsd:datetim'\n\\item 'xt'\n\\item 'xu'\n\\item 'xx'\n\\item 'y'\n\\item 'ya'\n\\item 'ye'\n\\item 'year'\n\\item 'yellow'\n\\item 'yes'\n\\item 'yesterday'\n\\item 'yield'\n\\item 'yin1'\n\\item 'yo'\n\\item 'yoshinobu'\n\\item 'youden'\n\\item 'young'\n\\item 'yp'\n\\item 'ys'\n\\item 'ysis'\n\\item 'yt'\n\\item 'yw'\n\\item 'yy'\n\\item 'yyyi'\n\\item 'z'\n\\item 'za'\n\\item 'ze'\n\\item 'zero'\n\\item 'zh'\n\\item 'zhang'\n\\item 'zhu'\n\\item 'zhu1'\n\\item 'zimmermann3'\n\\item 'zip\\_cod'\n\\item 'zipcod'\n\\item 'zollinger-'\n\\item 'zoloft'\n\\item 'zoo'\n\\item 'zooanim'\n\\item 'zoonot'\n\\item 'zweigenbaum'\n\\item 'þ'\n\\end{enumerate*}\n\n\\item[\\$docs.removed] NULL\n\\item[\\$tokens.removed] 6962\n\\item[\\$wordcounts] \\begin{enumerate*}\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 2\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 5\n\\item 9\n\\item 1\n\\item 1\n\\item 3\n\\item 1\n\\item 15\n\\item 1\n\\item 2\n\\item 3\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 15\n\\item 1\n\\item 2\n\\item 1\n\\item 9\n\\item 1\n\\item 2\n\\item 1\n\\item 3\n\\item 6\n\\item 4\n\\item 1\n\\item 1\n\\item 1\n\\item 4\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 4\n\\item 1\n\\item 1\n\\item 5\n\\item 2\n\\item 3\n\\item 10\n\\item 1\n\\item 2\n\\item 6\n\\item 2\n\\item 7\n\\item 1\n\\item 1\n\\item 2\n\\item 12\n\\item 5\n\\item 1\n\\item 2\n\\item 12\n\\item 8\n\\item 2\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 3\n\\item 1\n\\item 2\n\\item 4\n\\item 1\n\\item 4\n\\item 4\n\\item 3\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 4\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 3\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 2\n\\item 1\n\\item 1\n\\item 5\n\\item 1\n\\item 1\n\\item 3\n\\item 1\n\\item ⋯\n\\item 1\n\\item 12\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 12\n\\item 5\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 3\n\\item 2\n\\item 4\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 4\n\\item 1\n\\item 2\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 13\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 8\n\\item 1\n\\item 1\n\\item 1\n\\item 4\n\\item 7\n\\item 1\n\\item 3\n\\item 2\n\\item 1\n\\item 4\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 4\n\\item 2\n\\item 1\n\\item 6\n\\item 4\n\\item 11\n\\item 3\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 3\n\\item 1\n\\item 1\n\\item 1\n\\item 6\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 8\n\\item 1\n\\item 1\n\\item 1\n\\item 3\n\\item 12\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 3\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 5\n\\item 1\n\\item 6\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 4\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 5\n\\item 1\n\\item 2\n\\item 4\n\\item 1\n\\item 1\n\\item 1\n\\item 3\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 2\n\\item 1\n\\item 2\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\end{enumerate*}\n\n\\end{description}\n",
            "text/plain": [
              "$documents\n",
              "$documents$text1\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    5    6    8   10   12    13    14    16    17    18\n",
              "[2,]    1    1    4    1    1    1    3    8    1    12     2     2     2     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    19    20    21    22    23    24    27    29    30    32    33    34\n",
              "[2,]     6     6     1     7     2     1     1     2     1     1     2     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    35    36    37    38    39    40    42    43    44    45    48    51\n",
              "[2,]     5     2     2     1     3     1     2    16     1     5     3     4\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    54    55    56    59    64    66    67    70    71    72    73    75\n",
              "[2,]    72    11    37     1     6     5     1     1     3     1     3     6\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    77    78    79    80    82    87    88    90    92    93    95    96\n",
              "[2,]     4     4     1     1    10     2     1     2     3     1     1     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    97    98    99   101   102   103   104   105   106   108   109   110\n",
              "[2,]     3     4     2     1     4     1     1     1     1     3     1     2\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   111   112   115   116   117   118   119   120   121   122   123   124\n",
              "[2,]     8     5     4     1   197     2     9     3     1    10     3     1\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   129   132   133   136   137   139   141   143   145   146   147   148\n",
              "[2,]     7     1     1     5     1    12     7     7     3     1     9     2\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   149    151    153    154    156    157    158    160    161    162\n",
              "[2,]     1      5      2      5      4      5      1      1      1      3\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    163    165    167    170    171    172    179    180    182    183\n",
              "[2,]      2      2      1     16      1      1      1      5      1      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    184    185    186    194    196    198    199    205    207    208\n",
              "[2,]     16      3      1      2      1      6      4      1      1      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    211    212    213    214    215    216    217    218    219    220\n",
              "[2,]      3     10      1      1      2      2     19      2      1     10\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    224    225    227    228    229    230    231    232    233    234\n",
              "[2,]      2      1      1      1      4      2     11      2      9      2\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    235    236    237    238    242    244    245    247    249    250\n",
              "[2,]      1      2     17      1      1      1      1      1      1      7\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    251    253    254    255    256    258    260    261    262    263\n",
              "[2,]      2      1      7      1      4      4      1      1     68      3\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    264    265    268    269    270    272    273    274    275    278\n",
              "[2,]      1      1      5      1      1      1      1      9      5      7\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    280    281    283    285    286    288    290    293    294    295\n",
              "[2,]      1      7      1      1      2      1      4      1      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    296    298    299    301    302    305    306    308    310    312\n",
              "[2,]      2      4      1      2      1      1      2      1     32      8\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    314    315    316    317    319    320    323    326    327    330\n",
              "[2,]      2      5      1      6      1      1      1      8      3      2\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    332    333    334    337    338    340    341    342    343    344\n",
              "[2,]      9      4     10      3     12      1      1      1      4      2\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    345    346    347    348    349    351    352    353    359    362\n",
              "[2,]      5      7      3      7      2      1      1      1     19      7\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    363    364    365    366    367    368    369    371    372    373\n",
              "[2,]      2      1      3     46      3     13      1      4      2     52\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    375    376    377    380    381    382    384    385    386    388\n",
              "[2,]     11     23      8      2      1      4      1      2     23      5\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    389    391    394    395    399    400    401    402    403    405\n",
              "[2,]      1      1      2      1      4      4      6      6      1      1\n",
              "     [,259] [,260] [,261] [,262] [,263]\n",
              "[1,]    409    411    413    415    416\n",
              "[2,]      1      1      2     32      1\n",
              "\n",
              "$documents$text2\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3   11   24   35   36   39   57   79    86   121   128   130   153\n",
              "[2,]    1    1    1    1    1    1    1    3    2     1     1     1     1     3\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]   155   161   164   172   184   188   194   197   204   216   218   241\n",
              "[2,]     1     1     1     3     3     2     5     1     1     1     1     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]   253   255   263   271   295   296   309   313   315   318   319   351\n",
              "[2,]     1     1     1     1     1     1     2     1     1     1     1     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45]\n",
              "[1,]   355   358   365   376   387   390   399\n",
              "[2,]     1     1     2     1     1     1     1\n",
              "\n",
              "$documents$text3\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    4    6    8    9   10   11   12    13    15    17    18    22\n",
              "[2,]    3    2    5    3    5    1    2    1    2    15     2     2     1     2\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    23    24    25    26    27    28    31    32    33    36    37    38\n",
              "[2,]     1     2     5     2     1     2     2     2    63     4     1     8\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    39    41    42    43    45    46    48    49    52    53    57    58\n",
              "[2,]     2     2     6     6    19     5     1     1     1     1     1     2\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    59    60    62    64    66    71    72    77    79    80    81    83\n",
              "[2,]     8     1     5     2     6     2     2     4     3     2     1     3\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    84    85    87    88    89    90    91    92    93    94    95    96\n",
              "[2,]     1     1     1     1     2     1     2     5     3     1     1    41\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    97    98   100   101   102   103   106   109   111   112   114   119\n",
              "[2,]    11     1     2     1     5     2     2     1    11     3     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   120   121   122   125   127   129   132   134   138   139   140   141\n",
              "[2,]     2    28     1     1     1     1     1     2     4     5     1     2\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   144   145   147   148   152   153   155   156   157   159   160   161\n",
              "[2,]     1     1     1     9     2     1     5     2     2     1     4     6\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   163    165    166    167    169    170    171    174    176    177\n",
              "[2,]     2      2      3      3      1      6      1      3      1      2\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    178    179    181    182    183    184    186    188    190    191\n",
              "[2,]      1      1      1      3      9     15      1      9      1      6\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    194    195    196    197    198    200    201    203    204    205\n",
              "[2,]     21      1      1      1      1      1     17      1      4      2\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    208    209    210    211    212    213    214    216    218    219\n",
              "[2,]      7      2      4     13     13      1      7      5      1      2\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    220    221    222    223    224    225    226    227    228    229\n",
              "[2,]      1      3      1      4      2      2      2      3      9      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    231    232    233    234    235    237    238    240    241    242\n",
              "[2,]      1      2     10      3      1      3     18      6      1      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    243    244    245    246    249    251    253    254    256    258\n",
              "[2,]      3      1      3      1      1      8      3     23      1     25\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    259    260    261    263    264    267    268    269    270    271\n",
              "[2,]      1      8      8      2      1      1     14      1      2      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    274    276    280    281    282    283    287    291    292    294\n",
              "[2,]      5      1      4     10      1      3      1      1      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    296    297    298    299    300    302    303    304    305    308\n",
              "[2,]      5      1      5     32      5      1     11     18      2      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    310    311    312    313    315    317    318    323    325    326\n",
              "[2,]      2      1      1      3      5      1      2      2      1     44\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    327    328    330    332    333    334    335    337    338    339\n",
              "[2,]      8      8     20      4      9      6      1      1      8      3\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    340    341    342    343    344    346    347    348    350    352\n",
              "[2,]      2      1      1      3      1      1      2      2      1      4\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    353    354    355    356    358    360    366    367    368    369\n",
              "[2,]      1      1      2      1      4      1      1      9     10      1\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    372    373    376    377    378    379    380    381    382    383\n",
              "[2,]     15     15      9      1      3     12      9      2      3      7\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    385    388    391    392    393    394    395    399    402    403\n",
              "[2,]     11      1      1      1      2      5      2      2      1      6\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266]\n",
              "[1,]    407    408    409    411    412    413    415    416\n",
              "[2,]      1      1      1      1      2      1      6      3\n",
              "\n",
              "$documents$text4\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    5    6    8    9   10   11    12    13    14    15    16\n",
              "[2,]    6    1    3    1    1    2    1    1    3     5     1     1     2     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    18    20    21    22    23    24    25    27    28    30    32    33\n",
              "[2,]     2     6     2     2     2     1     2    10     1     1     2     4\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    34    36    38    39    40    41    42    43    44    45    48    50\n",
              "[2,]     9     2     2     2     6     1     1     7     1     3     3     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    51    52    53    54    55    56    58    61    62    63    64    65\n",
              "[2,]     1     1     1    16     3     1     2     1     3     1     1     2\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    66    69    71    72    73    74    77    78    79    80    81    82\n",
              "[2,]     8     2     3     4     2     5     2     1     3     4    19     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    83    84    85    86    88    89    90    91    92    93    94    95\n",
              "[2,]     1     1     1     3     1     1     2     1     5     3     4     3\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]    96    97    98    99   101   103   106   107   108   109   110   112\n",
              "[2,]     3     7     6     1     1     1     4     6     8     2     1    17\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   113   114   115   116   118   119   120   121   122   123   124   125\n",
              "[2,]     1     4     1     1     1     1     1     1    34     1     1     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   129    134    135    136    139    140    142    143    144    145\n",
              "[2,]     2      1      1      2      1      1      1     13      1      2\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    146    147    148    150    151    154    156    158    159    160\n",
              "[2,]      1      1      1      3      2     17      1      4      5      2\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    161    163    165    166    167    168    169    171    175    176\n",
              "[2,]      1      1      1      1      1      2      7      1      5      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    178    179    184    185    186    187    188    189    191    193\n",
              "[2,]      1      1      2     17      1      1      4      5      1      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    194    196    199    200    202    203    204    205    207    208\n",
              "[2,]      6      5     18      3      2      3      2      4      5      3\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    209    210    212    214    216    218    220    221    223    224\n",
              "[2,]     12      1      7      1     10     14      6      4      4      2\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    225    226    227    228    229    230    232    233    234    235\n",
              "[2,]      6      4      6      6      1     15      3      4      1      7\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    237    240    241    244    245    246    247    248    249    250\n",
              "[2,]      1      8      2      1      3      2      2     23      1     13\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    251    253    254    255    257    258    259    260    261    262\n",
              "[2,]      8      2      1      1      1      1     10      1      1    153\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    263    265    266    267    269    270    274    275    276    277\n",
              "[2,]      2      1      3      1      2      2      6      5      1      3\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    280    282    283    284    285    286    287    289    291    292\n",
              "[2,]      4      1      8      2      3      4      7      3      1      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    293    294    295    296    298    299    301    302    303    305\n",
              "[2,]      6      1      4      4      6      1      4      7      1     10\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    306    308    310    311    312    313    314    315    316    317\n",
              "[2,]     24      3      1      1     11      3      1      9      5      1\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    318    319    320    321    325    326    327    328    331    332\n",
              "[2,]     22      1      6      1      4     14      5      2      3      7\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    333    334    335    337    338    339    340    344    345    347\n",
              "[2,]      1     11      2      1      7      1      3      7      2      1\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    348    349    350    351    352    354    355    357    358    359\n",
              "[2,]      4      1      3      5      4      1      1      1      2      3\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    360    361    362    363    364    366    368    369    370    371\n",
              "[2,]     22      2      1      3      2      1      5      4      1      6\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    373    377    378    379    380    382    383    388    389    390\n",
              "[2,]      1      1      3      2      3      8      1      6      2      1\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    391    394    395    396    397    399    405    406    407    408\n",
              "[2,]      1      9      2      1      1      1      2     13      2      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294]\n",
              "[1,]    409    410    411    413    414    416\n",
              "[2,]      1      2      3      1      1      2\n",
              "\n",
              "$documents$text5\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    4    5    6    9   10   11   13    14    15    17    19    20\n",
              "[2,]    1    4    1    2    6    1    6    2   11     2     4     3     1     3\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    22    23    24    28    29    32    33    35    36    37    38    39\n",
              "[2,]     2     1     1     3     6     1     8     1     1     1     3     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    40    41    43    45    46    52    56    57    58    59    62    63\n",
              "[2,]     1     2    10     5     1     1     1     2     1    26     2     3\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    64    65    66    67    68    69    70    71    72    73    74    75\n",
              "[2,]     1     1    14     8     1     1     3     6     7     1     2     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    76    77    78    79    80    81    82    83    84    85    87    88\n",
              "[2,]     3     3     6     2     1    19     4     1     1     1     1     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    92    93    96    97    98    99   101   102   103   104   105   106\n",
              "[2,]     5     3     2     2     3     2     1     3     4     2     7     5\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   107   108   109   111   114   116   120   121   122   123   125   126\n",
              "[2,]     3     3     1     1     1    24     3     1     3     1     1     9\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   129   132   133   134   136   139   142   143   144   145   146   147\n",
              "[2,]     1     2    29     1     1    11     1     4     1     4     5    23\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   148    149    150    151    153    154    156    157    159    160\n",
              "[2,]     1      2      2      1     11     10      9      2      5      3\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    161    162    163    166    167    168    169    170    171    172\n",
              "[2,]      2      2      1      3      3      1      4      1      2     17\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    174    175    176    178    179    181    182    185    186    187\n",
              "[2,]      2      2      1      1      1      2      3      1      1      2\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    189    191    193    194    196    197    198    199    200    202\n",
              "[2,]     11      2      4     13      3      3      1      3      1      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    203    204    205    208    210    211    212    213    214    216\n",
              "[2,]      1      1      1      3      2      4     10      1      3      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    217    218    220    221    223    224    226    227    228    229\n",
              "[2,]     10      2     20      3      4      2      2      1     13      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    232    233    234    235    238    239    240    241    242    244\n",
              "[2,]      2      1      2      7      1      5      3      1      5      1\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    245    248    250    251    252    253    254    255    256    258\n",
              "[2,]      1      8      5      2      2      3      5      1      1     15\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    259    260    261    263    264    265    267    269    270    272\n",
              "[2,]      6      4      9      2      1      7      3      2      2      5\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    273    274    275    276    277    278    279    280    281    282\n",
              "[2,]      1      9      1      1      1      2      4      1      2      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    283    286    287    288    289    290    292    293    294    295\n",
              "[2,]      3      3      1      1      1      8      9      1      3      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    296    298    299    302    303    304    307    308    311    313\n",
              "[2,]      3      5      1      5      1      5      2      2      1      2\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    315    316    317    318    320    322    323    324    325    326\n",
              "[2,]      5     15      1      2      4      2      3      1      1     33\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    329    331    332    334    337    338    339    341    343    344\n",
              "[2,]     14      2      4     17      2      8      7      3      6      3\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    345    346    347    348    350    351    352    354    358    361\n",
              "[2,]      1      1      5      1      1      2      1      1      4      1\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    364    365    366    367    368    369    371    373    374    376\n",
              "[2,]      2      1      2     11      6      1     11     11      1     17\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    378    379    380    384    386    387    389    392    393    394\n",
              "[2,]      2      1      8      1      8      6      1      1      1      2\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    395    396    398    402    404    405    407    408    409    410\n",
              "[2,]      2      3      1      4      1      1      2      1      1      2\n",
              "     [,279] [,280] [,281] [,282]\n",
              "[1,]    412    415    416    417\n",
              "[2,]      1     15     14      1\n",
              "\n",
              "$documents$text6\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    4    7    9   10   12   13   14    15    16    17    18    20\n",
              "[2,]    1   16    6    5    2    9    1   15    3     7     1     1     2     7\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    22    23    24    30    32    33    34    35    36    38    39    40\n",
              "[2,]     3     2     1     4     2     6     3     2    11     3     2     6\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    41    42    43    44    45    47    48    52    53    54    58    59\n",
              "[2,]     3     1    10     1     3     1     6     2     1    20     3     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    62    63    64    66    67    70    71    72    73    74    75    76\n",
              "[2,]     3     1     2     7     1     3     1     2    11     2     1     2\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    77    78    79    80    82    83    84    85    86    87    88    91\n",
              "[2,]     4     2     9     2     7     1     2     1     2     3    12     4\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    92    93    95    96    97    98    99   101   102   103   104   105\n",
              "[2,]     5     3     1     3   118     2    20     1     3     7     3     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   106   107   108   110   111   113   114   116   117   118   120   122\n",
              "[2,]    10     4     2     1     2     1    23     1     1     1     4    11\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   123   124   125   126   127   128   129   130   131   133   134   135\n",
              "[2,]     4     1     2     4     4     1     2     1    14    11     3     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   136    137    138    139    140    141    142    143    144    145\n",
              "[2,]     3      2      1     15      1      5      2      7      1     10\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    149    150    151    152    153    154    156    157    159    160\n",
              "[2,]      8      1      4      1     48      2      8      1      2      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    161    162    163    165    168    169    170    171    172    173\n",
              "[2,]      2      4      1      3      1      9      1      1      8      4\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    174    176    177    178    179    180    184    185    186    187\n",
              "[2,]      2      1      2      1      1      1      3      2      1      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    188    191    192    193    194    196    197    199    200    202\n",
              "[2,]      4      1      1      7     18      6      5      3      2     11\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    203    204    205    206    207    209    210    212    213    214\n",
              "[2,]      3      1      1      1      7      6      1      8      7      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    215    216    218    219    220    223    224    225    226    227\n",
              "[2,]      1     12      5      2      2      4      2      4     16      4\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    228    231    232    233    234    235    236    240    243    244\n",
              "[2,]      9      2      3      1      3      3      1      6      1      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    245    247    248    250    251    253    257    258    259    261\n",
              "[2,]      2      3      1      4      1      2      2      5      5      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    262    263    265    266    267    269    270    274    275    276\n",
              "[2,]      9      3      2     11     16      7      2      7      2      3\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    277    278    280    281    282    283    284    285    286    287\n",
              "[2,]      1      1      4      2      1      3      6      1      3      6\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    291    292    293    294    296    298    299    300    301    303\n",
              "[2,]      1      5      5      1      7     14      3      5      2     49\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    305    306    308    309    310    311    312    313    314    315\n",
              "[2,]      4      2      2      1      2      2      6      8      2      2\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    316    317    318    319    320    321    322    324    325    326\n",
              "[2,]      2      1      7      6      2      2      1      1      3     27\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    327    328    331    333    334    335    336    338    339    344\n",
              "[2,]      1      2      2     10     39      1      1      8     12      7\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    345    347    348    349    350    352    353    354    355    356\n",
              "[2,]      4      9      3      1      4      2      1      1      7      2\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    357    358    359    360    361    362    363    364    366    369\n",
              "[2,]      5     12      2      6      1      1      2     13     11      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    370    371    372    373    374    377    378    379    380    381\n",
              "[2,]      1      3      3      5      9      1     11      4      9      2\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    382    386    387    388    389    390    391    394    395    396\n",
              "[2,]      2      3      1     10      2      5      1      1      3      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294] [,295] [,296] [,297] [,298]\n",
              "[1,]    397    398    400    401    402    404    405    406    407    408\n",
              "[2,]     10      4      1      2      3      1      2      2      1      1\n",
              "     [,299] [,300] [,301] [,302] [,303] [,304]\n",
              "[1,]    409    410    411    413    414    416\n",
              "[2,]      1      1     10      1     13     10\n",
              "\n",
              "$documents$text7\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    9   11   12   13   20   21   22    23    24    25    30    31\n",
              "[2,]    1    8    1    1    2    3   10    1    1     8     1    14     2     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    32    33    36    39    41    42    43    44    47    48    49    51\n",
              "[2,]     1     4    18     2     1     1     6     4     5    28     1     1\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    52    55    57    58    60    62    65    66    67    69    71    73\n",
              "[2,]     6     1    55     1     2     1     1     7     2     2     2     2\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    74    79    80    81    82    83    84    85    88    89    91    92\n",
              "[2,]     3     1     5     1     3     1     1     1     2     1     5     5\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    93    95    97    98   100   101   104   106   107   108   110   112\n",
              "[2,]     3     2    16     2     1     4     1     4     2     3     3     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]   114   116   119   120   121   122   123   126   130   133   134   136\n",
              "[2,]     2     2     1     2     7     2     3     1     1     1     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   137   139   140   142   143   144   145   146   147   151   153   159\n",
              "[2,]     1     3     3     3     1     1     7     1     1     1     3     3\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   160   163   164   165   166   168   169   171   172   173   175   176\n",
              "[2,]     1     1     1     1     1     1     1     1     2     4     5     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   177    178    179    180    181    183    184    185    186    188\n",
              "[2,]     3      1      1      1      1      3      1      3      3      3\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    189    191    192    194    195    196    197    199    201    202\n",
              "[2,]      2      1      2     11      1      1      5      4      1      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    203    204    205    207    209    212    213    214    215    218\n",
              "[2,]      1      1      1      1      5      4      2      1      2      3\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    221    222    223    224    226    227    228    229    231    232\n",
              "[2,]      1      1      4      2      2      1      1      6      1      4\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    233    235    240    242    243    244    248    251    252    253\n",
              "[2,]      4      2      4     23      2      1      1      1      2      2\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    256    257    260    261    263    267    269    270    274    275\n",
              "[2,]      2      3      1      2      3      1      1      2      2      2\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    276    278    279    281    282    283    284    285    288    289\n",
              "[2,]      1      1      9      1      2      4      2      1      1      2\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    290    291    292    293    294    295    296    297    298    299\n",
              "[2,]      1      3      1      1      1      1      6      1     13     12\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    300    301    302    306    308    309    311    312    314    315\n",
              "[2,]      4      7      1      5      2      1      1      2      1      2\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    317    319    320    321    323    324    326    328    331    334\n",
              "[2,]      1      1      1      1      1      1      4     14      2      9\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    336    338    339    342    343    347    348    351    352    354\n",
              "[2,]      1      4      4      1      8      5      4      1      1      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    355    356    358    361    363    364    366    370    371    375\n",
              "[2,]      1      1      3      1      1      2      1      1      5      4\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    376    380    382    384    385    386    387    388    389    392\n",
              "[2,]      1      3     14      1      1      3      3      1      6      1\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    393    394    395    396    400    402    403    405    406    407\n",
              "[2,]      1      1      2      1      3      1      1      6      2      2\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245]\n",
              "[1,]    408    409    412    413    415    416    417\n",
              "[2,]      1      1      1      1      2      8      1\n",
              "\n",
              "$documents$text8\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    5    6    7    8    9    10    11    13    15    17\n",
              "[2,]    4    1    3    2    1    1    1    4    1     2     5     6     4     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    18    19    21    22    23    24    26    27    29    31    32    33\n",
              "[2,]     1     2     2     1     3     7     2     3     2     5     1     6\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    36    39    40    41    42    43    47    48    50    51    52    54\n",
              "[2,]     3     2     4     2     4    11    33     2     1     1     6    26\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    57    60    62    63    65    66    68    70    71    72    73    74\n",
              "[2,]     4     4     7     1     1    17     1     2     1     3     3     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    75    77    78    79    80    81    82    84    85    86    87    88\n",
              "[2,]     2     3     1     5     2     1     5     1     1     1     1     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    89    90    91    92    93    94    95    96    97    98    99   100\n",
              "[2,]     1     1     1     6     3     1     1     1    33     7     1     6\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   101   102   103   104   106   108   109   111   112   114   116   117\n",
              "[2,]     2    24     5     4     1     4     2     1     7     4     1     5\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   118   120   121   122   123   127   128   129   130   133   134   135\n",
              "[2,]     1     1     4     5     1     2    16     2     4     2     2     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   136    138    139    141    142    143    144    145    147    148\n",
              "[2,]    26      3      6      4      1     11      1      8      1      1\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    150    151    152    153    155    156    158    160    161    162\n",
              "[2,]      3      3      2      5      1      2      2      5      2      5\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    163    165    167    168    169    170    171    172    173    174\n",
              "[2,]      1      2     11      2     10      1      1     45      3      2\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    176    178    179    180    184    185    186    187    188    189\n",
              "[2,]      1      1      1      7      4      1      1      3      3      6\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    190    191    192    193    194    195    196    197    198    199\n",
              "[2,]      2      1      1      3     22      4      4      1      2      7\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    200    201    202    203    205    208    209    211    212    213\n",
              "[2,]      1      1      4      3      1      5      3      1      6      3\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    214    215    217    218    221    222    223    224    225    226\n",
              "[2,]      1      2      1      2      1      1      4      5      1      3\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    227    228    230    232    233    234    235    236    239    240\n",
              "[2,]      1      3      3      3      1      2      8      3      1     32\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    241    242    243    244    245    247    249    250    251    253\n",
              "[2,]      2     13      1      1      2      1      1      5      2      2\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    255    257    258    261    262    263    264    266    267    268\n",
              "[2,]      2      4      7      2     57      3      1      1      4      8\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    269    270    271    272    274    275    276    277    278    279\n",
              "[2,]      1      4      1      1      7     16      1     13      4     53\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    281    282    283    284    286    287    292    293    294    295\n",
              "[2,]      8      1      3      9      5     16      3      6      5      3\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    296    297    298    299    301    302    303    304    306    308\n",
              "[2,]     19      1     33      1      3      5      2      1      2      3\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    309    310    311    312    313    314    315    316    317    319\n",
              "[2,]      7      3      1     24      3      1     21     15      1      2\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    320    321    323    325    326    327    328    330    331    332\n",
              "[2,]      2      1      2     70     22      4      1      1      3      2\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    333    334    335    336    337    338    339    340    341    342\n",
              "[2,]      2     14      2      3      1      6      3      1      1      1\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    343    344    345    346    347    348    349    350    351    352\n",
              "[2,]      1      1      6      1      3     10      4      6      1      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    353    354    355    356    357    358    359    361    363    364\n",
              "[2,]      1      1      1      2      5      8      3      1      1      1\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    366    367    368    371    372    373    374    376    378    379\n",
              "[2,]     16      2      4      1      1     21      1      1      2      8\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294] [,295] [,296] [,297] [,298]\n",
              "[1,]    380    382    387    388    389    391    392    395    397    398\n",
              "[2,]      7      2      2      6      2      1      1      5      2      2\n",
              "     [,299] [,300] [,301] [,302] [,303] [,304] [,305] [,306] [,307] [,308]\n",
              "[1,]    399    401    402    403    404    406    407    408    409    410\n",
              "[2,]      1      1      1      1      1      2      1      2      1      7\n",
              "     [,309] [,310] [,311] [,312] [,313] [,314]\n",
              "[1,]    411    412    413    414    415    416\n",
              "[2,]      5      3      1      3      1     10\n",
              "\n",
              "$documents$text9\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    4    8    9   10   12   13   15    16    17    19    21    22\n",
              "[2,]    2    2    1    3    1    5    1   18    1     1     2     3     1     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    23    24    25    26    27    28    29    32    33    35    36    37\n",
              "[2,]     2     1     2     2     1     9    13     2     4     1     6     4\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    38    39    42    43    46    47    48    49    51    53    55    56\n",
              "[2,]     5     3     2    12     4     2    16    10     3     2     9     8\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    57    58    59    60    63    64    66    68    69    70    72    77\n",
              "[2,]     2     1     1     6     1     1    10     2     1     1     2     4\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    78    79    80    81    84    85    87    88    91    92    93    94\n",
              "[2,]     5     2     3     2     2     1     3     3     5     5     3     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    95    97    98   100   101   102   103   104   105   106   107   109\n",
              "[2,]     1     6     5     3     1     5     3     1     3     5     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   111   112   113   114   115   116   117   118   119   120   121   122\n",
              "[2,]     8     1     1    32     8     1   135     4     1     1     1     2\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   123   125   129   130   131   132   133   134   135   136   138   139\n",
              "[2,]     2     2     2     1     1     1     1     1     1     4     1    18\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   141    142    143    144    145    147    148    149    152    153\n",
              "[2,]     6      2      7      1      1      4      4      2      1      6\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    154    155    156    157    158    159    160    161    162    163\n",
              "[2,]     19      3      6      2      2      1      1      7      1      3\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    165    166    167    169    170    171    172    174    175    176\n",
              "[2,]      1      1      1      3      1      1      1      1      1      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    177    178    179    181    183    184    186    189    190    192\n",
              "[2,]      2      1      1      1      1     29      1      5      2      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    194    195    197    200    202    203    205    206    208    210\n",
              "[2,]     20      1      1      1      1      1      2      1      2      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    211    212    213    214    216    218    219    220    221    222\n",
              "[2,]      1      7      1      1     27      1      3      5      3      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    223    224    226    227    228    229    230    231    232    234\n",
              "[2,]      4      2      2      2      1      6      1      5      3      3\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    235    237    238    239    240    241    242    243    244    245\n",
              "[2,]      2      5      6      4      4      4      8      1      1      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    247    248    252    254    255    257    258    259    261    263\n",
              "[2,]      2      1      2      6      1      1      5      7      2      2\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    264    266    269    270    273    274    275    276    277    279\n",
              "[2,]      2      1      1      2      3      6      2      1      1      8\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    281    282    283    285    286    287    288    289    290    291\n",
              "[2,]     21      1      3      5      3      1      1      1      2      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    292    294    295    296    298    299    300    304    305    307\n",
              "[2,]      3      2      1      5      6      1      1      1      6      2\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    308    309    310    311    312    314    315    317    318    319\n",
              "[2,]      1      4     21      1      7      3     15      1      4      1\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    320    322    324    326    329    330    331    332    334    336\n",
              "[2,]      2      1      4      2      1      1      3      1      7      1\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    337    338    339    340    341    343    345    346    347    348\n",
              "[2,]      1     66     11      1      4      1      2      2      6      4\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    349    350    352    353    354    356    358    359    360    361\n",
              "[2,]      3      7      2      1      1      1      1      3      6      2\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    362    364    366    367    368    371    374    375    377    378\n",
              "[2,]      6      1      3      1     10      2      1      4      4      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    379    380    381    383    385    386    387    388    390    391\n",
              "[2,]      1      2      2      1      3      2      3      3      3      2\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    392    394    395    396    397    400    402    403    404    407\n",
              "[2,]      1      1      2      1      1      1      4      7      1      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294]\n",
              "[1,]    408    409    410    413    414    416\n",
              "[2,]      1      1      1      1      4     11\n",
              "\n",
              "$documents$text10\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    6    7    9   10   12    13    15    16    17    18\n",
              "[2,]    3    2    2   11    1    4    1    4    4    17     4     1     1     2\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    19    21    22    23    24    27    28    29    30    31    32    33\n",
              "[2,]     1     3     4     1     1     3     8    28     1     1     1     3\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    34    35    36    37    38    39    40    43    45    46    47    48\n",
              "[2,]     3     2     4     8     2     4     1     9     2     5    15     5\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    49    50    51    52    54    55    56    57    58    60    61    62\n",
              "[2,]     1     2     1     2    17    41    16     9     4     3     4     3\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    63    64    65    66    68    69    70    72    73    76    77    78\n",
              "[2,]     1     1     2     6     2     1     1     1     2     3     4     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    79    80    81    82    83    84    85    86    87    88    89    90\n",
              "[2,]     1     6     1     1     1     1     1     2    13     7     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]    92    93    94    95    96    97    99   101   102   104   106   107\n",
              "[2,]     5     3     1     2     3    20     2     1     2     3     7     4\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   109   110   111   112   113   114   115   118   120   121   122   123\n",
              "[2,]     2     2     1     2     1     5     1     2     1    21     9     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   124    127    129    130    131    132    133    134    137    139\n",
              "[2,]     1      2      1      1      3      1      2      2      1     17\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    141    142    143    144    146    147    148    150    153    154\n",
              "[2,]     35      2      8      2      1      2      5      1      3      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    156    159    160    161    163    164    165    166    167    168\n",
              "[2,]      3      3     13     12      1      5      1      3      2      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    170    171    172    174    175    176    177    178    179    180\n",
              "[2,]      4      1      1      1      1      1      6      1      1      1\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    181    184    186    187    188    193    194    196    197    198\n",
              "[2,]      2      5      1      4      1      6      8     13      3      9\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    199    200    202    203    204    205    206    210    212    214\n",
              "[2,]      2      4      2      6      1      1      2      2      8      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    215    216    217    218    219    220    221    222    223    224\n",
              "[2,]      1      2      8      2      1      6     14      2      4      2\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    225    226    227    228    229    230    231    232    233    234\n",
              "[2,]      1      2      1      1      2      1      3      4      4      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    235    236    237    238    239    240    241    242    244    245\n",
              "[2,]      3      6      7      2      1      3      4      4      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    247    248    249    251    253    254    255    256    258    259\n",
              "[2,]      1      1      1      1      2      5      2      1      9      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    261    262    263    264    265    267    269    270    271    272\n",
              "[2,]      3      1      2      1      4      2      3      2      1      5\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    273    274    275    276    278    279    281    282    283    286\n",
              "[2,]      1      7      2      1      2      8     14      1      3      4\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    289    290    291    292    295    296    298    299    301    302\n",
              "[2,]      1      4      3      2      1      4      6      1      2      2\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    305    307    308    309    311    312    313    315    316    317\n",
              "[2,]      5      1      1      2      1      2      1      1      6      1\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    318    319    320    322    323    326    329    331    332    333\n",
              "[2,]      5      1      1      3      3     18      3     15      9      8\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    334    335    337    338    339    340    341    343    344    345\n",
              "[2,]     11      1      3     11      1      3      2     14      4      1\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    346    347    348    350    351    352    353    354    355    356\n",
              "[2,]      1      6      3     10      1      1      3      1     17      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    357    358    360    362    363    364    366    368    369    370\n",
              "[2,]      1     19     53      4      4      3     50     12      2     11\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    371    372    373    375    376    378    379    380    381    382\n",
              "[2,]     12      1      3      7     34      6     12      6      1      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294] [,295] [,296] [,297] [,298]\n",
              "[1,]    383    384    385    386    387    388    390    391    392    393\n",
              "[2,]      9      1      3     14      2      3      1      1      3      2\n",
              "     [,299] [,300] [,301] [,302] [,303] [,304] [,305] [,306] [,307] [,308]\n",
              "[1,]    394    395    400    401    403    404    405    406    407    408\n",
              "[2,]      2      2      1      2     10      1      2      9      1      1\n",
              "     [,309] [,310] [,311] [,312] [,313] [,314] [,315] [,316]\n",
              "[1,]    409    410    412    413    414    415    416    417\n",
              "[2,]      1      1      1      1      9      3      7      4\n",
              "\n",
              "$documents$text11\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    7    8    9   10   11   13    14    15    17    18    19\n",
              "[2,]    1    2    2    2    5    3    8    6   16     1     1     4     2     3\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    20    21    22    24    26    29    30    32    33    35    36    37\n",
              "[2,]    42     6    15     1     2     4     2     1     3     4    10     3\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    39    41    42    43    44    45    46    47    51    53    56    57\n",
              "[2,]     2     2     1     8     1     1     9     1     1     7     2    11\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    60    61    62    64    66    68    69    70    75    76    78    80\n",
              "[2,]     2     1     3     5     5     1     4     1     2     3     2     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    82    83    84    85    88    89    90    91    92    93    94    95\n",
              "[2,]     1     1     1     1     4     1     1     4     5     3     1     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    96    97    98    99   100   101   102   104   105   106   107   108\n",
              "[2,]     4    23     2    85     9     1     1     1     2     5     5     2\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   109   112   113   114   116   120   121   122   124   126   128   129\n",
              "[2,]     6     1     3     3     1     1    14     5     5     1    37     4\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   130   132   134   135   136   138   139   141   142   143   144   145\n",
              "[2,]     6     1     1     2     8     1    16    15     2     5     1     3\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   146    147    150    153    154    155    156    157    159    161\n",
              "[2,]     1      4      5      3      4      2      2      4      1      3\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    162    163    164    165    168    169    171    172    173    175\n",
              "[2,]      3      3      1      2      1      1      1      7      4      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    176    177    178    179    180    181    182    183    184    186\n",
              "[2,]      1     33      1      1      2      1      5      8      2      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    187    188    189    190    193    194    195    198    199    201\n",
              "[2,]      8      1      2      3      2     10      2      1      1      2\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    202    203    205    207    209    212    214    218    220    221\n",
              "[2,]      1      2      1      2      2      8      3     10     12      5\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    222    223    224    225    226    227    228    230    231    232\n",
              "[2,]      1      4      3      1      2      1      2      1      7      3\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    233    235    237    239    240    241    242    244    245    247\n",
              "[2,]      2      3      9      9      3      2     10      1      2      1\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    251    252    254    255    257    258    259    260    261    263\n",
              "[2,]     38      1      1      7      1      9      1      3      9      2\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    266    268    269    270    272    273    274    275    276    277\n",
              "[2,]      2      2     10      2      3      3      7      4      1      4\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    279    280    281    282    283    284    285    286    289    290\n",
              "[2,]      8      3     20      1      3     16     12      2      5      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    291    292    296    297    298    299    300    304    305    306\n",
              "[2,]      2      2      3      1      3      5      1     10      1      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    307    308    309    311    317    318    319    320    321    323\n",
              "[2,]      1      3     12      1      1      5      1      2      1      3\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    326    329    333    334    336    337    338    339    340    341\n",
              "[2,]     11      6      1      8      1      1     17     12      3      2\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    343    345    346    347    348    350    352    354    356    358\n",
              "[2,]      2      4      3      8      2      5      1      1      1      2\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    360    361    362    364    366    367    368    370    371    373\n",
              "[2,]      1      2      3      2     20      1     13      4     28      6\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    375    376    377    379    380    382    384    385    386    387\n",
              "[2,]      6      4      2     15      1      1      1      3     23      1\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    388    389    393    394    395    399    401    402    403    404\n",
              "[2,]      5      1      2      1      2      1      2      2      3      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275]\n",
              "[1,]    406    407    408    409    415    416    417\n",
              "[2,]      1      1      2      1     10     12      5\n",
              "\n",
              "$documents$text12\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    5    6    7   13   15   19    22    25    28    33    34\n",
              "[2,]    1    1    1    1    1    4    1    2    1     1     2     3     1     2\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    36    39    43    59    61    67    88    89    95    97   106   108\n",
              "[2,]     1     3     1     1     1     1     1     1     1     2     1     1\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]   112   123   125   127   133   134   139   141   153   165   170   173\n",
              "[2,]     1     1     1     2     1     1     1     1     1     1     1     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]   184   194   204   206   212   214   216   220   221   228   229   231\n",
              "[2,]     4     2     1     1     1     1     3     1     2     1     2     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]   235   242   246   252   263   267   271   280   284   286   287   297\n",
              "[2,]     1     9     2     3     1     3     1     1     1     1     1     2\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]   298   299   300   313   327   328   334   347   358   366   375   376\n",
              "[2,]     3     3     3     1     1     4     5     3     1     1     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81]\n",
              "[1,]   379   382   383   390   394   399   416\n",
              "[2,]     2     2     6     1     1     1     1\n",
              "\n",
              "$documents$text13\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    5    6    9   10   12   13    15    17    18    21    22\n",
              "[2,]    2    1    7    1    1    1    2    1    6     1     1     1     1     5\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    23    24    26    27    31    32    33    35    36    37    39    42\n",
              "[2,]     3     2     2     1     2     1     3     1     2     2     1     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    43    44    47    48    50    52    53    57    61    62    66    67\n",
              "[2,]    21     1     2     3     1     3     2     3     1     1     6     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    70    71    75    76    79    80    83    84    85    88    92    93\n",
              "[2,]     2     4     2     3     2     2     1     1     1     1     5     3\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    95    96    97    98    99   101   102   103   105   106   108   109\n",
              "[2,]     4     1    35    14     6     1     4     1     1     2    12     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]   111   112   114   115   117   118   120   122   123   125   126   127\n",
              "[2,]     1     2     3     3    54     1     1     3     2     1     1     5\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   129   131   133   134   136   137   138   139   140   143   144   145\n",
              "[2,]     1     1     1     1     2     1     3     6     1     3     1     1\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   148   149   153   155   159   160   161   163   164   165   168   170\n",
              "[2,]     1     1     4     3     1     2     3     3     1     1     1     4\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   171    172    176    177    178    179    180    184    186    188\n",
              "[2,]     1     11      1      1      1      1      4      5      1      2\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    189    190    193    194    195    196    197    199    201    202\n",
              "[2,]      3      4      1     19      1      1      3      1      4     24\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    203    204    205    206    207    208    209    212    214    216\n",
              "[2,]      2      1      3      2      2      1      1      3      1     21\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    217    218    219    223    224    225    226    227    228    232\n",
              "[2,]      1      3      1      4      2      2      2      1      2      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    233    234    235    236    237    238    239    240    241    242\n",
              "[2,]      1      1      1      1      7      3      1      4      1      2\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    244    246    250    251    252    254    261    262    263    268\n",
              "[2,]      1      1      3      1      6      1      1      8      2      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    269    270    274    275    276    278    279    282    283    285\n",
              "[2,]      1      2      2      1      1      3      5      1      3      1\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    287    288    291    295    296    297    298    299    307    311\n",
              "[2,]      1      2      1      1      7      1     11      5      1      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    312    313    314    315    317    318    319    323    325    326\n",
              "[2,]      3      1      1      2      1      1     14      1      1      5\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    327    328    331    334    336    339    341    342    343    345\n",
              "[2,]      5      5      4      8      2      3      2      1      3      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    347    348    351    352    354    355    357    358    361    364\n",
              "[2,]      3      2      1      1      1      1      1      1      1      5\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    365    366    368    369    370    371    372    376    378    379\n",
              "[2,]      2      1      4      1      1      2      1      1      1      3\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    385    387    388    390    391    392    393    395    396    398\n",
              "[2,]      1      3      4      1      3      7      2      2      1      1\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236]\n",
              "[1,]    401    407    408    409    411    412    413    416\n",
              "[2,]      4      2      1      1      1      1      2      2\n",
              "\n",
              "$documents$text14\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    6    8    9   10   13    14    20    22    23    24\n",
              "[2,]    1    3    3    1    1   10    1    4   69    20     7     4     1     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    26    28    29    31    32    33    35    36    38    39    41    43\n",
              "[2,]     2     1     3     1     1     2     1     5     6     3     2     8\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    45    47    48    49    50    55    56    57    60    66    68    69\n",
              "[2,]   114     4     1     1     4     2     2    19     3     5     1     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    70    71    73    75    76    78    80    81    84    85    88    89\n",
              "[2,]     1     4    24     1     1     4     2     2     1     1     1     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    90    91    92    93    94    95    96    97    98    99   101   102\n",
              "[2,]     1     3     5     3     3     3   209    10     4     1     1     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]   103   106   107   109   110   112   114   120   121   122   124   126\n",
              "[2,]     3     6     4     1    16     9     2     1     4     3     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   128   130   131   133   136   138   139   141   143   144   145   149\n",
              "[2,]     7     3     1     2     5     2    23    20     3     1     4     1\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   150   152   153   155   156   158   161   162   163   164   166   168\n",
              "[2,]     2     4     5     1     1     1     3     1     3    16     1     5\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   171    172    174    176    178    179    180    181    182    183\n",
              "[2,]     1      6      1      1      1      1      2      2      8      2\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    184    186    187    189    191    192    193    194    195    196\n",
              "[2,]      4      1      7      2      1      1      1     10      2      2\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    200    201    203    204    205    206    210    211    212    214\n",
              "[2,]      2      1      1      2      1      1      1      1      8      2\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    218    222    223    224    225    226    227    228    229    232\n",
              "[2,]      8      2      4      2      4      2      5      5      4      2\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    233    235    238    239    240    241    242    243    244    246\n",
              "[2,]      2      2     11      1      3      1      8      3      1      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    253    255    256    259    260    261    262    263    264    265\n",
              "[2,]      5      1      4      5      2      1     19      2      1      2\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    267    269    270    271    274    276    279    281    282    283\n",
              "[2,]      1      1      2      3      9      1     17     10      1      3\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    286    287    288    290    291    292    296    298    299    301\n",
              "[2,]      1      1      1      7      1      1     11      2      8      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    302    303    304    309    310    311    312    315    317    322\n",
              "[2,]      1      1     46      4      1      1      1      1      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    323    324    326    329    330    333    334    338    339    344\n",
              "[2,]     11      1    150      1      4      3      9      2      1      2\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    345    346    347    348    350    352    354    358    360    364\n",
              "[2,]      1      1      3      2      5      1      1      7      1      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    365    366    367    368    371    374    375    376    377    380\n",
              "[2,]      1      2     79      2      5      1      1     33      5      4\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    382    383    388    392    394    395    397    398    402    403\n",
              "[2,]      1      1      3      1      1      2      1      2      8      2\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235]\n",
              "[1,]    407    408    409    410    413    415    417\n",
              "[2,]      3      4      1      1      1      1      1\n",
              "\n",
              "$documents$text15\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    6    7    9   10   11    12    13    14    15    16\n",
              "[2,]    3    3    6    2    1    1    1    8    2     4    10    11     1     2\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    20    21    22    23    24    25    26    29    31    32    33    34\n",
              "[2,]     9     1     2     1     4     1     2     1     1    13     3     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    35    36    38    39    40    42    43    44    45    46    47    48\n",
              "[2,]     4    13     4     2     2     1     8     1     4    15     3     6\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    49    50    51    52    53    54    55    56    57    60    61    63\n",
              "[2,]     1     1     1     2     2    59    12     9    18    14     1     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    64    65    66    68    69    70    71    72    73    74    75    77\n",
              "[2,]    14     1     8     1     3     3    13     4     6     1     4     4\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    78    80    81    82    84    85    86    87    88    91    92    93\n",
              "[2,]     3     3     2     3     1     1     9     6    18     5     5     3\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]    94    95    96    97    98    99   100   101   102   103   104   105\n",
              "[2,]     8     2     2    61     3     5     2     2    18     4     5    13\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   106   107   108   109   111   112   113   114   115   117   118   119\n",
              "[2,]     2     5     5     1     5     3     1     5     1     5     1     2\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   120    121    122    128    129    130    131    132    133    134\n",
              "[2,]     1      5      4      6      1      2      3      1      1      1\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    135    136    137    138    139    140    141    143    144    145\n",
              "[2,]      1      5      2      2     10      1     14     10      1      4\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    150    151    152    153    154    155    156    158    159    160\n",
              "[2,]     10      1      1      5      2      3     20      2      1      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    161    162    163    165    167    169    170    171    172    173\n",
              "[2,]     10      3      3      1      9     10      6      1      3      7\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    176    177    181    182    184    186    187    188    190    192\n",
              "[2,]      1      3      1      1      1      1     13      5      4      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    193    194    195    196    197    198    199    200    202    203\n",
              "[2,]      6      6      4      5      3      4     14      2      5      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    205    209    212    213    214    215    216    217    218    221\n",
              "[2,]      1      2      8      3      1      1      4      1      3      2\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    222    223    224    226    227    228    230    231    232    233\n",
              "[2,]      1      4      4      2      2      1      1      1      3      4\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    235    236    238    239    240    241    242    244    245    246\n",
              "[2,]      4      2      4      3      3      1      8      1      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    248    249    251    253    258    259    260    261    262    263\n",
              "[2,]     11      1      5      1      1      3     14      1     34      2\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    264    266    267    268    269    270    271    272    273    274\n",
              "[2,]      6      2      1      6      2      2      1      2      2      7\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    275    276    277    279    281    282    283    284    286    291\n",
              "[2,]      6      1      4     14      2      1      3      1      4      1\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    294    295    296    297    298    299    301    302    303    304\n",
              "[2,]      2      2      6      1     14      2      1      1      8      3\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    305    306    307    308    309    310    311    312    313    314\n",
              "[2,]      6     14      3      2      3      1      1      7     12      1\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    315    316    317    318    319    320    321    322    324    325\n",
              "[2,]      7      5      1     11     22      1     11      2      3      3\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    326    329    330    331    332    333    334    335    336    337\n",
              "[2,]      3      6     18      8      6      6      8      1      2      1\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    338    339    341    342    345    347    348    349    350    351\n",
              "[2,]     10      2      2      2     17      4     28     19      4      2\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    352    353    354    355    356    357    358    359    360    363\n",
              "[2,]      1      2      1      2      1      3     11      7      1      2\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    364    365    366    369    371    372    373    374    375    378\n",
              "[2,]     11      2      7      1      3      2      2      4      7      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294] [,295] [,296] [,297] [,298]\n",
              "[1,]    380    381    382    384    387    388    390    391    393    394\n",
              "[2,]      7      1      3      1      1     16      1      1      3      4\n",
              "     [,299] [,300] [,301] [,302] [,303] [,304] [,305] [,306] [,307] [,308]\n",
              "[1,]    395    396    397    398    400    401    402    403    405    407\n",
              "[2,]      2      1      1      1      2      1      2     24      2      2\n",
              "     [,309] [,310] [,311] [,312] [,313] [,314] [,315]\n",
              "[1,]    408    409    410    411    413    414    417\n",
              "[2,]      1      1      4      1      1      1      1\n",
              "\n",
              "\n",
              "$vocab\n",
              "  [1] \"abl\"                                              \n",
              "  [2] \"abstract\"                                         \n",
              "  [3] \"access\"                                           \n",
              "  [4] \"accord\"                                           \n",
              "  [5] \"accur\"                                            \n",
              "  [6] \"activ\"                                            \n",
              "  [7] \"actual\"                                           \n",
              "  [8] \"ad\"                                               \n",
              "  [9] \"adapt\"                                            \n",
              " [10] \"addit\"                                            \n",
              " [11] \"address\"                                          \n",
              " [12] \"aim\"                                              \n",
              " [13] \"al\"                                               \n",
              " [14] \"algorithm\"                                        \n",
              " [15] \"allow\"                                            \n",
              " [16] \"alreadi\"                                          \n",
              " [17] \"although\"                                         \n",
              " [18] \"amount\"                                           \n",
              " [19] \"analyz\"                                           \n",
              " [20] \"annot\"                                            \n",
              " [21] \"anoth\"                                            \n",
              " [22] \"appli\"                                            \n",
              " [23] \"applic\"                                           \n",
              " [24] \"appropri\"                                         \n",
              " [25] \"area\"                                             \n",
              " [26] \"articl\"                                           \n",
              " [27] \"aspect\"                                           \n",
              " [28] \"assess\"                                           \n",
              " [29] \"assign\"                                           \n",
              " [30] \"assum\"                                            \n",
              " [31] \"ation\"                                            \n",
              " [32] \"attribut\"                                         \n",
              " [33] \"author\"                                           \n",
              " [34] \"autom\"                                            \n",
              " [35] \"automat\"                                          \n",
              " [36] \"avail\"                                            \n",
              " [37] \"averag\"                                           \n",
              " [38] \"b\"                                                \n",
              " [39] \"background\"                                       \n",
              " [40] \"basic\"                                            \n",
              " [41] \"becom\"                                            \n",
              " [42] \"biolog\"                                           \n",
              " [43] \"biomed\"                                           \n",
              " [44] \"built\"                                            \n",
              " [45] \"c\"                                                \n",
              " [46] \"calcul\"                                           \n",
              " [47] \"care\"                                             \n",
              " [48] \"case\"                                             \n",
              " [49] \"categor\"                                          \n",
              " [50] \"categori\"                                         \n",
              " [51] \"caus\"                                             \n",
              " [52] \"challeng\"                                         \n",
              " [53] \"characterist\"                                     \n",
              " [54] \"class\"                                            \n",
              " [55] \"classif\"                                          \n",
              " [56] \"classifi\"                                         \n",
              " [57] \"clinic\"                                           \n",
              " [58] \"close\"                                            \n",
              " [59] \"cluster\"                                          \n",
              " [60] \"code\"                                             \n",
              " [61] \"coher\"                                            \n",
              " [62] \"collect\"                                          \n",
              " [63] \"com-\"                                             \n",
              " [64] \"combin\"                                           \n",
              " [65] \"come\"                                             \n",
              " [66] \"common\"                                           \n",
              " [67] \"communiti\"                                        \n",
              " [68] \"compar\"                                           \n",
              " [69] \"comparison\"                                       \n",
              " [70] \"complex\"                                          \n",
              " [71] \"comput\"                                           \n",
              " [72] \"con-\"                                             \n",
              " [73] \"concept\"                                          \n",
              " [74] \"concern\"                                          \n",
              " [75] \"condit\"                                           \n",
              " [76] \"conduct\"                                          \n",
              " [77] \"consid\"                                           \n",
              " [78] \"consist\"                                          \n",
              " [79] \"construct\"                                        \n",
              " [80] \"contain\"                                          \n",
              " [81] \"content\"                                          \n",
              " [82] \"context\"                                          \n",
              " [83] \"contribut\"                                        \n",
              " [84] \"copi\"                                             \n",
              " [85] \"copyright\"                                        \n",
              " [86] \"core\"                                             \n",
              " [87] \"correct\"                                          \n",
              " [88] \"correspond\"                                       \n",
              " [89] \"countri\"                                          \n",
              " [90] \"cover\"                                            \n",
              " [91] \"creat\"                                            \n",
              " [92] \"creativ\"                                          \n",
              " [93] \"credit\"                                           \n",
              " [94] \"criteria\"                                         \n",
              " [95] \"current\"                                          \n",
              " [96] \"d\"                                                \n",
              " [97] \"data\"                                             \n",
              " [98] \"databas\"                                          \n",
              " [99] \"dataset\"                                          \n",
              "[100] \"de-\"                                              \n",
              "[101] \"dedic\"                                            \n",
              "[102] \"defin\"                                            \n",
              "[103] \"definit\"                                          \n",
              "[104] \"depend\"                                           \n",
              "[105] \"deriv\"                                            \n",
              "[106] \"describ\"                                          \n",
              "[107] \"descript\"                                         \n",
              "[108] \"design\"                                           \n",
              "[109] \"detail\"                                           \n",
              "[110] \"detect\"                                           \n",
              "[111] \"determin\"                                         \n",
              "[112] \"develop\"                                          \n",
              "[113] \"differ-\"                                          \n",
              "[114] \"direct\"                                           \n",
              "[115] \"dis-\"                                             \n",
              "[116] \"discuss\"                                          \n",
              "[117] \"diseas\"                                           \n",
              "[118] \"distinct\"                                         \n",
              "[119] \"distinguish\"                                      \n",
              "[120] \"distribut\"                                        \n",
              "[121] \"document\"                                         \n",
              "[122] \"domain\"                                           \n",
              "[123] \"due\"                                              \n",
              "[124] \"earli\"                                            \n",
              "[125] \"edg\"                                              \n",
              "[126] \"effect\"                                           \n",
              "[127] \"effort\"                                           \n",
              "[128] \"ehr\"                                              \n",
              "[129] \"either\"                                           \n",
              "[130] \"electron\"                                         \n",
              "[131] \"enabl\"                                            \n",
              "[132] \"enc\"                                              \n",
              "[133] \"encod\"                                            \n",
              "[134] \"end\"                                              \n",
              "[135] \"ent\"                                              \n",
              "[136] \"entiti\"                                           \n",
              "[137] \"equal\"                                            \n",
              "[138] \"establish\"                                        \n",
              "[139] \"et\"                                               \n",
              "[140] \"etc\"                                              \n",
              "[141] \"evalu\"                                            \n",
              "[142] \"even\"                                             \n",
              "[143] \"exampl\"                                           \n",
              "[144] \"exceed\"                                           \n",
              "[145] \"exist\"                                            \n",
              "[146] \"expect\"                                           \n",
              "[147] \"experi\"                                           \n",
              "[148] \"expert\"                                           \n",
              "[149] \"explicit\"                                         \n",
              "[150] \"express\"                                          \n",
              "[151] \"extend\"                                           \n",
              "[152] \"extern\"                                           \n",
              "[153] \"extract\"                                          \n",
              "[154] \"featur\"                                           \n",
              "[155] \"field\"                                            \n",
              "[156] \"fig\"                                              \n",
              "[157] \"figur\"                                            \n",
              "[158] \"file\"                                             \n",
              "[159] \"final\"                                            \n",
              "[160] \"focus\"                                            \n",
              "[161] \"form\"                                             \n",
              "[162] \"formal\"                                           \n",
              "[163] \"format\"                                           \n",
              "[164] \"free\"                                             \n",
              "[165] \"full\"                                             \n",
              "[166] \"fulli\"                                            \n",
              "[167] \"function\"                                         \n",
              "[168] \"futur\"                                            \n",
              "[169] \"general\"                                          \n",
              "[170] \"generat\"                                          \n",
              "[171] \"give\"                                             \n",
              "[172] \"health\"                                           \n",
              "[173] \"healthcar\"                                        \n",
              "[174] \"help\"                                             \n",
              "[175] \"histori\"                                          \n",
              "[176] \"holder\"                                           \n",
              "[177] \"hospit\"                                           \n",
              "[178] \"http://creativecommons.org/licenses/by/4.0/.\"     \n",
              "[179] \"http://creativecommons.org/publicdomain/zero/1.0/\"\n",
              "[180] \"human\"                                            \n",
              "[181] \"ical\"                                             \n",
              "[182] \"id\"                                               \n",
              "[183] \"identif\"                                          \n",
              "[184] \"identifi\"                                         \n",
              "[185] \"illustr\"                                          \n",
              "[186] \"imag\"                                             \n",
              "[187] \"implement\"                                        \n",
              "[188] \"import\"                                           \n",
              "[189] \"improv\"                                           \n",
              "[190] \"in-\"                                              \n",
              "[191] \"inclus\"                                           \n",
              "[192] \"index\"                                            \n",
              "[193] \"individu\"                                         \n",
              "[194] \"inform\"                                           \n",
              "[195] \"informat\"                                         \n",
              "[196] \"ing\"                                              \n",
              "[197] \"initi\"                                            \n",
              "[198] \"input\"                                            \n",
              "[199] \"instanc\"                                          \n",
              "[200] \"instead\"                                          \n",
              "[201] \"institut\"                                         \n",
              "[202] \"integr\"                                           \n",
              "[203] \"intend\"                                           \n",
              "[204] \"interest\"                                         \n",
              "[205] \"intern\"                                           \n",
              "[206] \"interpret\"                                        \n",
              "[207] \"introduc\"                                         \n",
              "[208] \"involv\"                                           \n",
              "[209] \"issu\"                                             \n",
              "[210] \"iti\"                                              \n",
              "[211] \"j\"                                                \n",
              "[212] \"journal\"                                          \n",
              "[213] \"key\"                                              \n",
              "[214] \"keyword\"                                          \n",
              "[215] \"kind\"                                             \n",
              "[216] \"knowledg\"                                         \n",
              "[217] \"label\"                                            \n",
              "[218] \"languag\"                                          \n",
              "[219] \"lead\"                                             \n",
              "[220] \"learn\"                                            \n",
              "[221] \"level\"                                            \n",
              "[222] \"librari\"                                          \n",
              "[223] \"licenc\"                                           \n",
              "[224] \"licens\"                                           \n",
              "[225] \"limit\"                                            \n",
              "[226] \"line\"                                             \n",
              "[227] \"link\"                                             \n",
              "[228] \"list\"                                             \n",
              "[229] \"literatur\"                                        \n",
              "[230] \"logic\"                                            \n",
              "[231] \"machin\"                                           \n",
              "[232] \"made\"                                             \n",
              "[233] \"main\"                                             \n",
              "[234] \"major\"                                            \n",
              "[235] \"make\"                                             \n",
              "[236] \"manner\"                                           \n",
              "[237] \"manual\"                                           \n",
              "[238] \"map\"                                              \n",
              "[239] \"match\"                                            \n",
              "[240] \"materi\"                                           \n",
              "[241] \"mean\"                                             \n",
              "[242] \"medic\"                                            \n",
              "[243] \"medicin\"                                          \n",
              "[244] \"medium\"                                           \n",
              "[245] \"ment\"                                             \n",
              "[246] \"methodolog\"                                       \n",
              "[247] \"might\"                                            \n",
              "[248] \"model\"                                            \n",
              "[249] \"moreov\"                                           \n",
              "[250] \"multipl\"                                          \n",
              "[251] \"name\"                                             \n",
              "[252] \"nation\"                                           \n",
              "[253] \"natur\"                                            \n",
              "[254] \"network\"                                          \n",
              "[255] \"next\"                                             \n",
              "[256] \"normal\"                                           \n",
              "[257] \"now\"                                              \n",
              "[258] \"number\"                                           \n",
              "[259] \"object\"                                           \n",
              "[260] \"observ\"                                           \n",
              "[261] \"obtain\"                                           \n",
              "[262] \"ontolog\"                                          \n",
              "[263] \"open\"                                             \n",
              "[264] \"oper\"                                             \n",
              "[265] \"optim\"                                            \n",
              "[266] \"option\"                                           \n",
              "[267] \"order\"                                            \n",
              "[268] \"organ\"                                            \n",
              "[269] \"origin\"                                           \n",
              "[270] \"otherwis\"                                         \n",
              "[271] \"outcom\"                                           \n",
              "[272] \"output\"                                           \n",
              "[273] \"overal\"                                           \n",
              "[274] \"page\"                                             \n",
              "[275] \"part\"                                             \n",
              "[276] \"parti\"                                            \n",
              "[277] \"particip\"                                         \n",
              "[278] \"particular\"                                       \n",
              "[279] \"patient\"                                          \n",
              "[280] \"pattern\"                                          \n",
              "[281] \"perform\"                                          \n",
              "[282] \"permiss\"                                          \n",
              "[283] \"permit\"                                           \n",
              "[284] \"person\"                                           \n",
              "[285] \"point\"                                            \n",
              "[286] \"possibl\"                                          \n",
              "[287] \"practic\"                                          \n",
              "[288] \"pre-\"                                             \n",
              "[289] \"precis\"                                           \n",
              "[290] \"predict\"                                          \n",
              "[291] \"present\"                                          \n",
              "[292] \"previous\"                                         \n",
              "[293] \"principl\"                                         \n",
              "[294] \"pro-\"                                             \n",
              "[295] \"problem\"                                          \n",
              "[296] \"process\"                                          \n",
              "[297] \"program\"                                          \n",
              "[298] \"provid\"                                           \n",
              "[299] \"public\"                                           \n",
              "[300] \"publish\"                                          \n",
              "[301] \"purpos\"                                           \n",
              "[302] \"qualiti\"                                          \n",
              "[303] \"queri\"                                            \n",
              "[304] \"r\"                                                \n",
              "[305] \"rang\"                                             \n",
              "[306] \"reason\"                                           \n",
              "[307] \"receiv\"                                           \n",
              "[308] \"recent\"                                           \n",
              "[309] \"record\"                                           \n",
              "[310] \"refer\"                                            \n",
              "[311] \"regul\"                                            \n",
              "[312] \"relat\"                                            \n",
              "[313] \"relev\"                                            \n",
              "[314] \"remain\"                                           \n",
              "[315] \"repres\"                                           \n",
              "[316] \"represent\"                                        \n",
              "[317] \"reproduct\"                                        \n",
              "[318] \"requir\"                                           \n",
              "[319] \"resourc\"                                          \n",
              "[320] \"respect\"                                          \n",
              "[321] \"restrict\"                                         \n",
              "[322] \"result\"                                           \n",
              "[323] \"review\"                                           \n",
              "[324] \"risk\"                                             \n",
              "[325] \"role\"                                             \n",
              "[326] \"s\"                                                \n",
              "[327] \"scienc\"                                           \n",
              "[328] \"scientif\"                                         \n",
              "[329] \"score\"                                            \n",
              "[330] \"search\"                                           \n",
              "[331] \"section\"                                          \n",
              "[332] \"see\"                                              \n",
              "[333] \"select\"                                           \n",
              "[334] \"semant\"                                           \n",
              "[335] \"sens\"                                             \n",
              "[336] \"sent\"                                             \n",
              "[337] \"separ\"                                            \n",
              "[338] \"set\"                                              \n",
              "[339] \"share\"                                            \n",
              "[340] \"show\"                                             \n",
              "[341] \"shown\"                                            \n",
              "[342] \"sign\"                                             \n",
              "[343] \"similar\"                                          \n",
              "[344] \"sinc\"                                             \n",
              "[345] \"singl\"                                            \n",
              "[346] \"size\"                                             \n",
              "[347] \"sourc\"                                            \n",
              "[348] \"specif\"                                           \n",
              "[349] \"specifi\"                                          \n",
              "[350] \"standard\"                                         \n",
              "[351] \"start\"                                            \n",
              "[352] \"state\"                                            \n",
              "[353] \"statist\"                                          \n",
              "[354] \"statutori\"                                        \n",
              "[355] \"step\"                                             \n",
              "[356] \"still\"                                            \n",
              "[357] \"store\"                                            \n",
              "[358] \"structur\"                                         \n",
              "[359] \"subclass\"                                         \n",
              "[360] \"subject\"                                          \n",
              "[361] \"subset\"                                           \n",
              "[362] \"suggest\"                                          \n",
              "[363] \"suitabl\"                                          \n",
              "[364] \"support\"                                          \n",
              "[365] \"symptom\"                                          \n",
              "[366] \"system\"                                           \n",
              "[367] \"t\"                                                \n",
              "[368] \"tabl\"                                             \n",
              "[369] \"take\"                                             \n",
              "[370] \"target\"                                           \n",
              "[371] \"task\"                                             \n",
              "[372] \"technolog\"                                        \n",
              "[373] \"term\"                                             \n",
              "[374] \"terminolog\"                                       \n",
              "[375] \"test\"                                             \n",
              "[376] \"text\"                                             \n",
              "[377] \"therefor\"                                         \n",
              "[378] \"thus\"                                             \n",
              "[379] \"time\"                                             \n",
              "[380] \"tion\"                                             \n",
              "[381] \"tive\"                                             \n",
              "[382] \"tool\"                                             \n",
              "[383] \"topic\"                                            \n",
              "[384] \"tor\"                                              \n",
              "[385] \"total\"                                            \n",
              "[386] \"train\"                                            \n",
              "[387] \"treatment\"                                        \n",
              "[388] \"type\"                                             \n",
              "[389] \"typic\"                                            \n",
              "[390] \"under\"                                            \n",
              "[391] \"understand\"                                       \n",
              "[392] \"uniqu\"                                            \n",
              "[393] \"unit\"                                             \n",
              "[394] \"univers\"                                          \n",
              "[395] \"unless\"                                           \n",
              "[396] \"updat\"                                            \n",
              "[397] \"upon\"                                             \n",
              "[398] \"usag\"                                             \n",
              "[399] \"use\"                                              \n",
              "[400] \"usual\"                                            \n",
              "[401] \"util\"                                             \n",
              "[402] \"valid\"                                            \n",
              "[403] \"valu\"                                             \n",
              "[404] \"variat\"                                           \n",
              "[405] \"various\"                                          \n",
              "[406] \"version\"                                          \n",
              "[407] \"view\"                                             \n",
              "[408] \"visit\"                                            \n",
              "[409] \"waiver\"                                           \n",
              "[410] \"way\"                                              \n",
              "[411] \"web\"                                              \n",
              "[412] \"whole\"                                            \n",
              "[413] \"wide\"                                             \n",
              "[414] \"without\"                                          \n",
              "[415] \"word\"                                             \n",
              "[416] \"work\"                                             \n",
              "[417] \"written\"                                          \n",
              "\n",
              "$meta\n",
              "data frame with 0 columns and 15 rows\n",
              "\n",
              "$words.removed\n",
              "   [1] \"-0.42\"                                                                       \n",
              "   [2] \"-1\"                                                                          \n",
              "   [3] \"-2\"                                                                          \n",
              "   [4] \"-9\"                                                                          \n",
              "   [5] \"-b\"                                                                          \n",
              "   [6] \"-base\"                                                                       \n",
              "   [7] \"-c\"                                                                          \n",
              "   [8] \"-e\"                                                                          \n",
              "   [9] \"-fr\"                                                                         \n",
              "  [10] \"-h\"                                                                          \n",
              "  [11] \"-ii\"                                                                         \n",
              "  [12] \"-j\"                                                                          \n",
              "  [13] \"-n\"                                                                          \n",
              "  [14] \"-s\"                                                                          \n",
              "  [15] \"-t\"                                                                          \n",
              "  [16] \"-w\"                                                                          \n",
              "  [17] \"0-\"                                                                          \n",
              "  [18] \"1-3\"                                                                         \n",
              "  [19] \"1-4\"                                                                         \n",
              "  [20] \"1.7m\"                                                                        \n",
              "  [21] \"10-15\"                                                                       \n",
              "  [22] \"10-fold\"                                                                     \n",
              "  [23] \"10th\"                                                                        \n",
              "  [24] \"1558-6\"                                                                      \n",
              "  [25] \"1a\"                                                                          \n",
              "  [26] \"1amsterdam\"                                                                  \n",
              "  [27] \"1cnrs\"                                                                       \n",
              "  [28] \"1comput\"                                                                     \n",
              "  [29] \"1depart\"                                                                     \n",
              "  [30] \"1divis\"                                                                      \n",
              "  [31] \"1faculti\"                                                                    \n",
              "  [32] \"1http://www.w3.org/tr/2003/pr-owl-guide-20031209/win\"                        \n",
              "  [33] \"1https://www.atilika.com/ja/kuromoji/\"                                       \n",
              "  [34] \"1https://www.healthboards.com/\"                                              \n",
              "  [35] \"1https://www.ncbi.nlm.nih.gov/pubm\"                                          \n",
              "  [36] \"1in\"                                                                         \n",
              "  [37] \"1informatik\"                                                                 \n",
              "  [38] \"1institut\"                                                                   \n",
              "  [39] \"1k\"                                                                          \n",
              "  [40] \"1like\"                                                                       \n",
              "  [41] \"1nation\"                                                                     \n",
              "  [42] \"1sourc\"                                                                      \n",
              "  [43] \"1space\"                                                                      \n",
              "  [44] \"1univers\"                                                                    \n",
              "  [45] \"2-3\"                                                                         \n",
              "  [46] \"2012-2015\"                                                                   \n",
              "  [47] \"23955-6900\"                                                                  \n",
              "  [48] \"26-11-2018\"                                                                  \n",
              "  [49] \"2a\"                                                                          \n",
              "  [50] \"2b\"                                                                          \n",
              "  [51] \"2c\"                                                                          \n",
              "  [52] \"2castor\"                                                                     \n",
              "  [53] \"2center\"                                                                     \n",
              "  [54] \"2comput\"                                                                     \n",
              "  [55] \"2d\"                                                                          \n",
              "  [56] \"2here\"                                                                       \n",
              "  [57] \"2http://mallet.cs.umass.edu/\"                                                \n",
              "  [58] \"2http://ontofox.hegroup.org\"                                                 \n",
              "  [59] \"2http://xiphoid.biostr.washington.edu/fma/shirt_ontology/shirt_ontology_1.\"  \n",
              "  [60] \"2https://hal.archives-ouvertes.fr/\"                                          \n",
              "  [61] \"2https://medhelp.org/\"                                                       \n",
              "  [62] \"2nd\"                                                                         \n",
              "  [63] \"2univ\"                                                                       \n",
              "  [64] \"3-5-1\"                                                                       \n",
              "  [65] \"3d\"                                                                          \n",
              "  [66] \"3hereinaft\"                                                                  \n",
              "  [67] \"3http://purl.org/sig/ont/fma/fma290055\"                                      \n",
              "  [68] \"3http://www.workingontologist.org/examples.zip\"                              \n",
              "  [69] \"3https://www.istex.fr/\"                                                      \n",
              "  [70] \"432-8011\"                                                                    \n",
              "  [71] \"4a\"                                                                          \n",
              "  [72] \"4a3\"                                                                         \n",
              "  [73] \"4http://www.cl.ecei.tohoku.ac.jp/~m~suzuki/jawiki_vector/\"                   \n",
              "  [74] \"4https://www.i2b2.org/nlp/datasets/main.php\"                                 \n",
              "  [75] \"4recent\"                                                                     \n",
              "  [76] \"5https://github.com/johokugsk\"                                               \n",
              "  [77] \"5https://n2c2.dbmi.hms.harvard.edu/\"                                         \n",
              "  [78] \"5in\"                                                                         \n",
              "  [79] \"60gb\"                                                                        \n",
              "  [80] \"64-year-old\"                                                                 \n",
              "  [81] \"6https://sites.google.com/site/shareclefehealth/\"                            \n",
              "  [82] \"7http://www.cepidc.inserm.fr/\"                                               \n",
              "  [83] \"8-core\"                                                                      \n",
              "  [84] \"86-year-old\"                                                                 \n",
              "  [85] \"8gb\"                                                                         \n",
              "  [86] \"8https://knowledge-learning.github.io/ehealthkd-2019\"                        \n",
              "  [87] \"9c\"                                                                          \n",
              "  [88] \"a.m\"                                                                         \n",
              "  [89] \"a1\"                                                                          \n",
              "  [90] \"a2\"                                                                          \n",
              "  [91] \"aachen\"                                                                      \n",
              "  [92] \"ab\"                                                                          \n",
              "  [93] \"abbrevi\"                                                                     \n",
              "  [94] \"abdelhakim1,2\"                                                               \n",
              "  [95] \"abdomin\"                                                                     \n",
              "  [96] \"abdullah\"                                                                    \n",
              "  [97] \"abil\"                                                                        \n",
              "  [98] \"ablat\"                                                                       \n",
              "  [99] \"abli\"                                                                        \n",
              " [100] \"abnorm\"                                                                      \n",
              " [101] \"absenc\"                                                                      \n",
              " [102] \"abu-hanna1\"                                                                  \n",
              " [103] \"ac\"                                                                          \n",
              " [104] \"ac-\"                                                                         \n",
              " [105] \"acad-\"                                                                       \n",
              " [106] \"academ\"                                                                      \n",
              " [107] \"academicstaff\"                                                               \n",
              " [108] \"acceler\"                                                                     \n",
              " [109] \"accept\"                                                                      \n",
              " [110] \"accom-\"                                                                      \n",
              " [111] \"accommod\"                                                                    \n",
              " [112] \"accomplish\"                                                                  \n",
              " [113] \"accordingly.w\"                                                               \n",
              " [114] \"account\"                                                                     \n",
              " [115] \"accumu-\"                                                                     \n",
              " [116] \"accumul\"                                                                     \n",
              " [117] \"accuraci\"                                                                    \n",
              " [118] \"aceruloplasminemia\"                                                          \n",
              " [119] \"ach\"                                                                         \n",
              " [120] \"acharya8\"                                                                    \n",
              " [121] \"achiev\"                                                                      \n",
              " [122] \"acknow-\"                                                                     \n",
              " [123] \"acknowl-\"                                                                    \n",
              " [124] \"acknowledg\"                                                                  \n",
              " [125] \"acl\"                                                                         \n",
              " [126] \"acm\"                                                                         \n",
              " [127] \"acquir\"                                                                      \n",
              " [128] \"acquisit\"                                                                    \n",
              " [129] \"acroparesthesia\"                                                             \n",
              " [130] \"act\"                                                                         \n",
              " [131] \"acterist\"                                                                    \n",
              " [132] \"action\"                                                                      \n",
              " [133] \"activ-\"                                                                      \n",
              " [134] \"actor\"                                                                       \n",
              " [135] \"acut\"                                                                        \n",
              " [136] \"ad-\"                                                                         \n",
              " [137] \"ad-hoc\"                                                                      \n",
              " [138] \"adam\"                                                                        \n",
              " [139] \"add\"                                                                         \n",
              " [140] \"add-\"                                                                        \n",
              " [141] \"addi-\"                                                                       \n",
              " [142] \"adequ\"                                                                       \n",
              " [143] \"adher\"                                                                       \n",
              " [144] \"adisinsight\"                                                                 \n",
              " [145] \"adjac\"                                                                       \n",
              " [146] \"adjacentregion\"                                                              \n",
              " [147] \"adjust\"                                                                      \n",
              " [148] \"admin-\"                                                                      \n",
              " [149] \"administr\"                                                                   \n",
              " [150] \"admission_num\"                                                               \n",
              " [151] \"admit\"                                                                       \n",
              " [152] \"adopt\"                                                                       \n",
              " [153] \"adr\"                                                                         \n",
              " [154] \"adult\"                                                                       \n",
              " [155] \"advanc\"                                                                      \n",
              " [156] \"advantag\"                                                                    \n",
              " [157] \"advers\"                                                                      \n",
              " [158] \"adverse-drug\"                                                                \n",
              " [159] \"advic\"                                                                       \n",
              " [160] \"advis\"                                                                       \n",
              " [161] \"aesthet\"                                                                     \n",
              " [162] \"af\"                                                                          \n",
              " [163] \"affect\"                                                                      \n",
              " [164] \"affili\"                                                                      \n",
              " [165] \"affili-\"                                                                     \n",
              " [166] \"affilia-\"                                                                    \n",
              " [167] \"afford\"                                                                      \n",
              " [168] \"aforement\"                                                                   \n",
              " [169] \"africa\"                                                                      \n",
              " [170] \"african\"                                                                     \n",
              " [171] \"africanwildlif\"                                                              \n",
              " [172] \"africanwildlifeontology.xml\"                                                 \n",
              " [173] \"africanwildlifeontology0\"                                                    \n",
              " [174] \"africanwildlifeontology1\"                                                    \n",
              " [175] \"africanwildlifeontology1a.owl\"                                               \n",
              " [176] \"africanwildlifeontology2\"                                                    \n",
              " [177] \"africanwildlifeontology2a.owl\"                                               \n",
              " [178] \"africanwildlifeontology3\"                                                    \n",
              " [179] \"africanwildlifeontologyweb.owl\"                                              \n",
              " [180] \"afrikaan\"                                                                    \n",
              " [181] \"afterward\"                                                                   \n",
              " [182] \"ag\"                                                                          \n",
              " [183] \"ag-\"                                                                         \n",
              " [184] \"agar-\"                                                                       \n",
              " [185] \"age\"                                                                         \n",
              " [186] \"agenc\"                                                                       \n",
              " [187] \"agent\"                                                                       \n",
              " [188] \"aggreg\"                                                                      \n",
              " [189] \"aggress\"                                                                     \n",
              " [190] \"agnost\"                                                                      \n",
              " [191] \"ago\"                                                                         \n",
              " [192] \"agre\"                                                                        \n",
              " [193] \"agreement\"                                                                   \n",
              " [194] \"ah\"                                                                          \n",
              " [195] \"ahornstr\"                                                                    \n",
              " [196] \"ai\"                                                                          \n",
              " [197] \"aid\"                                                                         \n",
              " [198] \"airplan\"                                                                     \n",
              " [199] \"airway\"                                                                      \n",
              " [200] \"ak\"                                                                          \n",
              " [201] \"aka\"                                                                         \n",
              " [202] \"akker\"                                                                       \n",
              " [203] \"akker3,4\"                                                                    \n",
              " [204] \"al-\"                                                                         \n",
              " [205] \"alan\"                                                                        \n",
              " [206] \"alcin\"                                                                       \n",
              " [207] \"alciq\"                                                                       \n",
              " [208] \"alert\"                                                                       \n",
              " [209] \"alexandr\"                                                                    \n",
              " [210] \"algo-\"                                                                       \n",
              " [211] \"ali\"                                                                         \n",
              " [212] \"alia\"                                                                        \n",
              " [213] \"alias\"                                                                       \n",
              " [214] \"align\"                                                                       \n",
              " [215] \"aliti\"                                                                       \n",
              " [216] \"allevi\"                                                                      \n",
              " [217] \"alli\"                                                                        \n",
              " [218] \"alloc\"                                                                       \n",
              " [219] \"almost\"                                                                      \n",
              " [220] \"alon\"                                                                        \n",
              " [221] \"alongsid\"                                                                    \n",
              " [222] \"alpha\"                                                                       \n",
              " [223] \"alt1\"                                                                        \n",
              " [224] \"alt2\"                                                                        \n",
              " [225] \"alter\"                                                                       \n",
              " [226] \"altern\"                                                                      \n",
              " [227] \"althuba\"                                                                     \n",
              " [228] \"althubaiti1,2\"                                                               \n",
              " [229] \"altmetr\"                                                                     \n",
              " [230] \"alway\"                                                                       \n",
              " [231] \"amalgam\"                                                                     \n",
              " [232] \"amax-\"                                                                       \n",
              " [233] \"ambigu\"                                                                      \n",
              " [234] \"ameen\"                                                                       \n",
              " [235] \"amel\"                                                                        \n",
              " [236] \"ameloblast\"                                                                  \n",
              " [237] \"amen\"                                                                        \n",
              " [238] \"amer-\"                                                                       \n",
              " [239] \"america\"                                                                     \n",
              " [240] \"amit\"                                                                        \n",
              " [241] \"amodel\"                                                                      \n",
              " [242] \"amongst\"                                                                     \n",
              " [243] \"ampl\"                                                                        \n",
              " [244] \"amrf\"                                                                        \n",
              " [245] \"amsterdam\"                                                                   \n",
              " [246] \"amyloidosi\"                                                                  \n",
              " [247] \"an-\"                                                                         \n",
              " [248] \"ana-\"                                                                        \n",
              " [249] \"anal-\"                                                                       \n",
              " [250] \"analog\"                                                                      \n",
              " [251] \"analys\"                                                                      \n",
              " [252] \"analyt\"                                                                      \n",
              " [253] \"anatom\"                                                                      \n",
              " [254] \"anatomi\"                                                                     \n",
              " [255] \"and-link\"                                                                    \n",
              " [256] \"andré\"                                                                       \n",
              " [257] \"andworkflow\"                                                                 \n",
              " [258] \"angiokeratoma\"                                                               \n",
              " [259] \"angular\"                                                                     \n",
              " [260] \"anim\"                                                                        \n",
              " [261] \"ann\"                                                                         \n",
              " [262] \"anno-\"                                                                       \n",
              " [263] \"annota-\"                                                                     \n",
              " [264] \"annotat\"                                                                     \n",
              " [265] \"annotation-bas\"                                                              \n",
              " [266] \"anonym\"                                                                      \n",
              " [267] \"anowl\"                                                                       \n",
              " [268] \"answer\"                                                                      \n",
              " [269] \"antelop\"                                                                     \n",
              " [270] \"antholog\"                                                                    \n",
              " [271] \"anti-\"                                                                       \n",
              " [272] \"anti-depress\"                                                                \n",
              " [273] \"anticip\"                                                                     \n",
              " [274] \"anxieti\"                                                                     \n",
              " [275] \"anyth\"                                                                       \n",
              " [276] \"ao\"                                                                          \n",
              " [277] \"ão\"                                                                          \n",
              " [278] \"ap\"                                                                          \n",
              " [279] \"ap-\"                                                                         \n",
              " [280] \"apach\"                                                                       \n",
              " [281] \"apart\"                                                                       \n",
              " [282] \"aphthosi\"                                                                    \n",
              " [283] \"api\"                                                                         \n",
              " [284] \"app\"                                                                         \n",
              " [285] \"appar\"                                                                       \n",
              " [286] \"appeal\"                                                                      \n",
              " [287] \"appear\"                                                                      \n",
              " [288] \"append\"                                                                      \n",
              " [289] \"appendix\"                                                                    \n",
              " [290] \"appetit\"                                                                     \n",
              " [291] \"appli-\"                                                                      \n",
              " [292] \"application-independ\"                                                        \n",
              " [293] \"appropri-\"                                                                   \n",
              " [294] \"approv\"                                                                      \n",
              " [295] \"approxi-\"                                                                    \n",
              " [296] \"approxim\"                                                                    \n",
              " [297] \"apt\"                                                                         \n",
              " [298] \"ar\"                                                                          \n",
              " [299] \"arabia\"                                                                      \n",
              " [300] \"archi-\"                                                                      \n",
              " [301] \"archic\"                                                                      \n",
              " [302] \"architectur\"                                                                 \n",
              " [303] \"archiv\"                                                                      \n",
              " [304] \"ard\"                                                                         \n",
              " [305] \"aremor\"                                                                      \n",
              " [306] \"argu\"                                                                        \n",
              " [307] \"arguabl\"                                                                     \n",
              " [308] \"aris\"                                                                        \n",
              " [309] \"around\"                                                                      \n",
              " [310] \"array\"                                                                       \n",
              " [311] \"arrow\"                                                                       \n",
              " [312] \"art\"                                                                         \n",
              " [313] \"art-\"                                                                        \n",
              " [314] \"art-decor\"                                                                   \n",
              " [315] \"artefact\"                                                                    \n",
              " [316] \"arthralgia\"                                                                  \n",
              " [317] \"arthriti\"                                                                    \n",
              " [318] \"arthritis-precedes-heart\"                                                    \n",
              " [319] \"arti-\"                                                                       \n",
              " [320] \"artifact\"                                                                    \n",
              " [321] \"artifici\"                                                                    \n",
              " [322] \"arts1,2\"                                                                     \n",
              " [323] \"as-\"                                                                         \n",
              " [324] \"asap\"                                                                        \n",
              " [325] \"ascertain\"                                                                   \n",
              " [326] \"asia\"                                                                        \n",
              " [327] \"ask\"                                                                         \n",
              " [328] \"ass\"                                                                         \n",
              " [329] \"assert\"                                                                      \n",
              " [330] \"asses-\"                                                                      \n",
              " [331] \"assess-\"                                                                     \n",
              " [332] \"assessor\"                                                                    \n",
              " [333] \"assist\"                                                                      \n",
              " [334] \"associ\"                                                                      \n",
              " [335] \"assump-\"                                                                     \n",
              " [336] \"assumpt\"                                                                     \n",
              " [337] \"asymmetr\"                                                                    \n",
              " [338] \"át\"                                                                          \n",
              " [339] \"at-\"                                                                         \n",
              " [340] \"atala\"                                                                       \n",
              " [341] \"ate\"                                                                         \n",
              " [342] \"attach\"                                                                      \n",
              " [343] \"attack\"                                                                      \n",
              " [344] \"attempt\"                                                                     \n",
              " [345] \"atten-\"                                                                      \n",
              " [346] \"attend\"                                                                      \n",
              " [347] \"attent\"                                                                      \n",
              " [348] \"attention-bas\"                                                               \n",
              " [349] \"attention-weight\"                                                            \n",
              " [350] \"attract\"                                                                     \n",
              " [351] \"au\"                                                                          \n",
              " [352] \"au-\"                                                                         \n",
              " [353] \"auc\"                                                                         \n",
              " [354] \"auciteli@imise.uni-leipzig.d\"                                                \n",
              " [355] \"audienc\"                                                                     \n",
              " [356] \"august\"                                                                      \n",
              " [357] \"australian\"                                                                  \n",
              " [358] \"authorit\"                                                                    \n",
              " [359] \"auto-\"                                                                       \n",
              " [360] \"autoimmun\"                                                                   \n",
              " [361] \"autoinflammatori\"                                                            \n",
              " [362] \"autopsi\"                                                                     \n",
              " [363] \"autosom\"                                                                     \n",
              " [364] \"auxiliari\"                                                                   \n",
              " [365] \"av\"                                                                          \n",
              " [366] \"avenu\"                                                                       \n",
              " [367] \"avoid\"                                                                       \n",
              " [368] \"awar\"                                                                        \n",
              " [369] \"awo\"                                                                         \n",
              " [370] \"awo_12\"                                                                      \n",
              " [371] \"axi\"                                                                         \n",
              " [372] \"axiom\"                                                                       \n",
              " [373] \"axiomat\"                                                                     \n",
              " [374] \"ay\"                                                                          \n",
              " [375] \"az\"                                                                          \n",
              " [376] \"azael\"                                                                       \n",
              " [377] \"b-\"                                                                          \n",
              " [378] \"b1\"                                                                          \n",
              " [379] \"b12\"                                                                         \n",
              " [380] \"b2\"                                                                          \n",
              " [381] \"b3\"                                                                          \n",
              " [382] \"ba\"                                                                          \n",
              " [383] \"bac-\"                                                                        \n",
              " [384] \"back\"                                                                        \n",
              " [385] \"back-\"                                                                       \n",
              " [386] \"back-propag\"                                                                 \n",
              " [387] \"backbon\"                                                                     \n",
              " [388] \"backend\"                                                                     \n",
              " [389] \"backward\"                                                                    \n",
              " [390] \"bacteri\"                                                                     \n",
              " [391] \"bacteria\"                                                                    \n",
              " [392] \"bags-of-word\"                                                                \n",
              " [393] \"balanc\"                                                                      \n",
              " [394] \"balli\"                                                                       \n",
              " [395] \"bang\"                                                                        \n",
              " [396] \"bank\"                                                                        \n",
              " [397] \"barrier\"                                                                     \n",
              " [398] \"base\"                                                                        \n",
              " [399] \"base-url\"                                                                    \n",
              " [400] \"based-method\"                                                                \n",
              " [401] \"based-system\"                                                                \n",
              " [402] \"baselin\"                                                                     \n",
              " [403] \"basi\"                                                                        \n",
              " [404] \"basqu\"                                                                       \n",
              " [405] \"bc\"                                                                          \n",
              " [406] \"be\"                                                                          \n",
              " [407] \"be-\"                                                                         \n",
              " [408] \"be4class\"                                                                    \n",
              " [409] \"bear\"                                                                        \n",
              " [410] \"bearer\"                                                                      \n",
              " [411] \"becam\"                                                                       \n",
              " [412] \"bed\"                                                                         \n",
              " [413] \"bedridden\"                                                                   \n",
              " [414] \"bedsid\"                                                                      \n",
              " [415] \"beforehand\"                                                                  \n",
              " [416] \"beger1,3\"                                                                    \n",
              " [417] \"begin\"                                                                       \n",
              " [418] \"begun\"                                                                       \n",
              " [419] \"behav\"                                                                       \n",
              " [420] \"behavior\"                                                                    \n",
              " [421] \"behaviour\"                                                                   \n",
              " [422] \"behind\"                                                                      \n",
              " [423] \"believ\"                                                                      \n",
              " [424] \"belong\"                                                                      \n",
              " [425] \"belong-\"                                                                     \n",
              " [426] \"bemerged.w\"                                                                  \n",
              " [427] \"benedetti\"                                                                   \n",
              " [428] \"benefici\"                                                                    \n",
              " [429] \"benefit\"                                                                     \n",
              " [430] \"benign\"                                                                      \n",
              " [431] \"ber\"                                                                         \n",
              " [432] \"besid\"                                                                       \n",
              " [433] \"between\"                                                                     \n",
              " [434] \"between-\"                                                                    \n",
              " [435] \"beyan1,2\"                                                                    \n",
              " [436] \"beyond\"                                                                      \n",
              " [437] \"bfo\"                                                                         \n",
              " [438] \"bfo-\"                                                                        \n",
              " [439] \"bi\"                                                                          \n",
              " [440] \"bi-direct\"                                                                   \n",
              " [441] \"bi-lstm\"                                                                     \n",
              " [442] \"bias\"                                                                        \n",
              " [443] \"biblio-\"                                                                     \n",
              " [444] \"bibliograph\"                                                                 \n",
              " [445] \"bidirect\"                                                                    \n",
              " [446] \"big\"                                                                         \n",
              " [447] \"biggest\"                                                                     \n",
              " [448] \"bigmouth\"                                                                    \n",
              " [449] \"bilirubin\"                                                                   \n",
              " [450] \"biliti\"                                                                      \n",
              " [451] \"bill\"                                                                        \n",
              " [452] \"bill-\"                                                                       \n",
              " [453] \"binari\"                                                                      \n",
              " [454] \"binary-classif\"                                                              \n",
              " [455] \"binary-valu\"                                                                 \n",
              " [456] \"binat\"                                                                       \n",
              " [457] \"bind\"                                                                        \n",
              " [458] \"binder\"                                                                      \n",
              " [459] \"bino-\"                                                                       \n",
              " [460] \"bio\"                                                                         \n",
              " [461] \"bio-\"                                                                        \n",
              " [462] \"biochem\"                                                                     \n",
              " [463] \"biofabr\"                                                                     \n",
              " [464] \"bioinformat\"                                                                 \n",
              " [465] \"biomark\"                                                                     \n",
              " [466] \"biomed-\"                                                                     \n",
              " [467] \"biomedicin\"                                                                  \n",
              " [468] \"biometr\"                                                                     \n",
              " [469] \"biomolecu-\"                                                                  \n",
              " [470] \"biomolecular\"                                                                \n",
              " [471] \"bioontology.org\"                                                             \n",
              " [472] \"biopor-\"                                                                     \n",
              " [473] \"bioport\"                                                                     \n",
              " [474] \"bioprint\"                                                                    \n",
              " [475] \"bioscienc\"                                                                   \n",
              " [476] \"biotechnolog\"                                                                \n",
              " [477] \"biotop\"                                                                      \n",
              " [478] \"biotoplit\"                                                                   \n",
              " [479] \"bird\"                                                                        \n",
              " [480] \"birn\"                                                                        \n",
              " [481] \"birth\"                                                                       \n",
              " [482] \"birthdat\"                                                                    \n",
              " [483] \"bittar1\"                                                                     \n",
              " [484] \"bl\"                                                                          \n",
              " [485] \"black\"                                                                       \n",
              " [486] \"blank\"                                                                       \n",
              " [487] \"ble\"                                                                         \n",
              " [488] \"block\"                                                                       \n",
              " [489] \"blood\"                                                                       \n",
              " [490] \"blue\"                                                                        \n",
              " [491] \"blur\"                                                                        \n",
              " [492] \"bm\"                                                                          \n",
              " [493] \"bmi\"                                                                         \n",
              " [494] \"board\"                                                                       \n",
              " [495] \"bod-\"                                                                        \n",
              " [496] \"bodi\"                                                                        \n",
              " [497] \"bodili\"                                                                      \n",
              " [498] \"bold\"                                                                        \n",
              " [499] \"bone\"                                                                        \n",
              " [500] \"book\"                                                                        \n",
              " [501] \"boolean\"                                                                     \n",
              " [502] \"boost\"                                                                       \n",
              " [503] \"bordeaux\"                                                                    \n",
              " [504] \"border\"                                                                      \n",
              " [505] \"born\"                                                                        \n",
              " [506] \"boston\"                                                                      \n",
              " [507] \"bottom\"                                                                      \n",
              " [508] \"bottom-up\"                                                                   \n",
              " [509] \"bound\"                                                                       \n",
              " [510] \"boundari\"                                                                    \n",
              " [511] \"bowel\"                                                                       \n",
              " [512] \"box\"                                                                         \n",
              " [513] \"br\"                                                                          \n",
              " [514] \"brain\"                                                                       \n",
              " [515] \"branch\"                                                                      \n",
              " [516] \"brazilian\"                                                                   \n",
              " [517] \"breast\"                                                                      \n",
              " [518] \"bridg\"                                                                       \n",
              " [519] \"brief\"                                                                       \n",
              " [520] \"briefli\"                                                                     \n",
              " [521] \"brigham\"                                                                     \n",
              " [522] \"brighton\"                                                                    \n",
              " [523] \"bring\"                                                                       \n",
              " [524] \"broad\"                                                                       \n",
              " [525] \"broaden\"                                                                     \n",
              " [526] \"broader\"                                                                     \n",
              " [527] \"bronchi\"                                                                     \n",
              " [528] \"brows\"                                                                       \n",
              " [529] \"bs\"                                                                          \n",
              " [530] \"bu\"                                                                          \n",
              " [531] \"buffalo\"                                                                     \n",
              " [532] \"build\"                                                                       \n",
              " [533] \"builder\"                                                                     \n",
              " [534] \"bulgaria\"                                                                    \n",
              " [535] \"bundl\"                                                                       \n",
              " [536] \"bur-\"                                                                        \n",
              " [537] \"burden\"                                                                      \n",
              " [538] \"burn\"                                                                        \n",
              " [539] \"busi\"                                                                        \n",
              " [540] \"butterfli\"                                                                   \n",
              " [541] \"c.f\"                                                                         \n",
              " [542] \"c1\"                                                                          \n",
              " [543] \"c2\"                                                                          \n",
              " [544] \"c3\"                                                                          \n",
              " [545] \"c4\"                                                                          \n",
              " [546] \"c5\"                                                                          \n",
              " [547] \"c50\"                                                                         \n",
              " [548] \"c6\"                                                                          \n",
              " [549] \"c7\"                                                                          \n",
              " [550] \"ca\"                                                                          \n",
              " [551] \"ca49y29ra9\"                                                                  \n",
              " [552] \"cabernet\"                                                                    \n",
              " [553] \"cal\"                                                                         \n",
              " [554] \"cal-\"                                                                        \n",
              " [555] \"calcula-\"                                                                    \n",
              " [556] \"call\"                                                                        \n",
              " [557] \"callosum\"                                                                    \n",
              " [558] \"canal\"                                                                       \n",
              " [559] \"cancer\"                                                                      \n",
              " [560] \"candid\"                                                                      \n",
              " [561] \"candida\"                                                                     \n",
              " [562] \"candidiasi\"                                                                  \n",
              " [563] \"canin\"                                                                       \n",
              " [564] \"canon\"                                                                       \n",
              " [565] \"cant\"                                                                        \n",
              " [566] \"capabl\"                                                                      \n",
              " [567] \"cape\"                                                                        \n",
              " [568] \"caplan9\"                                                                     \n",
              " [569] \"captur\"                                                                      \n",
              " [570] \"car-\"                                                                        \n",
              " [571] \"card\"                                                                        \n",
              " [572] \"cardi-\"                                                                      \n",
              " [573] \"cardiolog\"                                                                   \n",
              " [574] \"cardiovas-\"                                                                  \n",
              " [575] \"cardiovascular\"                                                              \n",
              " [576] \"care-requir\"                                                                 \n",
              " [577] \"caregiv\"                                                                     \n",
              " [578] \"caret\"                                                                       \n",
              " [579] \"cari\"                                                                        \n",
              " [580] \"carlo\"                                                                       \n",
              " [581] \"carnivor\"                                                                    \n",
              " [582] \"carnivorouspl\"                                                               \n",
              " [583] \"caro\"                                                                        \n",
              " [584] \"carrasco3\"                                                                   \n",
              " [585] \"carri\"                                                                       \n",
              " [586] \"cas\"                                                                         \n",
              " [587] \"castor\"                                                                      \n",
              " [588] \"casu\"                                                                        \n",
              " [589] \"casual\"                                                                      \n",
              " [590] \"cat-\"                                                                        \n",
              " [591] \"catalog\"                                                                     \n",
              " [592] \"catalysi\"                                                                    \n",
              " [593] \"cate\"                                                                        \n",
              " [594] \"catego-\"                                                                     \n",
              " [595] \"categoris\"                                                                   \n",
              " [596] \"cater\"                                                                       \n",
              " [597] \"caterpil-\"                                                                   \n",
              " [598] \"cation\"                                                                      \n",
              " [599] \"causal\"                                                                      \n",
              " [600] \"causat\"                                                                      \n",
              " [601] \"caution\"                                                                     \n",
              " [602] \"caviti\"                                                                      \n",
              " [603] \"cb\"                                                                          \n",
              " [604] \"cc\"                                                                          \n",
              " [605] \"cdm\"                                                                         \n",
              " [606] \"ce\"                                                                          \n",
              " [607] \"ceas\"                                                                        \n",
              " [608] \"cede\"                                                                        \n",
              " [609] \"cedur\"                                                                       \n",
              " [610] \"ceiv\"                                                                        \n",
              " [611] \"cell\"                                                                        \n",
              " [612] \"cell-laden\"                                                                  \n",
              " [613] \"cellular\"                                                                    \n",
              " [614] \"cen-\"                                                                        \n",
              " [615] \"centag\"                                                                      \n",
              " [616] \"center\"                                                                      \n",
              " [617] \"centli\"                                                                      \n",
              " [618] \"centr\"                                                                       \n",
              " [619] \"central\"                                                                     \n",
              " [620] \"cephalea\"                                                                    \n",
              " [621] \"cépidc\"                                                                      \n",
              " [622] \"cept\"                                                                        \n",
              " [623] \"cer-\"                                                                        \n",
              " [624] \"certain\"                                                                     \n",
              " [625] \"certif\"                                                                      \n",
              " [626] \"ceruloplasmin\"                                                               \n",
              " [627] \"cess\"                                                                        \n",
              " [628] \"cessibl\"                                                                     \n",
              " [629] \"cf\"                                                                          \n",
              " [630] \"ch\"                                                                          \n",
              " [631] \"challenge8\"                                                                  \n",
              " [632] \"challenges6\"                                                                 \n",
              " [633] \"chamber\"                                                                     \n",
              " [634] \"chanc\"                                                                       \n",
              " [635] \"chang\"                                                                       \n",
              " [636] \"channel\"                                                                     \n",
              " [637] \"chapter\"                                                                     \n",
              " [638] \"char-\"                                                                       \n",
              " [639] \"charac-\"                                                                     \n",
              " [640] \"charact\"                                                                     \n",
              " [641] \"character\"                                                                   \n",
              " [642] \"character-bas\"                                                               \n",
              " [643] \"character-embed\"                                                             \n",
              " [644] \"characteris\"                                                                 \n",
              " [645] \"chardon-\"                                                                    \n",
              " [646] \"charg\"                                                                       \n",
              " [647] \"chart\"                                                                       \n",
              " [648] \"chase\"                                                                       \n",
              " [649] \"checker\"                                                                     \n",
              " [650] \"checkup\"                                                                     \n",
              " [651] \"cheiliti\"                                                                    \n",
              " [652] \"chemic\"                                                                      \n",
              " [653] \"chi-squar\"                                                                   \n",
              " [654] \"child\"                                                                       \n",
              " [655] \"children\"                                                                    \n",
              " [656] \"chile\"                                                                       \n",
              " [657] \"chilean\"                                                                     \n",
              " [658] \"chill\"                                                                       \n",
              " [659] \"china\"                                                                       \n",
              " [660] \"chine\"                                                                       \n",
              " [661] \"chines\"                                                                      \n",
              " [662] \"chlorophyl\"                                                                  \n",
              " [663] \"chloroplast\"                                                                 \n",
              " [664] \"choic\"                                                                       \n",
              " [665] \"choos\"                                                                       \n",
              " [666] \"choos-\"                                                                      \n",
              " [667] \"choriocarcinoma\"                                                             \n",
              " [668] \"chosen\"                                                                      \n",
              " [669] \"christoph\"                                                                   \n",
              " [670] \"chronic\"                                                                     \n",
              " [671] \"ci\"                                                                          \n",
              " [672] \"cialli\"                                                                      \n",
              " [673] \"cian\"                                                                        \n",
              " [674] \"cident\"                                                                      \n",
              " [675] \"cie\"                                                                         \n",
              " [676] \"cientli\"                                                                     \n",
              " [677] \"cimen\"                                                                       \n",
              " [678] \"circuit\"                                                                     \n",
              " [679] \"circumfer\"                                                                   \n",
              " [680] \"circumscriptum\"                                                              \n",
              " [681] \"circumv\"                                                                     \n",
              " [682] \"cismef\"                                                                      \n",
              " [683] \"citat\"                                                                       \n",
              " [684] \"cite\"                                                                        \n",
              " [685] \"citi\"                                                                        \n",
              " [686] \"citizen\"                                                                     \n",
              " [687] \"civil\"                                                                       \n",
              " [688] \"ck\"                                                                          \n",
              " [689] \"cl\"                                                                          \n",
              " [690] \"claim\"                                                                       \n",
              " [691] \"clas-\"                                                                       \n",
              " [692] \"class-as-inst\"                                                               \n",
              " [693] \"classi-\"                                                                     \n",
              " [694] \"classic\"                                                                     \n",
              " [695] \"classifica-\"                                                                 \n",
              " [696] \"classificationmethod\"                                                        \n",
              " [697] \"claus\"                                                                       \n",
              " [698] \"claveau\"                                                                     \n",
              " [699] \"claveau3\"                                                                    \n",
              " [700] \"clavijo1\"                                                                    \n",
              " [701] \"clé\"                                                                         \n",
              " [702] \"clean\"                                                                       \n",
              " [703] \"clear\"                                                                       \n",
              " [704] \"clearer\"                                                                     \n",
              " [705] \"clef-ehealth\"                                                                \n",
              " [706] \"clément\"                                                                     \n",
              " [707] \"cles\"                                                                        \n",
              " [708] \"client\"                                                                      \n",
              " [709] \"clin-\"                                                                       \n",
              " [710] \"clini-\"                                                                      \n",
              " [711] \"clinician\"                                                                   \n",
              " [712] \"clock\"                                                                       \n",
              " [713] \"closer\"                                                                      \n",
              " [714] \"closest\"                                                                     \n",
              " [715] \"clude\"                                                                       \n",
              " [716] \"clue\"                                                                        \n",
              " [717] \"clus-\"                                                                       \n",
              " [718] \"cluster-sensit\"                                                              \n",
              " [719] \"clusteriza-\"                                                                 \n",
              " [720] \"cn\"                                                                          \n",
              " [721] \"cnn\"                                                                         \n",
              " [722] \"cnn-base\"                                                                    \n",
              " [723] \"co\"                                                                          \n",
              " [724] \"co-\"                                                                         \n",
              " [725] \"co-author\"                                                                   \n",
              " [726] \"co-cit\"                                                                      \n",
              " [727] \"co-occur\"                                                                    \n",
              " [728] \"co-occurr\"                                                                   \n",
              " [729] \"coars\"                                                                       \n",
              " [730] \"cod-\"                                                                        \n",
              " [731] \"coeffici\"                                                                    \n",
              " [732] \"coexist\"                                                                     \n",
              " [733] \"cognis\"                                                                      \n",
              " [734] \"cohen\"                                                                       \n",
              " [735] \"cohes\"                                                                       \n",
              " [736] \"cohort\"                                                                      \n",
              " [737] \"coincid\"                                                                     \n",
              " [738] \"col-\"                                                                        \n",
              " [739] \"cold\"                                                                        \n",
              " [740] \"collab-\"                                                                     \n",
              " [741] \"collabo-\"                                                                    \n",
              " [742] \"collabor\"                                                                    \n",
              " [743] \"collabora-\"                                                                  \n",
              " [744] \"collaps\"                                                                     \n",
              " [745] \"collect-\"                                                                    \n",
              " [746] \"colloc\"                                                                      \n",
              " [747] \"colour\"                                                                      \n",
              " [748] \"column\"                                                                      \n",
              " [749] \"combin-\"                                                                     \n",
              " [750] \"combined_phenotyp\"                                                           \n",
              " [751] \"comma\"                                                                       \n",
              " [752] \"commensal-\"                                                                  \n",
              " [753] \"comment\"                                                                     \n",
              " [754] \"commerci\"                                                                    \n",
              " [755] \"commit\"                                                                      \n",
              " [756] \"commod\"                                                                      \n",
              " [757] \"commonsens\"                                                                  \n",
              " [758] \"commu-\"                                                                      \n",
              " [759] \"communic\"                                                                    \n",
              " [760] \"compani\"                                                                     \n",
              " [761] \"compari-\"                                                                    \n",
              " [762] \"compat\"                                                                      \n",
              " [763] \"compendium\"                                                                  \n",
              " [764] \"compet\"                                                                      \n",
              " [765] \"competit\"                                                                    \n",
              " [766] \"compil\"                                                                      \n",
              " [767] \"complaint\"                                                                   \n",
              " [768] \"complement\"                                                                  \n",
              " [769] \"complementari\"                                                               \n",
              " [770] \"complet\"                                                                     \n",
              " [771] \"compli-\"                                                                     \n",
              " [772] \"complianc\"                                                                   \n",
              " [773] \"complic\"                                                                     \n",
              " [774] \"complica-\"                                                                   \n",
              " [775] \"compon\"                                                                      \n",
              " [776] \"compos-\"                                                                     \n",
              " [777] \"composit\"                                                                    \n",
              " [778] \"composite_phenotyp\"                                                          \n",
              " [779] \"compound\"                                                                    \n",
              " [780] \"comprehen-\"                                                                  \n",
              " [781] \"comprehens\"                                                                  \n",
              " [782] \"compris\"                                                                     \n",
              " [783] \"compromis\"                                                                   \n",
              " [784] \"computer-assist\"                                                             \n",
              " [785] \"concate-\"                                                                    \n",
              " [786] \"concaten\"                                                                    \n",
              " [787] \"concentr\"                                                                    \n",
              " [788] \"conceptu\"                                                                    \n",
              " [789] \"concis\"                                                                      \n",
              " [790] \"conclud\"                                                                     \n",
              " [791] \"concret\"                                                                     \n",
              " [792] \"concur\"                                                                      \n",
              " [793] \"condi-\"                                                                      \n",
              " [794] \"confer\"                                                                      \n",
              " [795] \"confi-\"                                                                      \n",
              " [796] \"confid\"                                                                      \n",
              " [797] \"confidenti\"                                                                  \n",
              " [798] \"configur\"                                                                    \n",
              " [799] \"confirm\"                                                                     \n",
              " [800] \"conflict\"                                                                    \n",
              " [801] \"conform\"                                                                     \n",
              " [802] \"confound\"                                                                    \n",
              " [803] \"confront\"                                                                    \n",
              " [804] \"confus\"                                                                      \n",
              " [805] \"congenit\"                                                                    \n",
              " [806] \"conjunct\"                                                                    \n",
              " [807] \"conll\"                                                                       \n",
              " [808] \"connect\"                                                                     \n",
              " [809] \"conse-\"                                                                      \n",
              " [810] \"consec-\"                                                                     \n",
              " [811] \"consensus\"                                                                   \n",
              " [812] \"consent\"                                                                     \n",
              " [813] \"consequ\"                                                                     \n",
              " [814] \"consid-\"                                                                     \n",
              " [815] \"consider\"                                                                    \n",
              " [816] \"consortia\"                                                                   \n",
              " [817] \"consortium\"                                                                  \n",
              " [818] \"constip\"                                                                     \n",
              " [819] \"constitut\"                                                                   \n",
              " [820] \"constraint\"                                                                  \n",
              " [821] \"consult\"                                                                     \n",
              " [822] \"consum\"                                                                      \n",
              " [823] \"consumer-fac\"                                                                \n",
              " [824] \"contact\"                                                                     \n",
              " [825] \"context-\"                                                                    \n",
              " [826] \"context-bas\"                                                                 \n",
              " [827] \"context-rich\"                                                                \n",
              " [828] \"contextu\"                                                                    \n",
              " [829] \"contin\"                                                                      \n",
              " [830] \"continu\"                                                                     \n",
              " [831] \"contradict\"                                                                  \n",
              " [832] \"contrari\"                                                                    \n",
              " [833] \"contrast\"                                                                    \n",
              " [834] \"contribu-\"                                                                   \n",
              " [835] \"control\"                                                                     \n",
              " [836] \"controversi\"                                                                 \n",
              " [837] \"conv\"                                                                        \n",
              " [838] \"conveni\"                                                                     \n",
              " [839] \"convent\"                                                                     \n",
              " [840] \"convert\"                                                                     \n",
              " [841] \"convey\"                                                                      \n",
              " [842] \"convolut\"                                                                    \n",
              " [843] \"cooper\"                                                                      \n",
              " [844] \"coordin\"                                                                     \n",
              " [845] \"cop\"                                                                         \n",
              " [846] \"cop-bas\"                                                                     \n",
              " [847] \"cor-\"                                                                        \n",
              " [848] \"cord\"                                                                        \n",
              " [849] \"cornet1\"                                                                     \n",
              " [850] \"corpor\"                                                                      \n",
              " [851] \"corpora\"                                                                     \n",
              " [852] \"corpus\"                                                                      \n",
              " [853] \"corre-\"                                                                      \n",
              " [854] \"correct-\"                                                                    \n",
              " [855] \"correl\"                                                                      \n",
              " [856] \"correla-\"                                                                    \n",
              " [857] \"correspond-\"                                                                 \n",
              " [858] \"corrobor\"                                                                    \n",
              " [859] \"cose\"                                                                        \n",
              " [860] \"cosin\"                                                                       \n",
              " [861] \"cost\"                                                                        \n",
              " [862] \"cost-effici\"                                                                 \n",
              " [863] \"cough\"                                                                       \n",
              " [864] \"count\"                                                                       \n",
              " [865] \"counter\"                                                                     \n",
              " [866] \"coupl\"                                                                       \n",
              " [867] \"cours\"                                                                       \n",
              " [868] \"coursework\"                                                                  \n",
              " [869] \"cq\"                                                                          \n",
              " [870] \"cq5\"                                                                         \n",
              " [871] \"cq8\"                                                                         \n",
              " [872] \"cqa\"                                                                         \n",
              " [873] \"cqs\"                                                                         \n",
              " [874] \"cr\"                                                                          \n",
              " [875] \"cramp\"                                                                       \n",
              " [876] \"cre-\"                                                                        \n",
              " [877] \"creas\"                                                                       \n",
              " [878] \"creation\"                                                                    \n",
              " [879] \"cred-\"                                                                       \n",
              " [880] \"credi\"                                                                       \n",
              " [881] \"credi-\"                                                                      \n",
              " [882] \"credibil-\"                                                                   \n",
              " [883] \"credibl\"                                                                     \n",
              " [884] \"crf\"                                                                         \n",
              " [885] \"crf-base\"                                                                    \n",
              " [886] \"crfs\"                                                                        \n",
              " [887] \"cri-\"                                                                        \n",
              " [888] \"crimin\"                                                                      \n",
              " [889] \"crit-\"                                                                       \n",
              " [890] \"criterion\"                                                                   \n",
              " [891] \"critic\"                                                                      \n",
              " [892] \"cross\"                                                                       \n",
              " [893] \"cross-cultur\"                                                                \n",
              " [894] \"cross-entropi\"                                                               \n",
              " [895] \"cross-valid\"                                                                 \n",
              " [896] \"crown\"                                                                       \n",
              " [897] \"crucial\"                                                                     \n",
              " [898] \"cryptorchid\"                                                                 \n",
              " [899] \"cs\"                                                                          \n",
              " [900] \"cs_ai-sem1-2018\"                                                             \n",
              " [901] \"csv\"                                                                         \n",
              " [902] \"ct\"                                                                          \n",
              " [903] \"cti\"                                                                         \n",
              " [904] \"cu\"                                                                          \n",
              " [905] \"cular\"                                                                       \n",
              " [906] \"culat\"                                                                       \n",
              " [907] \"cult\"                                                                        \n",
              " [908] \"culti\"                                                                       \n",
              " [909] \"cultur\"                                                                      \n",
              " [910] \"cumul\"                                                                       \n",
              " [911] \"cur-\"                                                                        \n",
              " [912] \"curat\"                                                                       \n",
              " [913] \"currenc\"                                                                     \n",
              " [914] \"curv\"                                                                        \n",
              " [915] \"custom\"                                                                      \n",
              " [916] \"customis\"                                                                    \n",
              " [917] \"cut\"                                                                         \n",
              " [918] \"cutan\"                                                                       \n",
              " [919] \"cutoff\"                                                                      \n",
              " [920] \"cw\"                                                                          \n",
              " [921] \"cycl\"                                                                        \n",
              " [922] \"cyromazin\"                                                                   \n",
              " [923] \"d.info\"                                                                      \n",
              " [924] \"d1\"                                                                          \n",
              " [925] \"d2\"                                                                          \n",
              " [926] \"d3\"                                                                          \n",
              " [927] \"da\"                                                                          \n",
              " [928] \"dac-trung\"                                                                   \n",
              " [929] \"daili\"                                                                       \n",
              " [930] \"daliani\"                                                                     \n",
              " [931] \"dalloux\"                                                                     \n",
              " [932] \"dalloux3\"                                                                    \n",
              " [933] \"damag\"                                                                       \n",
              " [934] \"daniel\"                                                                      \n",
              " [935] \"danielsson-ojala3,4\"                                                         \n",
              " [936] \"danish\"                                                                      \n",
              " [937] \"dardis\"                                                                      \n",
              " [938] \"dardiz\"                                                                      \n",
              " [939] \"data-\"                                                                       \n",
              " [940] \"data-driven\"                                                                 \n",
              " [941] \"date\"                                                                        \n",
              " [942] \"date_part\"                                                                   \n",
              " [943] \"david\"                                                                       \n",
              " [944] \"day\"                                                                         \n",
              " [945] \"dd\"                                                                          \n",
              " [946] \"de\"                                                                          \n",
              " [947] \"de-identif\"                                                                  \n",
              " [948] \"de-identifi\"                                                                 \n",
              " [949] \"deafness-dystonia-\"                                                          \n",
              " [950] \"deal\"                                                                        \n",
              " [951] \"death\"                                                                       \n",
              " [952] \"debat\"                                                                       \n",
              " [953] \"debug\"                                                                       \n",
              " [954] \"decemb\"                                                                      \n",
              " [955] \"decid\"                                                                       \n",
              " [956] \"decis\"                                                                       \n",
              " [957] \"decision-mak\"                                                                \n",
              " [958] \"decker1,2\"                                                                   \n",
              " [959] \"decla-\"                                                                      \n",
              " [960] \"declar\"                                                                      \n",
              " [961] \"declin\"                                                                      \n",
              " [962] \"decompos\"                                                                    \n",
              " [963] \"decor\"                                                                       \n",
              " [964] \"deduct\"                                                                      \n",
              " [965] \"deem\"                                                                        \n",
              " [966] \"deep\"                                                                        \n",
              " [967] \"deeper\"                                                                      \n",
              " [968] \"def-\"                                                                        \n",
              " [969] \"default\"                                                                     \n",
              " [970] \"defer\"                                                                       \n",
              " [971] \"defici\"                                                                      \n",
              " [972] \"defin-\"                                                                      \n",
              " [973] \"defined1\"                                                                    \n",
              " [974] \"defini-\"                                                                     \n",
              " [975] \"degrad\"                                                                      \n",
              " [976] \"degre\"                                                                       \n",
              " [977] \"deist\"                                                                       \n",
              " [978] \"delay\"                                                                       \n",
              " [979] \"delet\"                                                                       \n",
              " [980] \"deliber\"                                                                     \n",
              " [981] \"demicstaff\"                                                                  \n",
              " [982] \"demiolog\"                                                                    \n",
              " [983] \"demograph\"                                                                   \n",
              " [984] \"demon-\"                                                                      \n",
              " [985] \"demonstr\"                                                                    \n",
              " [986] \"demonstrat-\"                                                                 \n",
              " [987] \"den\"                                                                         \n",
              " [988] \"den-\"                                                                        \n",
              " [989] \"denc\"                                                                        \n",
              " [990] \"denci\"                                                                       \n",
              " [991] \"dene\"                                                                        \n",
              " [992] \"denni\"                                                                       \n",
              " [993] \"denot\"                                                                       \n",
              " [994] \"dens\"                                                                        \n",
              " [995] \"dent\"                                                                        \n",
              " [996] \"dental\"                                                                      \n",
              " [997] \"dentist\"                                                                     \n",
              " [998] \"dentistri\"                                                                   \n",
              " [999] \"depart\"                                                                      \n",
              "[1000] \"depen-\"                                                                      \n",
              "[1001] \"depict\"                                                                      \n",
              "[1002] \"deploy\"                                                                      \n",
              "[1003] \"depress\"                                                                     \n",
              "[1004] \"depth\"                                                                       \n",
              "[1005] \"der\"                                                                         \n",
              "[1006] \"deriva-\"                                                                     \n",
              "[1007] \"derived_phenotyp\"                                                            \n",
              "[1008] \"derk\"                                                                        \n",
              "[1009] \"dermatolog\"                                                                  \n",
              "[1010] \"descend\"                                                                     \n",
              "[1011] \"descrip-\"                                                                    \n",
              "[1012] \"descriptionwithin\"                                                           \n",
              "[1013] \"descriptor\"                                                                  \n",
              "[1014] \"desiderata\"                                                                  \n",
              "[1015] \"desig-\"                                                                      \n",
              "[1016] \"desir\"                                                                       \n",
              "[1017] \"desktop\"                                                                     \n",
              "[1018] \"desmoid\"                                                                     \n",
              "[1019] \"despit\"                                                                      \n",
              "[1020] \"dessertwin\"                                                                  \n",
              "[1021] \"destruct\"                                                                    \n",
              "[1022] \"detailedmathemat\"                                                            \n",
              "[1023] \"devel-\"                                                                      \n",
              "[1024] \"develop-\"                                                                    \n",
              "[1025] \"deviat\"                                                                      \n",
              "[1026] \"devic\"                                                                       \n",
              "[1027] \"di\"                                                                          \n",
              "[1028] \"di-\"                                                                         \n",
              "[1029] \"diabet\"                                                                      \n",
              "[1030] \"diag-\"                                                                       \n",
              "[1031] \"diagnos\"                                                                     \n",
              "[1032] \"diagnosi\"                                                                    \n",
              "[1033] \"diagnost\"                                                                    \n",
              "[1034] \"diagram\"                                                                     \n",
              "[1035] \"dian\"                                                                        \n",
              "[1036] \"diarrhea\"                                                                    \n",
              "[1037] \"dictat\"                                                                      \n",
              "[1038] \"diction\"                                                                     \n",
              "[1039] \"dictionari\"                                                                  \n",
              "[1040] \"dictionary-bas\"                                                              \n",
              "[1041] \"dif-\"                                                                        \n",
              "[1042] \"differ\"                                                                      \n",
              "[1043] \"differenti\"                                                                  \n",
              "[1044] \"diffi-\"                                                                      \n",
              "[1045] \"difficult\"                                                                   \n",
              "[1046] \"difficulti\"                                                                  \n",
              "[1047] \"difutur\"                                                                     \n",
              "[1048] \"digest\"                                                                      \n",
              "[1049] \"digit\"                                                                       \n",
              "[1050] \"digitis\"                                                                     \n",
              "[1051] \"dimens\"                                                                      \n",
              "[1052] \"dimension\"                                                                   \n",
              "[1053] \"dimensional-\"                                                                \n",
              "[1054] \"ding\"                                                                        \n",
              "[1055] \"direc-\"                                                                      \n",
              "[1056] \"direction-\"                                                                  \n",
              "[1057] \"directori\"                                                                   \n",
              "[1058] \"dirichlet\"                                                                   \n",
              "[1059] \"disabl\"                                                                      \n",
              "[1060] \"disagre\"                                                                     \n",
              "[1061] \"disambigu\"                                                                   \n",
              "[1062] \"discard\"                                                                     \n",
              "[1063] \"discharg\"                                                                    \n",
              "[1064] \"disciplin\"                                                                   \n",
              "[1065] \"disclos\"                                                                     \n",
              "[1066] \"disconnect\"                                                                  \n",
              "[1067] \"discours\"                                                                    \n",
              "[1068] \"discov\"                                                                      \n",
              "[1069] \"discov-\"                                                                     \n",
              "[1070] \"discover\"                                                                    \n",
              "[1071] \"discoveri\"                                                                   \n",
              "[1072] \"discret\"                                                                     \n",
              "[1073] \"discrimin\"                                                                   \n",
              "[1074] \"discus-\"                                                                     \n",
              "[1075] \"discussion-bas\"                                                              \n",
              "[1076] \"disease-agnost\"                                                              \n",
              "[1077] \"disease-cod\"                                                                 \n",
              "[1078] \"disease-diseas\"                                                              \n",
              "[1079] \"disease-rel\"                                                                 \n",
              "[1080] \"disease-term\"                                                                \n",
              "[1081] \"diseasecard\"                                                                 \n",
              "[1082] \"disgenet\"                                                                    \n",
              "[1083] \"disjoint\"                                                                    \n",
              "[1084] \"disord\"                                                                      \n",
              "[1085] \"dispar\"                                                                      \n",
              "[1086] \"dispers\"                                                                     \n",
              "[1087] \"disposit\"                                                                    \n",
              "[1088] \"disregard\"                                                                   \n",
              "[1089] \"dissemin\"                                                                    \n",
              "[1090] \"distal\"                                                                      \n",
              "[1091] \"distanc\"                                                                     \n",
              "[1092] \"distance-bas\"                                                                \n",
              "[1093] \"distin-\"                                                                     \n",
              "[1094] \"distract\"                                                                    \n",
              "[1095] \"diverg\"                                                                      \n",
              "[1096] \"divers\"                                                                      \n",
              "[1097] \"divid\"                                                                       \n",
              "[1098] \"divis\"                                                                       \n",
              "[1099] \"dizzi\"                                                                       \n",
              "[1100] \"dl\"                                                                          \n",
              "[1101] \"dls\"                                                                         \n",
              "[1102] \"dm\"                                                                          \n",
              "[1103] \"dna\"                                                                         \n",
              "[1104] \"do-\"                                                                         \n",
              "[1105] \"doc-\"                                                                        \n",
              "[1106] \"doctor\"                                                                      \n",
              "[1107] \"doctor_nam\"                                                                  \n",
              "[1108] \"doctor_num\"                                                                  \n",
              "[1109] \"docu-\"                                                                       \n",
              "[1110] \"documen-\"                                                                    \n",
              "[1111] \"dodder\"                                                                      \n",
              "[1112] \"doid\"                                                                        \n",
              "[1113] \"dolc\"                                                                        \n",
              "[1114] \"dolce-lite.owl\"                                                              \n",
              "[1115] \"domain-\"                                                                     \n",
              "[1116] \"domain-accept\"                                                               \n",
              "[1117] \"domain-independ\"                                                             \n",
              "[1118] \"domain-neutr\"                                                                \n",
              "[1119] \"domain-specif\"                                                               \n",
              "[1120] \"domainconcept\"                                                               \n",
              "[1121] \"domainent\"                                                                   \n",
              "[1122] \"domin\"                                                                       \n",
              "[1123] \"don\"                                                                         \n",
              "[1124] \"done\"                                                                        \n",
              "[1125] \"doubl\"                                                                       \n",
              "[1126] \"download\"                                                                    \n",
              "[1127] \"downsid\"                                                                     \n",
              "[1128] \"downstream\"                                                                  \n",
              "[1129] \"dp\"                                                                          \n",
              "[1130] \"dp1\"                                                                         \n",
              "[1131] \"dp2\"                                                                         \n",
              "[1132] \"dp3\"                                                                         \n",
              "[1133] \"dp4\"                                                                         \n",
              "[1134] \"dp5\"                                                                         \n",
              "[1135] \"dr\"                                                                          \n",
              "[1136] \"draft\"                                                                       \n",
              "[1137] \"drag-and-drop\"                                                               \n",
              "[1138] \"drawn\"                                                                       \n",
              "[1139] \"dream\"                                                                       \n",
              "[1140] \"dri\"                                                                         \n",
              "[1141] \"drive\"                                                                       \n",
              "[1142] \"driven\"                                                                      \n",
              "[1143] \"driver\"                                                                      \n",
              "[1144] \"drool\"                                                                       \n",
              "[1145] \"drop\"                                                                        \n",
              "[1146] \"dropwizard\"                                                                  \n",
              "[1147] \"drowsi\"                                                                      \n",
              "[1148] \"drug\"                                                                        \n",
              "[1149] \"drymouth\"                                                                    \n",
              "[1150] \"drywin\"                                                                      \n",
              "[1151] \"ds\"                                                                          \n",
              "[1152] \"dt\"                                                                          \n",
              "[1153] \"du\"                                                                          \n",
              "[1154] \"duce\"                                                                        \n",
              "[1155] \"duct\"                                                                        \n",
              "[1156] \"dudá\"                                                                        \n",
              "[1157] \"dummi\"                                                                       \n",
              "[1158] \"dummy-ehr\"                                                                   \n",
              "[1159] \"dummy-famili\"                                                                \n",
              "[1160] \"dummy-first\"                                                                 \n",
              "[1161] \"duncan\"                                                                      \n",
              "[1162] \"duncan1,2\"                                                                   \n",
              "[1163] \"duodenum\"                                                                    \n",
              "[1164] \"dup\"                                                                         \n",
              "[1165] \"duplic\"                                                                      \n",
              "[1166] \"durat\"                                                                       \n",
              "[1167] \"dutch\"                                                                       \n",
              "[1168] \"dutta1,2\"                                                                    \n",
              "[1169] \"dv\"                                                                          \n",
              "[1170] \"dvd\"                                                                         \n",
              "[1171] \"dynam\"                                                                       \n",
              "[1172] \"dysfunct\"                                                                    \n",
              "[1173] \"dysplasia\"                                                                   \n",
              "[1174] \"e\"                                                                           \n",
              "[1175] \"e-\"                                                                          \n",
              "[1176] \"e-mail\"                                                                      \n",
              "[1177] \"e-port\"                                                                      \n",
              "[1178] \"e11\"                                                                         \n",
              "[1179] \"e47\"                                                                         \n",
              "[1180] \"ea\"                                                                          \n",
              "[1181] \"earlier\"                                                                     \n",
              "[1182] \"eas\"                                                                         \n",
              "[1183] \"easi\"                                                                        \n",
              "[1184] \"easier\"                                                                      \n",
              "[1185] \"easili\"                                                                      \n",
              "[1186] \"eat\"                                                                         \n",
              "[1187] \"eb\"                                                                          \n",
              "[1188] \"ebusi\"                                                                       \n",
              "[1189] \"ec\"                                                                          \n",
              "[1190] \"eco-\"                                                                        \n",
              "[1191] \"econom\"                                                                      \n",
              "[1192] \"economi\"                                                                     \n",
              "[1193] \"ecut\"                                                                        \n",
              "[1194] \"ed\"                                                                          \n",
              "[1195] \"edc\"                                                                         \n",
              "[1196] \"edit\"                                                                        \n",
              "[1197] \"editor\"                                                                      \n",
              "[1198] \"editor1\"                                                                     \n",
              "[1199] \"educ\"                                                                        \n",
              "[1200] \"ee\"                                                                          \n",
              "[1201] \"ef\"                                                                          \n",
              "[1202] \"effec-\"                                                                      \n",
              "[1203] \"efficaci\"                                                                    \n",
              "[1204] \"effici\"                                                                      \n",
              "[1205] \"efo\"                                                                         \n",
              "[1206] \"eg\"                                                                          \n",
              "[1207] \"egi\"                                                                         \n",
              "[1208] \"egori\"                                                                       \n",
              "[1209] \"eh\"                                                                          \n",
              "[1210] \"ehealth\"                                                                     \n",
              "[1211] \"ehealth-kd\"                                                                  \n",
              "[1212] \"ei\"                                                                          \n",
              "[1213] \"eight\"                                                                       \n",
              "[1214] \"ekp\"                                                                         \n",
              "[1215] \"el\"                                                                          \n",
              "[1216] \"el-\"                                                                         \n",
              "[1217] \"ele\"                                                                         \n",
              "[1218] \"ele-\"                                                                        \n",
              "[1219] \"electr\"                                                                      \n",
              "[1220] \"element\"                                                                     \n",
              "[1221] \"eleph\"                                                                       \n",
              "[1222] \"elig\"                                                                        \n",
              "[1223] \"elimin\"                                                                      \n",
              "[1224] \"elk\"                                                                         \n",
              "[1225] \"ellison\"                                                                     \n",
              "[1226] \"els\"                                                                         \n",
              "[1227] \"elsewher\"                                                                    \n",
              "[1228] \"em\"                                                                          \n",
              "[1229] \"em-\"                                                                         \n",
              "[1230] \"eman\"                                                                        \n",
              "[1231] \"emb\"                                                                         \n",
              "[1232] \"embas\"                                                                       \n",
              "[1233] \"embed\"                                                                       \n",
              "[1234] \"embed-\"                                                                      \n",
              "[1235] \"ement\"                                                                       \n",
              "[1236] \"emerg\"                                                                       \n",
              "[1237] \"emi\"                                                                         \n",
              "[1238] \"empha-\"                                                                      \n",
              "[1239] \"emphas\"                                                                      \n",
              "[1240] \"emphasi\"                                                                     \n",
              "[1241] \"employ\"                                                                      \n",
              "[1242] \"employe\"                                                                     \n",
              "[1243] \"empti\"                                                                       \n",
              "[1244] \"emul\"                                                                        \n",
              "[1245] \"en\"                                                                          \n",
              "[1246] \"en-\"                                                                         \n",
              "[1247] \"enamel\"                                                                      \n",
              "[1248] \"enay\"                                                                        \n",
              "[1249] \"enclav\"                                                                      \n",
              "[1250] \"encompass\"                                                                   \n",
              "[1251] \"encount\"                                                                     \n",
              "[1252] \"end-to-end\"                                                                  \n",
              "[1253] \"endang\"                                                                      \n",
              "[1254] \"endem\"                                                                       \n",
              "[1255] \"endnot\"                                                                      \n",
              "[1256] \"endocrin\"                                                                    \n",
              "[1257] \"endodon-\"                                                                    \n",
              "[1258] \"endodont\"                                                                    \n",
              "[1259] \"endpoint\"                                                                    \n",
              "[1260] \"enforc\"                                                                      \n",
              "[1261] \"engag\"                                                                       \n",
              "[1262] \"engend\"                                                                      \n",
              "[1263] \"engi-\"                                                                       \n",
              "[1264] \"engin\"                                                                       \n",
              "[1265] \"engineer-\"                                                                   \n",
              "[1266] \"english\"                                                                     \n",
              "[1267] \"english-languag\"                                                             \n",
              "[1268] \"enhanc\"                                                                      \n",
              "[1269] \"enjoy\"                                                                       \n",
              "[1270] \"enough\"                                                                      \n",
              "[1271] \"enrich\"                                                                      \n",
              "[1272] \"ensur\"                                                                       \n",
              "[1273] \"entail\"                                                                      \n",
              "[1274] \"entail-\"                                                                     \n",
              "[1275] \"enter\"                                                                       \n",
              "[1276] \"enterolog\"                                                                   \n",
              "[1277] \"entif\"                                                                       \n",
              "[1278] \"entir\"                                                                       \n",
              "[1279] \"entitl\"                                                                      \n",
              "[1280] \"entri\"                                                                       \n",
              "[1281] \"envi-\"                                                                       \n",
              "[1282] \"environ\"                                                                     \n",
              "[1283] \"environment\"                                                                 \n",
              "[1284] \"envis\"                                                                       \n",
              "[1285] \"ep\"                                                                          \n",
              "[1286] \"ép\"                                                                          \n",
              "[1287] \"epi-\"                                                                        \n",
              "[1288] \"epidemiolog\"                                                                 \n",
              "[1289] \"epidemiologist\"                                                              \n",
              "[1290] \"epr\"                                                                         \n",
              "[1291] \"eq\"                                                                          \n",
              "[1292] \"equat\"                                                                       \n",
              "[1293] \"equival\"                                                                     \n",
              "[1294] \"equivalentclass\"                                                             \n",
              "[1295] \"equivalentproperti\"                                                          \n",
              "[1296] \"er\"                                                                          \n",
              "[1297] \"erabl\"                                                                       \n",
              "[1298] \"eral\"                                                                        \n",
              "[1299] \"erasmus\"                                                                     \n",
              "[1300] \"erat\"                                                                        \n",
              "[1301] \"ere\"                                                                         \n",
              "[1302] \"erenc\"                                                                       \n",
              "[1303] \"eri\"                                                                         \n",
              "[1304] \"eric\"                                                                        \n",
              "[1305] \"erik\"                                                                        \n",
              "[1306] \"error\"                                                                       \n",
              "[1307] \"erti\"                                                                        \n",
              "[1308] \"erythrasma\"                                                                  \n",
              "[1309] \"es\"                                                                          \n",
              "[1310] \"eschew\"                                                                      \n",
              "[1311] \"espe-\"                                                                       \n",
              "[1312] \"especi\"                                                                      \n",
              "[1313] \"ess\"                                                                         \n",
              "[1314] \"essarili\"                                                                    \n",
              "[1315] \"essenti\"                                                                     \n",
              "[1316] \"est\"                                                                         \n",
              "[1317] \"establish-\"                                                                  \n",
              "[1318] \"estim\"                                                                       \n",
              "[1319] \"ethic\"                                                                       \n",
              "[1320] \"etiolog\"                                                                     \n",
              "[1321] \"eu\"                                                                          \n",
              "[1322] \"eureto\"                                                                      \n",
              "[1323] \"europ\"                                                                       \n",
              "[1324] \"europepmc.org\"                                                               \n",
              "[1325] \"ev\"                                                                          \n",
              "[1326] \"eva-\"                                                                        \n",
              "[1327] \"eval-\"                                                                       \n",
              "[1328] \"evalex\"                                                                      \n",
              "[1329] \"evalu-\"                                                                      \n",
              "[1330] \"event\"                                                                       \n",
              "[1331] \"ever\"                                                                        \n",
              "[1332] \"everi\"                                                                       \n",
              "[1333] \"evid\"                                                                        \n",
              "[1334] \"evidence-bas\"                                                                \n",
              "[1335] \"evolut\"                                                                      \n",
              "[1336] \"evolv\"                                                                       \n",
              "[1337] \"ew\"                                                                          \n",
              "[1338] \"ewui\"                                                                        \n",
              "[1339] \"ex\"                                                                          \n",
              "[1340] \"ex-\"                                                                         \n",
              "[1341] \"ex-hospit\"                                                                   \n",
              "[1342] \"ex:pati\"                                                                     \n",
              "[1343] \"ex:treatedat\"                                                                \n",
              "[1344] \"exact\"                                                                       \n",
              "[1345] \"exam-\"                                                                       \n",
              "[1346] \"examin\"                                                                      \n",
              "[1347] \"examin-\"                                                                     \n",
              "[1348] \"example.org\"                                                                 \n",
              "[1349] \"examples3\"                                                                   \n",
              "[1350] \"examplewächt\"                                                                \n",
              "[1351] \"excel\"                                                                       \n",
              "[1352] \"except\"                                                                      \n",
              "[1353] \"excess\"                                                                      \n",
              "[1354] \"exchang\"                                                                     \n",
              "[1355] \"exclu-\"                                                                      \n",
              "[1356] \"exclud\"                                                                      \n",
              "[1357] \"exclus\"                                                                      \n",
              "[1358] \"execut\"                                                                      \n",
              "[1359] \"exem-\"                                                                       \n",
              "[1360] \"exemplari\"                                                                   \n",
              "[1361] \"exemplifi\"                                                                   \n",
              "[1362] \"exerc\"                                                                       \n",
              "[1363] \"exercis\"                                                                     \n",
              "[1364] \"exhaust\"                                                                     \n",
              "[1365] \"exhibit\"                                                                     \n",
              "[1366] \"exist-\"                                                                      \n",
              "[1367] \"exp\"                                                                         \n",
              "[1368] \"expand\"                                                                      \n",
              "[1369] \"expe-\"                                                                       \n",
              "[1370] \"expens\"                                                                      \n",
              "[1371] \"exper-\"                                                                      \n",
              "[1372] \"experi-\"                                                                     \n",
              "[1373] \"experienc\"                                                                   \n",
              "[1374] \"experience-assess\"                                                           \n",
              "[1375] \"experience-bas\"                                                              \n",
              "[1376] \"experiment\"                                                                  \n",
              "[1377] \"expert-annot\"                                                                \n",
              "[1378] \"expert-driven\"                                                               \n",
              "[1379] \"expertis\"                                                                    \n",
              "[1380] \"explain\"                                                                     \n",
              "[1381] \"explan\"                                                                      \n",
              "[1382] \"explic\"                                                                      \n",
              "[1383] \"explic-\"                                                                     \n",
              "[1384] \"exploit\"                                                                     \n",
              "[1385] \"explor\"                                                                      \n",
              "[1386] \"exponenti\"                                                                   \n",
              "[1387] \"export\"                                                                      \n",
              "[1388] \"expos\"                                                                       \n",
              "[1389] \"exposur\"                                                                     \n",
              "[1390] \"extant\"                                                                      \n",
              "[1391] \"exten-\"                                                                      \n",
              "[1392] \"extens\"                                                                      \n",
              "[1393] \"extent\"                                                                      \n",
              "[1394] \"exter-\"                                                                      \n",
              "[1395] \"extrem\"                                                                      \n",
              "[1396] \"ey\"                                                                          \n",
              "[1397] \"eye\"                                                                         \n",
              "[1398] \"f\"                                                                           \n",
              "[1399] \"f-59000\"                                                                     \n",
              "[1400] \"f-score\"                                                                     \n",
              "[1401] \"f1\"                                                                          \n",
              "[1402] \"f1-\"                                                                         \n",
              "[1403] \"f1-measur\"                                                                   \n",
              "[1404] \"f1-score\"                                                                    \n",
              "[1405] \"fa\"                                                                          \n",
              "[1406] \"fabric\"                                                                      \n",
              "[1407] \"fac-\"                                                                        \n",
              "[1408] \"face\"                                                                        \n",
              "[1409] \"facil\"                                                                       \n",
              "[1410] \"facilit\"                                                                     \n",
              "[1411] \"fact\"                                                                        \n",
              "[1412] \"facto\"                                                                       \n",
              "[1413] \"factor\"                                                                      \n",
              "[1414] \"factori\"                                                                     \n",
              "[1415] \"faculti\"                                                                     \n",
              "[1416] \"fail\"                                                                        \n",
              "[1417] \"failur\"                                                                      \n",
              "[1418] \"failure-precedes-\"                                                           \n",
              "[1419] \"fair\"                                                                        \n",
              "[1420] \"fake\"                                                                        \n",
              "[1421] \"fall\"                                                                        \n",
              "[1422] \"falmer\"                                                                      \n",
              "[1423] \"fals\"                                                                        \n",
              "[1424] \"false-neg\"                                                                   \n",
              "[1425] \"false-posit\"                                                                 \n",
              "[1426] \"famili\"                                                                      \n",
              "[1427] \"familiar\"                                                                    \n",
              "[1428] \"far\"                                                                         \n",
              "[1429] \"fashion\"                                                                     \n",
              "[1430] \"fast\"                                                                        \n",
              "[1431] \"fast-\"                                                                       \n",
              "[1432] \"fast-grow\"                                                                   \n",
              "[1433] \"fasting_glu-\"                                                                \n",
              "[1434] \"fasting_glucos\"                                                              \n",
              "[1435] \"fasting_glucose_\"                                                            \n",
              "[1436] \"fasting_glucose_abnorm\"                                                      \n",
              "[1437] \"fatti\"                                                                       \n",
              "[1438] \"faultless\"                                                                   \n",
              "[1439] \"favour\"                                                                      \n",
              "[1440] \"fb\"                                                                          \n",
              "[1441] \"fda\"                                                                         \n",
              "[1442] \"fe\"                                                                          \n",
              "[1443] \"fea-\"                                                                        \n",
              "[1444] \"feasibl\"                                                                     \n",
              "[1445] \"feature-\"                                                                    \n",
              "[1446] \"februari\"                                                                    \n",
              "[1447] \"fed\"                                                                         \n",
              "[1448] \"feder\"                                                                       \n",
              "[1449] \"feed\"                                                                        \n",
              "[1450] \"feedback\"                                                                    \n",
              "[1451] \"feel\"                                                                        \n",
              "[1452] \"femal\"                                                                       \n",
              "[1453] \"ferenc\"                                                                      \n",
              "[1454] \"ferent\"                                                                      \n",
              "[1455] \"fertil\"                                                                      \n",
              "[1456] \"fever\"                                                                       \n",
              "[1457] \"fewer\"                                                                       \n",
              "[1458] \"ffe\"                                                                         \n",
              "[1459] \"fg\"                                                                          \n",
              "[1460] \"fhir\"                                                                        \n",
              "[1461] \"fibrodentinoma\"                                                              \n",
              "[1462] \"ficat\"                                                                       \n",
              "[1463] \"ficient\"                                                                     \n",
              "[1464] \"ficiti\"                                                                      \n",
              "[1465] \"ficm\"                                                                        \n",
              "[1466] \"ficnd\"                                                                       \n",
              "[1467] \"ficni\"                                                                       \n",
              "[1468] \"fie\"                                                                         \n",
              "[1469] \"fier\"                                                                        \n",
              "[1470] \"fifth\"                                                                       \n",
              "[1471] \"fifti\"                                                                       \n",
              "[1472] \"filip\"                                                                       \n",
              "[1473] \"fill\"                                                                        \n",
              "[1474] \"filter\"                                                                      \n",
              "[1475] \"fin-\"                                                                        \n",
              "[1476] \"fincc\"                                                                       \n",
              "[1477] \"findabl\"                                                                     \n",
              "[1478] \"fine\"                                                                        \n",
              "[1479] \"fine-grain\"                                                                  \n",
              "[1480] \"finer\"                                                                       \n",
              "[1481] \"finer-grain\"                                                                 \n",
              "[1482] \"finland\"                                                                     \n",
              "[1483] \"finnish\"                                                                     \n",
              "[1484] \"fir\"                                                                         \n",
              "[1485] \"first\"                                                                       \n",
              "[1486] \"first_nam\"                                                                   \n",
              "[1487] \"first-ord\"                                                                   \n",
              "[1488] \"fish\"                                                                        \n",
              "[1489] \"fit\"                                                                         \n",
              "[1490] \"fixtur\"                                                                      \n",
              "[1491] \"fl\"                                                                          \n",
              "[1492] \"flaccid\"                                                                     \n",
              "[1493] \"flexibl\"                                                                     \n",
              "[1494] \"fli\"                                                                         \n",
              "[1495] \"flight\"                                                                      \n",
              "[1496] \"flip\"                                                                        \n",
              "[1497] \"florentien\"                                                                  \n",
              "[1498] \"florenzano\"                                                                  \n",
              "[1499] \"flow\"                                                                        \n",
              "[1500] \"fluid\"                                                                       \n",
              "[1501] \"fluorid\"                                                                     \n",
              "[1502] \"fma\"                                                                         \n",
              "[1503] \"fo\"                                                                          \n",
              "[1504] \"foci\"                                                                        \n",
              "[1505] \"fol-\"                                                                        \n",
              "[1506] \"fold\"                                                                        \n",
              "[1507] \"folliculari\"                                                                 \n",
              "[1508] \"follow\"                                                                      \n",
              "[1509] \"follow-\"                                                                     \n",
              "[1510] \"follow-up\"                                                                   \n",
              "[1511] \"food\"                                                                        \n",
              "[1512] \"for-\"                                                                        \n",
              "[1513] \"forag\"                                                                       \n",
              "[1514] \"forc\"                                                                        \n",
              "[1515] \"fore\"                                                                        \n",
              "[1516] \"forese\"                                                                      \n",
              "[1517] \"forest\"                                                                      \n",
              "[1518] \"formanc\"                                                                     \n",
              "[1519] \"formani\"                                                                     \n",
              "[1520] \"former\"                                                                      \n",
              "[1521] \"formul\"                                                                      \n",
              "[1522] \"formula\"                                                                     \n",
              "[1523] \"forum\"                                                                       \n",
              "[1524] \"forummemb\"                                                                   \n",
              "[1525] \"forummoder\"                                                                  \n",
              "[1526] \"forward\"                                                                     \n",
              "[1527] \"foster\"                                                                      \n",
              "[1528] \"found\"                                                                       \n",
              "[1529] \"founda-\"                                                                     \n",
              "[1530] \"foundat\"                                                                     \n",
              "[1531] \"foundri\"                                                                     \n",
              "[1532] \"four-digit-numb\"                                                             \n",
              "[1533] \"four-fold\"                                                                   \n",
              "[1534] \"four-step\"                                                                   \n",
              "[1535] \"fourth\"                                                                      \n",
              "[1536] \"fp\"                                                                          \n",
              "[1537] \"fr\"                                                                          \n",
              "[1538] \"fra\"                                                                         \n",
              "[1539] \"fractur\"                                                                     \n",
              "[1540] \"fragment\"                                                                    \n",
              "[1541] \"frame\"                                                                       \n",
              "[1542] \"framework\"                                                                   \n",
              "[1543] \"franc\"                                                                       \n",
              "[1544] \"frank\"                                                                       \n",
              "[1545] \"fre\"                                                                         \n",
              "[1546] \"fre-\"                                                                        \n",
              "[1547] \"free-\"                                                                       \n",
              "[1548] \"free-text\"                                                                   \n",
              "[1549] \"freedom\"                                                                     \n",
              "[1550] \"freeli\"                                                                      \n",
              "[1551] \"french\"                                                                      \n",
              "[1552] \"french-\"                                                                     \n",
              "[1553] \"frequenc\"                                                                    \n",
              "[1554] \"frequent\"                                                                    \n",
              "[1555] \"fro\"                                                                         \n",
              "[1556] \"fromrdf\"                                                                     \n",
              "[1557] \"fs\"                                                                          \n",
              "[1558] \"ft\"                                                                          \n",
              "[1559] \"ftp\"                                                                         \n",
              "[1560] \"fu\"                                                                          \n",
              "[1561] \"ful-\"                                                                        \n",
              "[1562] \"full_\"                                                                       \n",
              "[1563] \"full-\"                                                                       \n",
              "[1564] \"full-text\"                                                                   \n",
              "[1565] \"func-\"                                                                       \n",
              "[1566] \"function-\"                                                                   \n",
              "[1567] \"fund\"                                                                        \n",
              "[1568] \"fundament\"                                                                   \n",
              "[1569] \"fungal\"                                                                      \n",
              "[1570] \"fur-\"                                                                        \n",
              "[1571] \"furnitur\"                                                                    \n",
              "[1572] \"furthermor\"                                                                  \n",
              "[1573] \"furthest\"                                                                    \n",
              "[1574] \"fy\"                                                                          \n",
              "[1575] \"g\"                                                                           \n",
              "[1576] \"g1\"                                                                          \n",
              "[1577] \"ga\"                                                                          \n",
              "[1578] \"gain\"                                                                        \n",
              "[1579] \"gamet\"                                                                       \n",
              "[1580] \"ganism\"                                                                      \n",
              "[1581] \"gap\"                                                                         \n",
              "[1582] \"garcía\"                                                                      \n",
              "[1583] \"garcía-\"                                                                     \n",
              "[1584] \"garcía-garcía\"                                                               \n",
              "[1585] \"garcía-garcía1\"                                                              \n",
              "[1586] \"gard\"                                                                        \n",
              "[1587] \"gase\"                                                                        \n",
              "[1588] \"gastrin\"                                                                     \n",
              "[1589] \"gastro-\"                                                                     \n",
              "[1590] \"gastrointestin\"                                                              \n",
              "[1591] \"gastroparesi\"                                                                \n",
              "[1592] \"gatekeep\"                                                                    \n",
              "[1593] \"gather\"                                                                      \n",
              "[1594] \"gave\"                                                                        \n",
              "[1595] \"gdpr\"                                                                        \n",
              "[1596] \"ge\"                                                                          \n",
              "[1597] \"gen-\"                                                                        \n",
              "[1598] \"gender\"                                                                      \n",
              "[1599] \"gene\"                                                                        \n",
              "[1600] \"gene-regulatori\"                                                             \n",
              "[1601] \"gener-\"                                                                      \n",
              "[1602] \"general-practition\"                                                          \n",
              "[1603] \"generaliz\"                                                                   \n",
              "[1604] \"generic\"                                                                     \n",
              "[1605] \"genet\"                                                                       \n",
              "[1606] \"genom\"                                                                       \n",
              "[1607] \"genotyp\"                                                                     \n",
              "[1608] \"gensim\"                                                                      \n",
              "[1609] \"genus\"                                                                       \n",
              "[1610] \"geo-\"                                                                        \n",
              "[1611] \"gephi\"                                                                       \n",
              "[1612] \"german\"                                                                      \n",
              "[1613] \"germani\"                                                                     \n",
              "[1614] \"gerprint\"                                                                    \n",
              "[1615] \"gest\"                                                                        \n",
              "[1616] \"gestat\"                                                                      \n",
              "[1617] \"get\"                                                                         \n",
              "[1618] \"gfo\"                                                                         \n",
              "[1619] \"gfo-data\"                                                                    \n",
              "[1620] \"ghz\"                                                                         \n",
              "[1621] \"gi\"                                                                          \n",
              "[1622] \"giddi\"                                                                       \n",
              "[1623] \"gie\"                                                                         \n",
              "[1624] \"ginter1\"                                                                     \n",
              "[1625] \"gislat\"                                                                      \n",
              "[1626] \"git\"                                                                         \n",
              "[1627] \"github\"                                                                      \n",
              "[1628] \"github.com\"                                                                  \n",
              "[1629] \"gl\"                                                                          \n",
              "[1630] \"glanc\"                                                                       \n",
              "[1631] \"gland\"                                                                       \n",
              "[1632] \"gleim\"                                                                       \n",
              "[1633] \"gleim@cs.rwth-aachen.d\"                                                      \n",
              "[1634] \"gleim1\"                                                                      \n",
              "[1635] \"glo-\"                                                                        \n",
              "[1636] \"global\"                                                                      \n",
              "[1637] \"glomangiomatosi\"                                                             \n",
              "[1638] \"glossiti\"                                                                    \n",
              "[1639] \"glove\"                                                                       \n",
              "[1640] \"glucos\"                                                                      \n",
              "[1641] \"gn\"                                                                          \n",
              "[1642] \"gnathostomiasi\"                                                              \n",
              "[1643] \"go\"                                                                          \n",
              "[1644] \"goal\"                                                                        \n",
              "[1645] \"goe\"                                                                         \n",
              "[1646] \"gold\"                                                                        \n",
              "[1647] \"gone\"                                                                        \n",
              "[1648] \"gonochorist\"                                                                 \n",
              "[1649] \"goodod\"                                                                      \n",
              "[1650] \"goodpastur\"                                                                  \n",
              "[1651] \"goodrela-\"                                                                   \n",
              "[1652] \"gorithm\"                                                                     \n",
              "[1653] \"govern\"                                                                      \n",
              "[1654] \"governmen-\"                                                                  \n",
              "[1655] \"government\"                                                                  \n",
              "[1656] \"gpl-3.0\"                                                                     \n",
              "[1657] \"gr\"                                                                          \n",
              "[1658] \"grabar\"                                                                      \n",
              "[1659] \"grabar1,2\"                                                                   \n",
              "[1660] \"grad-\"                                                                       \n",
              "[1661] \"gradient-bas\"                                                                \n",
              "[1662] \"gradual\"                                                                     \n",
              "[1663] \"graft\"                                                                       \n",
              "[1664] \"graña2\"                                                                      \n",
              "[1665] \"granular\"                                                                    \n",
              "[1666] \"grape\"                                                                       \n",
              "[1667] \"grape-bas\"                                                                   \n",
              "[1668] \"graph\"                                                                       \n",
              "[1669] \"graph-\"                                                                      \n",
              "[1670] \"graph-bas\"                                                                   \n",
              "[1671] \"graphic\"                                                                     \n",
              "[1672] \"grasp\"                                                                       \n",
              "[1673] \"grate\"                                                                       \n",
              "[1674] \"grationmethodolog\"                                                           \n",
              "[1675] \"gratzl\"                                                                      \n",
              "[1676] \"great\"                                                                       \n",
              "[1677] \"greater\"                                                                     \n",
              "[1678] \"greatest\"                                                                    \n",
              "[1679] \"gredient\"                                                                    \n",
              "[1680] \"green\"                                                                       \n",
              "[1681] \"gregat\"                                                                      \n",
              "[1682] \"gress\"                                                                       \n",
              "[1683] \"grishagin1\"                                                                  \n",
              "[1684] \"grouin\"                                                                      \n",
              "[1685] \"ground\"                                                                      \n",
              "[1686] \"group\"                                                                       \n",
              "[1687] \"grow\"                                                                        \n",
              "[1688] \"grown\"                                                                       \n",
              "[1689] \"growth\"                                                                      \n",
              "[1690] \"gt\"                                                                          \n",
              "[1691] \"gt1970\"                                                                      \n",
              "[1692] \"gt75\"                                                                        \n",
              "[1693] \"gu\"                                                                          \n",
              "[1694] \"guag\"                                                                        \n",
              "[1695] \"guarante\"                                                                    \n",
              "[1696] \"gui\"                                                                         \n",
              "[1697] \"guid\"                                                                        \n",
              "[1698] \"guidanc\"                                                                     \n",
              "[1699] \"guide-\"                                                                      \n",
              "[1700] \"guidelin\"                                                                    \n",
              "[1701] \"guish\"                                                                       \n",
              "[1702] \"gy\"                                                                          \n",
              "[1703] \"h\"                                                                           \n",
              "[1704] \"h-index\"                                                                     \n",
              "[1705] \"h1\"                                                                          \n",
              "[1706] \"h2\"                                                                          \n",
              "[1707] \"h3\"                                                                          \n",
              "[1708] \"ha\"                                                                          \n",
              "[1709] \"haendel4\"                                                                    \n",
              "[1710] \"hakala\"                                                                      \n",
              "[1711] \"hakala1,2\"                                                                   \n",
              "[1712] \"hal2\"                                                                        \n",
              "[1713] \"halder\"                                                                      \n",
              "[1714] \"hamamatsu\"                                                                   \n",
              "[1715] \"hamper\"                                                                      \n",
              "[1716] \"han\"                                                                         \n",
              "[1717] \"hand\"                                                                        \n",
              "[1718] \"handbook\"                                                                    \n",
              "[1719] \"handl\"                                                                       \n",
              "[1720] \"hanna-maria\"                                                                 \n",
              "[1721] \"happen\"                                                                      \n",
              "[1722] \"hard\"                                                                        \n",
              "[1723] \"harmon\"                                                                      \n",
              "[1724] \"harmonis\"                                                                    \n",
              "[1725] \"harvard\"                                                                     \n",
              "[1726] \"harvest\"                                                                     \n",
              "[1727] \"has_part\"                                                                    \n",
              "[1728] \"has_specified_\"                                                              \n",
              "[1729] \"has_specified_input\"                                                         \n",
              "[1730] \"has_valu\"                                                                    \n",
              "[1731] \"hasfil\"                                                                      \n",
              "[1732] \"hasheadboard\"                                                                \n",
              "[1733] \"hasiso-\"                                                                     \n",
              "[1734] \"hasparent.person\"                                                            \n",
              "[1735] \"hasproperpart.chloroplast\"                                                   \n",
              "[1736] \"hastop\"                                                                      \n",
              "[1737] \"hasx\"                                                                        \n",
              "[1738] \"have\"                                                                        \n",
              "[1739] \"hb\"                                                                          \n",
              "[1740] \"hb1\"                                                                         \n",
              "[1741] \"hb2\"                                                                         \n",
              "[1742] \"hbj\"                                                                         \n",
              "[1743] \"hbn\"                                                                         \n",
              "[1744] \"hci\"                                                                         \n",
              "[1745] \"hcls\"                                                                        \n",
              "[1746] \"hds\"                                                                         \n",
              "[1747] \"head\"                                                                        \n",
              "[1748] \"head-\"                                                                       \n",
              "[1749] \"headach\"                                                                     \n",
              "[1750] \"header\"                                                                      \n",
              "[1751] \"health_care_unit\"                                                            \n",
              "[1752] \"health-car\"                                                                  \n",
              "[1753] \"health-rel\"                                                                  \n",
              "[1754] \"healthboards1\"                                                               \n",
              "[1755] \"heard\"                                                                       \n",
              "[1756] \"heart\"                                                                       \n",
              "[1757] \"heart-burn\"                                                                  \n",
              "[1758] \"heart-rel\"                                                                   \n",
              "[1759] \"heartburn\"                                                                   \n",
              "[1760] \"heath\"                                                                       \n",
              "[1761] \"height\"                                                                      \n",
              "[1762] \"heinrich\"                                                                    \n",
              "[1763] \"heinrich.herre@imise.uni-\"                                                   \n",
              "[1764] \"held\"                                                                        \n",
              "[1765] \"hematolog\"                                                                   \n",
              "[1766] \"hematopoiet\"                                                                 \n",
              "[1767] \"hemoglobin\"                                                                  \n",
              "[1768] \"hemoglobinopathi\"                                                            \n",
              "[1769] \"henc\"                                                                        \n",
              "[1770] \"henri\"                                                                       \n",
              "[1771] \"herbivor\"                                                                    \n",
              "[1772] \"herd\"                                                                        \n",
              "[1773] \"hereinaft\"                                                                   \n",
              "[1774] \"hermit\"                                                                      \n",
              "[1775] \"hernandez6\"                                                                  \n",
              "[1776] \"herp\"                                                                        \n",
              "[1777] \"herpesvirida\"                                                                \n",
              "[1778] \"herpesvirus\"                                                                 \n",
              "[1779] \"herre1,2\"                                                                    \n",
              "[1780] \"heterogen\"                                                                   \n",
              "[1781] \"hetionet\"                                                                    \n",
              "[1782] \"heurist\"                                                                     \n",
              "[1783] \"hf\"                                                                          \n",
              "[1784] \"hf1\"                                                                         \n",
              "[1785] \"hf2\"                                                                         \n",
              "[1786] \"hfi\"                                                                         \n",
              "[1787] \"hfj\"                                                                         \n",
              "[1788] \"hfn\"                                                                         \n",
              "[1789] \"hi\"                                                                          \n",
              "[1790] \"hidden\"                                                                      \n",
              "[1791] \"hier-\"                                                                       \n",
              "[1792] \"hierarch\"                                                                    \n",
              "[1793] \"hierarchi\"                                                                   \n",
              "[1794] \"high\"                                                                        \n",
              "[1795] \"high-\"                                                                       \n",
              "[1796] \"high-level\"                                                                  \n",
              "[1797] \"high-qual\"                                                                   \n",
              "[1798] \"highest\"                                                                     \n",
              "[1799] \"highlight\"                                                                   \n",
              "[1800] \"himmelstein\"                                                                 \n",
              "[1801] \"hip\"                                                                         \n",
              "[1802] \"hipaa\"                                                                       \n",
              "[1803] \"hiragana\"                                                                    \n",
              "[1804] \"hiromasa\"                                                                    \n",
              "[1805] \"hk\"                                                                          \n",
              "[1806] \"hkj\"                                                                         \n",
              "[1807] \"hl7\"                                                                         \n",
              "[1808] \"hm\"                                                                          \n",
              "[1809] \"hn\"                                                                          \n",
              "[1810] \"hnsmoen@gmail.com\"                                                           \n",
              "[1811] \"ho\"                                                                          \n",
              "[1812] \"hoc\"                                                                         \n",
              "[1813] \"hoehndorf1,2\"                                                                \n",
              "[1814] \"hold\"                                                                        \n",
              "[1815] \"holger\"                                                                      \n",
              "[1816] \"holist\"                                                                      \n",
              "[1817] \"home\"                                                                        \n",
              "[1818] \"homodimer\"                                                                   \n",
              "[1819] \"homogen\"                                                                     \n",
              "[1820] \"hon\"                                                                         \n",
              "[1821] \"hook\"                                                                        \n",
              "[1822] \"horiguchi2\"                                                                  \n",
              "[1823] \"horizont\"                                                                    \n",
              "[1824] \"hort\"                                                                        \n",
              "[1825] \"hos-\"                                                                        \n",
              "[1826] \"hospi-\"                                                                      \n",
              "[1827] \"hosseini\"                                                                    \n",
              "[1828] \"how-\"                                                                        \n",
              "[1829] \"hpo\"                                                                         \n",
              "[1830] \"hr\"                                                                          \n",
              "[1831] \"http\"                                                                        \n",
              "[1832] \"http://creativecommons.org/\"                                                 \n",
              "[1833] \"http://creativecommons.org/licenses/by/4.0/\"                                 \n",
              "[1834] \"http://creativecommons.org/publicdomain/zero/1.\"                             \n",
              "[1835] \"http://disease-ontology.org/\"                                                \n",
              "[1836] \"http://example.org/\"                                                         \n",
              "[1837] \"http://loinc.org\"                                                            \n",
              "[1838] \"http://loinc.org?3141-9\"                                                     \n",
              "[1839] \"http://snomed.info/\"                                                         \n",
              "[1840] \"http://www.euretos.com\"                                                      \n",
              "[1841] \"http://www.iro.umontreal.ca/~lapalme/ift6281/owl/africanwildlifeontology.xml\"\n",
              "[1842] \"https\"                                                                       \n",
              "[1843] \"https://doi.org/10.1186/s13326-019-0218-0\"                                   \n",
              "[1844] \"https://doi.org/10.1186/s13326-020-00221-1\"                                  \n",
              "[1845] \"https://doi.org/10.1186/s13326-020-00222-0\"                                  \n",
              "[1846] \"https://doi.org/10.1186/s13326-020-00223-z\"                                  \n",
              "[1847] \"https://doi.org/10.1186/s13326-020-00224-i\"                                  \n",
              "[1848] \"https://doi.org/10.1186/s13326-020-00225-x\"                                  \n",
              "[1849] \"https://doi.org/10.1186/s13326-020-00226-w\"                                  \n",
              "[1850] \"https://doi.org/10.1186/s13326-020-00227-9\"                                  \n",
              "[1851] \"https://doi.org/10.1186/s13326-020-00228-8\"                                  \n",
              "[1852] \"https://doi.org/10.1186/s13326-020-00229-7\"                                  \n",
              "[1853] \"https://doi.org/10.1186/s13326-020-00230-0\"                                  \n",
              "[1854] \"https://doi.org/10.1186/s13326-020-00231-z\"                                  \n",
              "[1855] \"https://doi.org/10.1186/s13326-020-00232-i\"                                  \n",
              "[1856] \"https://doi.org/10.1186/s13326-020-0219-z\"                                   \n",
              "[1857] \"https://fairsharing.org/\"                                                    \n",
              "[1858] \"https://github.com/bio-ontology-research-group/ontology-extension.\"          \n",
              "[1859] \"https://github.com/onto-med/phe-\"                                            \n",
              "[1860] \"hu\"                                                                          \n",
              "[1861] \"hu-\"                                                                         \n",
              "[1862] \"huge\"                                                                        \n",
              "[1863] \"human-read\"                                                                  \n",
              "[1864] \"hundr\"                                                                       \n",
              "[1865] \"hurdl\"                                                                       \n",
              "[1866] \"hy\"                                                                          \n",
              "[1867] \"hygienist\"                                                                   \n",
              "[1868] \"hyperprolactinemiadiseas\"                                                    \n",
              "[1869] \"hypertens\"                                                                   \n",
              "[1870] \"hypohidrosi\"                                                                 \n",
              "[1871] \"hypomineralis\"                                                               \n",
              "[1872] \"hypophosphatasia\"                                                            \n",
              "[1873] \"hypothes\"                                                                    \n",
              "[1874] \"hypothesi\"                                                                   \n",
              "[1875] \"i-\"                                                                          \n",
              "[1876] \"i-iii\"                                                                       \n",
              "[1877] \"i2\"                                                                          \n",
              "[1878] \"i2b2\"                                                                        \n",
              "[1879] \"ia\"                                                                          \n",
              "[1880] \"ial\"                                                                         \n",
              "[1881] \"iao\"                                                                         \n",
              "[1882] \"ib\"                                                                          \n",
              "[1883] \"ibil\"                                                                        \n",
              "[1884] \"ic\"                                                                          \n",
              "[1885] \"ican\"                                                                        \n",
              "[1886] \"icat\"                                                                        \n",
              "[1887] \"icd\"                                                                         \n",
              "[1888] \"icd-\"                                                                        \n",
              "[1889] \"icd-10\"                                                                      \n",
              "[1890] \"icl\"                                                                         \n",
              "[1891] \"icpc\"                                                                        \n",
              "[1892] \"id3\"                                                                         \n",
              "[1893] \"idea\"                                                                        \n",
              "[1894] \"ideal\"                                                                       \n",
              "[1895] \"iden-\"                                                                       \n",
              "[1896] \"ident\"                                                                       \n",
              "[1897] \"identi-\"                                                                     \n",
              "[1898] \"identify-\"                                                                   \n",
              "[1899] \"ie\"                                                                          \n",
              "[1900] \"ieee\"                                                                        \n",
              "[1901] \"if-then-els\"                                                                 \n",
              "[1902] \"ifi\"                                                                         \n",
              "[1903] \"ig\"                                                                          \n",
              "[1904] \"ignor\"                                                                       \n",
              "[1905] \"ih\"                                                                          \n",
              "[1906] \"ii\"                                                                          \n",
              "[1907] \"iii\"                                                                         \n",
              "[1908] \"ij\"                                                                          \n",
              "[1909] \"ik\"                                                                          \n",
              "[1910] \"ilar\"                                                                        \n",
              "[1911] \"ild\"                                                                         \n",
              "[1912] \"ili\"                                                                         \n",
              "[1913] \"ill\"                                                                         \n",
              "[1914] \"illus-\"                                                                      \n",
              "[1915] \"im\"                                                                          \n",
              "[1916] \"im-\"                                                                         \n",
              "[1917] \"iment\"                                                                       \n",
              "[1918] \"imis\"                                                                        \n",
              "[1919] \"immedi\"                                                                      \n",
              "[1920] \"immun\"                                                                       \n",
              "[1921] \"impala\"                                                                      \n",
              "[1922] \"imped\"                                                                       \n",
              "[1923] \"imple-\"                                                                      \n",
              "[1924] \"impli\"                                                                       \n",
              "[1925] \"implicit\"                                                                    \n",
              "[1926] \"impor-\"                                                                      \n",
              "[1927] \"impos\"                                                                       \n",
              "[1928] \"imposs\"                                                                      \n",
              "[1929] \"improp\"                                                                      \n",
              "[1930] \"improv-\"                                                                     \n",
              "[1931] \"improve-\"                                                                    \n",
              "[1932] \"imum\"                                                                        \n",
              "[1933] \"inabl\"                                                                       \n",
              "[1934] \"inaccess\"                                                                    \n",
              "[1935] \"inadequ\"                                                                     \n",
              "[1936] \"inc\"                                                                         \n",
              "[1937] \"incid\"                                                                       \n",
              "[1938] \"incipi\"                                                                      \n",
              "[1939] \"incis\"                                                                       \n",
              "[1940] \"inclin\"                                                                      \n",
              "[1941] \"inclu-\"                                                                      \n",
              "[1942] \"inconclus\"                                                                   \n",
              "[1943] \"inconsist\"                                                                   \n",
              "[1944] \"inconveni\"                                                                   \n",
              "[1945] \"incor-\"                                                                      \n",
              "[1946] \"incorpor\"                                                                    \n",
              "[1947] \"incorrect\"                                                                   \n",
              "[1948] \"increas\"                                                                     \n",
              "[1949] \"increment\"                                                                   \n",
              "[1950] \"incur\"                                                                       \n",
              "[1951] \"inde\"                                                                        \n",
              "[1952] \"independ\"                                                                    \n",
              "[1953] \"index-\"                                                                      \n",
              "[1954] \"indi-\"                                                                       \n",
              "[1955] \"indianapoli\"                                                                 \n",
              "[1956] \"indic\"                                                                       \n",
              "[1957] \"indirect\"                                                                    \n",
              "[1958] \"indispens\"                                                                   \n",
              "[1959] \"indistinct\"                                                                  \n",
              "[1960] \"induc\"                                                                       \n",
              "[1961] \"induct\"                                                                      \n",
              "[1962] \"industri\"                                                                    \n",
              "[1963] \"infantile-ataxia\"                                                            \n",
              "[1964] \"infeas\"                                                                      \n",
              "[1965] \"infect\"                                                                      \n",
              "[1966] \"infecti\"                                                                     \n",
              "[1967] \"infer\"                                                                       \n",
              "[1968] \"inferenc\"                                                                    \n",
              "[1969] \"inflec-\"                                                                     \n",
              "[1970] \"influ-\"                                                                      \n",
              "[1971] \"influenti\"                                                                   \n",
              "[1972] \"influenza\"                                                                   \n",
              "[1973] \"infor-\"                                                                      \n",
              "[1974] \"informa-\"                                                                    \n",
              "[1975] \"ingest\"                                                                      \n",
              "[1976] \"ingredi\"                                                                     \n",
              "[1977] \"ingredi-\"                                                                    \n",
              "[1978] \"inher\"                                                                       \n",
              "[1979] \"inherit\"                                                                     \n",
              "[1980] \"inhibit\"                                                                     \n",
              "[1981] \"inhomogen\"                                                                   \n",
              "[1982] \"init\"                                                                        \n",
              "[1983] \"innov\"                                                                       \n",
              "[1984] \"insert\"                                                                      \n",
              "[1985] \"insid\"                                                                       \n",
              "[1986] \"insight\"                                                                     \n",
              "[1987] \"insomnia\"                                                                    \n",
              "[1988] \"inspect\"                                                                     \n",
              "[1989] \"inspir\"                                                                      \n",
              "[1990] \"instance-bas\"                                                                \n",
              "[1991] \"instanti\"                                                                    \n",
              "[1992] \"institu-\"                                                                    \n",
              "[1993] \"instru-\"                                                                     \n",
              "[1994] \"instruct\"                                                                    \n",
              "[1995] \"insuf-\"                                                                      \n",
              "[1996] \"insuffici\"                                                                   \n",
              "[1997] \"insur\"                                                                       \n",
              "[1998] \"inte-\"                                                                       \n",
              "[1999] \"intellig\"                                                                    \n",
              "[2000] \"intens\"                                                                      \n",
              "[2001] \"intension\"                                                                   \n",
              "[2002] \"inter\"                                                                       \n",
              "[2003] \"inter-\"                                                                      \n",
              "[2004] \"inter-annot\"                                                                 \n",
              "[2005] \"interact\"                                                                    \n",
              "[2006] \"interchang\"                                                                  \n",
              "[2007] \"interconnect\"                                                                \n",
              "[2008] \"interdisciplinari\"                                                           \n",
              "[2009] \"interfac\"                                                                    \n",
              "[2010] \"intermedi\"                                                                   \n",
              "[2011] \"internation\"                                                                 \n",
              "[2012] \"internet\"                                                                    \n",
              "[2013] \"interop-\"                                                                    \n",
              "[2014] \"interoper\"                                                                   \n",
              "[2015] \"interoper-\"                                                                  \n",
              "[2016] \"interoperabil-\"                                                              \n",
              "[2017] \"interquartil\"                                                                \n",
              "[2018] \"interre-\"                                                                    \n",
              "[2019] \"intersect\"                                                                   \n",
              "[2020] \"intervent\"                                                                   \n",
              "[2021] \"interview\"                                                                   \n",
              "[2022] \"intestin\"                                                                    \n",
              "[2023] \"intracoron\"                                                                  \n",
              "[2024] \"intro-\"                                                                      \n",
              "[2025] \"introduc-\"                                                                   \n",
              "[2026] \"introduct\"                                                                   \n",
              "[2027] \"introductori\"                                                                \n",
              "[2028] \"introspection-assist\"                                                        \n",
              "[2029] \"intuit\"                                                                      \n",
              "[2030] \"inventori\"                                                                   \n",
              "[2031] \"invers\"                                                                      \n",
              "[2032] \"investig\"                                                                    \n",
              "[2033] \"invit\"                                                                       \n",
              "[2034] \"inxight\"                                                                     \n",
              "[2035] \"io\"                                                                          \n",
              "[2036] \"iob\"                                                                         \n",
              "[2037] \"iob2\"                                                                        \n",
              "[2038] \"ioniz\"                                                                       \n",
              "[2039] \"ip\"                                                                          \n",
              "[2040] \"ir\"                                                                          \n",
              "[2041] \"ira\"                                                                         \n",
              "[2042] \"iri\"                                                                         \n",
              "[2043] \"iro\"                                                                         \n",
              "[2044] \"iron\"                                                                        \n",
              "[2045] \"irreflex\"                                                                    \n",
              "[2046] \"irrespect\"                                                                   \n",
              "[2047] \"irrit\"                                                                       \n",
              "[2048] \"is-a\"                                                                        \n",
              "[2049] \"isat\"                                                                        \n",
              "[2050] \"isblank\"                                                                     \n",
              "[2051] \"ischem\"                                                                      \n",
              "[2052] \"ish\"                                                                         \n",
              "[2053] \"isizulu\"                                                                     \n",
              "[2054] \"ism\"                                                                         \n",
              "[2055] \"isn\"                                                                         \n",
              "[2056] \"isoimmun\"                                                                    \n",
              "[2057] \"isol\"                                                                        \n",
              "[2058] \"istex3\"                                                                      \n",
              "[2059] \"istrat\"                                                                      \n",
              "[2060] \"it\"                                                                          \n",
              "[2061] \"ita\"                                                                         \n",
              "[2062] \"italian\"                                                                     \n",
              "[2063] \"itch\"                                                                        \n",
              "[2064] \"ite\"                                                                         \n",
              "[2065] \"item\"                                                                        \n",
              "[2066] \"iter\"                                                                        \n",
              "[2067] \"ith\"                                                                         \n",
              "[2068] \"ition\"                                                                       \n",
              "[2069] \"itiv\"                                                                        \n",
              "[2070] \"itu\"                                                                         \n",
              "[2071] \"iu\"                                                                          \n",
              "[2072] \"iv\"                                                                          \n",
              "[2073] \"ivan\"                                                                        \n",
              "[2074] \"ive\"                                                                         \n",
              "[2075] \"iw\"                                                                          \n",
              "[2076] \"iz\"                                                                          \n",
              "[2077] \"j.-\"                                                                         \n",
              "[2078] \"j.a\"                                                                         \n",
              "[2079] \"j.j\"                                                                         \n",
              "[2080] \"j.t\"                                                                         \n",
              "[2081] \"j09\"                                                                         \n",
              "[2082] \"j1b-109\"                                                                     \n",
              "[2083] \"ja\"                                                                          \n",
              "[2084] \"jaccard\"                                                                     \n",
              "[2085] \"jan\"                                                                         \n",
              "[2086] \"januari\"                                                                     \n",
              "[2087] \"japan\"                                                                       \n",
              "[2088] \"japanes\"                                                                     \n",
              "[2089] \"java\"                                                                        \n",
              "[2090] \"jaw\"                                                                         \n",
              "[2091] \"je\"                                                                          \n",
              "[2092] \"ject\"                                                                        \n",
              "[2093] \"jectori\"                                                                     \n",
              "[2094] \"jen-\"                                                                        \n",
              "[2095] \"jensen\"                                                                      \n",
              "[2096] \"jian\"                                                                        \n",
              "[2097] \"jie\"                                                                         \n",
              "[2098] \"jin\"                                                                         \n",
              "[2099] \"jn\"                                                                          \n",
              "[2100] \"jo\"                                                                          \n",
              "[2101] \"jochem\"                                                                      \n",
              "[2102] \"johannsen\"                                                                   \n",
              "[2103] \"johoku\"                                                                      \n",
              "[2104] \"joint\"                                                                       \n",
              "[2105] \"jour-\"                                                                       \n",
              "[2106] \"joyc\"                                                                        \n",
              "[2107] \"json\"                                                                        \n",
              "[2108] \"jth\"                                                                         \n",
              "[2109] \"ju\"                                                                          \n",
              "[2110] \"junit\"                                                                       \n",
              "[2111] \"just\"                                                                        \n",
              "[2112] \"justifi\"                                                                     \n",
              "[2113] \"juxtaposit\"                                                                  \n",
              "[2114] \"k-mean\"                                                                      \n",
              "[2115] \"ka\"                                                                          \n",
              "[2116] \"kabuki\"                                                                      \n",
              "[2117] \"kafkas1,2\"                                                                   \n",
              "[2118] \"kai\"                                                                         \n",
              "[2119] \"kajiyama\"                                                                    \n",
              "[2120] \"kajiyama1\"                                                                   \n",
              "[2121] \"kam1\"                                                                        \n",
              "[2122] \"kan\"                                                                         \n",
              "[2123] \"kanji\"                                                                       \n",
              "[2124] \"kano@inf.shizuoka.ac.jp\"                                                     \n",
              "[2125] \"kano1\"                                                                       \n",
              "[2126] \"kaposi\"                                                                      \n",
              "[2127] \"kappa\"                                                                       \n",
              "[2128] \"karczewski\"                                                                  \n",
              "[2129] \"karim1,2\"                                                                    \n",
              "[2130] \"katakana\"                                                                    \n",
              "[2131] \"kazunari\"                                                                    \n",
              "[2132] \"kdd\"                                                                         \n",
              "[2133] \"ke\"                                                                          \n",
              "[2134] \"keep\"                                                                        \n",
              "[2135] \"keet\"                                                                        \n",
              "[2136] \"kellou-menou\"                                                                \n",
              "[2137] \"kera\"                                                                        \n",
              "[2138] \"keratosi\"                                                                    \n",
              "[2139] \"kernel\"                                                                      \n",
              "[2140] \"kernicterus\"                                                                 \n",
              "[2141] \"kersloot\"                                                                    \n",
              "[2142] \"kersloot1,2\"                                                                 \n",
              "[2143] \"key-\"                                                                        \n",
              "[2144] \"key-valu\"                                                                    \n",
              "[2145] \"kg\"                                                                          \n",
              "[2146] \"kh\"                                                                          \n",
              "[2147] \"khadem-\"                                                                     \n",
              "[2148] \"khademhos-\"                                                                  \n",
              "[2149] \"khademhosseini\"                                                              \n",
              "[2150] \"ki\"                                                                          \n",
              "[2151] \"kidney\"                                                                      \n",
              "[2152] \"king\"                                                                        \n",
              "[2153] \"kirsi\"                                                                       \n",
              "[2154] \"kirsten2,4,5\"                                                                \n",
              "[2155] \"kishaloy\"                                                                    \n",
              "[2156] \"kl\"                                                                          \n",
              "[2157] \"kn\"                                                                          \n",
              "[2158] \"know\"                                                                        \n",
              "[2159] \"know-\"                                                                       \n",
              "[2160] \"knowl-\"                                                                      \n",
              "[2161] \"knowlesi\"                                                                    \n",
              "[2162] \"known\"                                                                       \n",
              "[2163] \"ko\"                                                                          \n",
              "[2164] \"kö\"                                                                          \n",
              "[2165] \"kohei\"                                                                       \n",
              "[2166] \"kohlbacher3,4,5,6,7\"                                                         \n",
              "[2167] \"kontou\"                                                                      \n",
              "[2168] \"koopman\"                                                                     \n",
              "[2169] \"kors1\"                                                                       \n",
              "[2170] \"ks\"                                                                          \n",
              "[2171] \"kuromoji\"                                                                    \n",
              "[2172] \"kuwata\"                                                                      \n",
              "[2173] \"kw\"                                                                          \n",
              "[2174] \"l\"                                                                           \n",
              "[2175] \"l.a.garcia-garcia@sussex.ac.uk\"                                              \n",
              "[2176] \"l.i\"                                                                         \n",
              "[2177] \"l0\"                                                                          \n",
              "[2178] \"l1\"                                                                          \n",
              "[2179] \"l2\"                                                                          \n",
              "[2180] \"l7\"                                                                          \n",
              "[2181] \"la\"                                                                          \n",
              "[2182] \"lab\"                                                                         \n",
              "[2183] \"label-bas\"                                                                   \n",
              "[2184] \"labo-\"                                                                       \n",
              "[2185] \"labor\"                                                                       \n",
              "[2186] \"laboratori\"                                                                  \n",
              "[2187] \"lack\"                                                                        \n",
              "[2188] \"lan-\"                                                                        \n",
              "[2189] \"lancet\"                                                                      \n",
              "[2190] \"landscap\"                                                                    \n",
              "[2191] \"langag\"                                                                      \n",
              "[2192] \"lar\"                                                                         \n",
              "[2193] \"lardizab\"                                                                    \n",
              "[2194] \"larg\"                                                                        \n",
              "[2195] \"large-scal\"                                                                  \n",
              "[2196] \"largest\"                                                                     \n",
              "[2197] \"lari\"                                                                        \n",
              "[2198] \"larynx\"                                                                      \n",
              "[2199] \"laser\"                                                                       \n",
              "[2200] \"last\"                                                                        \n",
              "[2201] \"last_nam\"                                                                    \n",
              "[2202] \"late\"                                                                        \n",
              "[2203] \"latent\"                                                                      \n",
              "[2204] \"later\"                                                                       \n",
              "[2205] \"lationship\"                                                                  \n",
              "[2206] \"lator\"                                                                       \n",
              "[2207] \"latter\"                                                                      \n",
              "[2208] \"laura-maria\"                                                                 \n",
              "[2209] \"law-\"                                                                        \n",
              "[2210] \"layer\"                                                                       \n",
              "[2211] \"lc\"                                                                          \n",
              "[2212] \"ld\"                                                                          \n",
              "[2213] \"le\"                                                                          \n",
              "[2214] \"le-\"                                                                         \n",
              "[2215] \"leaf\"                                                                        \n",
              "[2216] \"learnabl\"                                                                    \n",
              "[2217] \"learner\"                                                                     \n",
              "[2218] \"learningmethod\"                                                              \n",
              "[2219] \"least\"                                                                       \n",
              "[2220] \"leav\"                                                                        \n",
              "[2221] \"lect\"                                                                        \n",
              "[2222] \"led\"                                                                         \n",
              "[2223] \"ledg\"                                                                        \n",
              "[2224] \"ledgment\"                                                                    \n",
              "[2225] \"lee\"                                                                         \n",
              "[2226] \"left\"                                                                        \n",
              "[2227] \"legal\"                                                                       \n",
              "[2228] \"legisl\"                                                                      \n",
              "[2229] \"leipzig\"                                                                     \n",
              "[2230] \"leipzig.d\"                                                                   \n",
              "[2231] \"lem\"                                                                         \n",
              "[2232] \"lemmat\"                                                                      \n",
              "[2233] \"lend\"                                                                        \n",
              "[2234] \"length\"                                                                      \n",
              "[2235] \"leonardo\"                                                                    \n",
              "[2236] \"lesion\"                                                                      \n",
              "[2237] \"lesser\"                                                                      \n",
              "[2238] \"let\"                                                                         \n",
              "[2239] \"letter\"                                                                      \n",
              "[2240] \"leucoencephalopathi\"                                                         \n",
              "[2241] \"leverag\"                                                                     \n",
              "[2242] \"leverag-\"                                                                    \n",
              "[2243] \"lex\"                                                                         \n",
              "[2244] \"lexapro\"                                                                     \n",
              "[2245] \"lexic\"                                                                       \n",
              "[2246] \"lexicon\"                                                                     \n",
              "[2247] \"lf-\"                                                                         \n",
              "[2248] \"lg\"                                                                          \n",
              "[2249] \"lh\"                                                                          \n",
              "[2250] \"li\"                                                                          \n",
              "[2251] \"lia\"                                                                         \n",
              "[2252] \"licat\"                                                                       \n",
              "[2253] \"lid\"                                                                         \n",
              "[2254] \"lie\"                                                                         \n",
              "[2255] \"life\"                                                                        \n",
              "[2256] \"lifer\"                                                                       \n",
              "[2257] \"light\"                                                                       \n",
              "[2258] \"lightweight\"                                                                 \n",
              "[2259] \"lignant\"                                                                     \n",
              "[2260] \"like\"                                                                        \n",
              "[2261] \"lill\"                                                                        \n",
              "[2262] \"limb-shak\"                                                                   \n",
              "[2263] \"limi-\"                                                                       \n",
              "[2264] \"lin\"                                                                         \n",
              "[2265] \"linguist\"                                                                    \n",
              "[2266] \"lion\"                                                                        \n",
              "[2267] \"lis\"                                                                         \n",
              "[2268] \"lish\"                                                                        \n",
              "[2269] \"lisp\"                                                                        \n",
              "[2270] \"lit\"                                                                         \n",
              "[2271] \"litera-\"                                                                     \n",
              "[2272] \"literaci\"                                                                    \n",
              "[2273] \"literature-\"                                                                 \n",
              "[2274] \"literature-bas\"                                                              \n",
              "[2275] \"literature-support\"                                                          \n",
              "[2276] \"littl\"                                                                       \n",
              "[2277] \"liv\"                                                                         \n",
              "[2278] \"liv-\"                                                                        \n",
              "[2279] \"live\"                                                                        \n",
              "[2280] \"liver\"                                                                       \n",
              "[2281] \"liz\"                                                                         \n",
              "[2282] \"lk\"                                                                          \n",
              "[2283] \"ll\"                                                                          \n",
              "[2284] \"ll-\"                                                                         \n",
              "[2285] \"lla\"                                                                         \n",
              "[2286] \"lle\"                                                                         \n",
              "[2287] \"lli\"                                                                         \n",
              "[2288] \"lls\"                                                                         \n",
              "[2289] \"llu\"                                                                         \n",
              "[2290] \"ln\"                                                                          \n",
              "[2291] \"lo\"                                                                          \n",
              "[2292] \"load\"                                                                        \n",
              "[2293] \"local\"                                                                       \n",
              "[2294] \"locat\"                                                                       \n",
              "[2295] \"log\"                                                                         \n",
              "[2296] \"logic-bas\"                                                                   \n",
              "[2297] \"logics-\"                                                                     \n",
              "[2298] \"logics-bas\"                                                                  \n",
              "[2299] \"lohmann\"                                                                     \n",
              "[2300] \"loinc\"                                                                       \n",
              "[2301] \"loir\"                                                                        \n",
              "[2302] \"long-short\"                                                                  \n",
              "[2303] \"long-term\"                                                                   \n",
              "[2304] \"longer\"                                                                      \n",
              "[2305] \"longev\"                                                                      \n",
              "[2306] \"look\"                                                                        \n",
              "[2307] \"loop\"                                                                        \n",
              "[2308] \"loos\"                                                                        \n",
              "[2309] \"lose\"                                                                        \n",
              "[2310] \"loss\"                                                                        \n",
              "[2311] \"lost\"                                                                        \n",
              "[2312] \"lot\"                                                                         \n",
              "[2313] \"lov\"                                                                         \n",
              "[2314] \"low\"                                                                         \n",
              "[2315] \"low-health\"                                                                  \n",
              "[2316] \"lowercas\"                                                                    \n",
              "[2317] \"lowest\"                                                                      \n",
              "[2318] \"lp\"                                                                          \n",
              "[2319] \"lr\"                                                                          \n",
              "[2320] \"lri\"                                                                         \n",
              "[2321] \"ls\"                                                                          \n",
              "[2322] \"lstm\"                                                                        \n",
              "[2323] \"lstm-\"                                                                       \n",
              "[2324] \"lstm-base\"                                                                   \n",
              "[2325] \"lsw2\"                                                                        \n",
              "[2326] \"lta\"                                                                         \n",
              "[2327] \"ltc\"                                                                         \n",
              "[2328] \"lth\"                                                                         \n",
              "[2329] \"lti\"                                                                         \n",
              "[2330] \"lts\"                                                                         \n",
              "[2331] \"ltu\"                                                                         \n",
              "[2332] \"lu\"                                                                          \n",
              "[2333] \"lucia\"                                                                       \n",
              "[2334] \"luka\"                                                                        \n",
              "[2335] \"lung\"                                                                        \n",
              "[2336] \"luong\"                                                                       \n",
              "[2337] \"lv\"                                                                          \n",
              "[2338] \"lx\"                                                                          \n",
              "[2339] \"ly\"                                                                          \n",
              "[2340] \"lysi\"                                                                        \n",
              "[2341] \"m.g.kersloot@amsterdamumc.nl\"                                                \n",
              "[2342] \"ma-\"                                                                         \n",
              "[2343] \"macbookpro\"                                                                  \n",
              "[2344] \"macchiarini\"                                                                 \n",
              "[2345] \"machine-bas\"                                                                 \n",
              "[2346] \"machine-interpret\"                                                           \n",
              "[2347] \"machine-learn\"                                                               \n",
              "[2348] \"machine-process\"                                                             \n",
              "[2349] \"machine-read\"                                                                \n",
              "[2350] \"machineri\"                                                                   \n",
              "[2351] \"magnet\"                                                                      \n",
              "[2352] \"magnitud\"                                                                    \n",
              "[2353] \"maija\"                                                                       \n",
              "[2354] \"maintain\"                                                                    \n",
              "[2355] \"mainten\"                                                                     \n",
              "[2356] \"make-up\"                                                                     \n",
              "[2357] \"maker\"                                                                       \n",
              "[2358] \"makeup\"                                                                      \n",
              "[2359] \"mal-absorpt\"                                                                 \n",
              "[2360] \"malbecgrap\"                                                                  \n",
              "[2361] \"male\"                                                                        \n",
              "[2362] \"malign\"                                                                      \n",
              "[2363] \"mallet2\"                                                                     \n",
              "[2364] \"mammal\"                                                                      \n",
              "[2365] \"man\"                                                                         \n",
              "[2366] \"man-\"                                                                        \n",
              "[2367] \"manag\"                                                                       \n",
              "[2368] \"manage-\"                                                                     \n",
              "[2369] \"manc\"                                                                        \n",
              "[2370] \"manchest\"                                                                    \n",
              "[2371] \"manifest\"                                                                    \n",
              "[2372] \"manipul\"                                                                     \n",
              "[2373] \"mantic\"                                                                      \n",
              "[2374] \"manuel\"                                                                      \n",
              "[2375] \"manuel.grana@ehu.\"                                                           \n",
              "[2376] \"manufactur\"                                                                  \n",
              "[2377] \"maria\"                                                                       \n",
              "[2378] \"marineanim\"                                                                  \n",
              "[2379] \"maris\"                                                                       \n",
              "[2380] \"marisela\"                                                                    \n",
              "[2381] \"marjan\"                                                                      \n",
              "[2382] \"mark\"                                                                        \n",
              "[2383] \"markov\"                                                                      \n",
              "[2384] \"marri\"                                                                       \n",
              "[2385] \"mart\"                                                                        \n",
              "[2386] \"martijn\"                                                                     \n",
              "[2387] \"marwa\"                                                                       \n",
              "[2388] \"mass\"                                                                        \n",
              "[2389] \"massachusett\"                                                                \n",
              "[2390] \"massiv\"                                                                      \n",
              "[2391] \"master\"                                                                      \n",
              "[2392] \"mate\"                                                                        \n",
              "[2393] \"mate-\"                                                                       \n",
              "[2394] \"mater-\"                                                                      \n",
              "[2395] \"materials2\"                                                                  \n",
              "[2396] \"mathemat\"                                                                    \n",
              "[2397] \"matic\"                                                                       \n",
              "[2398] \"matinolli3\"                                                                  \n",
              "[2399] \"mation\"                                                                      \n",
              "[2400] \"matric\"                                                                      \n",
              "[2401] \"matrix\"                                                                      \n",
              "[2402] \"matur\"                                                                       \n",
              "[2403] \"max\"                                                                         \n",
              "[2404] \"maxillari\"                                                                   \n",
              "[2405] \"maxim\"                                                                       \n",
              "[2406] \"maximis\"                                                                     \n",
              "[2407] \"maximum\"                                                                     \n",
              "[2408] \"mayo\"                                                                        \n",
              "[2409] \"mayoclinic.org\"                                                              \n",
              "[2410] \"md\"                                                                          \n",
              "[2411] \"me-\"                                                                         \n",
              "[2412] \"mea-\"                                                                        \n",
              "[2413] \"meaning\"                                                                     \n",
              "[2414] \"meant\"                                                                       \n",
              "[2415] \"measur\"                                                                      \n",
              "[2416] \"mecha-\"                                                                      \n",
              "[2417] \"mechan\"                                                                      \n",
              "[2418] \"mechan-\"                                                                     \n",
              "[2419] \"mechanist\"                                                                   \n",
              "[2420] \"med_devic\"                                                                   \n",
              "[2421] \"medhelp2\"                                                                    \n",
              "[2422] \"media\"                                                                       \n",
              "[2423] \"median\"                                                                      \n",
              "[2424] \"médica\"                                                                      \n",
              "[2425] \"medical-rel\"                                                                 \n",
              "[2426] \"medicar\"                                                                     \n",
              "[2427] \"medication-rel\"                                                              \n",
              "[2428] \"medlin\"                                                                      \n",
              "[2429] \"mednlp\"                                                                      \n",
              "[2430] \"mednlp-1\"                                                                    \n",
              "[2431] \"mednlp-2\"                                                                    \n",
              "[2432] \"meet\"                                                                        \n",
              "[2433] \"meet-\"                                                                       \n",
              "[2434] \"mei\"                                                                         \n",
              "[2435] \"meineke1,2\"                                                                  \n",
              "[2436] \"melissa\"                                                                     \n",
              "[2437] \"mellitus\"                                                                    \n",
              "[2438] \"member\"                                                                      \n",
              "[2439] \"membership\"                                                                  \n",
              "[2440] \"memor\"                                                                       \n",
              "[2441] \"memori\"                                                                      \n",
              "[2442] \"men\"                                                                         \n",
              "[2443] \"men-\"                                                                        \n",
              "[2444] \"mench\"                                                                       \n",
              "[2445] \"mendelian\"                                                                   \n",
              "[2446] \"mening\"                                                                      \n",
              "[2447] \"mental\"                                                                      \n",
              "[2448] \"mentat\"                                                                      \n",
              "[2449] \"mention\"                                                                     \n",
              "[2450] \"mere\"                                                                        \n",
              "[2451] \"mereolog\"                                                                    \n",
              "[2452] \"merg\"                                                                        \n",
              "[2453] \"merit\"                                                                       \n",
              "[2454] \"mesh\"                                                                        \n",
              "[2455] \"messag\"                                                                      \n",
              "[2456] \"met\"                                                                         \n",
              "[2457] \"meta-analys\"                                                                 \n",
              "[2458] \"meta-ontolog\"                                                                \n",
              "[2459] \"metabol\"                                                                     \n",
              "[2460] \"metadata\"                                                                    \n",
              "[2461] \"metal\"                                                                       \n",
              "[2462] \"metapath\"                                                                    \n",
              "[2463] \"metapneumovirus\"                                                             \n",
              "[2464] \"metastasi\"                                                                   \n",
              "[2465] \"meth-\"                                                                       \n",
              "[2466] \"methodol-\"                                                                   \n",
              "[2467] \"metric\"                                                                      \n",
              "[2468] \"mg\"                                                                          \n",
              "[2469] \"mial\"                                                                        \n",
              "[2470] \"micro-\"                                                                      \n",
              "[2471] \"mid\"                                                                         \n",
              "[2472] \"middl\"                                                                       \n",
              "[2473] \"miguel\"                                                                      \n",
              "[2474] \"mii\"                                                                         \n",
              "[2475] \"million\"                                                                     \n",
              "[2476] \"mime-typ\"                                                                    \n",
              "[2477] \"mimic\"                                                                       \n",
              "[2478] \"min\"                                                                         \n",
              "[2479] \"min-yen\"                                                                     \n",
              "[2480] \"mind\"                                                                        \n",
              "[2481] \"mine\"                                                                        \n",
              "[2482] \"miner\"                                                                       \n",
              "[2483] \"minim\"                                                                       \n",
              "[2484] \"minimum\"                                                                     \n",
              "[2485] \"minolog\"                                                                     \n",
              "[2486] \"minus\"                                                                       \n",
              "[2487] \"minut\"                                                                       \n",
              "[2488] \"mireot\"                                                                      \n",
              "[2489] \"mironov\"                                                                     \n",
              "[2490] \"misclassif\"                                                                  \n",
              "[2491] \"miss\"                                                                        \n",
              "[2492] \"mission\"                                                                     \n",
              "[2493] \"mistak\"                                                                      \n",
              "[2494] \"misus\"                                                                       \n",
              "[2495] \"mit\"                                                                         \n",
              "[2496] \"mix\"                                                                         \n",
              "[2497] \"mixtur\"                                                                      \n",
              "[2498] \"mize\"                                                                        \n",
              "[2499] \"mizer\"                                                                       \n",
              "[2500] \"mizuki\"                                                                      \n",
              "[2501] \"mk\"                                                                          \n",
              "[2502] \"mkeet@cs.uct.ac.za\"                                                          \n",
              "[2503] \"mm\"                                                                          \n",
              "[2504] \"mod-\"                                                                        \n",
              "[2505] \"mode\"                                                                        \n",
              "[2506] \"moder\"                                                                       \n",
              "[2507] \"modif\"                                                                       \n",
              "[2508] \"modul\"                                                                       \n",
              "[2509] \"modular\"                                                                     \n",
              "[2510] \"moen\"                                                                        \n",
              "[2511] \"moen1\"                                                                       \n",
              "[2512] \"molecular\"                                                                   \n",
              "[2513] \"molewaterplein\"                                                              \n",
              "[2514] \"moment\"                                                                      \n",
              "[2515] \"mon\"                                                                         \n",
              "[2516] \"monarch\"                                                                     \n",
              "[2517] \"mondo\"                                                                       \n",
              "[2518] \"monitor\"                                                                     \n",
              "[2519] \"month\"                                                                       \n",
              "[2520] \"more-\"                                                                       \n",
              "[2521] \"morita4\"                                                                     \n",
              "[2522] \"morn\"                                                                        \n",
              "[2523] \"morphem\"                                                                     \n",
              "[2524] \"morpho-syntact\"                                                              \n",
              "[2525] \"morpholog\"                                                                   \n",
              "[2526] \"mortal\"                                                                      \n",
              "[2527] \"moscow\"                                                                      \n",
              "[2528] \"most\"                                                                        \n",
              "[2529] \"motiv\"                                                                       \n",
              "[2530] \"mouth\"                                                                       \n",
              "[2531] \"move\"                                                                        \n",
              "[2532] \"movement\"                                                                    \n",
              "[2533] \"mrconso\"                                                                     \n",
              "[2534] \"mri\"                                                                         \n",
              "[2535] \"mrrel\"                                                                       \n",
              "[2536] \"mucos\"                                                                       \n",
              "[2537] \"mujtaba\"                                                                     \n",
              "[2538] \"mula\"                                                                        \n",
              "[2539] \"mulat\"                                                                       \n",
              "[2540] \"mulligen1\"                                                                   \n",
              "[2541] \"multi\"                                                                       \n",
              "[2542] \"multi-\"                                                                      \n",
              "[2543] \"multi-caus\"                                                                  \n",
              "[2544] \"multi-class\"                                                                 \n",
              "[2545] \"multi-hot\"                                                                   \n",
              "[2546] \"multi-label\"                                                                 \n",
              "[2547] \"multiclass\"                                                                  \n",
              "[2548] \"multilingu\"                                                                  \n",
              "[2549] \"multistag\"                                                                   \n",
              "[2550] \"multisystem\"                                                                 \n",
              "[2551] \"muniti\"                                                                      \n",
              "[2552] \"muscl\"                                                                       \n",
              "[2553] \"musculoskelet\"                                                               \n",
              "[2554] \"must\"                                                                        \n",
              "[2555] \"myco-heterotroph\"                                                            \n",
              "[2556] \"myeloma\"                                                                     \n",
              "[2557] \"myelomatosi\"                                                                 \n",
              "[2558] \"myiasi\"                                                                      \n",
              "[2559] \"n\"                                                                           \n",
              "[2560] \"n-ari\"                                                                       \n",
              "[2561] \"n-th\"                                                                        \n",
              "[2562] \"n2c2\"                                                                        \n",
              "[2563] \"na\"                                                                          \n",
              "[2564] \"nacrolepsi\"                                                                  \n",
              "[2565] \"naka-ku\"                                                                     \n",
              "[2566] \"nal\"                                                                         \n",
              "[2567] \"named-ent\"                                                                   \n",
              "[2568] \"namespac\"                                                                    \n",
              "[2569] \"nanyang\"                                                                     \n",
              "[2570] \"narcolepsi\"                                                                  \n",
              "[2571] \"narra-\"                                                                      \n",
              "[2572] \"narrat\"                                                                      \n",
              "[2573] \"narrow\"                                                                      \n",
              "[2574] \"natalia\"                                                                     \n",
              "[2575] \"natalia.grabar@univ-lille.fr\"                                                \n",
              "[2576] \"nativ\"                                                                       \n",
              "[2577] \"nausea\"                                                                      \n",
              "[2578] \"navig\"                                                                       \n",
              "[2579] \"nay\"                                                                         \n",
              "[2580] \"nb\"                                                                          \n",
              "[2581] \"nc\"                                                                          \n",
              "[2582] \"ncat\"                                                                        \n",
              "[2583] \"ncbo\"                                                                        \n",
              "[2584] \"ncop\"                                                                        \n",
              "[2585] \"nd\"                                                                          \n",
              "[2586] \"ndep\"                                                                        \n",
              "[2587] \"ne\"                                                                          \n",
              "[2588] \"near\"                                                                        \n",
              "[2589] \"neat\"                                                                        \n",
              "[2590] \"necessari\"                                                                   \n",
              "[2591] \"necessarili\"                                                                 \n",
              "[2592] \"necessit\"                                                                    \n",
              "[2593] \"need\"                                                                        \n",
              "[2594] \"needl\"                                                                       \n",
              "[2595] \"neer\"                                                                        \n",
              "[2596] \"nega-\"                                                                       \n",
              "[2597] \"negat\"                                                                       \n",
              "[2598] \"neither\"                                                                     \n",
              "[2599] \"nent\"                                                                        \n",
              "[2600] \"neo-\"                                                                        \n",
              "[2601] \"neo4j\"                                                                       \n",
              "[2602] \"neoplasia\"                                                                   \n",
              "[2603] \"neoplasm\"                                                                    \n",
              "[2604] \"ner\"                                                                         \n",
              "[2605] \"ner-\"                                                                        \n",
              "[2606] \"nerv\"                                                                        \n",
              "[2607] \"nervous\"                                                                     \n",
              "[2608] \"ness\"                                                                        \n",
              "[2609] \"net-\"                                                                        \n",
              "[2610] \"netherland\"                                                                  \n",
              "[2611] \"network-bas\"                                                                 \n",
              "[2612] \"neu-\"                                                                        \n",
              "[2613] \"neural\"                                                                      \n",
              "[2614] \"neurodegen\"                                                                  \n",
              "[2615] \"neurodegener\"                                                                \n",
              "[2616] \"neurognathostomiasi\"                                                         \n",
              "[2617] \"neurolog\"                                                                    \n",
              "[2618] \"neuron\"                                                                      \n",
              "[2619] \"neuronopathi\"                                                                \n",
              "[2620] \"neurosci\"                                                                    \n",
              "[2621] \"névéol\"                                                                      \n",
              "[2622] \"nevertheless\"                                                                \n",
              "[2623] \"newli\"                                                                       \n",
              "[2624] \"newspap\"                                                                     \n",
              "[2625] \"ng\"                                                                          \n",
              "[2626] \"nguyen\"                                                                      \n",
              "[2627] \"nguyen1\"                                                                     \n",
              "[2628] \"ni\"                                                                          \n",
              "[2629] \"nical\"                                                                       \n",
              "[2630] \"nicat\"                                                                       \n",
              "[2631] \"nific\"                                                                       \n",
              "[2632] \"night\"                                                                       \n",
              "[2633] \"nightmar\"                                                                    \n",
              "[2634] \"nih\"                                                                         \n",
              "[2635] \"nih-fund\"                                                                    \n",
              "[2636] \"ning\"                                                                        \n",
              "[2637] \"niqu\"                                                                        \n",
              "[2638] \"nism\"                                                                        \n",
              "[2639] \"niti\"                                                                        \n",
              "[2640] \"nition\"                                                                      \n",
              "[2641] \"nizat\"                                                                       \n",
              "[2642] \"nj\"                                                                          \n",
              "[2643] \"nk\"                                                                          \n",
              "[2644] \"nlm\"                                                                         \n",
              "[2645] \"nlp\"                                                                         \n",
              "[2646] \"nlp-relat\"                                                                   \n",
              "[2647] \"nltk\"                                                                        \n",
              "[2648] \"nnnnnnnnnnnnnn\"                                                              \n",
              "[2649] \"nnnnnynnnnnnnnnnnn\"                                                          \n",
              "[2650] \"node\"                                                                        \n",
              "[2651] \"noel\"                                                                        \n",
              "[2652] \"nolog\"                                                                       \n",
              "[2653] \"noman\"                                                                       \n",
              "[2654] \"noman-editor\"                                                                \n",
              "[2655] \"nomerg\"                                                                      \n",
              "[2656] \"nomic\"                                                                       \n",
              "[2657] \"non-\"                                                                        \n",
              "[2658] \"non-au-\"                                                                     \n",
              "[2659] \"non-comput\"                                                                  \n",
              "[2660] \"non-diseas\"                                                                  \n",
              "[2661] \"non-exhaust\"                                                                 \n",
              "[2662] \"non-expert\"                                                                  \n",
              "[2663] \"non-extract\"                                                                 \n",
              "[2664] \"non-fund\"                                                                    \n",
              "[2665] \"non-infecti\"                                                                 \n",
              "[2666] \"non-med\"                                                                     \n",
              "[2667] \"non-monoton\"                                                                 \n",
              "[2668] \"non-neur\"                                                                    \n",
              "[2669] \"non-ontolog\"                                                                 \n",
              "[2670] \"non-profession\"                                                              \n",
              "[2671] \"non-profit\"                                                                  \n",
              "[2672] \"non-reproduc\"                                                                \n",
              "[2673] \"non-restrict\"                                                                \n",
              "[2674] \"non-trajectori\"                                                              \n",
              "[2675] \"non-x\"                                                                       \n",
              "[2676] \"nonaca-\"                                                                     \n",
              "[2677] \"none\"                                                                        \n",
              "[2678] \"nonethe-\"                                                                    \n",
              "[2679] \"nonetheless\"                                                                 \n",
              "[2680] \"noon\"                                                                        \n",
              "[2681] \"norm\"                                                                        \n",
              "[2682] \"nose\"                                                                        \n",
              "[2683] \"nosi\"                                                                        \n",
              "[2684] \"nosql\"                                                                       \n",
              "[2685] \"notabl\"                                                                      \n",
              "[2686] \"notat\"                                                                       \n",
              "[2687] \"note\"                                                                        \n",
              "[2688] \"noteworthi\"                                                                  \n",
              "[2689] \"notic\"                                                                       \n",
              "[2690] \"notif\"                                                                       \n",
              "[2691] \"notion\"                                                                      \n",
              "[2692] \"nottingham\"                                                                  \n",
              "[2693] \"noun\"                                                                        \n",
              "[2694] \"novel\"                                                                       \n",
              "[2695] \"novelti\"                                                                     \n",
              "[2696] \"novemb\"                                                                      \n",
              "[2697] \"novic\"                                                                       \n",
              "[2698] \"nowa-\"                                                                       \n",
              "[2699] \"ns\"                                                                          \n",
              "[2700] \"nsip\"                                                                        \n",
              "[2701] \"nt\"                                                                          \n",
              "[2702] \"nu-\"                                                                         \n",
              "[2703] \"nuanc\"                                                                       \n",
              "[2704] \"nui\"                                                                         \n",
              "[2705] \"num-\"                                                                        \n",
              "[2706] \"numer\"                                                                       \n",
              "[2707] \"nurs\"                                                                        \n",
              "[2708] \"nurs-\"                                                                       \n",
              "[2709] \"nurtur\"                                                                      \n",
              "[2710] \"ny\"                                                                          \n",
              "[2711] \"nz\"                                                                          \n",
              "[2712] \"o\"                                                                           \n",
              "[2713] \"o-\"                                                                          \n",
              "[2714] \"ob\"                                                                          \n",
              "[2715] \"ob-\"                                                                         \n",
              "[2716] \"obi\"                                                                         \n",
              "[2717] \"objec-\"                                                                      \n",
              "[2718] \"obo\"                                                                         \n",
              "[2719] \"obser-\"                                                                      \n",
              "[2720] \"observa-\"                                                                    \n",
              "[2721] \"obstetr\"                                                                     \n",
              "[2722] \"obviat\"                                                                      \n",
              "[2723] \"obvious\"                                                                     \n",
              "[2724] \"oc\"                                                                          \n",
              "[2725] \"oc-\"                                                                         \n",
              "[2726] \"occupi\"                                                                      \n",
              "[2727] \"occur\"                                                                       \n",
              "[2728] \"occurr\"                                                                      \n",
              "[2729] \"od\"                                                                          \n",
              "[2730] \"odolog\"                                                                      \n",
              "[2731] \"odontogen\"                                                                   \n",
              "[2732] \"odp\"                                                                         \n",
              "[2733] \"oe\"                                                                          \n",
              "[2734] \"of-vocabulari\"                                                               \n",
              "[2735] \"off-put\"                                                                     \n",
              "[2736] \"offer\"                                                                       \n",
              "[2737] \"offic\"                                                                       \n",
              "[2738] \"ofmultipl\"                                                                   \n",
              "[2739] \"oftentim\"                                                                    \n",
              "[2740] \"og\"                                                                          \n",
              "[2741] \"ogi\"                                                                         \n",
              "[2742] \"ogm\"                                                                         \n",
              "[2743] \"oh\"                                                                          \n",
              "[2744] \"ohd\"                                                                         \n",
              "[2745] \"ohd-\"                                                                        \n",
              "[2746] \"oi\"                                                                          \n",
              "[2747] \"oj\"                                                                          \n",
              "[2748] \"okumura3\"                                                                    \n",
              "[2749] \"ol\"                                                                          \n",
              "[2750] \"older\"                                                                       \n",
              "[2751] \"oliv\"                                                                        \n",
              "[2752] \"olog\"                                                                        \n",
              "[2753] \"om\"                                                                          \n",
              "[2754] \"omic\"                                                                        \n",
              "[2755] \"omim\"                                                                        \n",
              "[2756] \"omiss\"                                                                       \n",
              "[2757] \"omit\"                                                                        \n",
              "[2758] \"omnivor\"                                                                     \n",
              "[2759] \"omrs\"                                                                        \n",
              "[2760] \"oncolog\"                                                                     \n",
              "[2761] \"ond\"                                                                         \n",
              "[2762] \"ondari\"                                                                      \n",
              "[2763] \"one\"                                                                         \n",
              "[2764] \"one-fourth\"                                                                  \n",
              "[2765] \"ongo\"                                                                        \n",
              "[2766] \"onlin\"                                                                       \n",
              "[2767] \"onmed\"                                                                       \n",
              "[2768] \"onomi\"                                                                       \n",
              "[2769] \"onset\"                                                                       \n",
              "[2770] \"onto\"                                                                        \n",
              "[2771] \"onto-\"                                                                       \n",
              "[2772] \"ontofox\"                                                                     \n",
              "[2773] \"ontofox2\"                                                                    \n",
              "[2774] \"ontol-\"                                                                      \n",
              "[2775] \"ontolo-\"                                                                     \n",
              "[2776] \"ontologi-\"                                                                   \n",
              "[2777] \"ontologist\"                                                                  \n",
              "[2778] \"ontology-bas\"                                                                \n",
              "[2779] \"onym\"                                                                        \n",
              "[2780] \"oo\"                                                                          \n",
              "[2781] \"oop\"                                                                         \n",
              "[2782] \"op\"                                                                          \n",
              "[2783] \"ope\"                                                                         \n",
              "[2784] \"open-end\"                                                                    \n",
              "[2785] \"open-sourc\"                                                                  \n",
              "[2786] \"openllet\"                                                                    \n",
              "[2787] \"opera-\"                                                                      \n",
              "[2788] \"opinion\"                                                                     \n",
              "[2789] \"opment\"                                                                      \n",
              "[2790] \"opportun\"                                                                    \n",
              "[2791] \"opposit\"                                                                     \n",
              "[2792] \"opti-\"                                                                       \n",
              "[2793] \"optic\"                                                                       \n",
              "[2794] \"optimis\"                                                                     \n",
              "[2795] \"optimiza-\"                                                                   \n",
              "[2796] \"or-\"                                                                         \n",
              "[2797] \"oral\"                                                                        \n",
              "[2798] \"oral-health-and-disease-ontolog\"                                             \n",
              "[2799] \"oralles\"                                                                     \n",
              "[2800] \"orat\"                                                                        \n",
              "[2801] \"orcid\"                                                                       \n",
              "[2802] \"ordr\"                                                                        \n",
              "[2803] \"orga-\"                                                                       \n",
              "[2804] \"organis\"                                                                     \n",
              "[2805] \"organiz\"                                                                     \n",
              "[2806] \"origi-\"                                                                      \n",
              "[2807] \"oropharyng\"                                                                  \n",
              "[2808] \"orpha\"                                                                       \n",
              "[2809] \"orphan\"                                                                      \n",
              "[2810] \"orphanet\"                                                                    \n",
              "[2811] \"ortholog\"                                                                    \n",
              "[2812] \"os\"                                                                          \n",
              "[2813] \"osmot\"                                                                       \n",
              "[2814] \"ostosi\"                                                                      \n",
              "[2815] \"ot\"                                                                          \n",
              "[2816] \"other\"                                                                       \n",
              "[2817] \"ou\"                                                                          \n",
              "[2818] \"ous\"                                                                         \n",
              "[2819] \"ousli\"                                                                       \n",
              "[2820] \"out-\"                                                                        \n",
              "[2821] \"outer\"                                                                       \n",
              "[2822] \"outlin\"                                                                      \n",
              "[2823] \"outlook\"                                                                     \n",
              "[2824] \"outofmemoryerror\"                                                            \n",
              "[2825] \"outpati\"                                                                     \n",
              "[2826] \"outperform\"                                                                  \n",
              "[2827] \"outsid\"                                                                      \n",
              "[2828] \"outspoken\"                                                                   \n",
              "[2829] \"ov\"                                                                          \n",
              "[2830] \"ovari\"                                                                       \n",
              "[2831] \"ovarian\"                                                                     \n",
              "[2832] \"overarch\"                                                                    \n",
              "[2833] \"overcom\"                                                                     \n",
              "[2834] \"overhead\"                                                                    \n",
              "[2835] \"overlap\"                                                                     \n",
              "[2836] \"overnight\"                                                                   \n",
              "[2837] \"overview\"                                                                    \n",
              "[2838] \"ow\"                                                                          \n",
              "[2839] \"owl\"                                                                         \n",
              "[2840] \"owl-is\"                                                                      \n",
              "[2841] \"owl:equivalentclass\"                                                         \n",
              "[2842] \"owl:equivalentproperti\"                                                      \n",
              "[2843] \"owl:samea\"                                                                   \n",
              "[2844] \"owl:th\"                                                                      \n",
              "[2845] \"owl1\"                                                                        \n",
              "[2846] \"owldl\"                                                                       \n",
              "[2847] \"oya\"                                                                         \n",
              "[2848] \"p-valu\"                                                                      \n",
              "[2849] \"p.m\"                                                                         \n",
              "[2850] \"p1\"                                                                          \n",
              "[2851] \"p2\"                                                                          \n",
              "[2852] \"pa\"                                                                          \n",
              "[2853] \"pa-\"                                                                         \n",
              "[2854] \"pace\"                                                                        \n",
              "[2855] \"packag\"                                                                      \n",
              "[2856] \"paid\"                                                                        \n",
              "[2857] \"pain\"                                                                        \n",
              "[2858] \"pair\"                                                                        \n",
              "[2859] \"pairing-\"                                                                    \n",
              "[2860] \"pallidum\"                                                                    \n",
              "[2861] \"pancrea\"                                                                     \n",
              "[2862] \"pancreat\"                                                                    \n",
              "[2863] \"pandem\"                                                                      \n",
              "[2864] \"pant\"                                                                        \n",
              "[2865] \"paper\"                                                                       \n",
              "[2866] \"par-\"                                                                        \n",
              "[2867] \"para-\"                                                                       \n",
              "[2868] \"paragraph\"                                                                   \n",
              "[2869] \"paragraph-level\"                                                             \n",
              "[2870] \"paragraph-to-paragraph\"                                                      \n",
              "[2871] \"paragraphmerg\"                                                               \n",
              "[2872] \"paralysi\"                                                                    \n",
              "[2873] \"parame-\"                                                                     \n",
              "[2874] \"paramet\"                                                                     \n",
              "[2875] \"parameteris\"                                                                 \n",
              "[2876] \"parasit\"                                                                     \n",
              "[2877] \"parenchym\"                                                                   \n",
              "[2878] \"parent\"                                                                      \n",
              "[2879] \"parentofrobert\"                                                              \n",
              "[2880] \"paresi\"                                                                      \n",
              "[2881] \"pariser2\"                                                                    \n",
              "[2882] \"pars\"                                                                        \n",
              "[2883] \"part-of-\"                                                                    \n",
              "[2884] \"part-of-speech\"                                                              \n",
              "[2885] \"parthood\"                                                                    \n",
              "[2886] \"partial\"                                                                     \n",
              "[2887] \"partic-\"                                                                     \n",
              "[2888] \"partici-\"                                                                    \n",
              "[2889] \"participat-\"                                                                 \n",
              "[2890] \"pass\"                                                                        \n",
              "[2891] \"passag\"                                                                      \n",
              "[2892] \"past\"                                                                        \n",
              "[2893] \"pat\"                                                                         \n",
              "[2894] \"pat-\"                                                                        \n",
              "[2895] \"patel1,2\"                                                                    \n",
              "[2896] \"patent\"                                                                      \n",
              "[2897] \"patentometr\"                                                                 \n",
              "[2898] \"path\"                                                                        \n",
              "[2899] \"patho-\"                                                                      \n",
              "[2900] \"pathobiolog\"                                                                 \n",
              "[2901] \"pathogenesi\"                                                                 \n",
              "[2902] \"patholog\"                                                                    \n",
              "[2903] \"pathological_num\"                                                            \n",
              "[2904] \"pathophysiolog\"                                                              \n",
              "[2905] \"pathway\"                                                                     \n",
              "[2906] \"patient-report\"                                                              \n",
              "[2907] \"patricio\"                                                                    \n",
              "[2908] \"pattern-bas\"                                                                 \n",
              "[2909] \"pattern-match\"                                                               \n",
              "[2910] \"paxil\"                                                                       \n",
              "[2911] \"payload\"                                                                     \n",
              "[2912] \"pc\"                                                                          \n",
              "[2913] \"pca\"                                                                         \n",
              "[2914] \"pd\"                                                                          \n",
              "[2915] \"pdomain\"                                                                     \n",
              "[2916] \"pe\"                                                                          \n",
              "[2917] \"pé\"                                                                          \n",
              "[2918] \"pear-\"                                                                       \n",
              "[2919] \"pearson\"                                                                     \n",
              "[2920] \"pediatr\"                                                                     \n",
              "[2921] \"pedro\"                                                                       \n",
              "[2922] \"peel\"                                                                        \n",
              "[2923] \"peer\"                                                                        \n",
              "[2924] \"pelizaeus-merzbach\"                                                          \n",
              "[2925] \"pellet\"                                                                      \n",
              "[2926] \"peltonen3\"                                                                   \n",
              "[2927] \"penal\"                                                                       \n",
              "[2928] \"penalti\"                                                                     \n",
              "[2929] \"pend\"                                                                        \n",
              "[2930] \"penguin\"                                                                     \n",
              "[2931] \"peopl\"                                                                       \n",
              "[2932] \"peptic\"                                                                      \n",
              "[2933] \"per\"                                                                         \n",
              "[2934] \"per-\"                                                                        \n",
              "[2935] \"perceiv\"                                                                     \n",
              "[2936] \"percentag\"                                                                   \n",
              "[2937] \"percept\"                                                                     \n",
              "[2938] \"perfect\"                                                                     \n",
              "[2939] \"perfor-\"                                                                     \n",
              "[2940] \"perhap\"                                                                      \n",
              "[2941] \"pericard\"                                                                    \n",
              "[2942] \"perio\"                                                                       \n",
              "[2943] \"period\"                                                                      \n",
              "[2944] \"perman\"                                                                      \n",
              "[2945] \"perpetu\"                                                                     \n",
              "[2946] \"personnel\"                                                                   \n",
              "[2947] \"perspect\"                                                                    \n",
              "[2948] \"persuad\"                                                                     \n",
              "[2949] \"pert\"                                                                        \n",
              "[2950] \"pertain\"                                                                     \n",
              "[2951] \"ph\"                                                                          \n",
              "[2952] \"phan\"                                                                        \n",
              "[2953] \"pharmaceut\"                                                                  \n",
              "[2954] \"pharmacotherapi\"                                                             \n",
              "[2955] \"pharmacovigil\"                                                               \n",
              "[2956] \"phase\"                                                                       \n",
              "[2957] \"phe-\"                                                                        \n",
              "[2958] \"pheno-\"                                                                      \n",
              "[2959] \"phenoman\"                                                                    \n",
              "[2960] \"phenomena\"                                                                   \n",
              "[2961] \"phenomenon\"                                                                  \n",
              "[2962] \"phenotyp\"                                                                    \n",
              "[2963] \"phenotype-\"                                                                  \n",
              "[2964] \"phenotype-specif\"                                                            \n",
              "[2965] \"phep\"                                                                        \n",
              "[2966] \"pheso\"                                                                       \n",
              "[2967] \"phi\"                                                                         \n",
              "[2968] \"philosoph\"                                                                   \n",
              "[2969] \"phone\"                                                                       \n",
              "[2970] \"phone_numb\"                                                                  \n",
              "[2971] \"photopatholog\"                                                               \n",
              "[2972] \"php\"                                                                         \n",
              "[2973] \"phrase\"                                                                      \n",
              "[2974] \"pht\"                                                                         \n",
              "[2975] \"phys-\"                                                                       \n",
              "[2976] \"physic\"                                                                      \n",
              "[2977] \"physician\"                                                                   \n",
              "[2978] \"physio-\"                                                                     \n",
              "[2979] \"physiolog\"                                                                   \n",
              "[2980] \"pi\"                                                                          \n",
              "[2981] \"pillar\"                                                                      \n",
              "[2982] \"pin\"                                                                         \n",
              "[2983] \"pineal\"                                                                      \n",
              "[2984] \"pineocytoma\"                                                                 \n",
              "[2985] \"pinpoint\"                                                                    \n",
              "[2986] \"pipelin\"                                                                     \n",
              "[2987] \"pital\"                                                                       \n",
              "[2988] \"pitfal\"                                                                      \n",
              "[2989] \"pituitari\"                                                                   \n",
              "[2990] \"pizza\"                                                                       \n",
              "[2991] \"pl\"                                                                          \n",
              "[2992] \"place\"                                                                       \n",
              "[2993] \"plain\"                                                                       \n",
              "[2994] \"plan\"                                                                        \n",
              "[2995] \"plan-\"                                                                       \n",
              "[2996] \"plane\"                                                                       \n",
              "[2997] \"plant\"                                                                       \n",
              "[2998] \"plantat\"                                                                     \n",
              "[2999] \"plaqu\"                                                                       \n",
              "[3000] \"plari\"                                                                       \n",
              "[3001] \"plasm\"                                                                       \n",
              "[3002] \"plasma\"                                                                      \n",
              "[3003] \"plat-\"                                                                       \n",
              "[3004] \"platform\"                                                                    \n",
              "[3005] \"plausibl\"                                                                    \n",
              "[3006] \"play\"                                                                        \n",
              "[3007] \"player\"                                                                      \n",
              "[3008] \"ple\"                                                                         \n",
              "[3009] \"pleas\"                                                                       \n",
              "[3010] \"ples\"                                                                        \n",
              "[3011] \"plethora\"                                                                    \n",
              "[3012] \"pli\"                                                                         \n",
              "[3013] \"plish\"                                                                       \n",
              "[3014] \"plot\"                                                                        \n",
              "[3015] \"pm\"                                                                          \n",
              "[3016] \"pmc\"                                                                         \n",
              "[3017] \"po\"                                                                          \n",
              "[3018] \"po-\"                                                                         \n",
              "[3019] \"pointer\"                                                                     \n",
              "[3020] \"poison\"                                                                      \n",
              "[3021] \"polici\"                                                                      \n",
              "[3022] \"policymak\"                                                                   \n",
              "[3023] \"polit\"                                                                       \n",
              "[3024] \"polycyst\"                                                                    \n",
              "[3025] \"ponent\"                                                                      \n",
              "[3026] \"pool\"                                                                        \n",
              "[3027] \"poor\"                                                                        \n",
              "[3028] \"popul\"                                                                       \n",
              "[3029] \"popular\"                                                                     \n",
              "[3030] \"pora\"                                                                        \n",
              "[3031] \"poral\"                                                                       \n",
              "[3032] \"port\"                                                                        \n",
              "[3033] \"portabl\"                                                                     \n",
              "[3034] \"portal\"                                                                      \n",
              "[3035] \"portal1\"                                                                     \n",
              "[3036] \"portanc\"                                                                     \n",
              "[3037] \"portant\"                                                                     \n",
              "[3038] \"portion\"                                                                     \n",
              "[3039] \"portrait\"                                                                    \n",
              "[3040] \"portugues\"                                                                   \n",
              "[3041] \"pos\"                                                                         \n",
              "[3042] \"pos-\"                                                                        \n",
              "[3043] \"pos-tag\"                                                                     \n",
              "[3044] \"pose\"                                                                        \n",
              "[3045] \"posi-\"                                                                       \n",
              "[3046] \"posit\"                                                                       \n",
              "[3047] \"possess\"                                                                     \n",
              "[3048] \"post\"                                                                        \n",
              "[3049] \"post-\"                                                                       \n",
              "[3050] \"post-process\"                                                                \n",
              "[3051] \"post-thread\"                                                                 \n",
              "[3052] \"poten-\"                                                                      \n",
              "[3053] \"potenti\"                                                                     \n",
              "[3054] \"pow-\"                                                                        \n",
              "[3055] \"power\"                                                                       \n",
              "[3056] \"pp\"                                                                          \n",
              "[3057] \"pr\"                                                                          \n",
              "[3058] \"prac-\"                                                                       \n",
              "[3059] \"practi-\"                                                                     \n",
              "[3060] \"practition\"                                                                  \n",
              "[3061] \"prang\"                                                                       \n",
              "[3062] \"pre-clin\"                                                                    \n",
              "[3063] \"pre-defin\"                                                                   \n",
              "[3064] \"pre-process\"                                                                 \n",
              "[3065] \"pre-train\"                                                                   \n",
              "[3066] \"preced\"                                                                      \n",
              "[3067] \"pred-\"                                                                       \n",
              "[3068] \"predat\"                                                                      \n",
              "[3069] \"predefin\"                                                                    \n",
              "[3070] \"predi-\"                                                                      \n",
              "[3071] \"prediabet\"                                                                   \n",
              "[3072] \"predic\"                                                                      \n",
              "[3073] \"predicate-object\"                                                            \n",
              "[3074] \"predictedmal-absorpt\"                                                        \n",
              "[3075] \"prediction:neat\"                                                             \n",
              "[3076] \"prefer\"                                                                      \n",
              "[3077] \"prefix\"                                                                      \n",
              "[3078] \"preliminari\"                                                                 \n",
              "[3079] \"prepar\"                                                                      \n",
              "[3080] \"prepar-\"                                                                     \n",
              "[3081] \"preprocess\"                                                                  \n",
              "[3082] \"prescrib\"                                                                    \n",
              "[3083] \"prescript\"                                                                   \n",
              "[3084] \"presenc\"                                                                     \n",
              "[3085] \"preserv\"                                                                     \n",
              "[3086] \"press\"                                                                       \n",
              "[3087] \"pressur\"                                                                     \n",
              "[3088] \"presum-\"                                                                     \n",
              "[3089] \"pretrain\"                                                                    \n",
              "[3090] \"preval\"                                                                      \n",
              "[3091] \"prevent\"                                                                     \n",
              "[3092] \"previ-\"                                                                      \n",
              "[3093] \"prim-\"                                                                       \n",
              "[3094] \"primari\"                                                                     \n",
              "[3095] \"primarili\"                                                                   \n",
              "[3096] \"princi-\"                                                                     \n",
              "[3097] \"princip\"                                                                     \n",
              "[3098] \"print\"                                                                       \n",
              "[3099] \"print-\"                                                                      \n",
              "[3100] \"prior\"                                                                       \n",
              "[3101] \"priori\"                                                                      \n",
              "[3102] \"priorit\"                                                                     \n",
              "[3103] \"prisma\"                                                                      \n",
              "[3104] \"privaci\"                                                                     \n",
              "[3105] \"privacy-sensit\"                                                              \n",
              "[3106] \"privat\"                                                                      \n",
              "[3107] \"proach\"                                                                      \n",
              "[3108] \"prob-\"                                                                       \n",
              "[3109] \"probabilist\"                                                                 \n",
              "[3110] \"probabl\"                                                                     \n",
              "[3111] \"probe\"                                                                       \n",
              "[3112] \"problemat\"                                                                   \n",
              "[3113] \"proc\"                                                                        \n",
              "[3114] \"proc-\"                                                                       \n",
              "[3115] \"procedur\"                                                                    \n",
              "[3116] \"proceed\"                                                                     \n",
              "[3117] \"proces-\"                                                                     \n",
              "[3118] \"procur\"                                                                      \n",
              "[3119] \"produc\"                                                                      \n",
              "[3120] \"product\"                                                                     \n",
              "[3121] \"profess\"                                                                     \n",
              "[3122] \"profession\"                                                                  \n",
              "[3123] \"professorinhciorai\"                                                          \n",
              "[3124] \"profil\"                                                                      \n",
              "[3125] \"programmat\"                                                                  \n",
              "[3126] \"progress\"                                                                    \n",
              "[3127] \"project\"                                                                     \n",
              "[3128] \"prolactin\"                                                                   \n",
              "[3129] \"prolif\"                                                                      \n",
              "[3130] \"prolifer\"                                                                    \n",
              "[3131] \"promi-\"                                                                      \n",
              "[3132] \"promin\"                                                                      \n",
              "[3133] \"promis\"                                                                      \n",
              "[3134] \"promot\"                                                                      \n",
              "[3135] \"proof\"                                                                       \n",
              "[3136] \"proof-of-concept\"                                                            \n",
              "[3137] \"prop-\"                                                                       \n",
              "[3138] \"propag\"                                                                      \n",
              "[3139] \"proper\"                                                                      \n",
              "[3140] \"proper-\"                                                                     \n",
              "[3141] \"properti\"                                                                    \n",
              "[3142] \"proport\"                                                                     \n",
              "[3143] \"pros-\"                                                                       \n",
              "[3144] \"prospect\"                                                                    \n",
              "[3145] \"prosthesi\"                                                                   \n",
              "[3146] \"prosthet\"                                                                    \n",
              "[3147] \"protect\"                                                                     \n",
              "[3148] \"protégé\"                                                                     \n",
              "[3149] \"protein\"                                                                     \n",
              "[3150] \"protein-protein\"                                                             \n",
              "[3151] \"protocol\"                                                                    \n",
              "[3152] \"prototyp\"                                                                    \n",
              "[3153] \"prove\"                                                                       \n",
              "[3154] \"proven\"                                                                      \n",
              "[3155] \"provinc\"                                                                     \n",
              "[3156] \"proxi\"                                                                       \n",
              "[3157] \"proxim-\"                                                                     \n",
              "[3158] \"ps\"                                                                          \n",
              "[3159] \"psycholog\"                                                                   \n",
              "[3160] \"psychosi\"                                                                    \n",
              "[3161] \"pt\"                                                                          \n",
              "[3162] \"pu\"                                                                          \n",
              "[3163] \"pub-\"                                                                        \n",
              "[3164] \"publica-\"                                                                    \n",
              "[3165] \"pubm\"                                                                        \n",
              "[3166] \"pulmonolog\"                                                                  \n",
              "[3167] \"pulp\"                                                                        \n",
              "[3168] \"pun\"                                                                         \n",
              "[3169] \"punctuat\"                                                                    \n",
              "[3170] \"pur-\"                                                                        \n",
              "[3171] \"pure\"                                                                        \n",
              "[3172] \"purl\"                                                                        \n",
              "[3173] \"purpose.w\"                                                                   \n",
              "[3174] \"pursu\"                                                                       \n",
              "[3175] \"pus\"                                                                         \n",
              "[3176] \"put\"                                                                         \n",
              "[3177] \"putat\"                                                                       \n",
              "[3178] \"puter\"                                                                       \n",
              "[3179] \"putten1\"                                                                     \n",
              "[3180] \"q\"                                                                           \n",
              "[3181] \"q1\"                                                                          \n",
              "[3182] \"q2\"                                                                          \n",
              "[3183] \"q3\"                                                                          \n",
              "[3184] \"qian\"                                                                        \n",
              "[3185] \"qian.zhu@nih.gov\"                                                            \n",
              "[3186] \"qt\"                                                                          \n",
              "[3187] \"qu\"                                                                          \n",
              "[3188] \"qual-\"                                                                       \n",
              "[3189] \"qualifi\"                                                                     \n",
              "[3190] \"qualit\"                                                                      \n",
              "[3191] \"quan-\"                                                                       \n",
              "[3192] \"quantifi\"                                                                    \n",
              "[3193] \"quantit\"                                                                     \n",
              "[3194] \"quantita-\"                                                                   \n",
              "[3195] \"quantiti\"                                                                    \n",
              "[3196] \"quenc\"                                                                       \n",
              "[3197] \"quenci\"                                                                      \n",
              "[3198] \"quentli\"                                                                     \n",
              "[3199] \"quest\"                                                                       \n",
              "[3200] \"question\"                                                                    \n",
              "[3201] \"questionnair\"                                                                \n",
              "[3202] \"quibbl\"                                                                      \n",
              "[3203] \"quick\"                                                                       \n",
              "[3204] \"quirk\"                                                                       \n",
              "[3205] \"quit\"                                                                        \n",
              "[3206] \"r-\"                                                                          \n",
              "[3207] \"ra\"                                                                          \n",
              "[3208] \"ra-\"                                                                         \n",
              "[3209] \"radiat\"                                                                      \n",
              "[3210] \"radic\"                                                                       \n",
              "[3211] \"radiolog\"                                                                    \n",
              "[3212] \"radioulnar\"                                                                  \n",
              "[3213] \"rais\"                                                                        \n",
              "[3214] \"ral\"                                                                         \n",
              "[3215] \"random\"                                                                      \n",
              "[3216] \"ranger\"                                                                      \n",
              "[3217] \"rank\"                                                                        \n",
              "[3218] \"rapid\"                                                                       \n",
              "[3219] \"rare\"                                                                        \n",
              "[3220] \"rash\"                                                                        \n",
              "[3221] \"rashmi\"                                                                      \n",
              "[3222] \"rat-\"                                                                        \n",
              "[3223] \"rate\"                                                                        \n",
              "[3224] \"rather\"                                                                      \n",
              "[3225] \"ratio\"                                                                       \n",
              "[3226] \"ration\"                                                                      \n",
              "[3227] \"rational\"                                                                    \n",
              "[3228] \"ray_num\"                                                                     \n",
              "[3229] \"rayyan\"                                                                      \n",
              "[3230] \"rb\"                                                                          \n",
              "[3231] \"rc\"                                                                          \n",
              "[3232] \"rcop\"                                                                        \n",
              "[3233] \"rd\"                                                                          \n",
              "[3234] \"rd-connect\"                                                                  \n",
              "[3235] \"rdep\"                                                                        \n",
              "[3236] \"rdf\"                                                                         \n",
              "[3237] \"rdf:class\"                                                                   \n",
              "[3238] \"rdf:properti\"                                                                \n",
              "[3239] \"rdf:type\"                                                                    \n",
              "[3240] \"rdfd2\"                                                                       \n",
              "[3241] \"rdfs\"                                                                        \n",
              "[3242] \"rdfs:class\"                                                                  \n",
              "[3243] \"rdfs:domain\"                                                                 \n",
              "[3244] \"rdfs:rang\"                                                                   \n",
              "[3245] \"rdfs:subclassof\"                                                             \n",
              "[3246] \"rdfs:subpropertyof\"                                                          \n",
              "[3247] \"rdfs11\"                                                                      \n",
              "[3248] \"rdfs2\"                                                                       \n",
              "[3249] \"rdfs3\"                                                                       \n",
              "[3250] \"rdfs5\"                                                                       \n",
              "[3251] \"rdfs7\"                                                                       \n",
              "[3252] \"rdfs9\"                                                                       \n",
              "[3253] \"re\"                                                                          \n",
              "[3254] \"re-\"                                                                         \n",
              "[3255] \"re-identif\"                                                                  \n",
              "[3256] \"re-includ\"                                                                   \n",
              "[3257] \"re-train\"                                                                    \n",
              "[3258] \"re-us\"                                                                       \n",
              "[3259] \"rea-\"                                                                        \n",
              "[3260] \"reach\"                                                                       \n",
              "[3261] \"reaction\"                                                                    \n",
              "[3262] \"read\"                                                                        \n",
              "[3263] \"readabl\"                                                                     \n",
              "[3264] \"reader\"                                                                      \n",
              "[3265] \"readili\"                                                                     \n",
              "[3266] \"real\"                                                                        \n",
              "[3267] \"real-\"                                                                       \n",
              "[3268] \"real-tim\"                                                                    \n",
              "[3269] \"realism\"                                                                     \n",
              "[3270] \"realism-bas\"                                                                 \n",
              "[3271] \"realist\"                                                                     \n",
              "[3272] \"realiti\"                                                                     \n",
              "[3273] \"realiz\"                                                                      \n",
              "[3274] \"realli\"                                                                      \n",
              "[3275] \"reasonably-pow\"                                                              \n",
              "[3276] \"recal\"                                                                       \n",
              "[3277] \"recalcul\"                                                                    \n",
              "[3278] \"receivedmor\"                                                                 \n",
              "[3279] \"recog-\"                                                                      \n",
              "[3280] \"recogn\"                                                                      \n",
              "[3281] \"recognit\"                                                                    \n",
              "[3282] \"recommend\"                                                                   \n",
              "[3283] \"recommenda-\"                                                                 \n",
              "[3284] \"recruit\"                                                                     \n",
              "[3285] \"rect\"                                                                        \n",
              "[3286] \"recur\"                                                                       \n",
              "[3287] \"recurr\"                                                                      \n",
              "[3288] \"red\"                                                                         \n",
              "[3289] \"reduc\"                                                                       \n",
              "[3290] \"reduc-\"                                                                      \n",
              "[3291] \"reduct\"                                                                      \n",
              "[3292] \"redwin\"                                                                      \n",
              "[3293] \"ref-\"                                                                        \n",
              "[3294] \"refer-\"                                                                      \n",
              "[3295] \"referenc\"                                                                    \n",
              "[3296] \"referenc-\"                                                                   \n",
              "[3297] \"refin\"                                                                       \n",
              "[3298] \"reflect\"                                                                     \n",
              "[3299] \"reformul\"                                                                    \n",
              "[3300] \"refrain\"                                                                     \n",
              "[3301] \"regard\"                                                                      \n",
              "[3302] \"regard-\"                                                                     \n",
              "[3303] \"regen\"                                                                       \n",
              "[3304] \"regenstrief\"                                                                 \n",
              "[3305] \"regim\"                                                                       \n",
              "[3306] \"region\"                                                                      \n",
              "[3307] \"region-bas\"                                                                  \n",
              "[3308] \"regist\"                                                                      \n",
              "[3309] \"registri\"                                                                    \n",
              "[3310] \"regular\"                                                                     \n",
              "[3311] \"reific\"                                                                      \n",
              "[3312] \"rein\"                                                                        \n",
              "[3313] \"reintro-\"                                                                    \n",
              "[3314] \"reject\"                                                                      \n",
              "[3315] \"rela-\"                                                                       \n",
              "[3316] \"relation-\"                                                                   \n",
              "[3317] \"rele-\"                                                                       \n",
              "[3318] \"releas\"                                                                      \n",
              "[3319] \"reli\"                                                                        \n",
              "[3320] \"reliabl\"                                                                     \n",
              "[3321] \"reliablemethod\"                                                              \n",
              "[3322] \"reliant\"                                                                     \n",
              "[3323] \"remaind\"                                                                     \n",
              "[3324] \"remark\"                                                                      \n",
              "[3325] \"remodel\"                                                                     \n",
              "[3326] \"remot\"                                                                       \n",
              "[3327] \"remov\"                                                                       \n",
              "[3328] \"renal-hepatic-\"                                                              \n",
              "[3329] \"renam\"                                                                       \n",
              "[3330] \"render\"                                                                      \n",
              "[3331] \"renn\"                                                                        \n",
              "[3332] \"rental\"                                                                      \n",
              "[3333] \"reoccur\"                                                                     \n",
              "[3334] \"reorganiz-\"                                                                  \n",
              "[3335] \"rep-\"                                                                        \n",
              "[3336] \"repeat\"                                                                      \n",
              "[3337] \"repetit\"                                                                     \n",
              "[3338] \"replac\"                                                                      \n",
              "[3339] \"replace-\"                                                                    \n",
              "[3340] \"replet\"                                                                      \n",
              "[3341] \"replic\"                                                                      \n",
              "[3342] \"report\"                                                                      \n",
              "[3343] \"report-\"                                                                     \n",
              "[3344] \"repositori\"                                                                  \n",
              "[3345] \"repre-\"                                                                      \n",
              "[3346] \"represen-\"                                                                   \n",
              "[3347] \"representa-\"                                                                 \n",
              "[3348] \"reproduc\"                                                                    \n",
              "[3349] \"repurpos\"                                                                    \n",
              "[3350] \"request\"                                                                     \n",
              "[3351] \"res-\"                                                                        \n",
              "[3352] \"researcherid\"                                                                \n",
              "[3353] \"resent\"                                                                      \n",
              "[3354] \"resid\"                                                                       \n",
              "[3355] \"resin\"                                                                       \n",
              "[3356] \"resl\"                                                                        \n",
              "[3357] \"resolu-\"                                                                     \n",
              "[3358] \"resolut\"                                                                     \n",
              "[3359] \"resolv\"                                                                      \n",
              "[3360] \"reson\"                                                                       \n",
              "[3361] \"resource-typ\"                                                                \n",
              "[3362] \"respond\"                                                                     \n",
              "[3363] \"respons\"                                                                     \n",
              "[3364] \"rest\"                                                                        \n",
              "[3365] \"rest-\"                                                                       \n",
              "[3366] \"rest-hook\"                                                                   \n",
              "[3367] \"restor\"                                                                      \n",
              "[3368] \"restor-\"                                                                     \n",
              "[3369] \"restora-\"                                                                    \n",
              "[3370] \"restric-\"                                                                    \n",
              "[3371] \"restructur\"                                                                  \n",
              "[3372] \"result-\"                                                                     \n",
              "[3373] \"results2\"                                                                    \n",
              "[3374] \"retain\"                                                                      \n",
              "[3375] \"retriev\"                                                                     \n",
              "[3376] \"retrospect\"                                                                  \n",
              "[3377] \"return\"                                                                      \n",
              "[3378] \"reus\"                                                                        \n",
              "[3379] \"reusabl\"                                                                     \n",
              "[3380] \"reveal\"                                                                      \n",
              "[3381] \"revers\"                                                                      \n",
              "[3382] \"revis\"                                                                       \n",
              "[3383] \"revista\"                                                                     \n",
              "[3384] \"rezaul\"                                                                      \n",
              "[3385] \"rg\"                                                                          \n",
              "[3386] \"rheumat\"                                                                     \n",
              "[3387] \"rheumatoid\"                                                                  \n",
              "[3388] \"rho\"                                                                         \n",
              "[3389] \"rhomboid\"                                                                    \n",
              "[3390] \"ri\"                                                                          \n",
              "[3391] \"ria\"                                                                         \n",
              "[3392] \"rial\"                                                                        \n",
              "[3393] \"rib\"                                                                         \n",
              "[3394] \"ric\"                                                                         \n",
              "[3395] \"rich\"                                                                        \n",
              "[3396] \"ricket\"                                                                      \n",
              "[3397] \"rie\"                                                                         \n",
              "[3398] \"rienc\"                                                                       \n",
              "[3399] \"right\"                                                                       \n",
              "[3400] \"rigid\"                                                                       \n",
              "[3401] \"rigor\"                                                                       \n",
              "[3402] \"riitta\"                                                                      \n",
              "[3403] \"rin\"                                                                         \n",
              "[3404] \"rina\"                                                                        \n",
              "[3405] \"ring\"                                                                        \n",
              "[3406] \"rio\"                                                                         \n",
              "[3407] \"ríos1\"                                                                       \n",
              "[3408] \"rious\"                                                                       \n",
              "[3409] \"ris\"                                                                         \n",
              "[3410] \"risen\"                                                                       \n",
              "[3411] \"rit\"                                                                         \n",
              "[3412] \"rithm\"                                                                       \n",
              "[3413] \"rive\"                                                                        \n",
              "[3414] \"rix\"                                                                         \n",
              "[3415] \"riz\"                                                                         \n",
              "[3416] \"rize\"                                                                        \n",
              "[3417] \"rla\"                                                                         \n",
              "[3418] \"rm\"                                                                          \n",
              "[3419] \"rn\"                                                                          \n",
              "[3420] \"ro\"                                                                          \n",
              "[3421] \"robert\"                                                                      \n",
              "[3422] \"robert.hoehndorf@kaust.edu.sa\"                                               \n",
              "[3423] \"robust\"                                                                      \n",
              "[3424] \"roc\"                                                                         \n",
              "[3425] \"rockvill\"                                                                    \n",
              "[3426] \"rodriguez-salvador\"                                                          \n",
              "[3427] \"rodríguez-salvador\"                                                          \n",
              "[3428] \"rodríguez-salvador2\"                                                         \n",
              "[3429] \"roles4\"                                                                      \n",
              "[3430] \"ronald\"                                                                      \n",
              "[3431] \"rondebosch\"                                                                  \n",
              "[3432] \"ronment\"                                                                     \n",
              "[3433] \"room\"                                                                        \n",
              "[3434] \"root\"                                                                        \n",
              "[3435] \"rot\"                                                                         \n",
              "[3436] \"rotterdam\"                                                                   \n",
              "[3437] \"round\"                                                                       \n",
              "[3438] \"routin\"                                                                      \n",
              "[3439] \"routinely-collect\"                                                           \n",
              "[3440] \"row\"                                                                         \n",
              "[3441] \"rp\"                                                                          \n",
              "[3442] \"rr\"                                                                          \n",
              "[3443] \"rs\"                                                                          \n",
              "[3444] \"rsip\"                                                                        \n",
              "[3445] \"rt\"                                                                          \n",
              "[3446] \"ru\"                                                                          \n",
              "[3447] \"rule\"                                                                        \n",
              "[3448] \"rule-bas\"                                                                    \n",
              "[3449] \"run\"                                                                         \n",
              "[3450] \"runni\"                                                                       \n",
              "[3451] \"runtim\"                                                                      \n",
              "[3452] \"russian\"                                                                     \n",
              "[3453] \"ruttenberg\"                                                                  \n",
              "[3454] \"ruttenberg11\"                                                                \n",
              "[3455] \"rv\"                                                                          \n",
              "[3456] \"rw\"                                                                          \n",
              "[3457] \"rwth\"                                                                        \n",
              "[3458] \"rx\"                                                                          \n",
              "[3459] \"ry\"                                                                          \n",
              "[3460] \"s1\"                                                                          \n",
              "[3461] \"s2\"                                                                          \n",
              "[3462] \"sa\"                                                                          \n",
              "[3463] \"sacrif\"                                                                      \n",
              "[3464] \"safe\"                                                                        \n",
              "[3465] \"said\"                                                                        \n",
              "[3466] \"sake\"                                                                        \n",
              "[3467] \"salakoski1\"                                                                  \n",
              "[3468] \"salanterä3,4\"                                                                \n",
              "[3469] \"salem\"                                                                       \n",
              "[3470] \"salli\"                                                                       \n",
              "[3471] \"samea\"                                                                       \n",
              "[3472] \"san\"                                                                         \n",
              "[3473] \"sand\"                                                                        \n",
              "[3474] \"sandwich\"                                                                    \n",
              "[3475] \"sanna\"                                                                       \n",
              "[3476] \"sara\"                                                                        \n",
              "[3477] \"sarcoma\"                                                                     \n",
              "[3478] \"satisfi\"                                                                     \n",
              "[3479] \"satoshi\"                                                                     \n",
              "[3480] \"saudi\"                                                                       \n",
              "[3481] \"sauvignon\"                                                                   \n",
              "[3482] \"save\"                                                                        \n",
              "[3483] \"savoir\"                                                                      \n",
              "[3484] \"sb\"                                                                          \n",
              "[3485] \"sc\"                                                                          \n",
              "[3486] \"scaffold-fre\"                                                                \n",
              "[3487] \"scalabl\"                                                                     \n",
              "[3488] \"scale\"                                                                       \n",
              "[3489] \"scalpel\"                                                                     \n",
              "[3490] \"scarc\"                                                                       \n",
              "[3491] \"scarlet\"                                                                     \n",
              "[3492] \"scc\"                                                                         \n",
              "[3493] \"scenario\"                                                                    \n",
              "[3494] \"schema\"                                                                      \n",
              "[3495] \"schema-\"                                                                     \n",
              "[3496] \"schema-introspect\"                                                           \n",
              "[3497] \"schema:domaininclud\"                                                         \n",
              "[3498] \"schema:rangeinclud\"                                                          \n",
              "[3499] \"schema.org\"                                                                  \n",
              "[3500] \"schemat\"                                                                     \n",
              "[3501] \"scheme\"                                                                      \n",
              "[3502] \"scheuermann\"                                                                 \n",
              "[3503] \"schleyer\"                                                                    \n",
              "[3504] \"schleyer2,10\"                                                                \n",
              "[3505] \"schober\"                                                                     \n",
              "[3506] \"school\"                                                                      \n",
              "[3507] \"schroeder\"                                                                   \n",
              "[3508] \"sci-\"                                                                        \n",
              "[3509] \"scien-\"                                                                      \n",
              "[3510] \"scientist\"                                                                   \n",
              "[3511] \"scientometr\"                                                                 \n",
              "[3512] \"scope\"                                                                       \n",
              "[3513] \"scopus\"                                                                      \n",
              "[3514] \"screen\"                                                                      \n",
              "[3515] \"script\"                                                                      \n",
              "[3516] \"sct\"                                                                         \n",
              "[3517] \"sd\"                                                                          \n",
              "[3518] \"sd1\"                                                                         \n",
              "[3519] \"sd2\"                                                                         \n",
              "[3520] \"se\"                                                                          \n",
              "[3521] \"se-\"                                                                         \n",
              "[3522] \"seamless\"                                                                    \n",
              "[3523] \"sebastián\"                                                                   \n",
              "[3524] \"sec-\"                                                                        \n",
              "[3525] \"second\"                                                                      \n",
              "[3526] \"second-best\"                                                                 \n",
              "[3527] \"secondari\"                                                                   \n",
              "[3528] \"secret\"                                                                      \n",
              "[3529] \"sector\"                                                                      \n",
              "[3530] \"secur\"                                                                       \n",
              "[3531] \"seed\"                                                                        \n",
              "[3532] \"seek\"                                                                        \n",
              "[3533] \"seem\"                                                                        \n",
              "[3534] \"seen\"                                                                        \n",
              "[3535] \"segment\"                                                                     \n",
              "[3536] \"segreg\"                                                                      \n",
              "[3537] \"seini\"                                                                       \n",
              "[3538] \"self-\"                                                                       \n",
              "[3539] \"self-supervis\"                                                               \n",
              "[3540] \"selv\"                                                                        \n",
              "[3541] \"semi-\"                                                                       \n",
              "[3542] \"semi-autom\"                                                                  \n",
              "[3543] \"semi-automat\"                                                                \n",
              "[3544] \"semi-or\"                                                                     \n",
              "[3545] \"semi-structur\"                                                               \n",
              "[3546] \"sen\"                                                                         \n",
              "[3547] \"sen-\"                                                                        \n",
              "[3548] \"senior\"                                                                      \n",
              "[3549] \"sensat\"                                                                      \n",
              "[3550] \"sensi-\"                                                                      \n",
              "[3551] \"sensibl\"                                                                     \n",
              "[3552] \"sensit\"                                                                      \n",
              "[3553] \"sentat\"                                                                      \n",
              "[3554] \"sentenc\"                                                                     \n",
              "[3555] \"sentence-\"                                                                   \n",
              "[3556] \"sentence-level\"                                                              \n",
              "[3557] \"separ-\"                                                                      \n",
              "[3558] \"sequenc\"                                                                     \n",
              "[3559] \"sequenti\"                                                                    \n",
              "[3560] \"ser-\"                                                                        \n",
              "[3561] \"seri\"                                                                        \n",
              "[3562] \"serial_numb\"                                                                 \n",
              "[3563] \"serious\"                                                                     \n",
              "[3564] \"sertion\"                                                                     \n",
              "[3565] \"serum\"                                                                       \n",
              "[3566] \"serv\"                                                                        \n",
              "[3567] \"server\"                                                                      \n",
              "[3568] \"servic\"                                                                      \n",
              "[3569] \"ses\"                                                                         \n",
              "[3570] \"set-\"                                                                        \n",
              "[3571] \"set.whil\"                                                                    \n",
              "[3572] \"seven\"                                                                       \n",
              "[3573] \"seventy-seven\"                                                               \n",
              "[3574] \"sever\"                                                                       \n",
              "[3575] \"sewebmeda-2018\"                                                              \n",
              "[3576] \"sex\"                                                                         \n",
              "[3577] \"sexual\"                                                                      \n",
              "[3578] \"sg\"                                                                          \n",
              "[3579] \"sh\"                                                                          \n",
              "[3580] \"shacl\"                                                                       \n",
              "[3581] \"shape\"                                                                       \n",
              "[3582] \"sharp\"                                                                       \n",
              "[3583] \"sharpli\"                                                                     \n",
              "[3584] \"sheep\"                                                                       \n",
              "[3585] \"shex\"                                                                        \n",
              "[3586] \"shift\"                                                                       \n",
              "[3587] \"ship\"                                                                        \n",
              "[3588] \"shirt\"                                                                       \n",
              "[3589] \"shizudai\"                                                                    \n",
              "[3590] \"shizuoka\"                                                                    \n",
              "[3591] \"sho\"                                                                         \n",
              "[3592] \"shock-\"                                                                      \n",
              "[3593] \"shoin\"                                                                       \n",
              "[3594] \"short-\"                                                                      \n",
              "[3595] \"short-termmemori\"                                                            \n",
              "[3596] \"shortcom\"                                                                    \n",
              "[3597] \"shortest\"                                                                    \n",
              "[3598] \"si\"                                                                          \n",
              "[3599] \"sibl\"                                                                        \n",
              "[3600] \"sic\"                                                                         \n",
              "[3601] \"sid2\"                                                                        \n",
              "[3602] \"side\"                                                                        \n",
              "[3603] \"sider\"                                                                       \n",
              "[3604] \"sific\"                                                                       \n",
              "[3605] \"sig-\"                                                                        \n",
              "[3606] \"sigmoid\"                                                                     \n",
              "[3607] \"signal\"                                                                      \n",
              "[3608] \"signifi-\"                                                                    \n",
              "[3609] \"silhouett\"                                                                   \n",
              "[3610] \"sim-\"                                                                        \n",
              "[3611] \"simi-\"                                                                       \n",
              "[3612] \"similar-\"                                                                    \n",
              "[3613] \"simpl\"                                                                       \n",
              "[3614] \"simpler\"                                                                     \n",
              "[3615] \"simplest\"                                                                    \n",
              "[3616] \"simplex\"                                                                     \n",
              "[3617] \"simpli\"                                                                      \n",
              "[3618] \"simplic\"                                                                     \n",
              "[3619] \"simplif\"                                                                     \n",
              "[3620] \"simplifi\"                                                                    \n",
              "[3621] \"simplifica-\"                                                                 \n",
              "[3622] \"simul\"                                                                       \n",
              "[3623] \"singapor\"                                                                    \n",
              "[3624] \"single_phenotyp\"                                                             \n",
              "[3625] \"single-cent\"                                                                 \n",
              "[3626] \"singular\"                                                                    \n",
              "[3627] \"sinus\"                                                                       \n",
              "[3628] \"sion\"                                                                        \n",
              "[3629] \"sirabl\"                                                                      \n",
              "[3630] \"sit-\"                                                                        \n",
              "[3631] \"site\"                                                                        \n",
              "[3632] \"situ-\"                                                                       \n",
              "[3633] \"situa-\"                                                                      \n",
              "[3634] \"situat\"                                                                      \n",
              "[3635] \"sive\"                                                                        \n",
              "[3636] \"six\"                                                                         \n",
              "[3637] \"sixteen\"                                                                     \n",
              "[3638] \"sizabl\"                                                                      \n",
              "[3639] \"sjogren\"                                                                     \n",
              "[3640] \"sk\"                                                                          \n",
              "[3641] \"skin\"                                                                        \n",
              "[3642] \"skip-gram\"                                                                   \n",
              "[3643] \"skull\"                                                                       \n",
              "[3644] \"slash\"                                                                       \n",
              "[3645] \"sleep\"                                                                       \n",
              "[3646] \"sleepi\"                                                                      \n",
              "[3647] \"slight\"                                                                      \n",
              "[3648] \"slot\"                                                                        \n",
              "[3649] \"slur\"                                                                        \n",
              "[3650] \"smallest\"                                                                    \n",
              "[3651] \"smart\"                                                                       \n",
              "[3652] \"smit\"                                                                        \n",
              "[3653] \"smith\"                                                                       \n",
              "[3654] \"smoke\"                                                                       \n",
              "[3655] \"sn\"                                                                          \n",
              "[3656] \"sneez\"                                                                       \n",
              "[3657] \"snodent\"                                                                     \n",
              "[3658] \"snome\"                                                                       \n",
              "[3659] \"snome-\"                                                                      \n",
              "[3660] \"snomed-ct\"                                                                   \n",
              "[3661] \"snomedct\"                                                                    \n",
              "[3662] \"so-cal\"                                                                      \n",
              "[3663] \"social\"                                                                      \n",
              "[3664] \"sociat\"                                                                      \n",
              "[3665] \"societ\"                                                                      \n",
              "[3666] \"socio-econom\"                                                                \n",
              "[3667] \"sofa\"                                                                        \n",
              "[3668] \"soft\"                                                                        \n",
              "[3669] \"softwar\"                                                                     \n",
              "[3670] \"software-support\"                                                            \n",
              "[3671] \"sole\"                                                                        \n",
              "[3672] \"solo\"                                                                        \n",
              "[3673] \"solut\"                                                                       \n",
              "[3674] \"solv\"                                                                        \n",
              "[3675] \"somehow\"                                                                     \n",
              "[3676] \"someth\"                                                                      \n",
              "[3677] \"sometim\"                                                                     \n",
              "[3678] \"somewhat\"                                                                    \n",
              "[3679] \"son\"                                                                         \n",
              "[3680] \"sone\"                                                                        \n",
              "[3681] \"song7\"                                                                       \n",
              "[3682] \"sor\"                                                                         \n",
              "[3683] \"sort\"                                                                        \n",
              "[3684] \"sos\"                                                                         \n",
              "[3685] \"sought\"                                                                      \n",
              "[3686] \"south\"                                                                       \n",
              "[3687] \"southall1\"                                                                   \n",
              "[3688] \"sp\"                                                                          \n",
              "[3689] \"space\"                                                                       \n",
              "[3690] \"spain\"                                                                       \n",
              "[3691] \"span\"                                                                        \n",
              "[3692] \"spanish\"                                                                     \n",
              "[3693] \"sparql\"                                                                      \n",
              "[3694] \"spars\"                                                                       \n",
              "[3695] \"spatial\"                                                                     \n",
              "[3696] \"spe-\"                                                                        \n",
              "[3697] \"speak\"                                                                       \n",
              "[3698] \"spearman\"                                                                    \n",
              "[3699] \"speci\"                                                                       \n",
              "[3700] \"speci-\"                                                                      \n",
              "[3701] \"special\"                                                                     \n",
              "[3702] \"specialist\"                                                                  \n",
              "[3703] \"specialti\"                                                                   \n",
              "[3704] \"specif-\"                                                                     \n",
              "[3705] \"specifi-\"                                                                    \n",
              "[3706] \"specifica-\"                                                                  \n",
              "[3707] \"spectiv\"                                                                     \n",
              "[3708] \"spectrum\"                                                                    \n",
              "[3709] \"specul\"                                                                      \n",
              "[3710] \"speech\"                                                                      \n",
              "[3711] \"speed\"                                                                       \n",
              "[3712] \"spell\"                                                                       \n",
              "[3713] \"spend\"                                                                       \n",
              "[3714] \"spent\"                                                                       \n",
              "[3715] \"spin\"                                                                        \n",
              "[3716] \"splenium\"                                                                    \n",
              "[3717] \"split\"                                                                       \n",
              "[3718] \"spond\"                                                                       \n",
              "[3719] \"spreadsheet\"                                                                 \n",
              "[3720] \"spring\"                                                                      \n",
              "[3721] \"spss\"                                                                        \n",
              "[3722] \"sql\"                                                                         \n",
              "[3723] \"sri\"                                                                         \n",
              "[3724] \"sroiq\"                                                                       \n",
              "[3725] \"ss\"                                                                          \n",
              "[3726] \"st\"                                                                          \n",
              "[3727] \"sta-\"                                                                        \n",
              "[3728] \"staff\"                                                                       \n",
              "[3729] \"stage\"                                                                       \n",
              "[3730] \"stakehold\"                                                                   \n",
              "[3731] \"stan-\"                                                                       \n",
              "[3732] \"stanc\"                                                                       \n",
              "[3733] \"stand\"                                                                       \n",
              "[3734] \"stand-\"                                                                      \n",
              "[3735] \"standalon\"                                                                   \n",
              "[3736] \"standardis\"                                                                  \n",
              "[3737] \"stard\"                                                                       \n",
              "[3738] \"state-of-the-\"                                                               \n",
              "[3739] \"state-of-the-art\"                                                            \n",
              "[3740] \"statement\"                                                                   \n",
              "[3741] \"statement-bas\"                                                               \n",
              "[3742] \"static\"                                                                      \n",
              "[3743] \"statis-\"                                                                     \n",
              "[3744] \"statisti-\"                                                                   \n",
              "[3745] \"statistically-deriv\"                                                         \n",
              "[3746] \"statistician\"                                                                \n",
              "[3747] \"status\"                                                                      \n",
              "[3748] \"stay\"                                                                        \n",
              "[3749] \"stefan\"                                                                      \n",
              "[3750] \"stenzhorn3,8\"                                                                \n",
              "[3751] \"stereotyp\"                                                                   \n",
              "[3752] \"stewart1,2\"                                                                  \n",
              "[3753] \"stipul\"                                                                      \n",
              "[3754] \"stl\"                                                                         \n",
              "[3755] \"stockholm\"                                                                   \n",
              "[3756] \"stomach\"                                                                     \n",
              "[3757] \"stop\"                                                                        \n",
              "[3758] \"storag\"                                                                      \n",
              "[3759] \"straight\"                                                                    \n",
              "[3760] \"straight-forward\"                                                            \n",
              "[3761] \"straightfor-\"                                                                \n",
              "[3762] \"straightforward\"                                                             \n",
              "[3763] \"strat-\"                                                                      \n",
              "[3764] \"strate\"                                                                      \n",
              "[3765] \"strategi\"                                                                    \n",
              "[3766] \"stratifi\"                                                                    \n",
              "[3767] \"street\"                                                                      \n",
              "[3768] \"strength\"                                                                    \n",
              "[3769] \"strengthen\"                                                                  \n",
              "[3770] \"streptococci\"                                                                \n",
              "[3771] \"streptococcus\"                                                               \n",
              "[3772] \"strict\"                                                                      \n",
              "[3773] \"string\"                                                                      \n",
              "[3774] \"strive\"                                                                      \n",
              "[3775] \"strobe\"                                                                      \n",
              "[3776] \"strong\"                                                                      \n",
              "[3777] \"strongest\"                                                                   \n",
              "[3778] \"struc-\"                                                                      \n",
              "[3779] \"struct\"                                                                      \n",
              "[3780] \"struction\"                                                                   \n",
              "[3781] \"struggl\"                                                                     \n",
              "[3782] \"strument\"                                                                    \n",
              "[3783] \"stu-\"                                                                        \n",
              "[3784] \"stud-\"                                                                       \n",
              "[3785] \"student\"                                                                     \n",
              "[3786] \"studi\"                                                                       \n",
              "[3787] \"stuffi\"                                                                      \n",
              "[3788] \"style\"                                                                       \n",
              "[3789] \"stylist\"                                                                     \n",
              "[3790] \"su\"                                                                          \n",
              "[3791] \"sub-\"                                                                        \n",
              "[3792] \"sub-optim\"                                                                   \n",
              "[3793] \"sub-properti\"                                                                \n",
              "[3794] \"sub-system\"                                                                  \n",
              "[3795] \"sub-word\"                                                                    \n",
              "[3796] \"subject-\"                                                                    \n",
              "[3797] \"subject-predicate-object\"                                                    \n",
              "[3798] \"submiss\"                                                                     \n",
              "[3799] \"suboptim\"                                                                    \n",
              "[3800] \"subscrip-\"                                                                   \n",
              "[3801] \"subscript\"                                                                   \n",
              "[3802] \"subse-\"                                                                      \n",
              "[3803] \"subsect\"                                                                     \n",
              "[3804] \"subsequ\"                                                                     \n",
              "[3805] \"substan-\"                                                                    \n",
              "[3806] \"substanc\"                                                                    \n",
              "[3807] \"substanti\"                                                                   \n",
              "[3808] \"subsum\"                                                                      \n",
              "[3809] \"subsumpt\"                                                                    \n",
              "[3810] \"subsystem\"                                                                   \n",
              "[3811] \"subtop\"                                                                      \n",
              "[3812] \"subtract\"                                                                    \n",
              "[3813] \"subtyp\"                                                                      \n",
              "[3814] \"success\"                                                                     \n",
              "[3815] \"succinct\"                                                                    \n",
              "[3816] \"suchweight\"                                                                  \n",
              "[3817] \"suffer\"                                                                      \n",
              "[3818] \"suffi-\"                                                                      \n",
              "[3819] \"suffici\"                                                                     \n",
              "[3820] \"suffix\"                                                                      \n",
              "[3821] \"sug-\"                                                                        \n",
              "[3822] \"sugiyama\"                                                                    \n",
              "[3823] \"suhonen3,4\"                                                                  \n",
              "[3824] \"suit\"                                                                        \n",
              "[3825] \"sum\"                                                                         \n",
              "[3826] \"sum-\"                                                                        \n",
              "[3827] \"sumithra\"                                                                    \n",
              "[3828] \"summar\"                                                                      \n",
              "[3829] \"summari\"                                                                     \n",
              "[3830] \"summaris\"                                                                    \n",
              "[3831] \"summat\"                                                                      \n",
              "[3832] \"summer\"                                                                      \n",
              "[3833] \"sun\"                                                                         \n",
              "[3834] \"sup-\"                                                                        \n",
              "[3835] \"super-class\"                                                                 \n",
              "[3836] \"superclass\"                                                                  \n",
              "[3837] \"superior\"                                                                    \n",
              "[3838] \"supervis\"                                                                    \n",
              "[3839] \"supplement\"                                                                  \n",
              "[3840] \"supplementari\"                                                               \n",
              "[3841] \"suppli\"                                                                      \n",
              "[3842] \"support-\"                                                                    \n",
              "[3843] \"suppos\"                                                                      \n",
              "[3844] \"sur-\"                                                                        \n",
              "[3845] \"sure\"                                                                        \n",
              "[3846] \"surfac\"                                                                      \n",
              "[3847] \"surgeri\"                                                                     \n",
              "[3848] \"surgic\"                                                                      \n",
              "[3849] \"surnam\"                                                                      \n",
              "[3850] \"surround\"                                                                    \n",
              "[3851] \"survey\"                                                                      \n",
              "[3852] \"surviv\"                                                                      \n",
              "[3853] \"suscept\"                                                                     \n",
              "[3854] \"susceptibil-\"                                                                \n",
              "[3855] \"sussex\"                                                                      \n",
              "[3856] \"sustain\"                                                                     \n",
              "[3857] \"svm\"                                                                         \n",
              "[3858] \"sweat\"                                                                       \n",
              "[3859] \"swedish\"                                                                     \n",
              "[3860] \"swing\"                                                                       \n",
              "[3861] \"swirl\"                                                                       \n",
              "[3862] \"sy\"                                                                          \n",
              "[3863] \"symbol\"                                                                      \n",
              "[3864] \"symmetr\"                                                                     \n",
              "[3865] \"syn-\"                                                                        \n",
              "[3866] \"syndrom\"                                                                     \n",
              "[3867] \"synonym\"                                                                     \n",
              "[3868] \"synostosi\"                                                                   \n",
              "[3869] \"syntact\"                                                                     \n",
              "[3870] \"syphili\"                                                                     \n",
              "[3871] \"syphilit\"                                                                    \n",
              "[3872] \"sys-\"                                                                        \n",
              "[3873] \"systemat\"                                                                    \n",
              "[3874] \"systol\"                                                                      \n",
              "[3875] \"t-sne\"                                                                       \n",
              "[3876] \"t-test\"                                                                      \n",
              "[3877] \"t1\"                                                                          \n",
              "[3878] \"t2\"                                                                          \n",
              "[3879] \"t2dm\"                                                                        \n",
              "[3880] \"t2dm_case_3\"                                                                 \n",
              "[3881] \"ta\"                                                                          \n",
              "[3882] \"tablewin\"                                                                    \n",
              "[3883] \"tabular\"                                                                     \n",
              "[3884] \"tackl\"                                                                       \n",
              "[3885] \"tag\"                                                                         \n",
              "[3886] \"tail\"                                                                        \n",
              "[3887] \"tailor\"                                                                      \n",
              "[3888] \"tain\"                                                                        \n",
              "[3889] \"takashi\"                                                                     \n",
              "[3890] \"taken\"                                                                       \n",
              "[3891] \"tal\"                                                                         \n",
              "[3892] \"tanc\"                                                                        \n",
              "[3893] \"tanh\"                                                                        \n",
              "[3894] \"tant\"                                                                        \n",
              "[3895] \"tapio\"                                                                       \n",
              "[3896] \"task-appropri\"                                                               \n",
              "[3897] \"task-relev\"                                                                  \n",
              "[3898] \"task-specif\"                                                                 \n",
              "[3899] \"tast\"                                                                        \n",
              "[3900] \"tate\"                                                                        \n",
              "[3901] \"tation\"                                                                      \n",
              "[3902] \"tator\"                                                                       \n",
              "[3903] \"taught\"                                                                      \n",
              "[3904] \"tax-\"                                                                        \n",
              "[3905] \"taxonomi\"                                                                    \n",
              "[3906] \"taylorport\"                                                                  \n",
              "[3907] \"tc\"                                                                          \n",
              "[3908] \"te\"                                                                          \n",
              "[3909] \"teach\"                                                                       \n",
              "[3910] \"teach-\"                                                                      \n",
              "[3911] \"team\"                                                                        \n",
              "[3912] \"tech-\"                                                                       \n",
              "[3913] \"techniqu\"                                                                    \n",
              "[3914] \"technol-\"                                                                    \n",
              "[3915] \"technology-bas\"                                                              \n",
              "[3916] \"tect\"                                                                        \n",
              "[3917] \"tection\"                                                                     \n",
              "[3918] \"tectur\"                                                                      \n",
              "[3919] \"teeth\"                                                                       \n",
              "[3920] \"tégé\"                                                                        \n",
              "[3921] \"tegrat\"                                                                      \n",
              "[3922] \"tein\"                                                                        \n",
              "[3923] \"telephon\"                                                                    \n",
              "[3924] \"tem\"                                                                         \n",
              "[3925] \"tem-\"                                                                        \n",
              "[3926] \"temat\"                                                                       \n",
              "[3927] \"templat\"                                                                     \n",
              "[3928] \"tempor\"                                                                      \n",
              "[3929] \"temporally-connect\"                                                          \n",
              "[3930] \"tempt\"                                                                       \n",
              "[3931] \"ten\"                                                                         \n",
              "[3932] \"ten-\"                                                                        \n",
              "[3933] \"tenc\"                                                                        \n",
              "[3934] \"tend\"                                                                        \n",
              "[3935] \"tendenc\"                                                                     \n",
              "[3936] \"tensorflow\"                                                                  \n",
              "[3937] \"tent\"                                                                        \n",
              "[3938] \"tential\"                                                                     \n",
              "[3939] \"ter\"                                                                         \n",
              "[3940] \"ter-\"                                                                        \n",
              "[3941] \"tere\"                                                                        \n",
              "[3942] \"terho3,4\"                                                                    \n",
              "[3943] \"teria\"                                                                       \n",
              "[3944] \"terial\"                                                                      \n",
              "[3945] \"termi-\"                                                                      \n",
              "[3946] \"termin\"                                                                      \n",
              "[3947] \"termin-\"                                                                     \n",
              "[3948] \"termmemori\"                                                                  \n",
              "[3949] \"terms1\"                                                                      \n",
              "[3950] \"tern\"                                                                        \n",
              "[3951] \"ternari\"                                                                     \n",
              "[3952] \"tertiari\"                                                                    \n",
              "[3953] \"testicl\"                                                                     \n",
              "[3954] \"testicular\"                                                                  \n",
              "[3955] \"text-min\"                                                                    \n",
              "[3956] \"textbook\"                                                                    \n",
              "[3957] \"textual\"                                                                     \n",
              "[3958] \"th\"                                                                          \n",
              "[3959] \"thalassemia\"                                                                 \n",
              "[3960] \"thank\"                                                                       \n",
              "[3961] \"thankam\"                                                                     \n",
              "[3962] \"thedcat\"                                                                     \n",
              "[3963] \"them-\"                                                                       \n",
              "[3964] \"themanu\"                                                                     \n",
              "[3965] \"themed\"                                                                      \n",
              "[3966] \"theoret\"                                                                     \n",
              "[3967] \"theori\"                                                                      \n",
              "[3968] \"ther\"                                                                        \n",
              "[3969] \"therapeut\"                                                                   \n",
              "[3970] \"there-\"                                                                      \n",
              "[3971] \"therebi\"                                                                     \n",
              "[3972] \"thereof\"                                                                     \n",
              "[3973] \"therewith\"                                                                   \n",
              "[3974] \"therewithmeet\"                                                               \n",
              "[3975] \"thermor\"                                                                     \n",
              "[3976] \"thesaurii\"                                                                   \n",
              "[3977] \"thesaurus\"                                                                   \n",
              "[3978] \"thetic\"                                                                      \n",
              "[3979] \"theword2vec\"                                                                 \n",
              "[3980] \"thin\"                                                                        \n",
              "[3981] \"thing\"                                                                       \n",
              "[3982] \"think\"                                                                       \n",
              "[3983] \"third-parti\"                                                                 \n",
              "[3984] \"thor\"                                                                        \n",
              "[3985] \"thou-\"                                                                       \n",
              "[3986] \"though\"                                                                      \n",
              "[3987] \"thought\"                                                                     \n",
              "[3988] \"thousand\"                                                                    \n",
              "[3989] \"thread\"                                                                      \n",
              "[3990] \"thread-level\"                                                                \n",
              "[3991] \"three-level\"                                                                 \n",
              "[3992] \"three-ontolog\"                                                               \n",
              "[3993] \"threshold\"                                                                   \n",
              "[3994] \"throat\"                                                                      \n",
              "[3995] \"throughout\"                                                                  \n",
              "[3996] \"throw\"                                                                       \n",
              "[3997] \"thuwal\"                                                                      \n",
              "[3998] \"thyvalikakath2,3\"                                                            \n",
              "[3999] \"ti\"                                                                          \n",
              "[4000] \"tia\"                                                                         \n",
              "[4001] \"tial\"                                                                        \n",
              "[4002] \"tialli\"                                                                      \n",
              "[4003] \"tic\"                                                                         \n",
              "[4004] \"tical\"                                                                       \n",
              "[4005] \"tice\"                                                                        \n",
              "[4006] \"ticip\"                                                                       \n",
              "[4007] \"ticular\"                                                                     \n",
              "[4008] \"tie\"                                                                         \n",
              "[4009] \"tient\"                                                                       \n",
              "[4010] \"tient_nam\"                                                                   \n",
              "[4011] \"tif\"                                                                         \n",
              "[4012] \"tifi\"                                                                        \n",
              "[4013] \"tific\"                                                                       \n",
              "[4014] \"time-consum\"                                                                 \n",
              "[4015] \"timothi\"                                                                     \n",
              "[4016] \"tin\"                                                                         \n",
              "[4017] \"ting\"                                                                        \n",
              "[4018] \"tingl\"                                                                       \n",
              "[4019] \"tinuiti\"                                                                     \n",
              "[4020] \"tio\"                                                                         \n",
              "[4021] \"tional\"                                                                      \n",
              "[4022] \"tioner\"                                                                      \n",
              "[4023] \"tip\"                                                                         \n",
              "[4024] \"tire\"                                                                        \n",
              "[4025] \"tired\"                                                                       \n",
              "[4026] \"tis\"                                                                         \n",
              "[4027] \"tissu\"                                                                       \n",
              "[4028] \"tissue-engin\"                                                                \n",
              "[4029] \"tistic\"                                                                      \n",
              "[4030] \"tit\"                                                                         \n",
              "[4031] \"titi\"                                                                        \n",
              "[4032] \"titl\"                                                                        \n",
              "[4033] \"titus\"                                                                       \n",
              "[4034] \"tiv\"                                                                         \n",
              "[4035] \"tiviti\"                                                                      \n",
              "[4036] \"tle\"                                                                         \n",
              "[4037] \"tli\"                                                                         \n",
              "[4038] \"tlo\"                                                                         \n",
              "[4039] \"tm\"                                                                          \n",
              "[4040] \"today\"                                                                       \n",
              "[4041] \"togeth\"                                                                      \n",
              "[4042] \"token\"                                                                       \n",
              "[4043] \"token3\"                                                                      \n",
              "[4044] \"tokens1\"                                                                     \n",
              "[4045] \"tomanu\"                                                                      \n",
              "[4046] \"tomic\"                                                                       \n",
              "[4047] \"tomorrow\"                                                                    \n",
              "[4048] \"tonom\"                                                                       \n",
              "[4049] \"took\"                                                                        \n",
              "[4050] \"toolbox\"                                                                     \n",
              "[4051] \"tooth\"                                                                       \n",
              "[4052] \"tooth3\"                                                                      \n",
              "[4053] \"top\"                                                                         \n",
              "[4054] \"top-\"                                                                        \n",
              "[4055] \"top-15\"                                                                      \n",
              "[4056] \"top-cit\"                                                                     \n",
              "[4057] \"top-level\"                                                                   \n",
              "[4058] \"topicmodel\"                                                                  \n",
              "[4059] \"topograph\"                                                                   \n",
              "[4060] \"topolog\"                                                                     \n",
              "[4061] \"toralf\"                                                                      \n",
              "[4062] \"torat\"                                                                       \n",
              "[4063] \"tori\"                                                                        \n",
              "[4064] \"torniai5\"                                                                    \n",
              "[4065] \"tot\"                                                                         \n",
              "[4066] \"tourism\"                                                                     \n",
              "[4067] \"toward\"                                                                      \n",
              "[4068] \"town\"                                                                        \n",
              "[4069] \"tp\"                                                                          \n",
              "[4070] \"tr\"                                                                          \n",
              "[4071] \"tra-\"                                                                        \n",
              "[4072] \"trachea\"                                                                     \n",
              "[4073] \"tracheobronchomalacia\"                                                       \n",
              "[4074] \"track\"                                                                       \n",
              "[4075] \"tract\"                                                                       \n",
              "[4076] \"traction\"                                                                    \n",
              "[4077] \"trade-off\"                                                                   \n",
              "[4078] \"tradit\"                                                                      \n",
              "[4079] \"train-\"                                                                      \n",
              "[4080] \"trainabl\"                                                                    \n",
              "[4081] \"trait\"                                                                       \n",
              "[4082] \"trajec-\"                                                                     \n",
              "[4083] \"trajectori\"                                                                  \n",
              "[4084] \"tral\"                                                                        \n",
              "[4085] \"trans-\"                                                                      \n",
              "[4086] \"transfer\"                                                                    \n",
              "[4087] \"transform\"                                                                   \n",
              "[4088] \"transforma-\"                                                                 \n",
              "[4089] \"transient\"                                                                   \n",
              "[4090] \"transit\"                                                                     \n",
              "[4091] \"translat\"                                                                    \n",
              "[4092] \"transmiss\"                                                                   \n",
              "[4093] \"transmit\"                                                                    \n",
              "[4094] \"transplant\"                                                                  \n",
              "[4095] \"transporta-\"                                                                 \n",
              "[4096] \"trate\"                                                                       \n",
              "[4097] \"treat\"                                                                       \n",
              "[4098] \"treat-\"                                                                      \n",
              "[4099] \"tree\"                                                                        \n",
              "[4100] \"tremor\"                                                                      \n",
              "[4101] \"trend\"                                                                       \n",
              "[4102] \"treponema\"                                                                   \n",
              "[4103] \"tri\"                                                                         \n",
              "[4104] \"tri-\"                                                                        \n",
              "[4105] \"trial\"                                                                       \n",
              "[4106] \"trient\"                                                                      \n",
              "[4107] \"trigger\"                                                                     \n",
              "[4108] \"tripe\"                                                                       \n",
              "[4109] \"tripl\"                                                                       \n",
              "[4110] \"tripod\"                                                                      \n",
              "[4111] \"trivial\"                                                                     \n",
              "[4112] \"trophoblast\"                                                                 \n",
              "[4113] \"true\"                                                                        \n",
              "[4114] \"trust-\"                                                                      \n",
              "[4115] \"trustworthi\"                                                                 \n",
              "[4116] \"trustworthi-\"                                                                \n",
              "[4117] \"truth\"                                                                       \n",
              "[4118] \"ts\"                                                                          \n",
              "[4119] \"tsinghua\"                                                                    \n",
              "[4120] \"tt\"                                                                          \n",
              "[4121] \"tu\"                                                                          \n",
              "[4122] \"tubular\"                                                                     \n",
              "[4123] \"tumor\"                                                                       \n",
              "[4124] \"tune\"                                                                        \n",
              "[4125] \"tupl\"                                                                        \n",
              "[4126] \"tural\"                                                                       \n",
              "[4127] \"ture\"                                                                        \n",
              "[4128] \"turkey\"                                                                      \n",
              "[4129] \"turku\"                                                                       \n",
              "[4130] \"turn\"                                                                        \n",
              "[4131] \"tuto-\"                                                                       \n",
              "[4132] \"tutori\"                                                                      \n",
              "[4133] \"tw\"                                                                          \n",
              "[4134] \"tween\"                                                                       \n",
              "[4135] \"twenti\"                                                                      \n",
              "[4136] \"twenty-two\"                                                                  \n",
              "[4137] \"twig\"                                                                        \n",
              "[4138] \"two-digit\"                                                                   \n",
              "[4139] \"two-dimension\"                                                               \n",
              "[4140] \"two-or-one-digit-\"                                                           \n",
              "[4141] \"two-or-one-digit-numb\"                                                       \n",
              "[4142] \"two-sid\"                                                                     \n",
              "[4143] \"two-step\"                                                                    \n",
              "[4144] \"twofold\"                                                                     \n",
              "[4145] \"tx\"                                                                          \n",
              "[4146] \"ty\"                                                                          \n",
              "[4147] \"u\"                                                                           \n",
              "[4148] \"u.\"                                                                          \n",
              "[4149] \"u1\"                                                                          \n",
              "[4150] \"u2\"                                                                          \n",
              "[4151] \"ua\"                                                                          \n",
              "[4152] \"ual\"                                                                         \n",
              "[4153] \"uat\"                                                                         \n",
              "[4154] \"uation\"                                                                      \n",
              "[4155] \"uator\"                                                                       \n",
              "[4156] \"ub\"                                                                          \n",
              "[4157] \"uc\"                                                                          \n",
              "[4158] \"ucit\"                                                                        \n",
              "[4159] \"uciteli1,2\"                                                                  \n",
              "[4160] \"ud\"                                                                          \n",
              "[4161] \"ue\"                                                                          \n",
              "[4162] \"uf\"                                                                          \n",
              "[4163] \"ug\"                                                                          \n",
              "[4164] \"ui\"                                                                          \n",
              "[4165] \"uk\"                                                                          \n",
              "[4166] \"ul\"                                                                          \n",
              "[4167] \"ular\"                                                                        \n",
              "[4168] \"ulcer\"                                                                       \n",
              "[4169] \"ultim\"                                                                       \n",
              "[4170] \"ultrasonic_num\"                                                              \n",
              "[4171] \"ultrasound\"                                                                  \n",
              "[4172] \"umc\"                                                                         \n",
              "[4173] \"ument\"                                                                       \n",
              "[4174] \"uml\"                                                                         \n",
              "[4175] \"umr\"                                                                         \n",
              "[4176] \"un\"                                                                          \n",
              "[4177] \"un-\"                                                                         \n",
              "[4178] \"unabl\"                                                                       \n",
              "[4179] \"unavail\"                                                                     \n",
              "[4180] \"unavail-\"                                                                    \n",
              "[4181] \"unbias\"                                                                      \n",
              "[4182] \"uncertainti\"                                                                 \n",
              "[4183] \"unchang\"                                                                     \n",
              "[4184] \"unclear\"                                                                     \n",
              "[4185] \"uncontroversial4\"                                                            \n",
              "[4186] \"uncov\"                                                                       \n",
              "[4187] \"unde-\"                                                                       \n",
              "[4188] \"under-\"                                                                      \n",
              "[4189] \"undergo\"                                                                     \n",
              "[4190] \"underly-\"                                                                    \n",
              "[4191] \"underneath\"                                                                  \n",
              "[4192] \"undersampl\"                                                                  \n",
              "[4193] \"underspecifi\"                                                                \n",
              "[4194] \"understood\"                                                                  \n",
              "[4195] \"undertak\"                                                                    \n",
              "[4196] \"undescend\"                                                                   \n",
              "[4197] \"undesir\"                                                                     \n",
              "[4198] \"undiagnos\"                                                                   \n",
              "[4199] \"undirect\"                                                                    \n",
              "[4200] \"unfortun\"                                                                    \n",
              "[4201] \"uni-\"                                                                        \n",
              "[4202] \"unifi\"                                                                       \n",
              "[4203] \"unii\"                                                                        \n",
              "[4204] \"unintend\"                                                                    \n",
              "[4205] \"unintent\"                                                                    \n",
              "[4206] \"union\"                                                                       \n",
              "[4207] \"unionsq\"                                                                     \n",
              "[4208] \"univer-\"                                                                     \n",
              "[4209] \"unlabel\"                                                                     \n",
              "[4210] \"unnec-\"                                                                      \n",
              "[4211] \"unnecessarili\"                                                               \n",
              "[4212] \"unravel\"                                                                     \n",
              "[4213] \"unreli\"                                                                      \n",
              "[4214] \"unrestrict\"                                                                  \n",
              "[4215] \"unsatisfi\"                                                                   \n",
              "[4216] \"unseen\"                                                                      \n",
              "[4217] \"unstruc-\"                                                                    \n",
              "[4218] \"unstructur\"                                                                  \n",
              "[4219] \"unsuccess\"                                                                   \n",
              "[4220] \"unsuit\"                                                                      \n",
              "[4221] \"unsupervis\"                                                                  \n",
              "[4222] \"untransform\"                                                                 \n",
              "[4223] \"untreat\"                                                                     \n",
              "[4224] \"unus\"                                                                        \n",
              "[4225] \"unusu\"                                                                       \n",
              "[4226] \"unveil\"                                                                      \n",
              "[4227] \"up-to-d\"                                                                     \n",
              "[4228] \"upper\"                                                                       \n",
              "[4229] \"upper-\"                                                                      \n",
              "[4230] \"upper-level\"                                                                 \n",
              "[4231] \"upstream\"                                                                    \n",
              "[4232] \"upvot\"                                                                       \n",
              "[4233] \"ur\"                                                                          \n",
              "[4234] \"uri\"                                                                         \n",
              "[4235] \"urin\"                                                                        \n",
              "[4236] \"url\"                                                                         \n",
              "[4237] \"urolog\"                                                                      \n",
              "[4238] \"us\"                                                                          \n",
              "[4239] \"usa\"                                                                         \n",
              "[4240] \"usabl\"                                                                       \n",
              "[4241] \"use-cas\"                                                                     \n",
              "[4242] \"user\"                                                                        \n",
              "[4243] \"user-cred\"                                                                   \n",
              "[4244] \"user-friend\"                                                                 \n",
              "[4245] \"user-gener\"                                                                  \n",
              "[4246] \"user-report\"                                                                 \n",
              "[4247] \"useword2vec\"                                                                 \n",
              "[4248] \"ut\"                                                                          \n",
              "[4249] \"utilis\"                                                                      \n",
              "[4250] \"utiv\"                                                                        \n",
              "[4251] \"utmost\"                                                                      \n",
              "[4252] \"v\"                                                                           \n",
              "[4253] \"v1\"                                                                          \n",
              "[4254] \"v2u\"                                                                         \n",
              "[4255] \"v4\"                                                                          \n",
              "[4256] \"va\"                                                                          \n",
              "[4257] \"vai\"                                                                         \n",
              "[4258] \"vaihj\"                                                                       \n",
              "[4259] \"vaihl\"                                                                       \n",
              "[4260] \"vali-\"                                                                       \n",
              "[4261] \"valid-\"                                                                      \n",
              "[4262] \"valta4\"                                                                      \n",
              "[4263] \"valuabl\"                                                                     \n",
              "[4264] \"value-quant\"                                                                 \n",
              "[4265] \"valuepartit\"                                                                 \n",
              "[4266] \"van\"                                                                         \n",
              "[4267] \"van-hoang\"                                                                   \n",
              "[4268] \"vant\"                                                                        \n",
              "[4269] \"vapour\"                                                                      \n",
              "[4270] \"vari\"                                                                        \n",
              "[4271] \"vari-\"                                                                       \n",
              "[4272] \"varia-\"                                                                      \n",
              "[4273] \"variabl\"                                                                     \n",
              "[4274] \"varianc\"                                                                     \n",
              "[4275] \"variant\"                                                                     \n",
              "[4276] \"varieti\"                                                                     \n",
              "[4277] \"vascular\"                                                                    \n",
              "[4278] \"vast\"                                                                        \n",
              "[4279] \"vation\"                                                                      \n",
              "[4280] \"vð\"                                                                          \n",
              "[4281] \"ve\"                                                                          \n",
              "[4282] \"vec-\"                                                                        \n",
              "[4283] \"vector\"                                                                      \n",
              "[4284] \"vector-spac\"                                                                 \n",
              "[4285] \"veer\"                                                                        \n",
              "[4286] \"velop\"                                                                       \n",
              "[4287] \"velupillai\"                                                                  \n",
              "[4288] \"velupillai1\"                                                                 \n",
              "[4289] \"vendor\"                                                                      \n",
              "[4290] \"veneer\"                                                                      \n",
              "[4291] \"vention\"                                                                     \n",
              "[4292] \"verifi\"                                                                      \n",
              "[4293] \"vers\"                                                                        \n",
              "[4294] \"versa\"                                                                       \n",
              "[4295] \"versatil\"                                                                    \n",
              "[4296] \"versiti\"                                                                     \n",
              "[4297] \"versus\"                                                                      \n",
              "[4298] \"vertex\"                                                                      \n",
              "[4299] \"vertic\"                                                                      \n",
              "[4300] \"vertigo\"                                                                     \n",
              "[4301] \"vesilinnanti\"                                                                \n",
              "[4302] \"vessel\"                                                                      \n",
              "[4303] \"vhnguyen@u.nus.edu\"                                                          \n",
              "[4304] \"vi\"                                                                          \n",
              "[4305] \"viani1\"                                                                      \n",
              "[4306] \"viation\"                                                                     \n",
              "[4307] \"vice\"                                                                        \n",
              "[4308] \"vide\"                                                                        \n",
              "[4309] \"video\"                                                                       \n",
              "[4310] \"vider\"                                                                       \n",
              "[4311] \"vidual\"                                                                      \n",
              "[4312] \"view-\"                                                                       \n",
              "[4313] \"viewer\"                                                                      \n",
              "[4314] \"viewpoint\"                                                                   \n",
              "[4315] \"vii\"                                                                         \n",
              "[4316] \"vincent\"                                                                     \n",
              "[4317] \"violat\"                                                                      \n",
              "[4318] \"vious\"                                                                       \n",
              "[4319] \"viral\"                                                                       \n",
              "[4320] \"virtual\"                                                                     \n",
              "[4321] \"virus\"                                                                       \n",
              "[4322] \"vis-\"                                                                        \n",
              "[4323] \"visibl\"                                                                      \n",
              "[4324] \"vision\"                                                                      \n",
              "[4325] \"visual\"                                                                      \n",
              "[4326] \"visual-\"                                                                     \n",
              "[4327] \"visualis\"                                                                    \n",
              "[4328] \"vital\"                                                                       \n",
              "[4329] \"vitamin\"                                                                     \n",
              "[4330] \"vival\"                                                                       \n",
              "[4331] \"vlietstra\"                                                                   \n",
              "[4332] \"vlietstra1\"                                                                  \n",
              "[4333] \"vo\"                                                                          \n",
              "[4334] \"vocabu-\"                                                                     \n",
              "[4335] \"vocabulari\"                                                                  \n",
              "[4336] \"voic\"                                                                        \n",
              "[4337] \"volum\"                                                                       \n",
              "[4338] \"volv\"                                                                        \n",
              "[4339] \"vomit\"                                                                       \n",
              "[4340] \"vos\"                                                                         \n",
              "[4341] \"vos-\"                                                                        \n",
              "[4342] \"vos1,2\"                                                                      \n",
              "[4343] \"vosview\"                                                                     \n",
              "[4344] \"vous\"                                                                        \n",
              "[4345] \"vp\"                                                                          \n",
              "[4346] \"vpi\"                                                                         \n",
              "[4347] \"vpui\"                                                                        \n",
              "[4348] \"vs\"                                                                          \n",
              "[4349] \"vt\"                                                                          \n",
              "[4350] \"vu\"                                                                          \n",
              "[4351] \"vui\"                                                                         \n",
              "[4352] \"vw1\"                                                                         \n",
              "[4353] \"vw2\"                                                                         \n",
              "[4354] \"vwi\"                                                                         \n",
              "[4355] \"vwj\"                                                                         \n",
              "[4356] \"vwn\"                                                                         \n",
              "[4357] \"vx\"                                                                          \n",
              "[4358] \"w\"                                                                           \n",
              "[4359] \"w.vlietstra@erasmusmc.nl\"                                                    \n",
              "[4360] \"w1\"                                                                          \n",
              "[4361] \"w2\"                                                                          \n",
              "[4362] \"w3c\"                                                                         \n",
              "[4363] \"waist\"                                                                       \n",
              "[4364] \"waist-hip\"                                                                   \n",
              "[4365] \"wait\"                                                                        \n",
              "[4366] \"waj\"                                                                         \n",
              "[4367] \"wajh\"                                                                        \n",
              "[4368] \"wajhbj\"                                                                      \n",
              "[4369] \"wajhkj\"                                                                      \n",
              "[4370] \"wake\"                                                                        \n",
              "[4371] \"wal\"                                                                         \n",
              "[4372] \"walk\"                                                                        \n",
              "[4373] \"wang\"                                                                        \n",
              "[4374] \"want\"                                                                        \n",
              "[4375] \"ward\"                                                                        \n",
              "[4376] \"warrant\"                                                                     \n",
              "[4377] \"wast\"                                                                        \n",
              "[4378] \"way2\"                                                                        \n",
              "[4379] \"wd\"                                                                          \n",
              "[4380] \"wdduncan@gmail.com\"                                                          \n",
              "[4381] \"weak\"                                                                        \n",
              "[4382] \"web-bas\"                                                                     \n",
              "[4383] \"websit\"                                                                      \n",
              "[4384] \"week\"                                                                        \n",
              "[4385] \"wei\"                                                                         \n",
              "[4386] \"weight\"                                                                      \n",
              "[4387] \"well-defin\"                                                                  \n",
              "[4388] \"well-establish\"                                                              \n",
              "[4389] \"well-form\"                                                                   \n",
              "[4390] \"well-known\"                                                                  \n",
              "[4391] \"well-suit\"                                                                   \n",
              "[4392] \"well-understood\"                                                             \n",
              "[4393] \"well-vers\"                                                                   \n",
              "[4394] \"whale\"                                                                       \n",
              "[4395] \"whatev\"                                                                      \n",
              "[4396] \"whatizit\"                                                                    \n",
              "[4397] \"whatsoev\"                                                                    \n",
              "[4398] \"wherea\"                                                                      \n",
              "[4399] \"wherebi\"                                                                     \n",
              "[4400] \"whim\"                                                                        \n",
              "[4401] \"whose\"                                                                       \n",
              "[4402] \"wider\"                                                                       \n",
              "[4403] \"wikipedia\"                                                                   \n",
              "[4404] \"wildlif\"                                                                     \n",
              "[4405] \"wilhelm\"                                                                     \n",
              "[4406] \"william\"                                                                     \n",
              "[4407] \"window\"                                                                      \n",
              "[4408] \"wine\"                                                                        \n",
              "[4409] \"wine-produc\"                                                                 \n",
              "[4410] \"wine.owl\"                                                                    \n",
              "[4411] \"wineri\"                                                                      \n",
              "[4412] \"winston\"                                                                     \n",
              "[4413] \"winter\"                                                                      \n",
              "[4414] \"wit\"                                                                         \n",
              "[4415] \"with-\"                                                                       \n",
              "[4416] \"withdraw\"                                                                    \n",
              "[4417] \"withhold\"                                                                    \n",
              "[4418] \"withmerg\"                                                                    \n",
              "[4419] \"wk\"                                                                          \n",
              "[4420] \"wn\"                                                                          \n",
              "[4421] \"wolff\"                                                                       \n",
              "[4422] \"wolff1\"                                                                      \n",
              "[4423] \"woman\"                                                                       \n",
              "[4424] \"women\"                                                                       \n",
              "[4425] \"word-\"                                                                       \n",
              "[4426] \"word-bas\"                                                                    \n",
              "[4427] \"word-embed\"                                                                  \n",
              "[4428] \"word2vec\"                                                                    \n",
              "[4429] \"workaround\"                                                                  \n",
              "[4430] \"workflow\"                                                                    \n",
              "[4431] \"workload\"                                                                    \n",
              "[4432] \"workshop\"                                                                    \n",
              "[4433] \"world\"                                                                       \n",
              "[4434] \"worldwid\"                                                                    \n",
              "[4435] \"wors\"                                                                        \n",
              "[4436] \"worth\"                                                                       \n",
              "[4437] \"worthi\"                                                                      \n",
              "[4438] \"wound\"                                                                       \n",
              "[4439] \"write\"                                                                       \n",
              "[4440] \"writer\"                                                                      \n",
              "[4441] \"wrong\"                                                                       \n",
              "[4442] \"wu\"                                                                          \n",
              "[4443] \"wui\"                                                                         \n",
              "[4444] \"wx\"                                                                          \n",
              "[4445] \"wytz\"                                                                        \n",
              "[4446] \"x\"                                                                           \n",
              "[4447] \"x-\"                                                                          \n",
              "[4448] \"x2\"                                                                          \n",
              "[4449] \"x9\"                                                                          \n",
              "[4450] \"xanax\"                                                                       \n",
              "[4451] \"xi\"                                                                          \n",
              "[4452] \"xiang\"                                                                       \n",
              "[4453] \"xie\"                                                                         \n",
              "[4454] \"xml\"                                                                         \n",
              "[4455] \"xn\"                                                                          \n",
              "[4456] \"xori\"                                                                        \n",
              "[4457] \"xpi\"                                                                         \n",
              "[4458] \"xsd:datetim\"                                                                 \n",
              "[4459] \"xt\"                                                                          \n",
              "[4460] \"xu\"                                                                          \n",
              "[4461] \"xx\"                                                                          \n",
              "[4462] \"y\"                                                                           \n",
              "[4463] \"ya\"                                                                          \n",
              "[4464] \"ye\"                                                                          \n",
              "[4465] \"year\"                                                                        \n",
              "[4466] \"yellow\"                                                                      \n",
              "[4467] \"yes\"                                                                         \n",
              "[4468] \"yesterday\"                                                                   \n",
              "[4469] \"yield\"                                                                       \n",
              "[4470] \"yin1\"                                                                        \n",
              "[4471] \"yo\"                                                                          \n",
              "[4472] \"yoshinobu\"                                                                   \n",
              "[4473] \"youden\"                                                                      \n",
              "[4474] \"young\"                                                                       \n",
              "[4475] \"yp\"                                                                          \n",
              "[4476] \"ys\"                                                                          \n",
              "[4477] \"ysis\"                                                                        \n",
              "[4478] \"yt\"                                                                          \n",
              "[4479] \"yw\"                                                                          \n",
              "[4480] \"yy\"                                                                          \n",
              "[4481] \"yyyi\"                                                                        \n",
              "[4482] \"z\"                                                                           \n",
              "[4483] \"za\"                                                                          \n",
              "[4484] \"ze\"                                                                          \n",
              "[4485] \"zero\"                                                                        \n",
              "[4486] \"zh\"                                                                          \n",
              "[4487] \"zhang\"                                                                       \n",
              "[4488] \"zhu\"                                                                         \n",
              "[4489] \"zhu1\"                                                                        \n",
              "[4490] \"zimmermann3\"                                                                 \n",
              "[4491] \"zip_cod\"                                                                     \n",
              "[4492] \"zipcod\"                                                                      \n",
              "[4493] \"zollinger-\"                                                                  \n",
              "[4494] \"zoloft\"                                                                      \n",
              "[4495] \"zoo\"                                                                         \n",
              "[4496] \"zooanim\"                                                                     \n",
              "[4497] \"zoonot\"                                                                      \n",
              "[4498] \"zweigenbaum\"                                                                 \n",
              "[4499] \"þ\"                                                                           \n",
              "\n",
              "$docs.removed\n",
              "NULL\n",
              "\n",
              "$tokens.removed\n",
              "[1] 6962\n",
              "\n",
              "$wordcounts\n",
              "   [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  2  1  1\n",
              "  [25]  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
              "  [49]  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
              "  [73]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  2  2  1  1  1\n",
              "  [97]  5  9  1  1  3  1 15  1  2  3  1  1  1  2  1 15  1  2  1  9  1  2  1  3\n",
              " [121]  6  4  1  1  1  4  1  1  1  1  1  4  1  1  5  2  3 10  1  2  6  2  7  1\n",
              " [145]  1  2 12  5  1  2 12  8  2  2  1  1  1  1  1  3  1  2  4  1  4  4  3  1\n",
              " [169]  1  1  1  1  1  4  1  1  1  1  3  1  1  1  1  1  1  1  1  1  1  1  1  2\n",
              " [193]  2  1  1  5  1  1  3  1  1  2  3  2  1  1  3  1  9  1  2  1  1  1  1 14\n",
              " [217]  2  1  1  1  1  1  4  7  1  1  1  2  2  1  1  1 11  1  1  1  1  6  1  1\n",
              " [241]  1  2  9  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  2  8  3  1  1  1\n",
              " [265]  1  2  1  1  2  1  7  2  2  1  1  1  1  1  2  2  2  8  3  1  1  5  9  1\n",
              " [289]  3  1  1  2  1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  3  1  2  1 14\n",
              " [313]  1 12  1 14  2  1  1  5  1  3  1  1  2  5  1  1  6  1  2  1  2  3  1  1\n",
              " [337]  3  2  1  1  1  1  1  1  7  3  5  1  2  1  1  1  3  7  1  2  1  7  1  1\n",
              " [361]  8  5  1  6  1  1  1  1  1  1  5  7  2  1  5  1  1  2  1  1  1 13  1  2\n",
              " [385]  2  1  3  1  1 14  2  5  1  1  6 10  1  1  1  1 15  1  7  3  2  1  1  1\n",
              " [409]  5  1  1  1  1  9  1  1  1  2  1  2  1  4  1  1  1  3 15  2  1  1  1  1\n",
              " [433]  1  1  1  1  5  1  1  1  3  1  7  1  1  1  5  1  2  1  1  8  1  1  2  1\n",
              " [457]  1  3  1  1  2  1  1  1  3  1  1  1  1  4  1  3  3  1  1  1  4  2  1  2\n",
              " [481]  1  1  3  1  1  2  1  1  1  2  1  1  1  3  1  1  1  2  1  1  2  4  1  1\n",
              " [505]  1 10  1 14  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  3  1\n",
              " [529]  1  1  2  3  2  1  1  2  1  1  4  1  2  2  1  2  1  1  1  1  1  1  1  1\n",
              " [553]  3  1  2  2  1  3  1  2  2  1  4  1  1  2  3  1  2  1  1  1  2  1  4  1\n",
              " [577]  7  1  1  1  1  1  2  1  9  1  3  2  2  1  1  1  1  1  3  1  1  2  1  6\n",
              " [601]  1  4  1  1  3  3  1  1  1  1  2  4  1  1  4  1  1  1  1  1  1  9  1  1\n",
              " [625]  1  1  1  1  1  1  1  3  1 11  1  1  1  1  2  1  5  1  6  6  1  1  1  2\n",
              " [649]  8  2  1  1  1  1  1  1  2  1  1  2  1  2  1  1  2  1  4  1  2  2  1  1\n",
              " [673]  3  2  4  2  1  2  1  1  2  9  1  1  1  1  3  1  1  2  1  3  3  1  1  1\n",
              " [697]  7  1  1  2  1  1  1  1  1  1  2  2  2  1  1  1  1  1  1  1  3  2  1  1\n",
              " [721]  3  2  3  3  1  1  1  1  1  1  1  1  1  1  1  1  3  1  1  1  1  1  2  2\n",
              " [745]  6  1  3  3  7  8  1  1  1  1  1  1  1  2  5  1  1  1  1  1  5  1 11  4\n",
              " [769]  1  7  3  1  2  1  1  6  1  1  1  1  1  2  1  1  1  1  1  1  1  8  2  1\n",
              " [793]  1  1  6  1  1  1  2  2  2  1  2  1  1  9  1  1  1  4  7  9  1  1  6  1\n",
              " [817]  1  2  1  2  1 13  1  2  4  6  2  7  1  8  1  1  1  2  1  1  1  2  3 10\n",
              " [841]  1  1  3  1  5  1  2  1  1  2  4  5  1 10  1  9  1  2  1  9  4  6  2  4\n",
              " [865]  2  1  1  8  6  1  1  1  1  2  1  2  1  1  1  3  1  2  1  5  1  1  1  1\n",
              " [889]  2  9  3  3 10  1  2  1  2  2 11  4  2  1  1 13  9  9  1  1  1  2  1  5\n",
              " [913]  1  1  3  1  8  3  1  1  1  2  1  2  2  1  2  1  1 12 12  2  1  6  1  1\n",
              " [937]  2  3  3  8  1  5  1 14  2  1  1  2  2  1  1  3  1  8  1  1  1  7  1  1\n",
              " [961]  1  1  1  1  1  1  1  9  4 13  1  1  1  1  1 13  1  1  1  1  1  1  8  1\n",
              " [985]  4  3  1  2  2  1  3  1  1  1  1  2  1  2  1  1  1  1  2  1  1  4  1 13\n",
              "[1009]  3  4  1  1  1  1  1  2  1 11  1  2  2  1  2  1  1  1  1  1  2  1  1  1\n",
              "[1033]  1  1 14  4  2 12 10  5  1  1  3  2  4  6  2  2  1  1  4  2  1  1  1  3\n",
              "[1057]  1  1  1  4  1  1  1 13  1  2  5  1  1  4  1  1 11  1  1  1 10  1  1  1\n",
              "[1081]  2  1  1  1  1  3  1  2  1  1  1  1  2  1  1  2  1  1  1  1  1  3  1  9\n",
              "[1105]  1  1  1  1  1  7  1  1  1  1  1 14  3  9  1  1  1  1 10  3  1  1  3  1\n",
              "[1129]  1 11  1  6  9  4 12  1  2  1  2  1  3  1  4  4  5  1  1  2  1  1  2  1\n",
              "[1153]  2  2  6  1  2  5  1  1  2  2  1  1  3  1  2  2 12  1  1  1  6  1  2  1\n",
              "[1177]  1  2  2  1  1  2  5  2  1  3  1  1  1  8  1  6  1  1  1  1  1  1  1  2\n",
              "[1201]  4  1  1  2  2  2  1  1  1  1  8  6  1 13  1  4  1  1  1  4  1  2  1  2\n",
              "[1225]  4  2  1  1  2  1 12  1  1  1  1 13  1  1  1  1  3  1  1  1  2  2  2  2\n",
              "[1249]  1  1  1  1  1  1  1  1  3  1  1  1  1  1  2  1  1  1  3  1  1  4  1  1\n",
              "[1273]  1  1  2  2  1  1 10  1  1  1  1  1  1  1  1  2  1  3  1  1  1  1  1  1\n",
              "[1297]  4  1  2  1  1  1  1  6  5  4  4  3  3  1  1  1  2  1  1  1  1  3  1  7\n",
              "[1321]  1  2  1  4  2  2  1  6  1  5  6  1  1  1  1  1  1  1  6  2  1 11  1  3\n",
              "[1345]  1  1  1  3  9  4  1  1  1  1  1  1  2  2  1  2  2  1  5  2  1  5  1  1\n",
              "[1369]  1  1  3  1  1  1  2  4  6  1  1  7  1 11  1  1 12  1  1  1  1  1  1  1\n",
              "[1393]  2  2  2  1  1  4  1  5  1  5  1  2  3  3  6  2  1  1  1  1  3 11  1  1\n",
              "[1417]  1  4  1  1  1  1  1  1  1  1  1  6  2  3  1  1  4  1  2  1  1  2  1  2\n",
              "[1441]  1  1  3  2  1  2  1  1  5  1  1  3  1  8  1  5 14  6  3  2  3  1  2  1\n",
              "[1465]  1  1  1  1 10  2  8  5  2  5  2  1  1  1  1  1  3  5  1  1  1  2  1  4\n",
              "[1489]  1 12  1  1  1 12  1  3  1  2  1  3  2  2  1  1  1  1  1  1  1 12  2  1\n",
              "[1513]  5  1  6  2  1  9  3  1  1  1  3  8  1  1  2  4  4  1  2  6  4  5  1  2\n",
              "[1537]  2  1  8  1  1  7  4  2  1  6 14  1  1  2  5  1  3  1  1  1  1  2  1  2\n",
              "[1561]  4  1  3  1  2  4  1  1  2  1  1  2  1  3  1  2  1  1  5  1  2  1  2  1\n",
              "[1585]  1  1  1  1  1  1  1  1  1  1  1  1  3  8  1  1  2  2  1  3  1  4  1  2\n",
              "[1609]  1  1  3  1  1  2  1  1  1  1  1  1  1  3  8  2  1  1 11  6  6  1  4  4\n",
              "[1633]  2 10  1  1  5  2  1  1  1  1  1  1  1  1  1  2  1  1  1  2  1  1  1  1\n",
              "[1657]  1  2  1  1  2  3  1 11  2  2  1  5  1  2  2  2  1  1  3  1  3 13  8  2\n",
              "[1681]  1 13  2  3  2  1  1  1  2  1  1  1  5  3  1  1  2  1  1  1  1  1  1  2\n",
              "[1705]  5  1  1  2  1  6  1  2  1  4  2  1  5  3  2  1  1  2  1  2  1 12  1  1\n",
              "[1729]  2  7  1  8  1  2  3  1  3  1  5  1  9  1  4  1  1  4  1  2  1  1  1  1\n",
              "[1753]  1  1  1  1  1  1  1  1  4  1  1  3  3  3  3  1  3  9  1  1 10  5  3  1\n",
              "[1777]  1  1  1  2  1  2  3  1  1  1  3  1  1  1  2  1  3  1  1  1  2  1 13  1\n",
              "[1801]  1  1  1  1  1  1  4  1  1  1  1  2  1  4  4  3  2  1  1  1  1  1  1  1\n",
              "[1825]  1  1  1  2  1  1  1  1  2  1  1  2  1  1  4  1  1  5  1  4  1  1  2  2\n",
              "[1849]  1  1  1  1  1  1  1  4  4  3  1  1  1  1  1  1  2  2  1  3  2  1  4  1\n",
              "[1873]  1  5  1  1  1  1  2  1  1  1  1  1  1  1  1  4  1  3  1  2  1  2  1  1\n",
              "[1897]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1\n",
              "[1921]  1 12  1  3  2  1  6  1  2  1  1  1  1  1  1  1  3  7  1  1  1  1  4  1\n",
              "[1945]  1  1  2  1  1  1  1  1  1  4  1  1  1  1  1  1  1  1  2  4  2  3  4  2\n",
              "[1969]  2  3  3  3  5  1  1  1  1  1  6  1  1  3  1  2  1  2  1  1  3 12  1  2\n",
              "[1993]  1  1  1  1  1  1  1  1  2  1  8  1  2  2  1  3  1  1 11  1 12  1  1  1\n",
              "[2017]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1\n",
              "[2041]  3  8  1  2  1  2  1  1  1  1  1  1  3  2  1  1  1  2  2  1  1  1  1  2\n",
              "[2065]  8  1  1  1  1  2  1  1  6  1  2  1  1  1  3  6 14  1  3  1  1  1  1  2\n",
              "[2089]  1  1  2  1  1  1  1  1  1  1  6  2  4 13  1  1  2  2  1  1  1  8  2  2\n",
              "[2113]  1 10  2  3  1  8  1  1  1  6  1  1  1  1  1  1  1  1  1  7  1  2  1  1\n",
              "[2137]  3  2  4  1  1  2  5  6  1  3  1  5  2  1  1  9  1  1  2  1  1  2  1  4\n",
              "[2161]  1  1  1  1  2  3 15  4  8 11  1  1  1  3  2  1  1  1 10  3  7  2  1  5\n",
              "[2185]  1  3  1 10  1  2  9  1  6  1  1  1  2  1  5 10  2 12  1  1  1  4  1  4\n",
              "[2209]  2  1  1 10  1  3 13  2  1  1  3  1  1  6  1  1  1  3  1  2  1  2  6  1\n",
              "[2233]  4  1  1  2  1  1  5  1  7  1  2  1  1  1  3  1  1  2  1  1  1  1  1  1\n",
              "[2257]  1  1  1  1  1  1  1  1  1  8  1  1  2  1  1  1  3  4  3  3  7  3  2  1\n",
              "[2281]  1  2  1  1  1  2  6  1  1  1  1  1  1  2  1  1  2  1  1  1  1  1  3  1\n",
              "[2305]  1  1  1  1  1  1  2  1  1  1  3  1 14  1  1  1  1  1  4  1  1  1  2  1\n",
              "[2329]  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  1  2  3  1  1  1  1  1  1\n",
              "[2353]  1  1  8  1  1 14  1  1  1  1  1  2  1  6  1  1  1  1  1  1  2  2  2 11\n",
              "[2377]  1  5  2  1  1  1  1  1  1  2  1  1  1  5  1  1  1  1  1  1  2  1  6  1\n",
              "[2401]  1  1  2  5  2  1  1  1 14  5  1  1  2  2  1  1  1  5  1  4  3  1  1  1\n",
              "[2425]  3  1  1  5  1  2  3  1  6  1  9  1  1  1  5  2  1  1  2  1  1  3  3  1\n",
              "[2449]  1  1  2  1  2  3  1  1  1  4  1  1 10  4  1  1  1  2  1  1  1  1  3  1\n",
              "[2473]  8  1 12 13  1  3  5  1  2  1  1  4  1  1  1  9  2 12  2 13  1  1  3  1\n",
              "[2497] 14  1  1  1  9  1  1  1  1  1  1  2  1  1  2  1  1  1  1  2  1  1  1  2\n",
              "[2521]  3  3  5  2  7  2  1  1  1  1  1  2  1  2  1  4  1  1  1  3  1  3  1  2\n",
              "[2545]  1  1  1  2  1  1  2  3  1  2  1  1  1  1  1  1  1  2  1  1  1  1  1  1\n",
              "[2569]  2  1  1  3  1  1  9  1  1  1  1  1  1 13  1  1  1 12  5  2  9 14  1  1\n",
              "[2593]  1  1  1  4  2  1  1  4  1  5  1  1  1  3  1  6  1  7  1  1  2  8  1  1\n",
              "[2617]  1  1  1  1  1  1  1  1  1  1  1  1  1  8  3  2  1 12  1  2  3  1  3  1\n",
              "[2641]  3  1  1  1  1  1  2  1  1  2  1  2 11  2  2  4  1  3  1  1  1  1  2  3\n",
              "[2665] 12  1  1  1  1  6 13  1  1  1  1  2  1  1  1  1  3  2  1  1  5  1  2  1\n",
              "[2689]  1  1 10  2  1  4  1  1  3  2  1  2  1  1  1  2  3  1  1  1  1  1  1  6\n",
              "[2713]  2  1  1  1  1  1  7  1  1  5  1  1  2  1  1  4  1  2  3  3  2  1  1  1\n",
              "[2737]  1  4  1  1  1  1  3  1  1  1  1  1  1  1  2  1  7  2  1  3  1  1  1  1\n",
              "[2761]  1  1  3  1  1  1  2  1  6  1  1  1  1  4  1  1  4  3  2  1  2  1  1  1\n",
              "[2785]  1  1  1  1  1  1  1  1  1  1  1  1  2  6  1  1  1  1  1  4  1  1  1  1\n",
              "[2809]  5  1  1  1  2  1  1  3 11  1  1  1  1  1  2  3  2  1  6  1 11  1  1  1\n",
              "[2833]  1  2  1  1  1  2  1  2  4  1  4  1  1  4  1  1  1  4  2  1  1  1  1  2\n",
              "[2857]  2  1  1  1  3  3  2  8  1  1  4  1  1  1  1  1  1  1  1  1  2  1  9  2\n",
              "[2881]  2  1  2  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  5  1  1  1  1\n",
              "[2905]  5  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
              "[2929]  1  1  1  3  1  1  1  1  6  1  1  1  3  2  5  1  2  1  3  1  1  5  1  1\n",
              "[2953]  1  6  1  2  1  2  1  1  1  3 10  4  4  1  1  1  1  5  1  2  1  2  1  9\n",
              "[2977]  3  1  8  2  1 13  1  1  2  1  1  5  5  2  1  1  1  3  1  1  4  1  1  1\n",
              "[3001]  2  5  1  1  1  1  2  1  1  2  1  1  3  2  1  2  1  1  1  1  1  2  1  5\n",
              "[3025]  1  3  4  1  1  3  1  1  1  1  4  3  8  1  2  1  3  1  1  2  2 15  1  3\n",
              "[3049]  1  8  2  2  1  3  1  1  1  6  1  1  6  3  2  1  1  2  1 10  1  1  6  2\n",
              "[3073]  1  1 13  1  1  1  2  1  2  1  1  2  3 13  2  1  3  2  7  1  2  1  1  1\n",
              "[3097]  1  6  1  1  2  1  1  6  1  3  1  3  1  3  1  5  1  1  1  1  1  1  1  1\n",
              "[3121]  1  1  1  1  2  2  2  1 13  1  2  5  1  1  1  1  1  1  3  3  1  2  1  1\n",
              "[3145]  1  1  1  5  1  2  1  2  1  1  1  2 11  1  1  1 12  2  1  1  7  1  7  1\n",
              "[3169]  1  3  1  1  1  1  1  4  1  1  1  3  1  1  1  9  1  1  7  1  1  1  1  1\n",
              "[3193]  1  1  1  2  1  1  2  1  1  1  1  1  1  1  1  1  1  1  5  1  4  3  1  3\n",
              "[3217]  1  1  1 11  1  1  1  4  1 12 13  1  7  1  1  1  2  2  2  1  1  1  1  3\n",
              "[3241]  1  1  1  1  1  5  1  1  1  1  1  1  1  1  1  1  5  1  1  4  3  1  2  2\n",
              "[3265]  1  1  1  1  1  1  2  1  1  1  2  3  2  2  1  1  1  1  1  1  1  2  1  3\n",
              "[3289]  2  4  1  1  1  2  1  2  1  1  1  1  2  1  7  1  1  2  1  2  2  1  2  2\n",
              "[3313]  2  4  1  1  5  1  4  1  1  1  2  1  1  1  2  1  3  1  4  2 11  1  1  1\n",
              "[3337]  1  1  3  1  2  1  2  3  1  9  3  1  6  2  1  1  1  3  6  1  1  1  2  1\n",
              "[3361]  2  1  7  1  1  2  2  2  1  1  1  1  2  1 10  1  1  1  1  1  1  4  4 10\n",
              "[3385]  1  2  4  1  3  6  1  1  4  1  2  1  2  1  1  9  1  2  1  5  1 10  1  1\n",
              "[3409]  1  4  1  1 14  1  4  2  2  3  1  3  7  1  3  5  1  1  2  1  1  2  4  1\n",
              "[3433]  1  2  1  2  1  5  1  1  2  1  1  4  3  1  1  2  2  4  3 14  1  1  1  1\n",
              "[3457]  1  1  1  1  3 14  2  6  2  1  1  1  1  1  1  1  8  1  1  1  2  1  1  1\n",
              "[3481]  1  1  1  1  1  1  1  2  1  1  2  9  1  2  1  1  1  2  1  1  7  1  5  1\n",
              "[3505]  1  2  1  1  7  1  2  1  2  1  1  1  2  1  5  8  1  3  2  5  1  1  1  4\n",
              "[3529]  4  2  2  2  1  1  1  2  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1\n",
              "[3553]  1  1  1  1  1  3  5  1  1  1  1  1  2  3  1  1  1  3  5  1  1  1  1  3\n",
              "[3577]  2  3  1  7  1  2  1  6  1 11  1  2  5  2  1  9  2  2  1  3  2  5  1  2\n",
              "[3601]  1  1  8  2  2  1  2  5  1  1  5  1  1  1  1  3  1  2  1 12  3  1  1  1\n",
              "[3625]  1  2 11  1  1  4 10  5  2  1  1  8  1  1  1  1  4  1  1  2  1  1  1  1\n",
              "[3649]  3  3  2  4  1  1  3  4  1  3  5 13  1  7  3  3 13  2  2 10  1  1  3  1\n",
              "[3673]  1  1  1  2  3  1 11  1 10  2  5  2  1  1  1  1  1  2  6  3  6  1  1  1\n",
              "[3697]  4  2  2  4  3  3  2  9  3  1  1  2  1  1  1  1  1  1  3  1  2  2  1  1\n",
              "[3721]  1  5  1  1  1  1  1  1  1  1  1  2  1  6  1  4  2  1  2  1  1  1  1  2\n",
              "[3745]  2  1  2  2  1  1  1  1  7  1  1  1  1  2  3  1  1  1  1  1  1  1  1  2\n",
              "[3769]  1  2  2  4  3  4  1  1  1  1  1  1  1  1  1  1 13  1  1  2  1  2  1  1\n",
              "[3793]  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  2  1  1  2  1  1  3  1  1\n",
              "[3817]  1  1  5  4  1  1  1  1  1  1  4  1  1  1  1  3  1  2  1  7  7  3  1  5\n",
              "[3841]  2  6  1  1  1  1  1  1  2  2  1  6  1  3  1  1  2  1  9  1  1  8  1  3\n",
              "[3865]  2  3  2  1  1  9  1  1  2 14  4  1  2  1  2  1  1  1  6  1  2  1  3  7\n",
              "[3889]  2  4  1  1  9  1  5  1  1  3  1  1  1  1  4  2  3  1 12  2  1  1  1  2\n",
              "[3913]  1  3  1  1  1  1  2 12  1  1  1  1  1  2  1  1  1  1  1  1  4  1  2  3\n",
              "[3937]  7  9  2  2  1  1  4  4  1  1  2  6  2  2  1  1  2  9  1  5  1  1  1  3\n",
              "[3961]  1  1  2  1  1  8  2 11  1  1  1  1  3  1  1  2  1  1  2  3  4  1  1  8\n",
              "[3985]  1  1  1  1  1  1  2  1  1  1  1  1  1  1  2  2  2  1  1  4  1  1  1  3\n",
              "[4009]  4  1  1  1  1  1  5  1  2  1  3  2  1  1  3  1  2  1  1  1  1  1  1 14\n",
              "[4033]  1  1  3  3  2  2  3  2  1  1  2  1  2  1  1  4  1  1 13  1  6  1  2  1\n",
              "[4057]  2  1  2  1  1  1  2  1  1  3  3  2  1  1  1  1  1  2  3  1  1  3  2  2\n",
              "[4081]  2  2  1  1 10  1  1  9 13  2  2  5  1  1  3  7  1  1  1  4 12  3  1  1\n",
              "[4105]  9  1  1  8  1  1  1  2  2  4  6  1  1  1  4  1  1  3  1  1  2  1  1  1\n",
              "[4129]  3  2  1  1  3  1  3  1  1 14  1  1  1  1  2  3  1  3  1  2  5  1  1  1\n",
              "[4153]  1  6  8  1  2  2  2  1  1  1  1  5  8  1  1  4  2  1  1  1  1  1  5  1\n",
              "[4177]  1  1  1  5  1  1  6  1  1  4  7  3  1  1  2  5  1  1  1  2  2  1  1  2\n",
              "[4201]  3  1  1  1 12  1  1  1  2  2  1  1  1  1  1  2  1  1  1  3  2  1  1  1\n",
              "[4225]  1  2  2  1  6  3  1  2  1  2  1  1  4 14  3  1  7  1  1  1  1  1  1  2\n",
              "[4249] 10  1  1  1  2  1  2  4  1  8  2  4  2  1  1  1  6 12  1  1  1  1  1  3\n",
              "[4273]  1  1  1  2  1  2  2  1  1  4  2  3  1  7  1  1  1  1  1  1  1  1  1  3\n",
              "[4297]  1  1  1  5  1  1  1  1  1  3  1  1  2  1  2  3  2  1  1  2  9  2  1  2\n",
              "[4321]  6  1  1  1  1  2  8  1  1 11  1  1  4  4  1  2  1  1  2  1  1  1  1  3\n",
              "[4345]  2  3  4  7  1  1  1  1  1  1  1  1  1  4  1  1  2  1  2  1  3  1  1  1\n",
              "[4369]  1  2  1  3  1  9  1  1  1  1  1  2  4  2  2  2  1  2  2  1  1  1  1 10\n",
              "[4393]  1  1  2  2  1  1  2 12  1  1  1  1  1  1  4  1  1  2  2  2  1  1  6  1\n",
              "[4417]  1  1  1  1  2  5  4  1  1  1  1  1  1  1 11  1  1  1  5  1  1  1  2  6\n",
              "[4441]  1  1  1  6  1  1  3  1  1  7  1  3  2  1  3  1  1  1  1  1  2  1  1  7\n",
              "[4465]  1  1  1  1  1  2  1  3  3  1  2  3  5  1  1  1  1  1  1  1 10  2  1  2\n",
              "[4489]  1  2  1  2  1  2  1  3  1  1  1  2  1  1  1  1  2  1  2  3  1  2  1  1\n",
              "[4513]  1  5  1  1  2  1  1  1  4  1  1  1  1  1  1  1  1  1  1  1  1 12  7  3\n",
              "[4537]  2  1  1  2  1  1  2  1  1  2  1  1  1  3  1  1  3  1  2  1  2  1  1  1\n",
              "[4561]  1  3  2  1  2  5  1  1  1  1  1  1  1  1  1  1  8  3  2  1  1  1  1  9\n",
              "[4585]  1  1  1  1  1  2  4  3  1  1  1  1  2  1  8  7  1 12  1 13  1  1  1  1\n",
              "[4609]  1  1  3  1  5  1  1  1  1  1  1  1  1  1  7  6  1  1  1  1  1  2  3  1\n",
              "[4633]  1  1  5  3  2  6  7  1  5  1  1  1  1  1  6  2  7  1  1  1  5  1  1  1\n",
              "[4657]  2  1  1  1  1 10  1  1  9  3  1  1  2  1  1  1  2  1  1  3  1  2  6  4\n",
              "[4681]  7  1  4  1  1  3  2  4  1  1  3  1  1  1  1  1  2  1  1  1  6  1  1  1\n",
              "[4705]  2  1  1  1  1  3  1  1  2  2  1  1  1 12  1  1  1  1  1  1  1  1  1  1\n",
              "[4729]  1  1  1 12  5  1  1  2  1  1  1  1  1  1  3  2  4  1  1  1  1  1  1  1\n",
              "[4753]  1  1  1  1  1  2  1  1  1  1  1  1  1  4  1  2  1  2  1  1  1 13  1  1\n",
              "[4777]  1  1  1  1  1  1  1  2  1  1  8  1  1  1  4  7  1  3  2  1  4  1  1  1\n",
              "[4801]  2  1  1  1  1  1  1  1  4  2  1  6  4 11  3  2  1  1  1  2  1  1  1  1\n",
              "[4825]  1  1  1  3  1  1  1  6  1  1  1  1  1  2  8  1  1  1  3 12  2  1  1  1\n",
              "[4849]  3  1  2  1  1  1  5  1  6  1  1  1  1  1  4  1  1  1  1  1  1  1  1  1\n",
              "[4873]  1  1  1  2  1  1  5  1  2  4  1  1  1  3  1  1  1  1  2  1  1  1  1  1\n",
              "[4897]  1  1  2  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning Spectral Initialization \n",
            "\t Calculating the gram matrix...\n",
            "\t Finding anchor words...\n",
            " \t....................\n",
            "\t Recovering initialization...\n",
            " \t....\n",
            "Initialization complete.\n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 1 (approx. per word bound = -5.043) \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 2 (approx. per word bound = -5.005, relative change = 7.606e-03) \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 3 (approx. per word bound = -4.989, relative change = 3.200e-03) \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 4 (approx. per word bound = -4.980, relative change = 1.693e-03) \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 5 (approx. per word bound = -4.975, relative change = 1.101e-03) \n",
            "Topic 1: d, s, c, t, al \n",
            " Topic 2: clinic, symptom, health, extract, field \n",
            " Topic 3: diseas, class, ontolog, term, refer \n",
            " Topic 4: medic, actual, topic, semant, provid \n",
            " Topic 5: ontolog, domain, reason, model, logic \n",
            " Topic 6: dataset, annot, name, ehr, hospit \n",
            " Topic 7: role, ontolog, patient, health, care \n",
            " Topic 8: data, queri, extract, semant, s \n",
            " Topic 9: s, encod, discuss, cluster, experi \n",
            " Topic 10: clinic, case, medic, avail, data \n",
            " Topic 11: author, public, document, s, number \n",
            " Topic 12: classif, subject, system, evalu, text \n",
            " Topic 13: set, direct, knowledg, identifi, perform \n",
            " Topic 14: integr, data, biomed, knowledg, inform \n",
            " Topic 15: class, data, specif, specifi, ontolog \n",
            " Topic 16: mean, record, inform, clinic, next \n",
            " Topic 17: clinic, structur, inform, outcom, avail \n",
            " Topic 18: text, identifi, use, background, open \n",
            " Topic 19: construct, inform, extract, health, process \n",
            " Topic 20: start, requir, import, address, relev \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 6 (approx. per word bound = -4.971, relative change = 8.073e-04) \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 7 (approx. per word bound = -4.968, relative change = 6.131e-04) \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 8 (approx. per word bound = -4.965, relative change = 4.978e-04) \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Completing Iteration 9 (approx. per word bound = -4.963, relative change = 4.260e-04) \n",
            "...............\n",
            "Completed E-Step (0 seconds). \n",
            "Completed M-Step. \n",
            "Model Terminated Before Convergence Reached \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Plot with title “Tokens Removed by Threshold”"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nEBBBYwAAAABJRU5ErkJggg=="
          },
          "metadata": {
            "image/png": {
              "width": 420,
              "height": 420
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Topic 1 Top Words:\n",
              " \t Highest Prob: d, s, c, t, al, r, text, et, concept, evalu \n",
              " \t FREX: d, c, t, s, r, al, concept, algorithm, detect, review \n",
              " \t Lift: d, c, t, detect, r, free, algorithm, s, review, concept \n",
              " \t Score: d, c, t, s, r, text, free, algorithm, al, ontolog \n",
              "Topic 2 Top Words:\n",
              " \t Highest Prob: clinic, symptom, health, import, extract, field, ehr, avail, inform, automat \n",
              " \t FREX: symptom, clinic, field, import, ehr, health, automat, resourc, extract, appropri \n",
              " \t Lift: symptom, field, automat, clinic, import, appropri, problem, abstract, electron, initi \n",
              " \t Score: symptom, clinic, ehr, field, import, automat, health, resourc, extract, record \n",
              "Topic 3 Top Words:\n",
              " \t Highest Prob: diseas, class, ontolog, term, system, classifi, refer, word, train, manual \n",
              " \t FREX: diseas, classifi, refer, term, subclass, class, word, label, manual, system \n",
              " \t Lift: diseas, distinguish, refer, subclass, classifi, size, label, manual, word, term \n",
              " \t Score: diseas, class, ontolog, classifi, refer, word, term, train, subclass, label \n",
              "Topic 4 Top Words:\n",
              " \t Highest Prob: actual, medic, semant, topic, order, provid, identifi, public, sourc, background \n",
              " \t FREX: actual, topic, order, medic, effort, autom, assess, scientif, sourc, publish \n",
              " \t Lift: actual, topic, effort, program, methodolog, countri, interpret, nation, publish, autom \n",
              " \t Score: actual, topic, medic, effort, assess, scientif, nation, autom, publish, order \n",
              "Topic 5 Top Words:\n",
              " \t Highest Prob: ontolog, domain, reason, logic, model, subject, instanc, illustr, content, requir \n",
              " \t FREX: ontolog, logic, reason, model, domain, illustr, content, instanc, subject, multipl \n",
              " \t Lift: logic, illustr, aspect, reason, model, ontolog, autom, version, content, domain \n",
              " \t Score: logic, ontolog, reason, model, illustr, class, subject, multipl, version, featur \n",
              "Topic 6 Top Words:\n",
              " \t Highest Prob: dataset, annot, name, ehr, hospit, task, train, data, perform, system \n",
              " \t FREX: dataset, name, annot, hospit, task, ehr, train, person, point, perform \n",
              " \t Lift: dataset, hospit, earli, name, ehr, annot, point, characterist, person, differ- \n",
              " \t Score: dataset, ehr, annot, train, hospit, person, name, point, task, learn \n",
              "Topic 7 Top Words:\n",
              " \t Highest Prob: role, ontolog, patient, health, care, materi, provid, data, entiti, relat \n",
              " \t FREX: role, care, patient, materi, entiti, relat, defin, practic, part, particip \n",
              " \t Lift: role, care, particip, practic, entiti, health, patient, materi, ation, way \n",
              " \t Score: role, ontolog, patient, class, ehr, care, health, represent, particip, entiti \n",
              "Topic 8 Top Words:\n",
              " \t Highest Prob: data, queri, extract, semant, s, direct, class, line, order, access \n",
              " \t FREX: queri, data, enabl, line, without, order, option, direct, thus, web \n",
              " \t Lift: queri, upon, enabl, option, terminolog, order, web, explicit, extract, without \n",
              " \t Score: queri, data, class, extract, enabl, upon, without, web, order, option \n",
              "Topic 9 Top Words:\n",
              " \t Highest Prob: s, discuss, encod, cluster, experi, learn, content, text, semant, health \n",
              " \t FREX: discuss, cluster, encod, experi, content, learn, score, represent, word, number \n",
              " \t Lift: discuss, cluster, encod, communiti, effect, experi, expect, score, optim, content \n",
              " \t Score: discuss, cluster, experi, learn, score, encod, word, represent, text, effect \n",
              "Topic 10 Top Words:\n",
              " \t Highest Prob: clinic, case, medic, avail, data, tool, area, scientif, typic, provid \n",
              " \t FREX: case, area, typic, scientif, tool, medic, purpos, applic, various, challeng \n",
              " \t Lift: typic, area, built, clinic, case, scientif, tool, various, etc, purpos \n",
              " \t Score: typic, clinic, area, scientif, case, medic, tool, annot, histori, reason \n",
              "Topic 11 Top Words:\n",
              " \t Highest Prob: author, public, document, s, number, institut, network, search, inform, map \n",
              " \t FREX: author, institut, network, technolog, search, public, number, j, map, organ \n",
              " \t Lift: institut, technolog, author, j, network, total, public, inclus, search, organ \n",
              " \t Score: institut, author, network, search, organ, j, map, technolog, number, total \n",
              "Topic 12 Top Words:\n",
              " \t Highest Prob: classif, subject, system, evalu, text, assign, data, document, s, class \n",
              " \t FREX: classif, subject, assign, evalu, system, correct, classifi, focus, section, accord \n",
              " \t Lift: classif, subject, target, assign, step, correct, focus, manner, accord, coher \n",
              " \t Score: classif, subject, assign, text, evalu, class, system, classifi, train, step \n",
              "Topic 13 Top Words:\n",
              " \t Highest Prob: set, direct, knowledg, identifi, perform, featur, inform, case, al, et \n",
              " \t FREX: set, direct, featur, categor, perform, refer, assign, dis-, case, knowledg \n",
              " \t Lift: categor, set, dis-, direct, risk, index, assess, knowledg, featur, identifi \n",
              " \t Score: categor, set, featur, knowledg, refer, assess, direct, assign, identifi, perform \n",
              "Topic 14 Top Words:\n",
              " \t Highest Prob: integr, data, biomed, knowledg, inform, resourc, databas, design, provid, health \n",
              " \t FREX: integr, databas, biomed, design, uniqu, resourc, data, knowledg, support, effort \n",
              " \t Lift: integr, uniqu, effort, nation, resourc, design, databas, understand, interpret, knowledg \n",
              " \t Score: integr, knowledg, resourc, design, data, nation, effort, uniqu, databas, scientif \n",
              "Topic 15 Top Words:\n",
              " \t Highest Prob: class, data, ontolog, specifi, specif, valu, fig, resourc, search, defin \n",
              " \t FREX: specifi, valu, specif, search, singl, class, combin, fig, observ, attribut \n",
              " \t Lift: specifi, restrict, attribut, deriv, core, valu, oper, combin, search, observ \n",
              " \t Score: specifi, class, ontolog, valu, search, calcul, core, reason, resourc, deriv \n",
              "Topic 16 Top Words:\n",
              " \t Highest Prob: mean, inform, record, clinic, treatment, next, free, requir, extract, form \n",
              " \t FREX: mean, record, treatment, next, free, inform, requir, electron, form, clinic \n",
              " \t Lift: mean, next, record, treatment, free, electron, problem, initi, appropri, form \n",
              " \t Score: mean, record, free, clinic, next, treatment, electron, core, requir, inform \n",
              "Topic 17 Top Words:\n",
              " \t Highest Prob: clinic, structur, inform, outcom, extract, identifi, avail, free, document, background \n",
              " \t FREX: structur, clinic, outcom, free, extract, avail, document, inform, identifi, background \n",
              " \t Lift: outcom, free, structur, clinic, abstract, background, natur, interest, avail, extract \n",
              " \t Score: outcom, clinic, free, structur, extract, identifi, avail, document, inform, natur \n",
              "Topic 18 Top Words:\n",
              " \t Highest Prob: text, identifi, use, background, open, access, avail, inform, tool, extract \n",
              " \t FREX: text, use, identifi, background, open, tool, access, avail, identif, next \n",
              " \t Lift: use, text, identifi, background, open, next, abstract, access, natur, tool \n",
              " \t Score: use, text, identifi, next, background, tool, identif, natur, open, access \n",
              "Topic 19 Top Words:\n",
              " \t Highest Prob: construct, inform, identifi, extract, health, process, open, access, repres, languag \n",
              " \t FREX: construct, inform, health, process, extract, step, repres, relev, identifi, open \n",
              " \t Lift: construct, interest, inform, open, step, under, natur, import, relev, appropri \n",
              " \t Score: construct, inform, step, relev, identifi, import, health, extract, interest, under \n",
              "Topic 20 Top Words:\n",
              " \t Highest Prob: start, import, languag, core, address, inform, relev, problem, requir, process \n",
              " \t FREX: start, core, address, import, relev, problem, languag, requir, repres, natur \n",
              " \t Lift: start, problem, core, address, interest, import, natur, relev, languag, requir \n",
              " \t Score: start, core, address, import, relev, problem, interest, requir, natur, languag "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 20 × 2</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>TopicNumber</th><th scope=col>TopicProportions</th></tr>\n",
              "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td> 1</td><td>0.075867534</td></tr>\n",
              "\t<tr><td> 2</td><td>0.028003396</td></tr>\n",
              "\t<tr><td> 3</td><td>0.079149772</td></tr>\n",
              "\t<tr><td> 4</td><td>0.068180003</td></tr>\n",
              "\t<tr><td> 5</td><td>0.063172738</td></tr>\n",
              "\t<tr><td> 6</td><td>0.070743661</td></tr>\n",
              "\t<tr><td> 7</td><td>0.060460284</td></tr>\n",
              "\t<tr><td> 8</td><td>0.060263936</td></tr>\n",
              "\t<tr><td> 9</td><td>0.066318523</td></tr>\n",
              "\t<tr><td>10</td><td>0.069234197</td></tr>\n",
              "\t<tr><td>11</td><td>0.058271790</td></tr>\n",
              "\t<tr><td>12</td><td>0.068718218</td></tr>\n",
              "\t<tr><td>13</td><td>0.047383721</td></tr>\n",
              "\t<tr><td>14</td><td>0.059440994</td></tr>\n",
              "\t<tr><td>15</td><td>0.055650463</td></tr>\n",
              "\t<tr><td>16</td><td>0.018586490</td></tr>\n",
              "\t<tr><td>17</td><td>0.012869762</td></tr>\n",
              "\t<tr><td>18</td><td>0.009479005</td></tr>\n",
              "\t<tr><td>19</td><td>0.012885918</td></tr>\n",
              "\t<tr><td>20</td><td>0.015319596</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 20 × 2\n\n| TopicNumber &lt;int&gt; | TopicProportions &lt;dbl&gt; |\n|---|---|\n|  1 | 0.075867534 |\n|  2 | 0.028003396 |\n|  3 | 0.079149772 |\n|  4 | 0.068180003 |\n|  5 | 0.063172738 |\n|  6 | 0.070743661 |\n|  7 | 0.060460284 |\n|  8 | 0.060263936 |\n|  9 | 0.066318523 |\n| 10 | 0.069234197 |\n| 11 | 0.058271790 |\n| 12 | 0.068718218 |\n| 13 | 0.047383721 |\n| 14 | 0.059440994 |\n| 15 | 0.055650463 |\n| 16 | 0.018586490 |\n| 17 | 0.012869762 |\n| 18 | 0.009479005 |\n| 19 | 0.012885918 |\n| 20 | 0.015319596 |\n\n",
            "text/latex": "A data.frame: 20 × 2\n\\begin{tabular}{ll}\n TopicNumber & TopicProportions\\\\\n <int> & <dbl>\\\\\n\\hline\n\t  1 & 0.075867534\\\\\n\t  2 & 0.028003396\\\\\n\t  3 & 0.079149772\\\\\n\t  4 & 0.068180003\\\\\n\t  5 & 0.063172738\\\\\n\t  6 & 0.070743661\\\\\n\t  7 & 0.060460284\\\\\n\t  8 & 0.060263936\\\\\n\t  9 & 0.066318523\\\\\n\t 10 & 0.069234197\\\\\n\t 11 & 0.058271790\\\\\n\t 12 & 0.068718218\\\\\n\t 13 & 0.047383721\\\\\n\t 14 & 0.059440994\\\\\n\t 15 & 0.055650463\\\\\n\t 16 & 0.018586490\\\\\n\t 17 & 0.012869762\\\\\n\t 18 & 0.009479005\\\\\n\t 19 & 0.012885918\\\\\n\t 20 & 0.015319596\\\\\n\\end{tabular}\n",
            "text/plain": [
              "   TopicNumber TopicProportions\n",
              "1   1          0.075867534     \n",
              "2   2          0.028003396     \n",
              "3   3          0.079149772     \n",
              "4   4          0.068180003     \n",
              "5   5          0.063172738     \n",
              "6   6          0.070743661     \n",
              "7   7          0.060460284     \n",
              "8   8          0.060263936     \n",
              "9   9          0.066318523     \n",
              "10 10          0.069234197     \n",
              "11 11          0.058271790     \n",
              "12 12          0.068718218     \n",
              "13 13          0.047383721     \n",
              "14 14          0.059440994     \n",
              "15 15          0.055650463     \n",
              "16 16          0.018586490     \n",
              "17 17          0.012869762     \n",
              "18 18          0.009479005     \n",
              "19 19          0.012885918     \n",
              "20 20          0.015319596     "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<dl>\n",
              "\t<dt>$text1</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 263 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>8</td><td>10</td><td>12</td><td>13</td><td>⋯</td><td>400</td><td>401</td><td>402</td><td>403</td><td>405</td><td>409</td><td>411</td><td>413</td><td>415</td><td>416</td></tr>\n",
              "\t<tr><td>1</td><td>1</td><td>4</td><td>1</td><td>1</td><td>1</td><td>3</td><td> 8</td><td> 1</td><td>12</td><td>⋯</td><td>  4</td><td>  6</td><td>  6</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td> 32</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text2</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 45 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>11</td><td>24</td><td>35</td><td>36</td><td>39</td><td>57</td><td>79</td><td>86</td><td>⋯</td><td>318</td><td>319</td><td>351</td><td>355</td><td>358</td><td>365</td><td>376</td><td>387</td><td>390</td><td>399</td></tr>\n",
              "\t<tr><td>1</td><td>1</td><td> 1</td><td> 1</td><td> 1</td><td> 1</td><td> 1</td><td> 3</td><td> 2</td><td> 1</td><td>⋯</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text3</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 266 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>4</td><td>6</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>13</td><td>⋯</td><td>402</td><td>403</td><td>407</td><td>408</td><td>409</td><td>411</td><td>412</td><td>413</td><td>415</td><td>416</td></tr>\n",
              "\t<tr><td>3</td><td>2</td><td>5</td><td>3</td><td>5</td><td>1</td><td> 2</td><td> 1</td><td> 2</td><td>15</td><td>⋯</td><td>  1</td><td>  6</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td>  6</td><td>  3</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text4</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 294 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>5</td><td>6</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>⋯</td><td>405</td><td>406</td><td>407</td><td>408</td><td>409</td><td>410</td><td>411</td><td>413</td><td>414</td><td>416</td></tr>\n",
              "\t<tr><td>6</td><td>1</td><td>3</td><td>1</td><td>1</td><td>2</td><td>1</td><td> 1</td><td> 3</td><td> 5</td><td>⋯</td><td>  2</td><td> 13</td><td>  2</td><td>  1</td><td>  1</td><td>  2</td><td>  3</td><td>  1</td><td>  1</td><td>  2</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text5</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 282 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>9</td><td>10</td><td>11</td><td>13</td><td>14</td><td>⋯</td><td>404</td><td>405</td><td>407</td><td>408</td><td>409</td><td>410</td><td>412</td><td>415</td><td>416</td><td>417</td></tr>\n",
              "\t<tr><td>1</td><td>4</td><td>1</td><td>2</td><td>6</td><td>1</td><td> 6</td><td> 2</td><td>11</td><td> 2</td><td>⋯</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td> 15</td><td> 14</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text6</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 304 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td> 3</td><td>4</td><td>7</td><td>9</td><td>10</td><td>12</td><td>13</td><td>14</td><td>15</td><td>⋯</td><td>405</td><td>406</td><td>407</td><td>408</td><td>409</td><td>410</td><td>411</td><td>413</td><td>414</td><td>416</td></tr>\n",
              "\t<tr><td>1</td><td>16</td><td>6</td><td>5</td><td>2</td><td> 9</td><td> 1</td><td>15</td><td> 3</td><td> 7</td><td>⋯</td><td>  2</td><td>  2</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td> 10</td><td>  1</td><td> 13</td><td> 10</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text7</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 245 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>9</td><td>11</td><td>12</td><td>13</td><td>20</td><td>21</td><td>22</td><td>23</td><td>⋯</td><td>405</td><td>406</td><td>407</td><td>408</td><td>409</td><td>412</td><td>413</td><td>415</td><td>416</td><td>417</td></tr>\n",
              "\t<tr><td>1</td><td>8</td><td>1</td><td> 1</td><td> 2</td><td> 3</td><td>10</td><td> 1</td><td> 1</td><td> 8</td><td>⋯</td><td>  6</td><td>  2</td><td>  2</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  8</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text8</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 314 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>⋯</td><td>407</td><td>408</td><td>409</td><td>410</td><td>411</td><td>412</td><td>413</td><td>414</td><td>415</td><td>416</td></tr>\n",
              "\t<tr><td>4</td><td>1</td><td>3</td><td>2</td><td>1</td><td>1</td><td>1</td><td>4</td><td>1</td><td> 2</td><td>⋯</td><td>  1</td><td>  2</td><td>  1</td><td>  7</td><td>  5</td><td>  3</td><td>  1</td><td>  3</td><td>  1</td><td> 10</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text9</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 294 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>2</td><td>3</td><td>4</td><td>8</td><td>9</td><td>10</td><td>12</td><td>13</td><td>15</td><td>16</td><td>⋯</td><td>402</td><td>403</td><td>404</td><td>407</td><td>408</td><td>409</td><td>410</td><td>413</td><td>414</td><td>416</td></tr>\n",
              "\t<tr><td>2</td><td>2</td><td>1</td><td>3</td><td>1</td><td> 5</td><td> 1</td><td>18</td><td> 1</td><td> 1</td><td>⋯</td><td>  4</td><td>  7</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  4</td><td> 11</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text10</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 316 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td> 4</td><td>6</td><td>7</td><td>9</td><td>10</td><td>12</td><td>13</td><td>⋯</td><td>407</td><td>408</td><td>409</td><td>410</td><td>412</td><td>413</td><td>414</td><td>415</td><td>416</td><td>417</td></tr>\n",
              "\t<tr><td>3</td><td>2</td><td>2</td><td>11</td><td>1</td><td>4</td><td>1</td><td> 4</td><td> 4</td><td>17</td><td>⋯</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  9</td><td>  3</td><td>  7</td><td>  4</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text11</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 275 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>13</td><td>14</td><td>⋯</td><td>402</td><td>403</td><td>404</td><td>406</td><td>407</td><td>408</td><td>409</td><td>415</td><td>416</td><td>417</td></tr>\n",
              "\t<tr><td>1</td><td>2</td><td>2</td><td>2</td><td>5</td><td>3</td><td> 8</td><td> 6</td><td>16</td><td> 1</td><td>⋯</td><td>  2</td><td>  3</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  1</td><td> 10</td><td> 12</td><td>  5</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text12</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 81 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>5</td><td>6</td><td>7</td><td>13</td><td>15</td><td>19</td><td>22</td><td>⋯</td><td>366</td><td>375</td><td>376</td><td>379</td><td>382</td><td>383</td><td>390</td><td>394</td><td>399</td><td>416</td></tr>\n",
              "\t<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>4</td><td> 1</td><td> 2</td><td> 1</td><td> 1</td><td>⋯</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  2</td><td>  6</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text13</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 236 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>5</td><td>6</td><td>9</td><td>10</td><td>12</td><td>13</td><td>15</td><td>⋯</td><td>396</td><td>398</td><td>401</td><td>407</td><td>408</td><td>409</td><td>411</td><td>412</td><td>413</td><td>416</td></tr>\n",
              "\t<tr><td>2</td><td>1</td><td>7</td><td>1</td><td>1</td><td>1</td><td> 2</td><td> 1</td><td> 6</td><td> 1</td><td>⋯</td><td>  1</td><td>  1</td><td>  4</td><td>  2</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  2</td><td>  2</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text14</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 235 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>4</td><td>6</td><td> 8</td><td>9</td><td>10</td><td>13</td><td>14</td><td>⋯</td><td>398</td><td>402</td><td>403</td><td>407</td><td>408</td><td>409</td><td>410</td><td>413</td><td>415</td><td>417</td></tr>\n",
              "\t<tr><td>1</td><td>3</td><td>3</td><td>1</td><td>1</td><td>10</td><td>1</td><td> 4</td><td>69</td><td>20</td><td>⋯</td><td>  2</td><td>  8</td><td>  2</td><td>  3</td><td>  4</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "\t<dt>$text15</dt>\n",
              "\t\t<dd><table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 315 of type int</caption>\n",
              "<tbody>\n",
              "\t<tr><td>1</td><td>2</td><td>3</td><td>4</td><td>6</td><td>7</td><td>9</td><td>10</td><td>11</td><td>12</td><td>⋯</td><td>403</td><td>405</td><td>407</td><td>408</td><td>409</td><td>410</td><td>411</td><td>413</td><td>414</td><td>417</td></tr>\n",
              "\t<tr><td>3</td><td>3</td><td>6</td><td>2</td><td>1</td><td>1</td><td>1</td><td> 8</td><td> 2</td><td> 4</td><td>⋯</td><td> 24</td><td>  2</td><td>  2</td><td>  1</td><td>  1</td><td>  4</td><td>  1</td><td>  1</td><td>  1</td><td>  1</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</dd>\n",
              "</dl>\n"
            ],
            "text/markdown": "$text1\n:   \nA matrix: 2 × 263 of type int\n\n| 1 | 2 | 3 | 4 | 5 | 6 | 8 | 10 | 12 | 13 | ⋯ | 400 | 401 | 402 | 403 | 405 | 409 | 411 | 413 | 415 | 416 |\n| 1 | 1 | 4 | 1 | 1 | 1 | 3 |  8 |  1 | 12 | ⋯ |   4 |   6 |   6 |   1 |   1 |   1 |   1 |   2 |  32 |   1 |\n\n\n$text2\n:   \nA matrix: 2 × 45 of type int\n\n| 2 | 3 | 11 | 24 | 35 | 36 | 39 | 57 | 79 | 86 | ⋯ | 318 | 319 | 351 | 355 | 358 | 365 | 376 | 387 | 390 | 399 |\n| 1 | 1 |  1 |  1 |  1 |  1 |  1 |  3 |  2 |  1 | ⋯ |   1 |   1 |   1 |   1 |   1 |   2 |   1 |   1 |   1 |   1 |\n\n\n$text3\n:   \nA matrix: 2 × 266 of type int\n\n| 2 | 3 | 4 | 6 | 8 | 9 | 10 | 11 | 12 | 13 | ⋯ | 402 | 403 | 407 | 408 | 409 | 411 | 412 | 413 | 415 | 416 |\n| 3 | 2 | 5 | 3 | 5 | 1 |  2 |  1 |  2 | 15 | ⋯ |   1 |   6 |   1 |   1 |   1 |   1 |   2 |   1 |   6 |   3 |\n\n\n$text4\n:   \nA matrix: 2 × 294 of type int\n\n| 1 | 2 | 3 | 5 | 6 | 8 | 9 | 10 | 11 | 12 | ⋯ | 405 | 406 | 407 | 408 | 409 | 410 | 411 | 413 | 414 | 416 |\n| 6 | 1 | 3 | 1 | 1 | 2 | 1 |  1 |  3 |  5 | ⋯ |   2 |  13 |   2 |   1 |   1 |   2 |   3 |   1 |   1 |   2 |\n\n\n$text5\n:   \nA matrix: 2 × 282 of type int\n\n| 2 | 3 | 4 | 5 | 6 | 9 | 10 | 11 | 13 | 14 | ⋯ | 404 | 405 | 407 | 408 | 409 | 410 | 412 | 415 | 416 | 417 |\n| 1 | 4 | 1 | 2 | 6 | 1 |  6 |  2 | 11 |  2 | ⋯ |   1 |   1 |   2 |   1 |   1 |   2 |   1 |  15 |  14 |   1 |\n\n\n$text6\n:   \nA matrix: 2 × 304 of type int\n\n| 2 |  3 | 4 | 7 | 9 | 10 | 12 | 13 | 14 | 15 | ⋯ | 405 | 406 | 407 | 408 | 409 | 410 | 411 | 413 | 414 | 416 |\n| 1 | 16 | 6 | 5 | 2 |  9 |  1 | 15 |  3 |  7 | ⋯ |   2 |   2 |   1 |   1 |   1 |   1 |  10 |   1 |  13 |  10 |\n\n\n$text7\n:   \nA matrix: 2 × 245 of type int\n\n| 2 | 3 | 9 | 11 | 12 | 13 | 20 | 21 | 22 | 23 | ⋯ | 405 | 406 | 407 | 408 | 409 | 412 | 413 | 415 | 416 | 417 |\n| 1 | 8 | 1 |  1 |  2 |  3 | 10 |  1 |  1 |  8 | ⋯ |   6 |   2 |   2 |   1 |   1 |   1 |   1 |   2 |   8 |   1 |\n\n\n$text8\n:   \nA matrix: 2 × 314 of type int\n\n| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | ⋯ | 407 | 408 | 409 | 410 | 411 | 412 | 413 | 414 | 415 | 416 |\n| 4 | 1 | 3 | 2 | 1 | 1 | 1 | 4 | 1 |  2 | ⋯ |   1 |   2 |   1 |   7 |   5 |   3 |   1 |   3 |   1 |  10 |\n\n\n$text9\n:   \nA matrix: 2 × 294 of type int\n\n| 2 | 3 | 4 | 8 | 9 | 10 | 12 | 13 | 15 | 16 | ⋯ | 402 | 403 | 404 | 407 | 408 | 409 | 410 | 413 | 414 | 416 |\n| 2 | 2 | 1 | 3 | 1 |  5 |  1 | 18 |  1 |  1 | ⋯ |   4 |   7 |   1 |   1 |   1 |   1 |   1 |   1 |   4 |  11 |\n\n\n$text10\n:   \nA matrix: 2 × 316 of type int\n\n| 1 | 2 | 3 |  4 | 6 | 7 | 9 | 10 | 12 | 13 | ⋯ | 407 | 408 | 409 | 410 | 412 | 413 | 414 | 415 | 416 | 417 |\n| 3 | 2 | 2 | 11 | 1 | 4 | 1 |  4 |  4 | 17 | ⋯ |   1 |   1 |   1 |   1 |   1 |   1 |   9 |   3 |   7 |   4 |\n\n\n$text11\n:   \nA matrix: 2 × 275 of type int\n\n| 1 | 2 | 3 | 7 | 8 | 9 | 10 | 11 | 13 | 14 | ⋯ | 402 | 403 | 404 | 406 | 407 | 408 | 409 | 415 | 416 | 417 |\n| 1 | 2 | 2 | 2 | 5 | 3 |  8 |  6 | 16 |  1 | ⋯ |   2 |   3 |   1 |   1 |   1 |   2 |   1 |  10 |  12 |   5 |\n\n\n$text12\n:   \nA matrix: 2 × 81 of type int\n\n| 1 | 2 | 3 | 5 | 6 | 7 | 13 | 15 | 19 | 22 | ⋯ | 366 | 375 | 376 | 379 | 382 | 383 | 390 | 394 | 399 | 416 |\n| 1 | 1 | 1 | 1 | 1 | 4 |  1 |  2 |  1 |  1 | ⋯ |   1 |   1 |   1 |   2 |   2 |   6 |   1 |   1 |   1 |   1 |\n\n\n$text13\n:   \nA matrix: 2 × 236 of type int\n\n| 1 | 2 | 3 | 5 | 6 | 9 | 10 | 12 | 13 | 15 | ⋯ | 396 | 398 | 401 | 407 | 408 | 409 | 411 | 412 | 413 | 416 |\n| 2 | 1 | 7 | 1 | 1 | 1 |  2 |  1 |  6 |  1 | ⋯ |   1 |   1 |   4 |   2 |   1 |   1 |   1 |   1 |   2 |   2 |\n\n\n$text14\n:   \nA matrix: 2 × 235 of type int\n\n| 1 | 2 | 3 | 4 | 6 |  8 | 9 | 10 | 13 | 14 | ⋯ | 398 | 402 | 403 | 407 | 408 | 409 | 410 | 413 | 415 | 417 |\n| 1 | 3 | 3 | 1 | 1 | 10 | 1 |  4 | 69 | 20 | ⋯ |   2 |   8 |   2 |   3 |   4 |   1 |   1 |   1 |   1 |   1 |\n\n\n$text15\n:   \nA matrix: 2 × 315 of type int\n\n| 1 | 2 | 3 | 4 | 6 | 7 | 9 | 10 | 11 | 12 | ⋯ | 403 | 405 | 407 | 408 | 409 | 410 | 411 | 413 | 414 | 417 |\n| 3 | 3 | 6 | 2 | 1 | 1 | 1 |  8 |  2 |  4 | ⋯ |  24 |   2 |   2 |   1 |   1 |   4 |   1 |   1 |   1 |   1 |\n\n\n\n\n",
            "text/latex": "\\begin{description}\n\\item[\\$text1] A matrix: 2 × 263 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 4 & 5 & 6 & 8 & 10 & 12 & 13 & ⋯ & 400 & 401 & 402 & 403 & 405 & 409 & 411 & 413 & 415 & 416\\\\\n\t 1 & 1 & 4 & 1 & 1 & 1 & 3 &  8 &  1 & 12 & ⋯ &   4 &   6 &   6 &   1 &   1 &   1 &   1 &   2 &  32 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text2] A matrix: 2 × 45 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 11 & 24 & 35 & 36 & 39 & 57 & 79 & 86 & ⋯ & 318 & 319 & 351 & 355 & 358 & 365 & 376 & 387 & 390 & 399\\\\\n\t 1 & 1 &  1 &  1 &  1 &  1 &  1 &  3 &  2 &  1 & ⋯ &   1 &   1 &   1 &   1 &   1 &   2 &   1 &   1 &   1 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text3] A matrix: 2 × 266 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 4 & 6 & 8 & 9 & 10 & 11 & 12 & 13 & ⋯ & 402 & 403 & 407 & 408 & 409 & 411 & 412 & 413 & 415 & 416\\\\\n\t 3 & 2 & 5 & 3 & 5 & 1 &  2 &  1 &  2 & 15 & ⋯ &   1 &   6 &   1 &   1 &   1 &   1 &   2 &   1 &   6 &   3\\\\\n\\end{tabular}\n\n\\item[\\$text4] A matrix: 2 × 294 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 5 & 6 & 8 & 9 & 10 & 11 & 12 & ⋯ & 405 & 406 & 407 & 408 & 409 & 410 & 411 & 413 & 414 & 416\\\\\n\t 6 & 1 & 3 & 1 & 1 & 2 & 1 &  1 &  3 &  5 & ⋯ &   2 &  13 &   2 &   1 &   1 &   2 &   3 &   1 &   1 &   2\\\\\n\\end{tabular}\n\n\\item[\\$text5] A matrix: 2 × 282 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 4 & 5 & 6 & 9 & 10 & 11 & 13 & 14 & ⋯ & 404 & 405 & 407 & 408 & 409 & 410 & 412 & 415 & 416 & 417\\\\\n\t 1 & 4 & 1 & 2 & 6 & 1 &  6 &  2 & 11 &  2 & ⋯ &   1 &   1 &   2 &   1 &   1 &   2 &   1 &  15 &  14 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text6] A matrix: 2 × 304 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 &  3 & 4 & 7 & 9 & 10 & 12 & 13 & 14 & 15 & ⋯ & 405 & 406 & 407 & 408 & 409 & 410 & 411 & 413 & 414 & 416\\\\\n\t 1 & 16 & 6 & 5 & 2 &  9 &  1 & 15 &  3 &  7 & ⋯ &   2 &   2 &   1 &   1 &   1 &   1 &  10 &   1 &  13 &  10\\\\\n\\end{tabular}\n\n\\item[\\$text7] A matrix: 2 × 245 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 9 & 11 & 12 & 13 & 20 & 21 & 22 & 23 & ⋯ & 405 & 406 & 407 & 408 & 409 & 412 & 413 & 415 & 416 & 417\\\\\n\t 1 & 8 & 1 &  1 &  2 &  3 & 10 &  1 &  1 &  8 & ⋯ &   6 &   2 &   2 &   1 &   1 &   1 &   1 &   2 &   8 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text8] A matrix: 2 × 314 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & ⋯ & 407 & 408 & 409 & 410 & 411 & 412 & 413 & 414 & 415 & 416\\\\\n\t 4 & 1 & 3 & 2 & 1 & 1 & 1 & 4 & 1 &  2 & ⋯ &   1 &   2 &   1 &   7 &   5 &   3 &   1 &   3 &   1 &  10\\\\\n\\end{tabular}\n\n\\item[\\$text9] A matrix: 2 × 294 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 2 & 3 & 4 & 8 & 9 & 10 & 12 & 13 & 15 & 16 & ⋯ & 402 & 403 & 404 & 407 & 408 & 409 & 410 & 413 & 414 & 416\\\\\n\t 2 & 2 & 1 & 3 & 1 &  5 &  1 & 18 &  1 &  1 & ⋯ &   4 &   7 &   1 &   1 &   1 &   1 &   1 &   1 &   4 &  11\\\\\n\\end{tabular}\n\n\\item[\\$text10] A matrix: 2 × 316 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 &  4 & 6 & 7 & 9 & 10 & 12 & 13 & ⋯ & 407 & 408 & 409 & 410 & 412 & 413 & 414 & 415 & 416 & 417\\\\\n\t 3 & 2 & 2 & 11 & 1 & 4 & 1 &  4 &  4 & 17 & ⋯ &   1 &   1 &   1 &   1 &   1 &   1 &   9 &   3 &   7 &   4\\\\\n\\end{tabular}\n\n\\item[\\$text11] A matrix: 2 × 275 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 7 & 8 & 9 & 10 & 11 & 13 & 14 & ⋯ & 402 & 403 & 404 & 406 & 407 & 408 & 409 & 415 & 416 & 417\\\\\n\t 1 & 2 & 2 & 2 & 5 & 3 &  8 &  6 & 16 &  1 & ⋯ &   2 &   3 &   1 &   1 &   1 &   2 &   1 &  10 &  12 &   5\\\\\n\\end{tabular}\n\n\\item[\\$text12] A matrix: 2 × 81 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 5 & 6 & 7 & 13 & 15 & 19 & 22 & ⋯ & 366 & 375 & 376 & 379 & 382 & 383 & 390 & 394 & 399 & 416\\\\\n\t 1 & 1 & 1 & 1 & 1 & 4 &  1 &  2 &  1 &  1 & ⋯ &   1 &   1 &   1 &   2 &   2 &   6 &   1 &   1 &   1 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text13] A matrix: 2 × 236 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 5 & 6 & 9 & 10 & 12 & 13 & 15 & ⋯ & 396 & 398 & 401 & 407 & 408 & 409 & 411 & 412 & 413 & 416\\\\\n\t 2 & 1 & 7 & 1 & 1 & 1 &  2 &  1 &  6 &  1 & ⋯ &   1 &   1 &   4 &   2 &   1 &   1 &   1 &   1 &   2 &   2\\\\\n\\end{tabular}\n\n\\item[\\$text14] A matrix: 2 × 235 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 4 & 6 &  8 & 9 & 10 & 13 & 14 & ⋯ & 398 & 402 & 403 & 407 & 408 & 409 & 410 & 413 & 415 & 417\\\\\n\t 1 & 3 & 3 & 1 & 1 & 10 & 1 &  4 & 69 & 20 & ⋯ &   2 &   8 &   2 &   3 &   4 &   1 &   1 &   1 &   1 &   1\\\\\n\\end{tabular}\n\n\\item[\\$text15] A matrix: 2 × 315 of type int\n\\begin{tabular}{lllllllllllllllllllll}\n\t 1 & 2 & 3 & 4 & 6 & 7 & 9 & 10 & 11 & 12 & ⋯ & 403 & 405 & 407 & 408 & 409 & 410 & 411 & 413 & 414 & 417\\\\\n\t 3 & 3 & 6 & 2 & 1 & 1 & 1 &  8 &  2 &  4 & ⋯ &  24 &   2 &   2 &   1 &   1 &   4 &   1 &   1 &   1 &   1\\\\\n\\end{tabular}\n\n\\end{description}\n",
            "text/plain": [
              "$text1\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    5    6    8   10   12    13    14    16    17    18\n",
              "[2,]    1    1    4    1    1    1    3    8    1    12     2     2     2     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    19    20    21    22    23    24    27    29    30    32    33    34\n",
              "[2,]     6     6     1     7     2     1     1     2     1     1     2     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    35    36    37    38    39    40    42    43    44    45    48    51\n",
              "[2,]     5     2     2     1     3     1     2    16     1     5     3     4\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    54    55    56    59    64    66    67    70    71    72    73    75\n",
              "[2,]    72    11    37     1     6     5     1     1     3     1     3     6\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    77    78    79    80    82    87    88    90    92    93    95    96\n",
              "[2,]     4     4     1     1    10     2     1     2     3     1     1     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    97    98    99   101   102   103   104   105   106   108   109   110\n",
              "[2,]     3     4     2     1     4     1     1     1     1     3     1     2\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   111   112   115   116   117   118   119   120   121   122   123   124\n",
              "[2,]     8     5     4     1   197     2     9     3     1    10     3     1\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   129   132   133   136   137   139   141   143   145   146   147   148\n",
              "[2,]     7     1     1     5     1    12     7     7     3     1     9     2\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   149    151    153    154    156    157    158    160    161    162\n",
              "[2,]     1      5      2      5      4      5      1      1      1      3\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    163    165    167    170    171    172    179    180    182    183\n",
              "[2,]      2      2      1     16      1      1      1      5      1      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    184    185    186    194    196    198    199    205    207    208\n",
              "[2,]     16      3      1      2      1      6      4      1      1      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    211    212    213    214    215    216    217    218    219    220\n",
              "[2,]      3     10      1      1      2      2     19      2      1     10\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    224    225    227    228    229    230    231    232    233    234\n",
              "[2,]      2      1      1      1      4      2     11      2      9      2\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    235    236    237    238    242    244    245    247    249    250\n",
              "[2,]      1      2     17      1      1      1      1      1      1      7\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    251    253    254    255    256    258    260    261    262    263\n",
              "[2,]      2      1      7      1      4      4      1      1     68      3\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    264    265    268    269    270    272    273    274    275    278\n",
              "[2,]      1      1      5      1      1      1      1      9      5      7\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    280    281    283    285    286    288    290    293    294    295\n",
              "[2,]      1      7      1      1      2      1      4      1      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    296    298    299    301    302    305    306    308    310    312\n",
              "[2,]      2      4      1      2      1      1      2      1     32      8\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    314    315    316    317    319    320    323    326    327    330\n",
              "[2,]      2      5      1      6      1      1      1      8      3      2\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    332    333    334    337    338    340    341    342    343    344\n",
              "[2,]      9      4     10      3     12      1      1      1      4      2\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    345    346    347    348    349    351    352    353    359    362\n",
              "[2,]      5      7      3      7      2      1      1      1     19      7\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    363    364    365    366    367    368    369    371    372    373\n",
              "[2,]      2      1      3     46      3     13      1      4      2     52\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    375    376    377    380    381    382    384    385    386    388\n",
              "[2,]     11     23      8      2      1      4      1      2     23      5\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    389    391    394    395    399    400    401    402    403    405\n",
              "[2,]      1      1      2      1      4      4      6      6      1      1\n",
              "     [,259] [,260] [,261] [,262] [,263]\n",
              "[1,]    409    411    413    415    416\n",
              "[2,]      1      1      2     32      1\n",
              "\n",
              "$text2\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3   11   24   35   36   39   57   79    86   121   128   130   153\n",
              "[2,]    1    1    1    1    1    1    1    3    2     1     1     1     1     3\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]   155   161   164   172   184   188   194   197   204   216   218   241\n",
              "[2,]     1     1     1     3     3     2     5     1     1     1     1     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]   253   255   263   271   295   296   309   313   315   318   319   351\n",
              "[2,]     1     1     1     1     1     1     2     1     1     1     1     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45]\n",
              "[1,]   355   358   365   376   387   390   399\n",
              "[2,]     1     1     2     1     1     1     1\n",
              "\n",
              "$text3\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    4    6    8    9   10   11   12    13    15    17    18    22\n",
              "[2,]    3    2    5    3    5    1    2    1    2    15     2     2     1     2\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    23    24    25    26    27    28    31    32    33    36    37    38\n",
              "[2,]     1     2     5     2     1     2     2     2    63     4     1     8\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    39    41    42    43    45    46    48    49    52    53    57    58\n",
              "[2,]     2     2     6     6    19     5     1     1     1     1     1     2\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    59    60    62    64    66    71    72    77    79    80    81    83\n",
              "[2,]     8     1     5     2     6     2     2     4     3     2     1     3\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    84    85    87    88    89    90    91    92    93    94    95    96\n",
              "[2,]     1     1     1     1     2     1     2     5     3     1     1    41\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    97    98   100   101   102   103   106   109   111   112   114   119\n",
              "[2,]    11     1     2     1     5     2     2     1    11     3     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   120   121   122   125   127   129   132   134   138   139   140   141\n",
              "[2,]     2    28     1     1     1     1     1     2     4     5     1     2\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   144   145   147   148   152   153   155   156   157   159   160   161\n",
              "[2,]     1     1     1     9     2     1     5     2     2     1     4     6\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   163    165    166    167    169    170    171    174    176    177\n",
              "[2,]     2      2      3      3      1      6      1      3      1      2\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    178    179    181    182    183    184    186    188    190    191\n",
              "[2,]      1      1      1      3      9     15      1      9      1      6\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    194    195    196    197    198    200    201    203    204    205\n",
              "[2,]     21      1      1      1      1      1     17      1      4      2\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    208    209    210    211    212    213    214    216    218    219\n",
              "[2,]      7      2      4     13     13      1      7      5      1      2\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    220    221    222    223    224    225    226    227    228    229\n",
              "[2,]      1      3      1      4      2      2      2      3      9      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    231    232    233    234    235    237    238    240    241    242\n",
              "[2,]      1      2     10      3      1      3     18      6      1      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    243    244    245    246    249    251    253    254    256    258\n",
              "[2,]      3      1      3      1      1      8      3     23      1     25\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    259    260    261    263    264    267    268    269    270    271\n",
              "[2,]      1      8      8      2      1      1     14      1      2      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    274    276    280    281    282    283    287    291    292    294\n",
              "[2,]      5      1      4     10      1      3      1      1      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    296    297    298    299    300    302    303    304    305    308\n",
              "[2,]      5      1      5     32      5      1     11     18      2      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    310    311    312    313    315    317    318    323    325    326\n",
              "[2,]      2      1      1      3      5      1      2      2      1     44\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    327    328    330    332    333    334    335    337    338    339\n",
              "[2,]      8      8     20      4      9      6      1      1      8      3\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    340    341    342    343    344    346    347    348    350    352\n",
              "[2,]      2      1      1      3      1      1      2      2      1      4\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    353    354    355    356    358    360    366    367    368    369\n",
              "[2,]      1      1      2      1      4      1      1      9     10      1\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    372    373    376    377    378    379    380    381    382    383\n",
              "[2,]     15     15      9      1      3     12      9      2      3      7\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    385    388    391    392    393    394    395    399    402    403\n",
              "[2,]     11      1      1      1      2      5      2      2      1      6\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266]\n",
              "[1,]    407    408    409    411    412    413    415    416\n",
              "[2,]      1      1      1      1      2      1      6      3\n",
              "\n",
              "$text4\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    5    6    8    9   10   11    12    13    14    15    16\n",
              "[2,]    6    1    3    1    1    2    1    1    3     5     1     1     2     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    18    20    21    22    23    24    25    27    28    30    32    33\n",
              "[2,]     2     6     2     2     2     1     2    10     1     1     2     4\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    34    36    38    39    40    41    42    43    44    45    48    50\n",
              "[2,]     9     2     2     2     6     1     1     7     1     3     3     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    51    52    53    54    55    56    58    61    62    63    64    65\n",
              "[2,]     1     1     1    16     3     1     2     1     3     1     1     2\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    66    69    71    72    73    74    77    78    79    80    81    82\n",
              "[2,]     8     2     3     4     2     5     2     1     3     4    19     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    83    84    85    86    88    89    90    91    92    93    94    95\n",
              "[2,]     1     1     1     3     1     1     2     1     5     3     4     3\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]    96    97    98    99   101   103   106   107   108   109   110   112\n",
              "[2,]     3     7     6     1     1     1     4     6     8     2     1    17\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   113   114   115   116   118   119   120   121   122   123   124   125\n",
              "[2,]     1     4     1     1     1     1     1     1    34     1     1     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   129    134    135    136    139    140    142    143    144    145\n",
              "[2,]     2      1      1      2      1      1      1     13      1      2\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    146    147    148    150    151    154    156    158    159    160\n",
              "[2,]      1      1      1      3      2     17      1      4      5      2\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    161    163    165    166    167    168    169    171    175    176\n",
              "[2,]      1      1      1      1      1      2      7      1      5      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    178    179    184    185    186    187    188    189    191    193\n",
              "[2,]      1      1      2     17      1      1      4      5      1      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    194    196    199    200    202    203    204    205    207    208\n",
              "[2,]      6      5     18      3      2      3      2      4      5      3\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    209    210    212    214    216    218    220    221    223    224\n",
              "[2,]     12      1      7      1     10     14      6      4      4      2\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    225    226    227    228    229    230    232    233    234    235\n",
              "[2,]      6      4      6      6      1     15      3      4      1      7\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    237    240    241    244    245    246    247    248    249    250\n",
              "[2,]      1      8      2      1      3      2      2     23      1     13\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    251    253    254    255    257    258    259    260    261    262\n",
              "[2,]      8      2      1      1      1      1     10      1      1    153\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    263    265    266    267    269    270    274    275    276    277\n",
              "[2,]      2      1      3      1      2      2      6      5      1      3\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    280    282    283    284    285    286    287    289    291    292\n",
              "[2,]      4      1      8      2      3      4      7      3      1      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    293    294    295    296    298    299    301    302    303    305\n",
              "[2,]      6      1      4      4      6      1      4      7      1     10\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    306    308    310    311    312    313    314    315    316    317\n",
              "[2,]     24      3      1      1     11      3      1      9      5      1\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    318    319    320    321    325    326    327    328    331    332\n",
              "[2,]     22      1      6      1      4     14      5      2      3      7\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    333    334    335    337    338    339    340    344    345    347\n",
              "[2,]      1     11      2      1      7      1      3      7      2      1\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    348    349    350    351    352    354    355    357    358    359\n",
              "[2,]      4      1      3      5      4      1      1      1      2      3\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    360    361    362    363    364    366    368    369    370    371\n",
              "[2,]     22      2      1      3      2      1      5      4      1      6\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    373    377    378    379    380    382    383    388    389    390\n",
              "[2,]      1      1      3      2      3      8      1      6      2      1\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    391    394    395    396    397    399    405    406    407    408\n",
              "[2,]      1      9      2      1      1      1      2     13      2      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294]\n",
              "[1,]    409    410    411    413    414    416\n",
              "[2,]      1      2      3      1      1      2\n",
              "\n",
              "$text5\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    4    5    6    9   10   11   13    14    15    17    19    20\n",
              "[2,]    1    4    1    2    6    1    6    2   11     2     4     3     1     3\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    22    23    24    28    29    32    33    35    36    37    38    39\n",
              "[2,]     2     1     1     3     6     1     8     1     1     1     3     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    40    41    43    45    46    52    56    57    58    59    62    63\n",
              "[2,]     1     2    10     5     1     1     1     2     1    26     2     3\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    64    65    66    67    68    69    70    71    72    73    74    75\n",
              "[2,]     1     1    14     8     1     1     3     6     7     1     2     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    76    77    78    79    80    81    82    83    84    85    87    88\n",
              "[2,]     3     3     6     2     1    19     4     1     1     1     1     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    92    93    96    97    98    99   101   102   103   104   105   106\n",
              "[2,]     5     3     2     2     3     2     1     3     4     2     7     5\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   107   108   109   111   114   116   120   121   122   123   125   126\n",
              "[2,]     3     3     1     1     1    24     3     1     3     1     1     9\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   129   132   133   134   136   139   142   143   144   145   146   147\n",
              "[2,]     1     2    29     1     1    11     1     4     1     4     5    23\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   148    149    150    151    153    154    156    157    159    160\n",
              "[2,]     1      2      2      1     11     10      9      2      5      3\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    161    162    163    166    167    168    169    170    171    172\n",
              "[2,]      2      2      1      3      3      1      4      1      2     17\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    174    175    176    178    179    181    182    185    186    187\n",
              "[2,]      2      2      1      1      1      2      3      1      1      2\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    189    191    193    194    196    197    198    199    200    202\n",
              "[2,]     11      2      4     13      3      3      1      3      1      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    203    204    205    208    210    211    212    213    214    216\n",
              "[2,]      1      1      1      3      2      4     10      1      3      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    217    218    220    221    223    224    226    227    228    229\n",
              "[2,]     10      2     20      3      4      2      2      1     13      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    232    233    234    235    238    239    240    241    242    244\n",
              "[2,]      2      1      2      7      1      5      3      1      5      1\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    245    248    250    251    252    253    254    255    256    258\n",
              "[2,]      1      8      5      2      2      3      5      1      1     15\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    259    260    261    263    264    265    267    269    270    272\n",
              "[2,]      6      4      9      2      1      7      3      2      2      5\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    273    274    275    276    277    278    279    280    281    282\n",
              "[2,]      1      9      1      1      1      2      4      1      2      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    283    286    287    288    289    290    292    293    294    295\n",
              "[2,]      3      3      1      1      1      8      9      1      3      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    296    298    299    302    303    304    307    308    311    313\n",
              "[2,]      3      5      1      5      1      5      2      2      1      2\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    315    316    317    318    320    322    323    324    325    326\n",
              "[2,]      5     15      1      2      4      2      3      1      1     33\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    329    331    332    334    337    338    339    341    343    344\n",
              "[2,]     14      2      4     17      2      8      7      3      6      3\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    345    346    347    348    350    351    352    354    358    361\n",
              "[2,]      1      1      5      1      1      2      1      1      4      1\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    364    365    366    367    368    369    371    373    374    376\n",
              "[2,]      2      1      2     11      6      1     11     11      1     17\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    378    379    380    384    386    387    389    392    393    394\n",
              "[2,]      2      1      8      1      8      6      1      1      1      2\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    395    396    398    402    404    405    407    408    409    410\n",
              "[2,]      2      3      1      4      1      1      2      1      1      2\n",
              "     [,279] [,280] [,281] [,282]\n",
              "[1,]    412    415    416    417\n",
              "[2,]      1     15     14      1\n",
              "\n",
              "$text6\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    4    7    9   10   12   13   14    15    16    17    18    20\n",
              "[2,]    1   16    6    5    2    9    1   15    3     7     1     1     2     7\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    22    23    24    30    32    33    34    35    36    38    39    40\n",
              "[2,]     3     2     1     4     2     6     3     2    11     3     2     6\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    41    42    43    44    45    47    48    52    53    54    58    59\n",
              "[2,]     3     1    10     1     3     1     6     2     1    20     3     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    62    63    64    66    67    70    71    72    73    74    75    76\n",
              "[2,]     3     1     2     7     1     3     1     2    11     2     1     2\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    77    78    79    80    82    83    84    85    86    87    88    91\n",
              "[2,]     4     2     9     2     7     1     2     1     2     3    12     4\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    92    93    95    96    97    98    99   101   102   103   104   105\n",
              "[2,]     5     3     1     3   118     2    20     1     3     7     3     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   106   107   108   110   111   113   114   116   117   118   120   122\n",
              "[2,]    10     4     2     1     2     1    23     1     1     1     4    11\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   123   124   125   126   127   128   129   130   131   133   134   135\n",
              "[2,]     4     1     2     4     4     1     2     1    14    11     3     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   136    137    138    139    140    141    142    143    144    145\n",
              "[2,]     3      2      1     15      1      5      2      7      1     10\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    149    150    151    152    153    154    156    157    159    160\n",
              "[2,]      8      1      4      1     48      2      8      1      2      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    161    162    163    165    168    169    170    171    172    173\n",
              "[2,]      2      4      1      3      1      9      1      1      8      4\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    174    176    177    178    179    180    184    185    186    187\n",
              "[2,]      2      1      2      1      1      1      3      2      1      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    188    191    192    193    194    196    197    199    200    202\n",
              "[2,]      4      1      1      7     18      6      5      3      2     11\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    203    204    205    206    207    209    210    212    213    214\n",
              "[2,]      3      1      1      1      7      6      1      8      7      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    215    216    218    219    220    223    224    225    226    227\n",
              "[2,]      1     12      5      2      2      4      2      4     16      4\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    228    231    232    233    234    235    236    240    243    244\n",
              "[2,]      9      2      3      1      3      3      1      6      1      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    245    247    248    250    251    253    257    258    259    261\n",
              "[2,]      2      3      1      4      1      2      2      5      5      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    262    263    265    266    267    269    270    274    275    276\n",
              "[2,]      9      3      2     11     16      7      2      7      2      3\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    277    278    280    281    282    283    284    285    286    287\n",
              "[2,]      1      1      4      2      1      3      6      1      3      6\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    291    292    293    294    296    298    299    300    301    303\n",
              "[2,]      1      5      5      1      7     14      3      5      2     49\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    305    306    308    309    310    311    312    313    314    315\n",
              "[2,]      4      2      2      1      2      2      6      8      2      2\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    316    317    318    319    320    321    322    324    325    326\n",
              "[2,]      2      1      7      6      2      2      1      1      3     27\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    327    328    331    333    334    335    336    338    339    344\n",
              "[2,]      1      2      2     10     39      1      1      8     12      7\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    345    347    348    349    350    352    353    354    355    356\n",
              "[2,]      4      9      3      1      4      2      1      1      7      2\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    357    358    359    360    361    362    363    364    366    369\n",
              "[2,]      5     12      2      6      1      1      2     13     11      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    370    371    372    373    374    377    378    379    380    381\n",
              "[2,]      1      3      3      5      9      1     11      4      9      2\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    382    386    387    388    389    390    391    394    395    396\n",
              "[2,]      2      3      1     10      2      5      1      1      3      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294] [,295] [,296] [,297] [,298]\n",
              "[1,]    397    398    400    401    402    404    405    406    407    408\n",
              "[2,]     10      4      1      2      3      1      2      2      1      1\n",
              "     [,299] [,300] [,301] [,302] [,303] [,304]\n",
              "[1,]    409    410    411    413    414    416\n",
              "[2,]      1      1     10      1     13     10\n",
              "\n",
              "$text7\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    9   11   12   13   20   21   22    23    24    25    30    31\n",
              "[2,]    1    8    1    1    2    3   10    1    1     8     1    14     2     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    32    33    36    39    41    42    43    44    47    48    49    51\n",
              "[2,]     1     4    18     2     1     1     6     4     5    28     1     1\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    52    55    57    58    60    62    65    66    67    69    71    73\n",
              "[2,]     6     1    55     1     2     1     1     7     2     2     2     2\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    74    79    80    81    82    83    84    85    88    89    91    92\n",
              "[2,]     3     1     5     1     3     1     1     1     2     1     5     5\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    93    95    97    98   100   101   104   106   107   108   110   112\n",
              "[2,]     3     2    16     2     1     4     1     4     2     3     3     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]   114   116   119   120   121   122   123   126   130   133   134   136\n",
              "[2,]     2     2     1     2     7     2     3     1     1     1     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   137   139   140   142   143   144   145   146   147   151   153   159\n",
              "[2,]     1     3     3     3     1     1     7     1     1     1     3     3\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   160   163   164   165   166   168   169   171   172   173   175   176\n",
              "[2,]     1     1     1     1     1     1     1     1     2     4     5     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   177    178    179    180    181    183    184    185    186    188\n",
              "[2,]     3      1      1      1      1      3      1      3      3      3\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    189    191    192    194    195    196    197    199    201    202\n",
              "[2,]      2      1      2     11      1      1      5      4      1      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    203    204    205    207    209    212    213    214    215    218\n",
              "[2,]      1      1      1      1      5      4      2      1      2      3\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    221    222    223    224    226    227    228    229    231    232\n",
              "[2,]      1      1      4      2      2      1      1      6      1      4\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    233    235    240    242    243    244    248    251    252    253\n",
              "[2,]      4      2      4     23      2      1      1      1      2      2\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    256    257    260    261    263    267    269    270    274    275\n",
              "[2,]      2      3      1      2      3      1      1      2      2      2\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    276    278    279    281    282    283    284    285    288    289\n",
              "[2,]      1      1      9      1      2      4      2      1      1      2\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    290    291    292    293    294    295    296    297    298    299\n",
              "[2,]      1      3      1      1      1      1      6      1     13     12\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    300    301    302    306    308    309    311    312    314    315\n",
              "[2,]      4      7      1      5      2      1      1      2      1      2\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    317    319    320    321    323    324    326    328    331    334\n",
              "[2,]      1      1      1      1      1      1      4     14      2      9\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    336    338    339    342    343    347    348    351    352    354\n",
              "[2,]      1      4      4      1      8      5      4      1      1      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    355    356    358    361    363    364    366    370    371    375\n",
              "[2,]      1      1      3      1      1      2      1      1      5      4\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    376    380    382    384    385    386    387    388    389    392\n",
              "[2,]      1      3     14      1      1      3      3      1      6      1\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    393    394    395    396    400    402    403    405    406    407\n",
              "[2,]      1      1      2      1      3      1      1      6      2      2\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245]\n",
              "[1,]    408    409    412    413    415    416    417\n",
              "[2,]      1      1      1      1      2      8      1\n",
              "\n",
              "$text8\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    5    6    7    8    9    10    11    13    15    17\n",
              "[2,]    4    1    3    2    1    1    1    4    1     2     5     6     4     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    18    19    21    22    23    24    26    27    29    31    32    33\n",
              "[2,]     1     2     2     1     3     7     2     3     2     5     1     6\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    36    39    40    41    42    43    47    48    50    51    52    54\n",
              "[2,]     3     2     4     2     4    11    33     2     1     1     6    26\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    57    60    62    63    65    66    68    70    71    72    73    74\n",
              "[2,]     4     4     7     1     1    17     1     2     1     3     3     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    75    77    78    79    80    81    82    84    85    86    87    88\n",
              "[2,]     2     3     1     5     2     1     5     1     1     1     1     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    89    90    91    92    93    94    95    96    97    98    99   100\n",
              "[2,]     1     1     1     6     3     1     1     1    33     7     1     6\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   101   102   103   104   106   108   109   111   112   114   116   117\n",
              "[2,]     2    24     5     4     1     4     2     1     7     4     1     5\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   118   120   121   122   123   127   128   129   130   133   134   135\n",
              "[2,]     1     1     4     5     1     2    16     2     4     2     2     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   136    138    139    141    142    143    144    145    147    148\n",
              "[2,]    26      3      6      4      1     11      1      8      1      1\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    150    151    152    153    155    156    158    160    161    162\n",
              "[2,]      3      3      2      5      1      2      2      5      2      5\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    163    165    167    168    169    170    171    172    173    174\n",
              "[2,]      1      2     11      2     10      1      1     45      3      2\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    176    178    179    180    184    185    186    187    188    189\n",
              "[2,]      1      1      1      7      4      1      1      3      3      6\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    190    191    192    193    194    195    196    197    198    199\n",
              "[2,]      2      1      1      3     22      4      4      1      2      7\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    200    201    202    203    205    208    209    211    212    213\n",
              "[2,]      1      1      4      3      1      5      3      1      6      3\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    214    215    217    218    221    222    223    224    225    226\n",
              "[2,]      1      2      1      2      1      1      4      5      1      3\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    227    228    230    232    233    234    235    236    239    240\n",
              "[2,]      1      3      3      3      1      2      8      3      1     32\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    241    242    243    244    245    247    249    250    251    253\n",
              "[2,]      2     13      1      1      2      1      1      5      2      2\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    255    257    258    261    262    263    264    266    267    268\n",
              "[2,]      2      4      7      2     57      3      1      1      4      8\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    269    270    271    272    274    275    276    277    278    279\n",
              "[2,]      1      4      1      1      7     16      1     13      4     53\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    281    282    283    284    286    287    292    293    294    295\n",
              "[2,]      8      1      3      9      5     16      3      6      5      3\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    296    297    298    299    301    302    303    304    306    308\n",
              "[2,]     19      1     33      1      3      5      2      1      2      3\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    309    310    311    312    313    314    315    316    317    319\n",
              "[2,]      7      3      1     24      3      1     21     15      1      2\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    320    321    323    325    326    327    328    330    331    332\n",
              "[2,]      2      1      2     70     22      4      1      1      3      2\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    333    334    335    336    337    338    339    340    341    342\n",
              "[2,]      2     14      2      3      1      6      3      1      1      1\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    343    344    345    346    347    348    349    350    351    352\n",
              "[2,]      1      1      6      1      3     10      4      6      1      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    353    354    355    356    357    358    359    361    363    364\n",
              "[2,]      1      1      1      2      5      8      3      1      1      1\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    366    367    368    371    372    373    374    376    378    379\n",
              "[2,]     16      2      4      1      1     21      1      1      2      8\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294] [,295] [,296] [,297] [,298]\n",
              "[1,]    380    382    387    388    389    391    392    395    397    398\n",
              "[2,]      7      2      2      6      2      1      1      5      2      2\n",
              "     [,299] [,300] [,301] [,302] [,303] [,304] [,305] [,306] [,307] [,308]\n",
              "[1,]    399    401    402    403    404    406    407    408    409    410\n",
              "[2,]      1      1      1      1      1      2      1      2      1      7\n",
              "     [,309] [,310] [,311] [,312] [,313] [,314]\n",
              "[1,]    411    412    413    414    415    416\n",
              "[2,]      5      3      1      3      1     10\n",
              "\n",
              "$text9\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    2    3    4    8    9   10   12   13   15    16    17    19    21    22\n",
              "[2,]    2    2    1    3    1    5    1   18    1     1     2     3     1     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    23    24    25    26    27    28    29    32    33    35    36    37\n",
              "[2,]     2     1     2     2     1     9    13     2     4     1     6     4\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    38    39    42    43    46    47    48    49    51    53    55    56\n",
              "[2,]     5     3     2    12     4     2    16    10     3     2     9     8\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    57    58    59    60    63    64    66    68    69    70    72    77\n",
              "[2,]     2     1     1     6     1     1    10     2     1     1     2     4\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    78    79    80    81    84    85    87    88    91    92    93    94\n",
              "[2,]     5     2     3     2     2     1     3     3     5     5     3     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    95    97    98   100   101   102   103   104   105   106   107   109\n",
              "[2,]     1     6     5     3     1     5     3     1     3     5     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   111   112   113   114   115   116   117   118   119   120   121   122\n",
              "[2,]     8     1     1    32     8     1   135     4     1     1     1     2\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   123   125   129   130   131   132   133   134   135   136   138   139\n",
              "[2,]     2     2     2     1     1     1     1     1     1     4     1    18\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   141    142    143    144    145    147    148    149    152    153\n",
              "[2,]     6      2      7      1      1      4      4      2      1      6\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    154    155    156    157    158    159    160    161    162    163\n",
              "[2,]     19      3      6      2      2      1      1      7      1      3\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    165    166    167    169    170    171    172    174    175    176\n",
              "[2,]      1      1      1      3      1      1      1      1      1      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    177    178    179    181    183    184    186    189    190    192\n",
              "[2,]      2      1      1      1      1     29      1      5      2      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    194    195    197    200    202    203    205    206    208    210\n",
              "[2,]     20      1      1      1      1      1      2      1      2      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    211    212    213    214    216    218    219    220    221    222\n",
              "[2,]      1      7      1      1     27      1      3      5      3      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    223    224    226    227    228    229    230    231    232    234\n",
              "[2,]      4      2      2      2      1      6      1      5      3      3\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    235    237    238    239    240    241    242    243    244    245\n",
              "[2,]      2      5      6      4      4      4      8      1      1      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    247    248    252    254    255    257    258    259    261    263\n",
              "[2,]      2      1      2      6      1      1      5      7      2      2\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    264    266    269    270    273    274    275    276    277    279\n",
              "[2,]      2      1      1      2      3      6      2      1      1      8\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    281    282    283    285    286    287    288    289    290    291\n",
              "[2,]     21      1      3      5      3      1      1      1      2      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    292    294    295    296    298    299    300    304    305    307\n",
              "[2,]      3      2      1      5      6      1      1      1      6      2\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    308    309    310    311    312    314    315    317    318    319\n",
              "[2,]      1      4     21      1      7      3     15      1      4      1\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    320    322    324    326    329    330    331    332    334    336\n",
              "[2,]      2      1      4      2      1      1      3      1      7      1\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    337    338    339    340    341    343    345    346    347    348\n",
              "[2,]      1     66     11      1      4      1      2      2      6      4\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    349    350    352    353    354    356    358    359    360    361\n",
              "[2,]      3      7      2      1      1      1      1      3      6      2\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    362    364    366    367    368    371    374    375    377    378\n",
              "[2,]      6      1      3      1     10      2      1      4      4      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    379    380    381    383    385    386    387    388    390    391\n",
              "[2,]      1      2      2      1      3      2      3      3      3      2\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    392    394    395    396    397    400    402    403    404    407\n",
              "[2,]      1      1      2      1      1      1      4      7      1      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294]\n",
              "[1,]    408    409    410    413    414    416\n",
              "[2,]      1      1      1      1      4     11\n",
              "\n",
              "$text10\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    6    7    9   10   12    13    15    16    17    18\n",
              "[2,]    3    2    2   11    1    4    1    4    4    17     4     1     1     2\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    19    21    22    23    24    27    28    29    30    31    32    33\n",
              "[2,]     1     3     4     1     1     3     8    28     1     1     1     3\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    34    35    36    37    38    39    40    43    45    46    47    48\n",
              "[2,]     3     2     4     8     2     4     1     9     2     5    15     5\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    49    50    51    52    54    55    56    57    58    60    61    62\n",
              "[2,]     1     2     1     2    17    41    16     9     4     3     4     3\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    63    64    65    66    68    69    70    72    73    76    77    78\n",
              "[2,]     1     1     2     6     2     1     1     1     2     3     4     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    79    80    81    82    83    84    85    86    87    88    89    90\n",
              "[2,]     1     6     1     1     1     1     1     2    13     7     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]    92    93    94    95    96    97    99   101   102   104   106   107\n",
              "[2,]     5     3     1     2     3    20     2     1     2     3     7     4\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   109   110   111   112   113   114   115   118   120   121   122   123\n",
              "[2,]     2     2     1     2     1     5     1     2     1    21     9     1\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   124    127    129    130    131    132    133    134    137    139\n",
              "[2,]     1      2      1      1      3      1      2      2      1     17\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    141    142    143    144    146    147    148    150    153    154\n",
              "[2,]     35      2      8      2      1      2      5      1      3      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    156    159    160    161    163    164    165    166    167    168\n",
              "[2,]      3      3     13     12      1      5      1      3      2      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    170    171    172    174    175    176    177    178    179    180\n",
              "[2,]      4      1      1      1      1      1      6      1      1      1\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    181    184    186    187    188    193    194    196    197    198\n",
              "[2,]      2      5      1      4      1      6      8     13      3      9\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    199    200    202    203    204    205    206    210    212    214\n",
              "[2,]      2      4      2      6      1      1      2      2      8      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    215    216    217    218    219    220    221    222    223    224\n",
              "[2,]      1      2      8      2      1      6     14      2      4      2\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    225    226    227    228    229    230    231    232    233    234\n",
              "[2,]      1      2      1      1      2      1      3      4      4      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    235    236    237    238    239    240    241    242    244    245\n",
              "[2,]      3      6      7      2      1      3      4      4      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    247    248    249    251    253    254    255    256    258    259\n",
              "[2,]      1      1      1      1      2      5      2      1      9      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    261    262    263    264    265    267    269    270    271    272\n",
              "[2,]      3      1      2      1      4      2      3      2      1      5\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    273    274    275    276    278    279    281    282    283    286\n",
              "[2,]      1      7      2      1      2      8     14      1      3      4\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    289    290    291    292    295    296    298    299    301    302\n",
              "[2,]      1      4      3      2      1      4      6      1      2      2\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    305    307    308    309    311    312    313    315    316    317\n",
              "[2,]      5      1      1      2      1      2      1      1      6      1\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    318    319    320    322    323    326    329    331    332    333\n",
              "[2,]      5      1      1      3      3     18      3     15      9      8\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    334    335    337    338    339    340    341    343    344    345\n",
              "[2,]     11      1      3     11      1      3      2     14      4      1\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    346    347    348    350    351    352    353    354    355    356\n",
              "[2,]      1      6      3     10      1      1      3      1     17      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    357    358    360    362    363    364    366    368    369    370\n",
              "[2,]      1     19     53      4      4      3     50     12      2     11\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    371    372    373    375    376    378    379    380    381    382\n",
              "[2,]     12      1      3      7     34      6     12      6      1      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294] [,295] [,296] [,297] [,298]\n",
              "[1,]    383    384    385    386    387    388    390    391    392    393\n",
              "[2,]      9      1      3     14      2      3      1      1      3      2\n",
              "     [,299] [,300] [,301] [,302] [,303] [,304] [,305] [,306] [,307] [,308]\n",
              "[1,]    394    395    400    401    403    404    405    406    407    408\n",
              "[2,]      2      2      1      2     10      1      2      9      1      1\n",
              "     [,309] [,310] [,311] [,312] [,313] [,314] [,315] [,316]\n",
              "[1,]    409    410    412    413    414    415    416    417\n",
              "[2,]      1      1      1      1      9      3      7      4\n",
              "\n",
              "$text11\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    7    8    9   10   11   13    14    15    17    18    19\n",
              "[2,]    1    2    2    2    5    3    8    6   16     1     1     4     2     3\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    20    21    22    24    26    29    30    32    33    35    36    37\n",
              "[2,]    42     6    15     1     2     4     2     1     3     4    10     3\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    39    41    42    43    44    45    46    47    51    53    56    57\n",
              "[2,]     2     2     1     8     1     1     9     1     1     7     2    11\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    60    61    62    64    66    68    69    70    75    76    78    80\n",
              "[2,]     2     1     3     5     5     1     4     1     2     3     2     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    82    83    84    85    88    89    90    91    92    93    94    95\n",
              "[2,]     1     1     1     1     4     1     1     4     5     3     1     1\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    96    97    98    99   100   101   102   104   105   106   107   108\n",
              "[2,]     4    23     2    85     9     1     1     1     2     5     5     2\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   109   112   113   114   116   120   121   122   124   126   128   129\n",
              "[2,]     6     1     3     3     1     1    14     5     5     1    37     4\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   130   132   134   135   136   138   139   141   142   143   144   145\n",
              "[2,]     6     1     1     2     8     1    16    15     2     5     1     3\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   146    147    150    153    154    155    156    157    159    161\n",
              "[2,]     1      4      5      3      4      2      2      4      1      3\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    162    163    164    165    168    169    171    172    173    175\n",
              "[2,]      3      3      1      2      1      1      1      7      4      1\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    176    177    178    179    180    181    182    183    184    186\n",
              "[2,]      1     33      1      1      2      1      5      8      2      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    187    188    189    190    193    194    195    198    199    201\n",
              "[2,]      8      1      2      3      2     10      2      1      1      2\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    202    203    205    207    209    212    214    218    220    221\n",
              "[2,]      1      2      1      2      2      8      3     10     12      5\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    222    223    224    225    226    227    228    230    231    232\n",
              "[2,]      1      4      3      1      2      1      2      1      7      3\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    233    235    237    239    240    241    242    244    245    247\n",
              "[2,]      2      3      9      9      3      2     10      1      2      1\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    251    252    254    255    257    258    259    260    261    263\n",
              "[2,]     38      1      1      7      1      9      1      3      9      2\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    266    268    269    270    272    273    274    275    276    277\n",
              "[2,]      2      2     10      2      3      3      7      4      1      4\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    279    280    281    282    283    284    285    286    289    290\n",
              "[2,]      8      3     20      1      3     16     12      2      5      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    291    292    296    297    298    299    300    304    305    306\n",
              "[2,]      2      2      3      1      3      5      1     10      1      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    307    308    309    311    317    318    319    320    321    323\n",
              "[2,]      1      3     12      1      1      5      1      2      1      3\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    326    329    333    334    336    337    338    339    340    341\n",
              "[2,]     11      6      1      8      1      1     17     12      3      2\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    343    345    346    347    348    350    352    354    356    358\n",
              "[2,]      2      4      3      8      2      5      1      1      1      2\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    360    361    362    364    366    367    368    370    371    373\n",
              "[2,]      1      2      3      2     20      1     13      4     28      6\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    375    376    377    379    380    382    384    385    386    387\n",
              "[2,]      6      4      2     15      1      1      1      3     23      1\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    388    389    393    394    395    399    401    402    403    404\n",
              "[2,]      5      1      2      1      2      1      2      2      3      1\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275]\n",
              "[1,]    406    407    408    409    415    416    417\n",
              "[2,]      1      1      2      1     10     12      5\n",
              "\n",
              "$text12\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    5    6    7   13   15   19    22    25    28    33    34\n",
              "[2,]    1    1    1    1    1    4    1    2    1     1     2     3     1     2\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    36    39    43    59    61    67    88    89    95    97   106   108\n",
              "[2,]     1     3     1     1     1     1     1     1     1     2     1     1\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]   112   123   125   127   133   134   139   141   153   165   170   173\n",
              "[2,]     1     1     1     2     1     1     1     1     1     1     1     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]   184   194   204   206   212   214   216   220   221   228   229   231\n",
              "[2,]     4     2     1     1     1     1     3     1     2     1     2     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]   235   242   246   252   263   267   271   280   284   286   287   297\n",
              "[2,]     1     9     2     3     1     3     1     1     1     1     1     2\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]   298   299   300   313   327   328   334   347   358   366   375   376\n",
              "[2,]     3     3     3     1     1     4     5     3     1     1     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81]\n",
              "[1,]   379   382   383   390   394   399   416\n",
              "[2,]     2     2     6     1     1     1     1\n",
              "\n",
              "$text13\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    5    6    9   10   12   13    15    17    18    21    22\n",
              "[2,]    2    1    7    1    1    1    2    1    6     1     1     1     1     5\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    23    24    26    27    31    32    33    35    36    37    39    42\n",
              "[2,]     3     2     2     1     2     1     3     1     2     2     1     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    43    44    47    48    50    52    53    57    61    62    66    67\n",
              "[2,]    21     1     2     3     1     3     2     3     1     1     6     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    70    71    75    76    79    80    83    84    85    88    92    93\n",
              "[2,]     2     4     2     3     2     2     1     1     1     1     5     3\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    95    96    97    98    99   101   102   103   105   106   108   109\n",
              "[2,]     4     1    35    14     6     1     4     1     1     2    12     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]   111   112   114   115   117   118   120   122   123   125   126   127\n",
              "[2,]     1     2     3     3    54     1     1     3     2     1     1     5\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   129   131   133   134   136   137   138   139   140   143   144   145\n",
              "[2,]     1     1     1     1     2     1     3     6     1     3     1     1\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   148   149   153   155   159   160   161   163   164   165   168   170\n",
              "[2,]     1     1     4     3     1     2     3     3     1     1     1     4\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   171    172    176    177    178    179    180    184    186    188\n",
              "[2,]     1     11      1      1      1      1      4      5      1      2\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    189    190    193    194    195    196    197    199    201    202\n",
              "[2,]      3      4      1     19      1      1      3      1      4     24\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    203    204    205    206    207    208    209    212    214    216\n",
              "[2,]      2      1      3      2      2      1      1      3      1     21\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    217    218    219    223    224    225    226    227    228    232\n",
              "[2,]      1      3      1      4      2      2      2      1      2      3\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    233    234    235    236    237    238    239    240    241    242\n",
              "[2,]      1      1      1      1      7      3      1      4      1      2\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    244    246    250    251    252    254    261    262    263    268\n",
              "[2,]      1      1      3      1      6      1      1      8      2      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    269    270    274    275    276    278    279    282    283    285\n",
              "[2,]      1      2      2      1      1      3      5      1      3      1\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    287    288    291    295    296    297    298    299    307    311\n",
              "[2,]      1      2      1      1      7      1     11      5      1      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    312    313    314    315    317    318    319    323    325    326\n",
              "[2,]      3      1      1      2      1      1     14      1      1      5\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    327    328    331    334    336    339    341    342    343    345\n",
              "[2,]      5      5      4      8      2      3      2      1      3      1\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    347    348    351    352    354    355    357    358    361    364\n",
              "[2,]      3      2      1      1      1      1      1      1      1      5\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    365    366    368    369    370    371    372    376    378    379\n",
              "[2,]      2      1      4      1      1      2      1      1      1      3\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    385    387    388    390    391    392    393    395    396    398\n",
              "[2,]      1      3      4      1      3      7      2      2      1      1\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236]\n",
              "[1,]    401    407    408    409    411    412    413    416\n",
              "[2,]      4      2      1      1      1      1      2      2\n",
              "\n",
              "$text14\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    6    8    9   10   13    14    20    22    23    24\n",
              "[2,]    1    3    3    1    1   10    1    4   69    20     7     4     1     1\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    26    28    29    31    32    33    35    36    38    39    41    43\n",
              "[2,]     2     1     3     1     1     2     1     5     6     3     2     8\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    45    47    48    49    50    55    56    57    60    66    68    69\n",
              "[2,]   114     4     1     1     4     2     2    19     3     5     1     1\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    70    71    73    75    76    78    80    81    84    85    88    89\n",
              "[2,]     1     4    24     1     1     4     2     2     1     1     1     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    90    91    92    93    94    95    96    97    98    99   101   102\n",
              "[2,]     1     3     5     3     3     3   209    10     4     1     1     3\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]   103   106   107   109   110   112   114   120   121   122   124   126\n",
              "[2,]     3     6     4     1    16     9     2     1     4     3     1     1\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]   128   130   131   133   136   138   139   141   143   144   145   149\n",
              "[2,]     7     3     1     2     5     2    23    20     3     1     4     1\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   150   152   153   155   156   158   161   162   163   164   166   168\n",
              "[2,]     2     4     5     1     1     1     3     1     3    16     1     5\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   171    172    174    176    178    179    180    181    182    183\n",
              "[2,]     1      6      1      1      1      1      2      2      8      2\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    184    186    187    189    191    192    193    194    195    196\n",
              "[2,]      4      1      7      2      1      1      1     10      2      2\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    200    201    203    204    205    206    210    211    212    214\n",
              "[2,]      2      1      1      2      1      1      1      1      8      2\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    218    222    223    224    225    226    227    228    229    232\n",
              "[2,]      8      2      4      2      4      2      5      5      4      2\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    233    235    238    239    240    241    242    243    244    246\n",
              "[2,]      2      2     11      1      3      1      8      3      1      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    253    255    256    259    260    261    262    263    264    265\n",
              "[2,]      5      1      4      5      2      1     19      2      1      2\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    267    269    270    271    274    276    279    281    282    283\n",
              "[2,]      1      1      2      3      9      1     17     10      1      3\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    286    287    288    290    291    292    296    298    299    301\n",
              "[2,]      1      1      1      7      1      1     11      2      8      1\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    302    303    304    309    310    311    312    315    317    322\n",
              "[2,]      1      1     46      4      1      1      1      1      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    323    324    326    329    330    333    334    338    339    344\n",
              "[2,]     11      1    150      1      4      3      9      2      1      2\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    345    346    347    348    350    352    354    358    360    364\n",
              "[2,]      1      1      3      2      5      1      1      7      1      1\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    365    366    367    368    371    374    375    376    377    380\n",
              "[2,]      1      2     79      2      5      1      1     33      5      4\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    382    383    388    392    394    395    397    398    402    403\n",
              "[2,]      1      1      3      1      1      2      1      2      8      2\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235]\n",
              "[1,]    407    408    409    410    413    415    417\n",
              "[2,]      3      4      1      1      1      1      1\n",
              "\n",
              "$text15\n",
              "     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\n",
              "[1,]    1    2    3    4    6    7    9   10   11    12    13    14    15    16\n",
              "[2,]    3    3    6    2    1    1    1    8    2     4    10    11     1     2\n",
              "     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\n",
              "[1,]    20    21    22    23    24    25    26    29    31    32    33    34\n",
              "[2,]     9     1     2     1     4     1     2     1     1    13     3     2\n",
              "     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\n",
              "[1,]    35    36    38    39    40    42    43    44    45    46    47    48\n",
              "[2,]     4    13     4     2     2     1     8     1     4    15     3     6\n",
              "     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\n",
              "[1,]    49    50    51    52    53    54    55    56    57    60    61    63\n",
              "[2,]     1     1     1     2     2    59    12     9    18    14     1     1\n",
              "     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] [,62]\n",
              "[1,]    64    65    66    68    69    70    71    72    73    74    75    77\n",
              "[2,]    14     1     8     1     3     3    13     4     6     1     4     4\n",
              "     [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] [,74]\n",
              "[1,]    78    80    81    82    84    85    86    87    88    91    92    93\n",
              "[2,]     3     3     2     3     1     1     9     6    18     5     5     3\n",
              "     [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] [,86]\n",
              "[1,]    94    95    96    97    98    99   100   101   102   103   104   105\n",
              "[2,]     8     2     2    61     3     5     2     2    18     4     5    13\n",
              "     [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] [,98]\n",
              "[1,]   106   107   108   109   111   112   113   114   115   117   118   119\n",
              "[2,]     2     5     5     1     5     3     1     5     1     5     1     2\n",
              "     [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108]\n",
              "[1,]   120    121    122    128    129    130    131    132    133    134\n",
              "[2,]     1      5      4      6      1      2      3      1      1      1\n",
              "     [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118]\n",
              "[1,]    135    136    137    138    139    140    141    143    144    145\n",
              "[2,]      1      5      2      2     10      1     14     10      1      4\n",
              "     [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128]\n",
              "[1,]    150    151    152    153    154    155    156    158    159    160\n",
              "[2,]     10      1      1      5      2      3     20      2      1      1\n",
              "     [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138]\n",
              "[1,]    161    162    163    165    167    169    170    171    172    173\n",
              "[2,]     10      3      3      1      9     10      6      1      3      7\n",
              "     [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148]\n",
              "[1,]    176    177    181    182    184    186    187    188    190    192\n",
              "[2,]      1      3      1      1      1      1     13      5      4      1\n",
              "     [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158]\n",
              "[1,]    193    194    195    196    197    198    199    200    202    203\n",
              "[2,]      6      6      4      5      3      4     14      2      5      1\n",
              "     [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168]\n",
              "[1,]    205    209    212    213    214    215    216    217    218    221\n",
              "[2,]      1      2      8      3      1      1      4      1      3      2\n",
              "     [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178]\n",
              "[1,]    222    223    224    226    227    228    230    231    232    233\n",
              "[2,]      1      4      4      2      2      1      1      1      3      4\n",
              "     [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188]\n",
              "[1,]    235    236    238    239    240    241    242    244    245    246\n",
              "[2,]      4      2      4      3      3      1      8      1      1      1\n",
              "     [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198]\n",
              "[1,]    248    249    251    253    258    259    260    261    262    263\n",
              "[2,]     11      1      5      1      1      3     14      1     34      2\n",
              "     [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208]\n",
              "[1,]    264    266    267    268    269    270    271    272    273    274\n",
              "[2,]      6      2      1      6      2      2      1      2      2      7\n",
              "     [,209] [,210] [,211] [,212] [,213] [,214] [,215] [,216] [,217] [,218]\n",
              "[1,]    275    276    277    279    281    282    283    284    286    291\n",
              "[2,]      6      1      4     14      2      1      3      1      4      1\n",
              "     [,219] [,220] [,221] [,222] [,223] [,224] [,225] [,226] [,227] [,228]\n",
              "[1,]    294    295    296    297    298    299    301    302    303    304\n",
              "[2,]      2      2      6      1     14      2      1      1      8      3\n",
              "     [,229] [,230] [,231] [,232] [,233] [,234] [,235] [,236] [,237] [,238]\n",
              "[1,]    305    306    307    308    309    310    311    312    313    314\n",
              "[2,]      6     14      3      2      3      1      1      7     12      1\n",
              "     [,239] [,240] [,241] [,242] [,243] [,244] [,245] [,246] [,247] [,248]\n",
              "[1,]    315    316    317    318    319    320    321    322    324    325\n",
              "[2,]      7      5      1     11     22      1     11      2      3      3\n",
              "     [,249] [,250] [,251] [,252] [,253] [,254] [,255] [,256] [,257] [,258]\n",
              "[1,]    326    329    330    331    332    333    334    335    336    337\n",
              "[2,]      3      6     18      8      6      6      8      1      2      1\n",
              "     [,259] [,260] [,261] [,262] [,263] [,264] [,265] [,266] [,267] [,268]\n",
              "[1,]    338    339    341    342    345    347    348    349    350    351\n",
              "[2,]     10      2      2      2     17      4     28     19      4      2\n",
              "     [,269] [,270] [,271] [,272] [,273] [,274] [,275] [,276] [,277] [,278]\n",
              "[1,]    352    353    354    355    356    357    358    359    360    363\n",
              "[2,]      1      2      1      2      1      3     11      7      1      2\n",
              "     [,279] [,280] [,281] [,282] [,283] [,284] [,285] [,286] [,287] [,288]\n",
              "[1,]    364    365    366    369    371    372    373    374    375    378\n",
              "[2,]     11      2      7      1      3      2      2      4      7      1\n",
              "     [,289] [,290] [,291] [,292] [,293] [,294] [,295] [,296] [,297] [,298]\n",
              "[1,]    380    381    382    384    387    388    390    391    393    394\n",
              "[2,]      7      1      3      1      1     16      1      1      3      4\n",
              "     [,299] [,300] [,301] [,302] [,303] [,304] [,305] [,306] [,307] [,308]\n",
              "[1,]    395    396    397    398    400    401    402    403    405    407\n",
              "[2,]      2      1      1      1      2      1      2     24      2      2\n",
              "     [,309] [,310] [,311] [,312] [,313] [,314] [,315]\n",
              "[1,]    408    409    410    411    413    414    417\n",
              "[2,]      1      1      4      1      1      1      1\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Plot with title “UNCC Research Topics”"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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"
          },
          "metadata": {
            "image/png": {
              "width": 420,
              "height": 420
            }
          }
        }
      ],
      "source": [
        "library(stm)\n",
        "# use quanteda converter to convert our Dfm\n",
        "stmdfm <- convert(dfm, to = \"stm\", docvars = docvars(myCorpus))\n",
        "pl <- plotRemoved(stmdfm$documents, lower.thresh = seq(1,100,by=10))\n",
        "pl\n",
        "out <- prepDocuments(stmdfm$documents, stmdfm$vocab, stmdfm$meta, lower.thresh = 5)\n",
        "out\n",
        "k <- 20\n",
        "load(\"/content/stmFit.RData\")\n",
        "stmFit <- stm(out$documents, out$vocab, K = k,\n",
        "             max.em.its = 10 , data = out$meta, init.type = \"Spectral\", seed = 300)\n",
        "#save(stmFit, file = \"./stmFit.RData\")\n",
        "\n",
        "\n",
        "Res <- plot(stmFit, \n",
        "         type = \"summary\", \n",
        "         xlim = c(0,.16), \n",
        "         n = 10, \n",
        "         labeltype = \"prob\",\n",
        "         main = \"UNCC Research Topics\", \n",
        "         text.cex = 0.8)\n",
        "\n",
        "topicNames <- labelTopics(stmFit, n = 10)\n",
        "topicNames\n",
        "topic <- data.frame(\n",
        "  TopicNumber = 1:k,\n",
        "  TopicProportions = colMeans(stmFit$theta))\n",
        "topic\n",
        "\n",
        "out$documents\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zGe722FbGppN",
        "outputId": "0a033aa4-1f10-4dc4-879e-333f6bbbdc8a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A tibble: 150 × 22</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>term</th><th scope=col>topic1</th><th scope=col>topic2</th><th scope=col>topic3</th><th scope=col>topic4</th><th scope=col>topic5</th><th scope=col>topic6</th><th scope=col>topic7</th><th scope=col>topic8</th><th scope=col>topic9</th><th scope=col>⋯</th><th scope=col>topic12</th><th scope=col>topic13</th><th scope=col>topic14</th><th scope=col>topic15</th><th scope=col>topic16</th><th scope=col>topic17</th><th scope=col>topic18</th><th scope=col>topic19</th><th scope=col>topic20</th><th scope=col>log_ratio</th></tr>\n",
              "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>abstract  </td><td>9.891270e-04</td><td>2.183909e-02</td><td>8.046808e-06</td><td>6.637682e-03</td><td>6.036847e-06</td><td>1.077171e-03</td><td>8.071029e-06</td><td>1.530695e-04</td><td>6.162472e-04</td><td>⋯</td><td>1.102119e-04</td><td>1.924018e-03</td><td>2.689976e-04</td><td>9.583808e-04</td><td>1.170690e-02</td><td>2.898499e-02</td><td>1.853744e-02</td><td>1.111416e-02</td><td>1.387425e-02</td><td>  4.464613</td></tr>\n",
              "\t<tr><td>access    </td><td>1.120011e-03</td><td>2.391222e-02</td><td>1.711953e-03</td><td>6.193825e-03</td><td>8.722404e-06</td><td>7.095733e-04</td><td>2.523896e-05</td><td>1.172592e-02</td><td>3.055575e-03</td><td>⋯</td><td>8.936891e-05</td><td>1.200693e-03</td><td>1.136751e-02</td><td>2.171506e-03</td><td>7.584126e-03</td><td>2.464101e-02</td><td>4.074399e-02</td><td>2.737817e-02</td><td>3.289386e-02</td><td>  4.416163</td></tr>\n",
              "\t<tr><td>ad        </td><td>7.008042e-03</td><td>5.043826e-12</td><td>2.148814e-03</td><td>5.487457e-13</td><td>1.724233e-03</td><td>3.517472e-03</td><td>2.900027e-03</td><td>1.863381e-16</td><td>4.733777e-17</td><td>⋯</td><td>4.223413e-13</td><td>2.860197e-03</td><td>3.819004e-16</td><td>5.891280e-15</td><td>3.359898e-08</td><td>4.620764e-08</td><td>4.433034e-05</td><td>2.576116e-09</td><td>1.757723e-07</td><td>-30.371846</td></tr>\n",
              "\t<tr><td>addit     </td><td>2.818202e-03</td><td>1.401852e-07</td><td>6.693555e-03</td><td>4.732357e-08</td><td>6.800993e-04</td><td>6.487958e-03</td><td>1.077201e-03</td><td>6.914795e-03</td><td>5.576463e-03</td><td>⋯</td><td>3.139404e-03</td><td>3.081846e-03</td><td>2.100681e-03</td><td>6.077327e-03</td><td>1.709528e-06</td><td>8.510894e-07</td><td>4.164345e-06</td><td>7.519590e-06</td><td>7.008782e-06</td><td>-14.295153</td></tr>\n",
              "\t<tr><td>address   </td><td>2.030336e-12</td><td>7.448534e-03</td><td>1.353661e-44</td><td>6.197137e-17</td><td>4.158349e-06</td><td>4.409513e-03</td><td>3.340190e-03</td><td>1.090546e-15</td><td>1.321966e-03</td><td>⋯</td><td>1.656630e-25</td><td>6.748111e-30</td><td>8.039043e-26</td><td>3.299021e-10</td><td>2.190744e-04</td><td>5.100948e-05</td><td>8.407537e-04</td><td>6.037564e-04</td><td>5.438222e-02</td><td> 31.772591</td></tr>\n",
              "\t<tr><td>al        </td><td>4.959293e-02</td><td>2.594243e-05</td><td>8.723284e-03</td><td>7.642550e-03</td><td>5.579447e-06</td><td>1.218664e-02</td><td>3.652897e-03</td><td>1.043555e-02</td><td>9.920903e-03</td><td>⋯</td><td>1.277826e-02</td><td>1.803704e-02</td><td>8.846769e-03</td><td>6.283460e-03</td><td>9.834287e-05</td><td>7.495552e-04</td><td>9.761622e-04</td><td>3.656316e-04</td><td>1.152408e-04</td><td>-10.900605</td></tr>\n",
              "\t<tr><td>algorithm </td><td>1.279238e-02</td><td>1.612707e-08</td><td>9.667328e-04</td><td>5.851164e-12</td><td>7.702624e-04</td><td>6.149898e-04</td><td>1.249067e-17</td><td>2.209886e-03</td><td>1.773114e-03</td><td>⋯</td><td>1.191969e-11</td><td>2.025265e-21</td><td>1.405831e-10</td><td>8.973717e-03</td><td>2.390390e-12</td><td>3.667118e-09</td><td>5.244659e-06</td><td>2.354098e-08</td><td>7.188320e-07</td><td>-19.597369</td></tr>\n",
              "\t<tr><td>annot     </td><td>4.419111e-03</td><td>5.661054e-07</td><td>3.126208e-03</td><td>8.181356e-10</td><td>4.977265e-03</td><td>3.620148e-02</td><td>2.941685e-17</td><td>1.356005e-04</td><td>2.501730e-03</td><td>⋯</td><td>1.191836e-10</td><td>8.179991e-18</td><td>1.638960e-11</td><td>6.306197e-03</td><td>1.034064e-10</td><td>2.362144e-07</td><td>2.603015e-04</td><td>1.320177e-06</td><td>1.644091e-05</td><td>-12.930398</td></tr>\n",
              "\t<tr><td>appli     </td><td>2.822480e-03</td><td>8.203205e-06</td><td>5.067956e-03</td><td>8.439849e-03</td><td>1.611016e-03</td><td>1.251382e-02</td><td>2.661850e-04</td><td>6.401412e-05</td><td>1.708201e-03</td><td>⋯</td><td>2.723833e-03</td><td>3.345135e-07</td><td>8.724387e-03</td><td>8.248182e-04</td><td>5.331917e-06</td><td>2.024520e-04</td><td>6.931348e-04</td><td>4.255744e-05</td><td>1.351996e-04</td><td> -8.426560</td></tr>\n",
              "\t<tr><td>appropri  </td><td>4.287913e-04</td><td>2.916191e-02</td><td>2.452486e-04</td><td>3.578829e-10</td><td>2.675423e-07</td><td>3.091958e-04</td><td>4.699571e-03</td><td>2.994976e-05</td><td>4.784685e-04</td><td>⋯</td><td>3.279498e-05</td><td>3.134474e-04</td><td>1.993403e-03</td><td>1.590759e-03</td><td>1.957800e-02</td><td>3.009035e-03</td><td>7.550059e-03</td><td>1.535133e-02</td><td>1.301994e-02</td><td>  6.087666</td></tr>\n",
              "\t<tr><td>articl    </td><td>1.380737e-03</td><td>1.624354e-06</td><td>1.909452e-09</td><td>5.027072e-13</td><td>2.860082e-18</td><td>1.453109e-03</td><td>1.382428e-03</td><td>3.108460e-14</td><td>1.605046e-20</td><td>⋯</td><td>5.185167e-15</td><td>2.341135e-03</td><td>2.913642e-03</td><td>1.521318e-03</td><td>1.286733e-07</td><td>6.178286e-07</td><td>4.454018e-09</td><td>9.094123e-09</td><td>6.421294e-10</td><td> -9.731356</td></tr>\n",
              "\t<tr><td>assign    </td><td>1.738337e-03</td><td>3.188525e-08</td><td>2.204132e-06</td><td>7.048900e-08</td><td>9.007766e-17</td><td>2.936961e-03</td><td>1.447034e-03</td><td>2.066484e-16</td><td>5.605659e-03</td><td>⋯</td><td>2.225758e-02</td><td>1.409597e-02</td><td>2.012521e-13</td><td>4.321236e-09</td><td>1.325097e-04</td><td>6.902069e-05</td><td>2.123401e-06</td><td>5.852219e-08</td><td>9.346156e-09</td><td>-15.734459</td></tr>\n",
              "\t<tr><td>author    </td><td>1.216551e-03</td><td>2.655127e-07</td><td>8.583378e-11</td><td>9.327456e-03</td><td>3.436140e-03</td><td>1.447600e-05</td><td>3.812620e-03</td><td>4.595319e-03</td><td>7.707470e-03</td><td>⋯</td><td>2.136899e-03</td><td>3.170465e-03</td><td>2.411198e-12</td><td>2.069869e-03</td><td>4.019256e-05</td><td>2.997288e-04</td><td>1.483614e-04</td><td>2.255188e-05</td><td>3.301625e-05</td><td>-12.161729</td></tr>\n",
              "\t<tr><td>automat   </td><td>4.661787e-04</td><td>3.513100e-02</td><td>3.728234e-03</td><td>4.774023e-12</td><td>3.419297e-16</td><td>2.968896e-03</td><td>7.310384e-20</td><td>1.229760e-03</td><td>7.195086e-04</td><td>⋯</td><td>1.349661e-03</td><td>1.870065e-07</td><td>8.974396e-07</td><td>1.359410e-03</td><td>2.135563e-03</td><td>7.105686e-04</td><td>1.554800e-04</td><td>1.807164e-03</td><td>3.178214e-05</td><td>  6.235718</td></tr>\n",
              "\t<tr><td>avail     </td><td>1.221211e-03</td><td>3.786471e-02</td><td>4.109698e-06</td><td>6.181862e-03</td><td>2.457682e-12</td><td>7.214460e-03</td><td>4.520363e-08</td><td>7.910794e-03</td><td>3.174459e-05</td><td>⋯</td><td>5.147275e-04</td><td>6.497022e-03</td><td>1.371174e-03</td><td>7.677596e-03</td><td>1.492809e-02</td><td>6.199535e-02</td><td>2.983071e-02</td><td>1.652672e-02</td><td>2.643197e-03</td><td>  4.954470</td></tr>\n",
              "\t<tr><td>b         </td><td>4.204775e-03</td><td>1.475797e-09</td><td>3.420375e-04</td><td>2.738781e-09</td><td>1.750221e-03</td><td>3.273561e-15</td><td>1.914965e-18</td><td>2.423580e-03</td><td>2.777781e-03</td><td>⋯</td><td>1.545185e-03</td><td>5.732180e-03</td><td>7.452978e-13</td><td>3.096485e-03</td><td>1.909632e-08</td><td>3.990144e-07</td><td>1.408433e-06</td><td>7.057816e-08</td><td>8.071677e-07</td><td>-21.442103</td></tr>\n",
              "\t<tr><td>background</td><td>7.698933e-04</td><td>1.363058e-02</td><td>5.793790e-04</td><td>2.459302e-02</td><td>3.917252e-04</td><td>5.651355e-04</td><td>3.028366e-04</td><td>8.151740e-05</td><td>1.522143e-03</td><td>⋯</td><td>4.805344e-04</td><td>2.861941e-03</td><td>3.360595e-04</td><td>1.419968e-04</td><td>1.362272e-02</td><td>3.847877e-02</td><td>5.600014e-02</td><td>1.832391e-02</td><td>2.103449e-02</td><td>  4.146045</td></tr>\n",
              "\t<tr><td>becom     </td><td>1.406732e-03</td><td>2.558069e-12</td><td>7.595363e-29</td><td>1.076617e-09</td><td>7.562309e-04</td><td>1.516971e-03</td><td>1.431094e-03</td><td>2.255005e-03</td><td>1.828404e-03</td><td>⋯</td><td>6.329347e-24</td><td>6.294256e-21</td><td>4.202434e-13</td><td>7.087722e-19</td><td>5.068777e-10</td><td>2.123746e-07</td><td>1.618801e-08</td><td>5.051627e-06</td><td>6.109471e-06</td><td>-29.034645</td></tr>\n",
              "\t<tr><td>biomed    </td><td>5.611050e-03</td><td>1.255468e-05</td><td>1.248233e-02</td><td>8.166679e-03</td><td>5.493058e-03</td><td>6.187196e-03</td><td>6.957069e-03</td><td>3.645285e-03</td><td>8.982907e-03</td><td>⋯</td><td>6.494097e-03</td><td>9.737253e-03</td><td>3.853707e-02</td><td>4.708152e-03</td><td>7.029111e-05</td><td>3.727022e-04</td><td>5.252596e-04</td><td>4.449118e-04</td><td>5.668793e-04</td><td> -8.803902</td></tr>\n",
              "\t<tr><td>c         </td><td>8.515085e-02</td><td>1.761635e-07</td><td>2.583672e-03</td><td>2.535301e-08</td><td>2.231142e-03</td><td>5.441060e-07</td><td>2.643292e-16</td><td>1.848887e-03</td><td>4.091417e-03</td><td>⋯</td><td>8.976594e-04</td><td>3.676523e-15</td><td>7.780450e-10</td><td>2.590792e-03</td><td>2.234159e-08</td><td>4.866334e-06</td><td>4.215045e-05</td><td>2.434859e-06</td><td>1.507445e-05</td><td>-18.882746</td></tr>\n",
              "\t<tr><td>care      </td><td>2.459688e-03</td><td>5.620387e-05</td><td>2.121653e-10</td><td>1.859297e-06</td><td>3.613394e-16</td><td>6.871268e-04</td><td>2.514296e-02</td><td>1.833950e-05</td><td>2.095156e-16</td><td>⋯</td><td>1.138556e-02</td><td>1.552860e-03</td><td>3.350752e-03</td><td>1.211869e-03</td><td>2.237874e-04</td><td>8.938321e-04</td><td>2.818156e-10</td><td>2.098193e-05</td><td>2.922198e-07</td><td> -5.451662</td></tr>\n",
              "\t<tr><td>categori  </td><td>2.511380e-03</td><td>6.525686e-07</td><td>3.513442e-15</td><td>2.747414e-09</td><td>8.082483e-04</td><td>9.346778e-17</td><td>6.904345e-04</td><td>6.951239e-15</td><td>8.414317e-23</td><td>⋯</td><td>1.436452e-03</td><td>8.013386e-18</td><td>1.544790e-03</td><td>6.898158e-04</td><td>2.600569e-07</td><td>2.391057e-05</td><td>6.734726e-13</td><td>8.029861e-09</td><td>2.100183e-05</td><td>-11.910063</td></tr>\n",
              "\t<tr><td>classifi  </td><td>1.220839e-03</td><td>2.318718e-09</td><td>2.781941e-02</td><td>2.573913e-11</td><td>7.062520e-04</td><td>1.461470e-03</td><td>1.255792e-19</td><td>1.314125e-17</td><td>9.075287e-04</td><td>⋯</td><td>1.299924e-02</td><td>6.865868e-07</td><td>3.103685e-16</td><td>6.577172e-03</td><td>3.194381e-09</td><td>9.360424e-09</td><td>2.981869e-08</td><td>2.677354e-13</td><td>7.416383e-08</td><td>-19.006115</td></tr>\n",
              "\t<tr><td>clinic    </td><td>6.730783e-03</td><td>1.958323e-01</td><td>4.197374e-21</td><td>7.556034e-10</td><td>3.518779e-30</td><td>4.057729e-03</td><td>2.582586e-13</td><td>9.191410e-18</td><td>8.083128e-08</td><td>⋯</td><td>4.306903e-08</td><td>3.294139e-07</td><td>9.185019e-11</td><td>2.027203e-08</td><td>6.063059e-02</td><td>1.543232e-01</td><td>8.026830e-08</td><td>2.792345e-06</td><td>2.339808e-06</td><td>  4.862700</td></tr>\n",
              "\t<tr><td>code      </td><td>2.073523e-03</td><td>6.254227e-07</td><td>7.819911e-13</td><td>5.860721e-08</td><td>7.322117e-18</td><td>1.433327e-03</td><td>2.655431e-03</td><td>5.776552e-13</td><td>2.236108e-17</td><td>⋯</td><td>2.180668e-03</td><td>6.954577e-03</td><td>4.557209e-13</td><td>1.147115e-02</td><td>6.200373e-05</td><td>3.149030e-04</td><td>6.170539e-08</td><td>3.199847e-08</td><td>2.631757e-08</td><td>-11.694965</td></tr>\n",
              "\t<tr><td>common    </td><td>3.473133e-03</td><td>7.735280e-06</td><td>3.159602e-03</td><td>1.450731e-06</td><td>6.645984e-03</td><td>3.826069e-03</td><td>1.238090e-02</td><td>4.337335e-03</td><td>1.311777e-02</td><td>⋯</td><td>4.595924e-03</td><td>1.053259e-02</td><td>1.023464e-02</td><td>5.555479e-03</td><td>5.358095e-05</td><td>7.741992e-05</td><td>1.224595e-04</td><td>2.873647e-04</td><td>4.451912e-04</td><td> -8.810568</td></tr>\n",
              "\t<tr><td>comput    </td><td>2.819313e-03</td><td>6.211652e-07</td><td>1.603550e-03</td><td>6.152242e-11</td><td>2.567386e-03</td><td>7.209705e-15</td><td>2.405402e-06</td><td>4.202400e-05</td><td>5.578562e-03</td><td>⋯</td><td>6.546892e-13</td><td>2.728500e-18</td><td>7.221106e-03</td><td>1.096700e-02</td><td>2.337505e-10</td><td>2.747105e-07</td><td>6.406775e-05</td><td>4.368791e-06</td><td>5.358622e-05</td><td>-12.148079</td></tr>\n",
              "\t<tr><td>concept   </td><td>1.527003e-02</td><td>1.958033e-07</td><td>1.609858e-03</td><td>4.587695e-08</td><td>1.551003e-03</td><td>5.071337e-13</td><td>1.926047e-03</td><td>8.785473e-03</td><td>7.811333e-04</td><td>⋯</td><td>1.436969e-03</td><td>1.118669e-16</td><td>1.299925e-08</td><td>4.440545e-03</td><td>1.711155e-07</td><td>1.336094e-05</td><td>1.287919e-05</td><td>8.170237e-06</td><td>6.201736e-05</td><td>-16.250938</td></tr>\n",
              "\t<tr><td>consist   </td><td>2.534902e-03</td><td>2.012769e-09</td><td>2.837861e-03</td><td>3.373529e-09</td><td>8.270901e-04</td><td>1.567406e-03</td><td>6.090519e-04</td><td>1.431202e-03</td><td>5.652282e-03</td><td>⋯</td><td>2.361887e-03</td><td>4.879473e-03</td><td>1.436422e-10</td><td>2.276071e-03</td><td>1.566843e-07</td><td>9.141889e-08</td><td>2.644551e-08</td><td>3.189997e-07</td><td>6.931708e-07</td><td>-20.264317</td></tr>\n",
              "\t<tr><td>contain   </td><td>1.382895e-03</td><td>1.433249e-05</td><td>1.320740e-04</td><td>2.205983e-06</td><td>3.381046e-03</td><td>6.967243e-04</td><td>1.178004e-03</td><td>1.204953e-03</td><td>8.442831e-04</td><td>⋯</td><td>4.817845e-03</td><td>3.200685e-03</td><td>3.621419e-03</td><td>1.997298e-03</td><td>9.663356e-05</td><td>1.391307e-04</td><td>1.463067e-04</td><td>3.242602e-04</td><td>7.192874e-04</td><td> -6.592259</td></tr>\n",
              "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋱</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><td>requir   </td><td>2.651101e-13</td><td>5.433721e-03</td><td>7.053808e-12</td><td>2.145810e-07</td><td>1.635895e-02</td><td>3.769148e-03</td><td>9.784849e-18</td><td>5.037504e-03</td><td>1.168802e-03</td><td>⋯</td><td>2.297590e-03</td><td>3.170416e-03</td><td>4.743175e-05</td><td>7.436474e-03</td><td>3.533793e-02</td><td>7.542716e-05</td><td>1.259603e-05</td><td>5.893189e-03</td><td>4.764888e-02</td><td> 34.254630</td></tr>\n",
              "\t<tr><td>resourc  </td><td>9.563138e-26</td><td>2.991568e-02</td><td>3.196894e-06</td><td>2.042148e-08</td><td>2.130328e-08</td><td>2.640717e-04</td><td>3.530524e-07</td><td>2.306115e-03</td><td>4.363810e-20</td><td>⋯</td><td>8.668097e-06</td><td>6.345836e-07</td><td>2.583768e-02</td><td>1.693822e-02</td><td>1.231651e-02</td><td>1.631455e-05</td><td>2.517959e-04</td><td>7.876871e-03</td><td>5.553990e-03</td><td> 78.049692</td></tr>\n",
              "\t<tr><td>review   </td><td>7.780898e-03</td><td>2.583608e-07</td><td>4.528827e-04</td><td>5.217645e-08</td><td>2.797743e-14</td><td>2.102552e-03</td><td>1.393630e-03</td><td>5.804722e-16</td><td>2.738678e-03</td><td>⋯</td><td>2.157579e-03</td><td>1.574899e-15</td><td>1.291758e-03</td><td>4.963188e-16</td><td>4.160110e-06</td><td>5.245387e-05</td><td>4.980519e-04</td><td>4.278476e-07</td><td>3.997150e-08</td><td>-14.878262</td></tr>\n",
              "\t<tr><td>s        </td><td>1.073173e-01</td><td>5.905413e-06</td><td>3.209919e-03</td><td>4.677995e-06</td><td>1.162558e-02</td><td>6.919190e-03</td><td>1.568630e-02</td><td>2.030493e-02</td><td>3.064131e-02</td><td>⋯</td><td>1.361620e-02</td><td>3.087469e-05</td><td>6.546738e-03</td><td>1.484804e-05</td><td>5.490291e-05</td><td>1.609546e-04</td><td>2.289795e-03</td><td>5.131828e-04</td><td>3.988947e-04</td><td>-14.149486</td></tr>\n",
              "\t<tr><td>search   </td><td>2.729446e-03</td><td>2.574298e-08</td><td>5.038322e-04</td><td>1.640140e-14</td><td>1.271111e-11</td><td>7.019241e-18</td><td>5.191067e-05</td><td>1.532129e-12</td><td>1.162798e-16</td><td>⋯</td><td>3.583388e-10</td><td>2.613683e-04</td><td>7.210127e-16</td><td>1.601236e-02</td><td>1.686364e-08</td><td>5.831544e-07</td><td>5.474231e-04</td><td>8.443078e-10</td><td>8.052038e-10</td><td>-16.694070</td></tr>\n",
              "\t<tr><td>select   </td><td>2.071751e-03</td><td>1.317791e-07</td><td>1.888010e-03</td><td>2.237886e-07</td><td>7.299765e-04</td><td>6.575797e-04</td><td>1.172481e-03</td><td>8.356523e-03</td><td>1.544028e-16</td><td>⋯</td><td>6.275551e-03</td><td>6.968690e-17</td><td>5.809728e-11</td><td>4.368237e-03</td><td>1.093655e-07</td><td>9.925481e-06</td><td>3.003759e-03</td><td>1.473784e-06</td><td>4.679659e-06</td><td>-13.940441</td></tr>\n",
              "\t<tr><td>semant   </td><td>6.303475e-03</td><td>8.810193e-06</td><td>7.149435e-03</td><td>4.008171e-02</td><td>8.871820e-03</td><td>5.889596e-03</td><td>9.227885e-03</td><td>3.060020e-02</td><td>1.580750e-02</td><td>⋯</td><td>6.966860e-03</td><td>5.361345e-03</td><td>1.314493e-02</td><td>3.679974e-03</td><td>6.404972e-05</td><td>6.302600e-04</td><td>1.494241e-03</td><td>3.591847e-04</td><td>4.780490e-04</td><td> -9.482758</td></tr>\n",
              "\t<tr><td>sinc     </td><td>1.491130e-03</td><td>9.928983e-13</td><td>8.211691e-05</td><td>4.402393e-08</td><td>6.988457e-03</td><td>3.540485e-15</td><td>5.066808e-04</td><td>5.726458e-03</td><td>2.820657e-03</td><td>⋯</td><td>3.173610e-03</td><td>1.713077e-18</td><td>4.401057e-11</td><td>7.697566e-15</td><td>8.329998e-09</td><td>4.168947e-07</td><td>2.018334e-04</td><td>1.594476e-06</td><td>9.534243e-07</td><td>-30.484041</td></tr>\n",
              "\t<tr><td>sourc    </td><td>2.046465e-03</td><td>9.204202e-06</td><td>2.094225e-03</td><td>2.489374e-02</td><td>1.437286e-05</td><td>6.256792e-03</td><td>1.726439e-03</td><td>6.165052e-03</td><td>4.610696e-03</td><td>⋯</td><td>3.668140e-03</td><td>6.278889e-03</td><td>4.960376e-03</td><td>2.393391e-03</td><td>4.983254e-05</td><td>6.721379e-04</td><td>6.806780e-04</td><td>2.850932e-04</td><td>2.017853e-04</td><td> -7.796626</td></tr>\n",
              "\t<tr><td>specif   </td><td>1.399678e-03</td><td>4.200053e-06</td><td>5.405619e-03</td><td>6.274845e-07</td><td>3.200630e-03</td><td>1.536844e-03</td><td>6.870144e-03</td><td>1.849648e-03</td><td>8.361394e-04</td><td>⋯</td><td>2.346414e-03</td><td>1.982240e-03</td><td>2.615571e-03</td><td>2.364658e-02</td><td>3.394491e-05</td><td>2.841043e-05</td><td>1.028503e-04</td><td>1.551024e-04</td><td>2.696745e-04</td><td> -8.380472</td></tr>\n",
              "\t<tr><td>standard </td><td>3.554464e-03</td><td>4.671545e-07</td><td>4.974477e-12</td><td>8.714671e-07</td><td>2.396007e-03</td><td>3.965851e-03</td><td>4.482356e-03</td><td>2.969436e-03</td><td>8.533895e-04</td><td>⋯</td><td>7.531849e-03</td><td>7.952213e-03</td><td>1.797171e-09</td><td>2.600512e-03</td><td>4.430359e-05</td><td>9.311562e-05</td><td>5.160446e-08</td><td>4.028396e-05</td><td>1.532075e-04</td><td>-12.893445</td></tr>\n",
              "\t<tr><td>step     </td><td>5.165770e-14</td><td>1.146366e-02</td><td>1.164647e-21</td><td>3.503949e-07</td><td>5.810797e-08</td><td>7.889259e-13</td><td>1.058829e-05</td><td>5.215642e-03</td><td>1.684063e-18</td><td>⋯</td><td>1.218656e-02</td><td>2.414912e-19</td><td>2.081811e-05</td><td>3.527609e-06</td><td>3.789087e-03</td><td>8.476196e-04</td><td>2.344289e-05</td><td>2.534314e-02</td><td>9.894129e-03</td><td> 37.691222</td></tr>\n",
              "\t<tr><td>structur </td><td>1.011567e-03</td><td>1.246816e-02</td><td>1.561459e-25</td><td>5.458288e-03</td><td>2.496536e-08</td><td>8.926302e-04</td><td>4.669665e-03</td><td>9.726700e-03</td><td>3.302975e-03</td><td>⋯</td><td>1.017776e-02</td><td>9.140531e-14</td><td>7.741066e-08</td><td>5.960010e-03</td><td>1.082417e-02</td><td>1.289781e-01</td><td>2.903461e-05</td><td>1.137516e-02</td><td>2.584095e-02</td><td>  3.623585</td></tr>\n",
              "\t<tr><td>symptom  </td><td>0.000000e+00</td><td>8.132030e-02</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>⋯</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>0.000000e+00</td><td>       Inf</td></tr>\n",
              "\t<tr><td>system   </td><td>1.211972e-03</td><td>2.797292e-07</td><td>2.986720e-02</td><td>7.730213e-03</td><td>1.564045e-04</td><td>1.622899e-02</td><td>1.185282e-02</td><td>7.639730e-03</td><td>1.701542e-03</td><td>⋯</td><td>3.944566e-02</td><td>1.356433e-10</td><td>1.766462e-07</td><td>2.498503e-03</td><td>2.247524e-07</td><td>6.450466e-05</td><td>8.240585e-04</td><td>1.388125e-05</td><td>4.972751e-06</td><td>-12.081037</td></tr>\n",
              "\t<tr><td>t        </td><td>5.551403e-02</td><td>2.330568e-10</td><td>1.985560e-03</td><td>2.757272e-13</td><td>6.693228e-14</td><td>6.545120e-06</td><td>1.298768e-03</td><td>5.444586e-16</td><td>9.950903e-03</td><td>⋯</td><td>2.623400e-13</td><td>4.009014e-04</td><td>2.403538e-14</td><td>7.076270e-15</td><td>1.852922e-07</td><td>3.326125e-07</td><td>9.465422e-05</td><td>2.739388e-08</td><td>6.418931e-10</td><td>-27.827596</td></tr>\n",
              "\t<tr><td>tabl     </td><td>1.285310e-03</td><td>1.406033e-08</td><td>1.138762e-02</td><td>8.056325e-09</td><td>4.328746e-03</td><td>9.736944e-03</td><td>2.704599e-03</td><td>3.305840e-17</td><td>5.568040e-03</td><td>⋯</td><td>9.103520e-03</td><td>7.633059e-03</td><td>2.583879e-04</td><td>7.353773e-16</td><td>9.518755e-06</td><td>1.354660e-06</td><td>1.842895e-05</td><td>3.175289e-07</td><td>3.417238e-06</td><td>-16.480126</td></tr>\n",
              "\t<tr><td>task     </td><td>3.137330e-03</td><td>1.561246e-04</td><td>1.415095e-03</td><td>6.038269e-07</td><td>5.405819e-03</td><td>2.393325e-02</td><td>1.417641e-04</td><td>1.082273e-07</td><td>1.039887e-02</td><td>⋯</td><td>1.023637e-02</td><td>1.188407e-03</td><td>2.405421e-03</td><td>1.253703e-04</td><td>1.169725e-04</td><td>5.465355e-05</td><td>4.403144e-04</td><td>9.379979e-05</td><td>5.343848e-04</td><td> -4.328768</td></tr>\n",
              "\t<tr><td>text     </td><td>2.290442e-02</td><td>1.173857e-04</td><td>5.028892e-03</td><td>2.373132e-04</td><td>5.948906e-17</td><td>8.465194e-04</td><td>1.053856e-09</td><td>7.214198e-19</td><td>1.606346e-02</td><td>⋯</td><td>2.457642e-02</td><td>3.362304e-17</td><td>5.188903e-09</td><td>4.628823e-20</td><td>3.251529e-04</td><td>9.751288e-03</td><td>2.953765e-01</td><td>9.056318e-07</td><td>4.640419e-07</td><td> -7.608226</td></tr>\n",
              "\t<tr><td>therefor </td><td>3.693794e-03</td><td>1.800992e-14</td><td>6.230708e-03</td><td>4.264379e-11</td><td>8.643181e-04</td><td>1.517697e-03</td><td>1.571386e-18</td><td>6.254913e-04</td><td>1.803054e-17</td><td>⋯</td><td>1.119241e-12</td><td>2.654954e-03</td><td>1.828557e-12</td><td>7.835730e-16</td><td>1.197622e-10</td><td>1.630550e-10</td><td>5.969838e-07</td><td>2.073772e-07</td><td>2.348635e-09</td><td>-37.577521</td></tr>\n",
              "\t<tr><td>tion     </td><td>2.799772e-03</td><td>5.127146e-07</td><td>5.179386e-06</td><td>9.586946e-07</td><td>2.565223e-03</td><td>5.989842e-04</td><td>4.951837e-03</td><td>7.358868e-03</td><td>7.666712e-03</td><td>⋯</td><td>5.153927e-03</td><td>1.716977e-03</td><td>2.205865e-11</td><td>5.376250e-03</td><td>5.096748e-05</td><td>6.430693e-05</td><td>1.104739e-03</td><td>4.314718e-05</td><td>1.618196e-04</td><td>-12.414866</td></tr>\n",
              "\t<tr><td>treatment</td><td>4.081466e-25</td><td>8.341618e-03</td><td>3.006203e-11</td><td>1.184767e-07</td><td>7.806397e-22</td><td>3.489717e-04</td><td>2.365162e-04</td><td>1.173801e-06</td><td>5.348865e-03</td><td>⋯</td><td>5.110973e-05</td><td>2.139484e-03</td><td>4.936456e-03</td><td>1.443479e-07</td><td>5.309154e-02</td><td>5.456368e-05</td><td>4.789987e-08</td><td>8.885283e-03</td><td>3.507845e-05</td><td> 74.113658</td></tr>\n",
              "\t<tr><td>type     </td><td>2.123200e-03</td><td>1.198630e-06</td><td>3.607481e-03</td><td>3.334206e-07</td><td>5.207057e-03</td><td>4.006298e-03</td><td>3.876567e-03</td><td>7.505584e-03</td><td>3.934998e-16</td><td>⋯</td><td>2.293804e-03</td><td>1.925814e-03</td><td>6.562676e-03</td><td>1.289760e-02</td><td>1.568049e-05</td><td>1.725896e-05</td><td>3.461722e-05</td><td>4.182137e-05</td><td>3.990601e-05</td><td>-10.790638</td></tr>\n",
              "\t<tr><td>under    </td><td>9.428805e-39</td><td>9.865233e-03</td><td>1.171682e-10</td><td>8.158001e-03</td><td>2.474007e-09</td><td>2.315447e-17</td><td>1.432359e-22</td><td>3.344750e-03</td><td>5.831852e-25</td><td>⋯</td><td>2.914710e-09</td><td>3.357855e-03</td><td>9.886849e-04</td><td>2.756130e-07</td><td>4.836058e-03</td><td>1.851716e-05</td><td>9.752044e-08</td><td>1.067660e-02</td><td>9.645659e-03</td><td>119.654690</td></tr>\n",
              "\t<tr><td>unless   </td><td>1.402495e-03</td><td>2.165140e-06</td><td>5.562675e-04</td><td>3.940094e-07</td><td>1.620809e-03</td><td>1.559653e-03</td><td>3.641730e-03</td><td>2.027132e-03</td><td>1.818337e-03</td><td>⋯</td><td>1.547843e-03</td><td>2.078652e-03</td><td>3.496030e-03</td><td>1.267791e-03</td><td>1.452777e-05</td><td>2.121317e-05</td><td>3.817902e-05</td><td>7.758486e-05</td><td>1.238108e-04</td><td> -9.339319</td></tr>\n",
              "\t<tr><td>usag     </td><td>1.248938e-03</td><td>1.389423e-07</td><td>9.201359e-17</td><td>1.694846e-12</td><td>7.719359e-18</td><td>5.464123e-14</td><td>1.420730e-03</td><td>3.073837e-03</td><td>8.763122e-04</td><td>⋯</td><td>4.248518e-18</td><td>1.853693e-20</td><td>1.769525e-03</td><td>5.981724e-04</td><td>1.913948e-11</td><td>5.871247e-08</td><td>2.865082e-16</td><td>4.521851e-07</td><td>2.419262e-09</td><td>-13.133928</td></tr>\n",
              "\t<tr><td>valid    </td><td>5.634323e-03</td><td>2.342462e-08</td><td>4.637276e-03</td><td>2.703723e-10</td><td>2.062555e-13</td><td>1.505306e-03</td><td>5.817825e-04</td><td>2.308203e-03</td><td>3.742204e-03</td><td>⋯</td><td>1.446625e-11</td><td>3.275471e-03</td><td>6.182080e-11</td><td>1.427890e-03</td><td>3.151711e-08</td><td>1.098513e-07</td><td>2.875573e-06</td><td>1.678606e-06</td><td>4.656130e-09</td><td>-17.875857</td></tr>\n",
              "\t<tr><td>valu     </td><td>1.314954e-03</td><td>3.072823e-07</td><td>1.003235e-11</td><td>5.106381e-08</td><td>8.180532e-20</td><td>2.149304e-03</td><td>1.642292e-08</td><td>8.363430e-13</td><td>2.387654e-16</td><td>⋯</td><td>7.858560e-03</td><td>7.596886e-03</td><td>1.202411e-16</td><td>2.017054e-02</td><td>2.756694e-05</td><td>2.659602e-04</td><td>1.805244e-06</td><td>5.241958e-09</td><td>3.689306e-09</td><td>-12.063160</td></tr>\n",
              "\t<tr><td>view     </td><td>2.137289e-03</td><td>1.861859e-06</td><td>1.250499e-11</td><td>9.038059e-07</td><td>1.571591e-03</td><td>7.419902e-04</td><td>5.373367e-04</td><td>3.270751e-04</td><td>1.807505e-03</td><td>⋯</td><td>6.923461e-04</td><td>1.088518e-03</td><td>3.847237e-03</td><td>1.440582e-03</td><td>4.460264e-05</td><td>6.093482e-05</td><td>3.793833e-08</td><td>1.183248e-04</td><td>3.579252e-04</td><td>-10.164823</td></tr>\n",
              "\t<tr><td>visit    </td><td>2.854792e-03</td><td>2.164210e-06</td><td>4.855266e-11</td><td>6.607485e-07</td><td>7.448582e-04</td><td>1.570393e-03</td><td>1.437449e-03</td><td>5.028020e-04</td><td>8.788297e-04</td><td>⋯</td><td>7.055743e-04</td><td>1.130561e-03</td><td>1.799449e-03</td><td>6.167371e-04</td><td>3.092867e-05</td><td>5.634496e-05</td><td>4.870266e-08</td><td>9.697629e-05</td><td>2.095183e-04</td><td>-10.365329</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA tibble: 150 × 22\n\n| term &lt;chr&gt; | topic1 &lt;dbl&gt; | topic2 &lt;dbl&gt; | topic3 &lt;dbl&gt; | topic4 &lt;dbl&gt; | topic5 &lt;dbl&gt; | topic6 &lt;dbl&gt; | topic7 &lt;dbl&gt; | topic8 &lt;dbl&gt; | topic9 &lt;dbl&gt; | ⋯ ⋯ | topic12 &lt;dbl&gt; | topic13 &lt;dbl&gt; | topic14 &lt;dbl&gt; | topic15 &lt;dbl&gt; | topic16 &lt;dbl&gt; | topic17 &lt;dbl&gt; | topic18 &lt;dbl&gt; | topic19 &lt;dbl&gt; | topic20 &lt;dbl&gt; | log_ratio &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| abstract   | 9.891270e-04 | 2.183909e-02 | 8.046808e-06 | 6.637682e-03 | 6.036847e-06 | 1.077171e-03 | 8.071029e-06 | 1.530695e-04 | 6.162472e-04 | ⋯ | 1.102119e-04 | 1.924018e-03 | 2.689976e-04 | 9.583808e-04 | 1.170690e-02 | 2.898499e-02 | 1.853744e-02 | 1.111416e-02 | 1.387425e-02 |   4.464613 |\n| access     | 1.120011e-03 | 2.391222e-02 | 1.711953e-03 | 6.193825e-03 | 8.722404e-06 | 7.095733e-04 | 2.523896e-05 | 1.172592e-02 | 3.055575e-03 | ⋯ | 8.936891e-05 | 1.200693e-03 | 1.136751e-02 | 2.171506e-03 | 7.584126e-03 | 2.464101e-02 | 4.074399e-02 | 2.737817e-02 | 3.289386e-02 |   4.416163 |\n| ad         | 7.008042e-03 | 5.043826e-12 | 2.148814e-03 | 5.487457e-13 | 1.724233e-03 | 3.517472e-03 | 2.900027e-03 | 1.863381e-16 | 4.733777e-17 | ⋯ | 4.223413e-13 | 2.860197e-03 | 3.819004e-16 | 5.891280e-15 | 3.359898e-08 | 4.620764e-08 | 4.433034e-05 | 2.576116e-09 | 1.757723e-07 | -30.371846 |\n| addit      | 2.818202e-03 | 1.401852e-07 | 6.693555e-03 | 4.732357e-08 | 6.800993e-04 | 6.487958e-03 | 1.077201e-03 | 6.914795e-03 | 5.576463e-03 | ⋯ | 3.139404e-03 | 3.081846e-03 | 2.100681e-03 | 6.077327e-03 | 1.709528e-06 | 8.510894e-07 | 4.164345e-06 | 7.519590e-06 | 7.008782e-06 | -14.295153 |\n| address    | 2.030336e-12 | 7.448534e-03 | 1.353661e-44 | 6.197137e-17 | 4.158349e-06 | 4.409513e-03 | 3.340190e-03 | 1.090546e-15 | 1.321966e-03 | ⋯ | 1.656630e-25 | 6.748111e-30 | 8.039043e-26 | 3.299021e-10 | 2.190744e-04 | 5.100948e-05 | 8.407537e-04 | 6.037564e-04 | 5.438222e-02 |  31.772591 |\n| al         | 4.959293e-02 | 2.594243e-05 | 8.723284e-03 | 7.642550e-03 | 5.579447e-06 | 1.218664e-02 | 3.652897e-03 | 1.043555e-02 | 9.920903e-03 | ⋯ | 1.277826e-02 | 1.803704e-02 | 8.846769e-03 | 6.283460e-03 | 9.834287e-05 | 7.495552e-04 | 9.761622e-04 | 3.656316e-04 | 1.152408e-04 | -10.900605 |\n| algorithm  | 1.279238e-02 | 1.612707e-08 | 9.667328e-04 | 5.851164e-12 | 7.702624e-04 | 6.149898e-04 | 1.249067e-17 | 2.209886e-03 | 1.773114e-03 | ⋯ | 1.191969e-11 | 2.025265e-21 | 1.405831e-10 | 8.973717e-03 | 2.390390e-12 | 3.667118e-09 | 5.244659e-06 | 2.354098e-08 | 7.188320e-07 | -19.597369 |\n| annot      | 4.419111e-03 | 5.661054e-07 | 3.126208e-03 | 8.181356e-10 | 4.977265e-03 | 3.620148e-02 | 2.941685e-17 | 1.356005e-04 | 2.501730e-03 | ⋯ | 1.191836e-10 | 8.179991e-18 | 1.638960e-11 | 6.306197e-03 | 1.034064e-10 | 2.362144e-07 | 2.603015e-04 | 1.320177e-06 | 1.644091e-05 | -12.930398 |\n| appli      | 2.822480e-03 | 8.203205e-06 | 5.067956e-03 | 8.439849e-03 | 1.611016e-03 | 1.251382e-02 | 2.661850e-04 | 6.401412e-05 | 1.708201e-03 | ⋯ | 2.723833e-03 | 3.345135e-07 | 8.724387e-03 | 8.248182e-04 | 5.331917e-06 | 2.024520e-04 | 6.931348e-04 | 4.255744e-05 | 1.351996e-04 |  -8.426560 |\n| appropri   | 4.287913e-04 | 2.916191e-02 | 2.452486e-04 | 3.578829e-10 | 2.675423e-07 | 3.091958e-04 | 4.699571e-03 | 2.994976e-05 | 4.784685e-04 | ⋯ | 3.279498e-05 | 3.134474e-04 | 1.993403e-03 | 1.590759e-03 | 1.957800e-02 | 3.009035e-03 | 7.550059e-03 | 1.535133e-02 | 1.301994e-02 |   6.087666 |\n| articl     | 1.380737e-03 | 1.624354e-06 | 1.909452e-09 | 5.027072e-13 | 2.860082e-18 | 1.453109e-03 | 1.382428e-03 | 3.108460e-14 | 1.605046e-20 | ⋯ | 5.185167e-15 | 2.341135e-03 | 2.913642e-03 | 1.521318e-03 | 1.286733e-07 | 6.178286e-07 | 4.454018e-09 | 9.094123e-09 | 6.421294e-10 |  -9.731356 |\n| assign     | 1.738337e-03 | 3.188525e-08 | 2.204132e-06 | 7.048900e-08 | 9.007766e-17 | 2.936961e-03 | 1.447034e-03 | 2.066484e-16 | 5.605659e-03 | ⋯ | 2.225758e-02 | 1.409597e-02 | 2.012521e-13 | 4.321236e-09 | 1.325097e-04 | 6.902069e-05 | 2.123401e-06 | 5.852219e-08 | 9.346156e-09 | -15.734459 |\n| author     | 1.216551e-03 | 2.655127e-07 | 8.583378e-11 | 9.327456e-03 | 3.436140e-03 | 1.447600e-05 | 3.812620e-03 | 4.595319e-03 | 7.707470e-03 | ⋯ | 2.136899e-03 | 3.170465e-03 | 2.411198e-12 | 2.069869e-03 | 4.019256e-05 | 2.997288e-04 | 1.483614e-04 | 2.255188e-05 | 3.301625e-05 | -12.161729 |\n| automat    | 4.661787e-04 | 3.513100e-02 | 3.728234e-03 | 4.774023e-12 | 3.419297e-16 | 2.968896e-03 | 7.310384e-20 | 1.229760e-03 | 7.195086e-04 | ⋯ | 1.349661e-03 | 1.870065e-07 | 8.974396e-07 | 1.359410e-03 | 2.135563e-03 | 7.105686e-04 | 1.554800e-04 | 1.807164e-03 | 3.178214e-05 |   6.235718 |\n| avail      | 1.221211e-03 | 3.786471e-02 | 4.109698e-06 | 6.181862e-03 | 2.457682e-12 | 7.214460e-03 | 4.520363e-08 | 7.910794e-03 | 3.174459e-05 | ⋯ | 5.147275e-04 | 6.497022e-03 | 1.371174e-03 | 7.677596e-03 | 1.492809e-02 | 6.199535e-02 | 2.983071e-02 | 1.652672e-02 | 2.643197e-03 |   4.954470 |\n| b          | 4.204775e-03 | 1.475797e-09 | 3.420375e-04 | 2.738781e-09 | 1.750221e-03 | 3.273561e-15 | 1.914965e-18 | 2.423580e-03 | 2.777781e-03 | ⋯ | 1.545185e-03 | 5.732180e-03 | 7.452978e-13 | 3.096485e-03 | 1.909632e-08 | 3.990144e-07 | 1.408433e-06 | 7.057816e-08 | 8.071677e-07 | -21.442103 |\n| background | 7.698933e-04 | 1.363058e-02 | 5.793790e-04 | 2.459302e-02 | 3.917252e-04 | 5.651355e-04 | 3.028366e-04 | 8.151740e-05 | 1.522143e-03 | ⋯ | 4.805344e-04 | 2.861941e-03 | 3.360595e-04 | 1.419968e-04 | 1.362272e-02 | 3.847877e-02 | 5.600014e-02 | 1.832391e-02 | 2.103449e-02 |   4.146045 |\n| becom      | 1.406732e-03 | 2.558069e-12 | 7.595363e-29 | 1.076617e-09 | 7.562309e-04 | 1.516971e-03 | 1.431094e-03 | 2.255005e-03 | 1.828404e-03 | ⋯ | 6.329347e-24 | 6.294256e-21 | 4.202434e-13 | 7.087722e-19 | 5.068777e-10 | 2.123746e-07 | 1.618801e-08 | 5.051627e-06 | 6.109471e-06 | -29.034645 |\n| biomed     | 5.611050e-03 | 1.255468e-05 | 1.248233e-02 | 8.166679e-03 | 5.493058e-03 | 6.187196e-03 | 6.957069e-03 | 3.645285e-03 | 8.982907e-03 | ⋯ | 6.494097e-03 | 9.737253e-03 | 3.853707e-02 | 4.708152e-03 | 7.029111e-05 | 3.727022e-04 | 5.252596e-04 | 4.449118e-04 | 5.668793e-04 |  -8.803902 |\n| c          | 8.515085e-02 | 1.761635e-07 | 2.583672e-03 | 2.535301e-08 | 2.231142e-03 | 5.441060e-07 | 2.643292e-16 | 1.848887e-03 | 4.091417e-03 | ⋯ | 8.976594e-04 | 3.676523e-15 | 7.780450e-10 | 2.590792e-03 | 2.234159e-08 | 4.866334e-06 | 4.215045e-05 | 2.434859e-06 | 1.507445e-05 | -18.882746 |\n| care       | 2.459688e-03 | 5.620387e-05 | 2.121653e-10 | 1.859297e-06 | 3.613394e-16 | 6.871268e-04 | 2.514296e-02 | 1.833950e-05 | 2.095156e-16 | ⋯ | 1.138556e-02 | 1.552860e-03 | 3.350752e-03 | 1.211869e-03 | 2.237874e-04 | 8.938321e-04 | 2.818156e-10 | 2.098193e-05 | 2.922198e-07 |  -5.451662 |\n| categori   | 2.511380e-03 | 6.525686e-07 | 3.513442e-15 | 2.747414e-09 | 8.082483e-04 | 9.346778e-17 | 6.904345e-04 | 6.951239e-15 | 8.414317e-23 | ⋯ | 1.436452e-03 | 8.013386e-18 | 1.544790e-03 | 6.898158e-04 | 2.600569e-07 | 2.391057e-05 | 6.734726e-13 | 8.029861e-09 | 2.100183e-05 | -11.910063 |\n| classifi   | 1.220839e-03 | 2.318718e-09 | 2.781941e-02 | 2.573913e-11 | 7.062520e-04 | 1.461470e-03 | 1.255792e-19 | 1.314125e-17 | 9.075287e-04 | ⋯ | 1.299924e-02 | 6.865868e-07 | 3.103685e-16 | 6.577172e-03 | 3.194381e-09 | 9.360424e-09 | 2.981869e-08 | 2.677354e-13 | 7.416383e-08 | -19.006115 |\n| clinic     | 6.730783e-03 | 1.958323e-01 | 4.197374e-21 | 7.556034e-10 | 3.518779e-30 | 4.057729e-03 | 2.582586e-13 | 9.191410e-18 | 8.083128e-08 | ⋯ | 4.306903e-08 | 3.294139e-07 | 9.185019e-11 | 2.027203e-08 | 6.063059e-02 | 1.543232e-01 | 8.026830e-08 | 2.792345e-06 | 2.339808e-06 |   4.862700 |\n| code       | 2.073523e-03 | 6.254227e-07 | 7.819911e-13 | 5.860721e-08 | 7.322117e-18 | 1.433327e-03 | 2.655431e-03 | 5.776552e-13 | 2.236108e-17 | ⋯ | 2.180668e-03 | 6.954577e-03 | 4.557209e-13 | 1.147115e-02 | 6.200373e-05 | 3.149030e-04 | 6.170539e-08 | 3.199847e-08 | 2.631757e-08 | -11.694965 |\n| common     | 3.473133e-03 | 7.735280e-06 | 3.159602e-03 | 1.450731e-06 | 6.645984e-03 | 3.826069e-03 | 1.238090e-02 | 4.337335e-03 | 1.311777e-02 | ⋯ | 4.595924e-03 | 1.053259e-02 | 1.023464e-02 | 5.555479e-03 | 5.358095e-05 | 7.741992e-05 | 1.224595e-04 | 2.873647e-04 | 4.451912e-04 |  -8.810568 |\n| comput     | 2.819313e-03 | 6.211652e-07 | 1.603550e-03 | 6.152242e-11 | 2.567386e-03 | 7.209705e-15 | 2.405402e-06 | 4.202400e-05 | 5.578562e-03 | ⋯ | 6.546892e-13 | 2.728500e-18 | 7.221106e-03 | 1.096700e-02 | 2.337505e-10 | 2.747105e-07 | 6.406775e-05 | 4.368791e-06 | 5.358622e-05 | -12.148079 |\n| concept    | 1.527003e-02 | 1.958033e-07 | 1.609858e-03 | 4.587695e-08 | 1.551003e-03 | 5.071337e-13 | 1.926047e-03 | 8.785473e-03 | 7.811333e-04 | ⋯ | 1.436969e-03 | 1.118669e-16 | 1.299925e-08 | 4.440545e-03 | 1.711155e-07 | 1.336094e-05 | 1.287919e-05 | 8.170237e-06 | 6.201736e-05 | -16.250938 |\n| consist    | 2.534902e-03 | 2.012769e-09 | 2.837861e-03 | 3.373529e-09 | 8.270901e-04 | 1.567406e-03 | 6.090519e-04 | 1.431202e-03 | 5.652282e-03 | ⋯ | 2.361887e-03 | 4.879473e-03 | 1.436422e-10 | 2.276071e-03 | 1.566843e-07 | 9.141889e-08 | 2.644551e-08 | 3.189997e-07 | 6.931708e-07 | -20.264317 |\n| contain    | 1.382895e-03 | 1.433249e-05 | 1.320740e-04 | 2.205983e-06 | 3.381046e-03 | 6.967243e-04 | 1.178004e-03 | 1.204953e-03 | 8.442831e-04 | ⋯ | 4.817845e-03 | 3.200685e-03 | 3.621419e-03 | 1.997298e-03 | 9.663356e-05 | 1.391307e-04 | 1.463067e-04 | 3.242602e-04 | 7.192874e-04 |  -6.592259 |\n| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋱ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n| requir    | 2.651101e-13 | 5.433721e-03 | 7.053808e-12 | 2.145810e-07 | 1.635895e-02 | 3.769148e-03 | 9.784849e-18 | 5.037504e-03 | 1.168802e-03 | ⋯ | 2.297590e-03 | 3.170416e-03 | 4.743175e-05 | 7.436474e-03 | 3.533793e-02 | 7.542716e-05 | 1.259603e-05 | 5.893189e-03 | 4.764888e-02 |  34.254630 |\n| resourc   | 9.563138e-26 | 2.991568e-02 | 3.196894e-06 | 2.042148e-08 | 2.130328e-08 | 2.640717e-04 | 3.530524e-07 | 2.306115e-03 | 4.363810e-20 | ⋯ | 8.668097e-06 | 6.345836e-07 | 2.583768e-02 | 1.693822e-02 | 1.231651e-02 | 1.631455e-05 | 2.517959e-04 | 7.876871e-03 | 5.553990e-03 |  78.049692 |\n| review    | 7.780898e-03 | 2.583608e-07 | 4.528827e-04 | 5.217645e-08 | 2.797743e-14 | 2.102552e-03 | 1.393630e-03 | 5.804722e-16 | 2.738678e-03 | ⋯ | 2.157579e-03 | 1.574899e-15 | 1.291758e-03 | 4.963188e-16 | 4.160110e-06 | 5.245387e-05 | 4.980519e-04 | 4.278476e-07 | 3.997150e-08 | -14.878262 |\n| s         | 1.073173e-01 | 5.905413e-06 | 3.209919e-03 | 4.677995e-06 | 1.162558e-02 | 6.919190e-03 | 1.568630e-02 | 2.030493e-02 | 3.064131e-02 | ⋯ | 1.361620e-02 | 3.087469e-05 | 6.546738e-03 | 1.484804e-05 | 5.490291e-05 | 1.609546e-04 | 2.289795e-03 | 5.131828e-04 | 3.988947e-04 | -14.149486 |\n| search    | 2.729446e-03 | 2.574298e-08 | 5.038322e-04 | 1.640140e-14 | 1.271111e-11 | 7.019241e-18 | 5.191067e-05 | 1.532129e-12 | 1.162798e-16 | ⋯ | 3.583388e-10 | 2.613683e-04 | 7.210127e-16 | 1.601236e-02 | 1.686364e-08 | 5.831544e-07 | 5.474231e-04 | 8.443078e-10 | 8.052038e-10 | -16.694070 |\n| select    | 2.071751e-03 | 1.317791e-07 | 1.888010e-03 | 2.237886e-07 | 7.299765e-04 | 6.575797e-04 | 1.172481e-03 | 8.356523e-03 | 1.544028e-16 | ⋯ | 6.275551e-03 | 6.968690e-17 | 5.809728e-11 | 4.368237e-03 | 1.093655e-07 | 9.925481e-06 | 3.003759e-03 | 1.473784e-06 | 4.679659e-06 | -13.940441 |\n| semant    | 6.303475e-03 | 8.810193e-06 | 7.149435e-03 | 4.008171e-02 | 8.871820e-03 | 5.889596e-03 | 9.227885e-03 | 3.060020e-02 | 1.580750e-02 | ⋯ | 6.966860e-03 | 5.361345e-03 | 1.314493e-02 | 3.679974e-03 | 6.404972e-05 | 6.302600e-04 | 1.494241e-03 | 3.591847e-04 | 4.780490e-04 |  -9.482758 |\n| sinc      | 1.491130e-03 | 9.928983e-13 | 8.211691e-05 | 4.402393e-08 | 6.988457e-03 | 3.540485e-15 | 5.066808e-04 | 5.726458e-03 | 2.820657e-03 | ⋯ | 3.173610e-03 | 1.713077e-18 | 4.401057e-11 | 7.697566e-15 | 8.329998e-09 | 4.168947e-07 | 2.018334e-04 | 1.594476e-06 | 9.534243e-07 | -30.484041 |\n| sourc     | 2.046465e-03 | 9.204202e-06 | 2.094225e-03 | 2.489374e-02 | 1.437286e-05 | 6.256792e-03 | 1.726439e-03 | 6.165052e-03 | 4.610696e-03 | ⋯ | 3.668140e-03 | 6.278889e-03 | 4.960376e-03 | 2.393391e-03 | 4.983254e-05 | 6.721379e-04 | 6.806780e-04 | 2.850932e-04 | 2.017853e-04 |  -7.796626 |\n| specif    | 1.399678e-03 | 4.200053e-06 | 5.405619e-03 | 6.274845e-07 | 3.200630e-03 | 1.536844e-03 | 6.870144e-03 | 1.849648e-03 | 8.361394e-04 | ⋯ | 2.346414e-03 | 1.982240e-03 | 2.615571e-03 | 2.364658e-02 | 3.394491e-05 | 2.841043e-05 | 1.028503e-04 | 1.551024e-04 | 2.696745e-04 |  -8.380472 |\n| standard  | 3.554464e-03 | 4.671545e-07 | 4.974477e-12 | 8.714671e-07 | 2.396007e-03 | 3.965851e-03 | 4.482356e-03 | 2.969436e-03 | 8.533895e-04 | ⋯ | 7.531849e-03 | 7.952213e-03 | 1.797171e-09 | 2.600512e-03 | 4.430359e-05 | 9.311562e-05 | 5.160446e-08 | 4.028396e-05 | 1.532075e-04 | -12.893445 |\n| step      | 5.165770e-14 | 1.146366e-02 | 1.164647e-21 | 3.503949e-07 | 5.810797e-08 | 7.889259e-13 | 1.058829e-05 | 5.215642e-03 | 1.684063e-18 | ⋯ | 1.218656e-02 | 2.414912e-19 | 2.081811e-05 | 3.527609e-06 | 3.789087e-03 | 8.476196e-04 | 2.344289e-05 | 2.534314e-02 | 9.894129e-03 |  37.691222 |\n| structur  | 1.011567e-03 | 1.246816e-02 | 1.561459e-25 | 5.458288e-03 | 2.496536e-08 | 8.926302e-04 | 4.669665e-03 | 9.726700e-03 | 3.302975e-03 | ⋯ | 1.017776e-02 | 9.140531e-14 | 7.741066e-08 | 5.960010e-03 | 1.082417e-02 | 1.289781e-01 | 2.903461e-05 | 1.137516e-02 | 2.584095e-02 |   3.623585 |\n| symptom   | 0.000000e+00 | 8.132030e-02 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | ⋯ | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | 0.000000e+00 |        Inf |\n| system    | 1.211972e-03 | 2.797292e-07 | 2.986720e-02 | 7.730213e-03 | 1.564045e-04 | 1.622899e-02 | 1.185282e-02 | 7.639730e-03 | 1.701542e-03 | ⋯ | 3.944566e-02 | 1.356433e-10 | 1.766462e-07 | 2.498503e-03 | 2.247524e-07 | 6.450466e-05 | 8.240585e-04 | 1.388125e-05 | 4.972751e-06 | -12.081037 |\n| t         | 5.551403e-02 | 2.330568e-10 | 1.985560e-03 | 2.757272e-13 | 6.693228e-14 | 6.545120e-06 | 1.298768e-03 | 5.444586e-16 | 9.950903e-03 | ⋯ | 2.623400e-13 | 4.009014e-04 | 2.403538e-14 | 7.076270e-15 | 1.852922e-07 | 3.326125e-07 | 9.465422e-05 | 2.739388e-08 | 6.418931e-10 | -27.827596 |\n| tabl      | 1.285310e-03 | 1.406033e-08 | 1.138762e-02 | 8.056325e-09 | 4.328746e-03 | 9.736944e-03 | 2.704599e-03 | 3.305840e-17 | 5.568040e-03 | ⋯ | 9.103520e-03 | 7.633059e-03 | 2.583879e-04 | 7.353773e-16 | 9.518755e-06 | 1.354660e-06 | 1.842895e-05 | 3.175289e-07 | 3.417238e-06 | -16.480126 |\n| task      | 3.137330e-03 | 1.561246e-04 | 1.415095e-03 | 6.038269e-07 | 5.405819e-03 | 2.393325e-02 | 1.417641e-04 | 1.082273e-07 | 1.039887e-02 | ⋯ | 1.023637e-02 | 1.188407e-03 | 2.405421e-03 | 1.253703e-04 | 1.169725e-04 | 5.465355e-05 | 4.403144e-04 | 9.379979e-05 | 5.343848e-04 |  -4.328768 |\n| text      | 2.290442e-02 | 1.173857e-04 | 5.028892e-03 | 2.373132e-04 | 5.948906e-17 | 8.465194e-04 | 1.053856e-09 | 7.214198e-19 | 1.606346e-02 | ⋯ | 2.457642e-02 | 3.362304e-17 | 5.188903e-09 | 4.628823e-20 | 3.251529e-04 | 9.751288e-03 | 2.953765e-01 | 9.056318e-07 | 4.640419e-07 |  -7.608226 |\n| therefor  | 3.693794e-03 | 1.800992e-14 | 6.230708e-03 | 4.264379e-11 | 8.643181e-04 | 1.517697e-03 | 1.571386e-18 | 6.254913e-04 | 1.803054e-17 | ⋯ | 1.119241e-12 | 2.654954e-03 | 1.828557e-12 | 7.835730e-16 | 1.197622e-10 | 1.630550e-10 | 5.969838e-07 | 2.073772e-07 | 2.348635e-09 | -37.577521 |\n| tion      | 2.799772e-03 | 5.127146e-07 | 5.179386e-06 | 9.586946e-07 | 2.565223e-03 | 5.989842e-04 | 4.951837e-03 | 7.358868e-03 | 7.666712e-03 | ⋯ | 5.153927e-03 | 1.716977e-03 | 2.205865e-11 | 5.376250e-03 | 5.096748e-05 | 6.430693e-05 | 1.104739e-03 | 4.314718e-05 | 1.618196e-04 | -12.414866 |\n| treatment | 4.081466e-25 | 8.341618e-03 | 3.006203e-11 | 1.184767e-07 | 7.806397e-22 | 3.489717e-04 | 2.365162e-04 | 1.173801e-06 | 5.348865e-03 | ⋯ | 5.110973e-05 | 2.139484e-03 | 4.936456e-03 | 1.443479e-07 | 5.309154e-02 | 5.456368e-05 | 4.789987e-08 | 8.885283e-03 | 3.507845e-05 |  74.113658 |\n| type      | 2.123200e-03 | 1.198630e-06 | 3.607481e-03 | 3.334206e-07 | 5.207057e-03 | 4.006298e-03 | 3.876567e-03 | 7.505584e-03 | 3.934998e-16 | ⋯ | 2.293804e-03 | 1.925814e-03 | 6.562676e-03 | 1.289760e-02 | 1.568049e-05 | 1.725896e-05 | 3.461722e-05 | 4.182137e-05 | 3.990601e-05 | -10.790638 |\n| under     | 9.428805e-39 | 9.865233e-03 | 1.171682e-10 | 8.158001e-03 | 2.474007e-09 | 2.315447e-17 | 1.432359e-22 | 3.344750e-03 | 5.831852e-25 | ⋯ | 2.914710e-09 | 3.357855e-03 | 9.886849e-04 | 2.756130e-07 | 4.836058e-03 | 1.851716e-05 | 9.752044e-08 | 1.067660e-02 | 9.645659e-03 | 119.654690 |\n| unless    | 1.402495e-03 | 2.165140e-06 | 5.562675e-04 | 3.940094e-07 | 1.620809e-03 | 1.559653e-03 | 3.641730e-03 | 2.027132e-03 | 1.818337e-03 | ⋯ | 1.547843e-03 | 2.078652e-03 | 3.496030e-03 | 1.267791e-03 | 1.452777e-05 | 2.121317e-05 | 3.817902e-05 | 7.758486e-05 | 1.238108e-04 |  -9.339319 |\n| usag      | 1.248938e-03 | 1.389423e-07 | 9.201359e-17 | 1.694846e-12 | 7.719359e-18 | 5.464123e-14 | 1.420730e-03 | 3.073837e-03 | 8.763122e-04 | ⋯ | 4.248518e-18 | 1.853693e-20 | 1.769525e-03 | 5.981724e-04 | 1.913948e-11 | 5.871247e-08 | 2.865082e-16 | 4.521851e-07 | 2.419262e-09 | -13.133928 |\n| valid     | 5.634323e-03 | 2.342462e-08 | 4.637276e-03 | 2.703723e-10 | 2.062555e-13 | 1.505306e-03 | 5.817825e-04 | 2.308203e-03 | 3.742204e-03 | ⋯ | 1.446625e-11 | 3.275471e-03 | 6.182080e-11 | 1.427890e-03 | 3.151711e-08 | 1.098513e-07 | 2.875573e-06 | 1.678606e-06 | 4.656130e-09 | -17.875857 |\n| valu      | 1.314954e-03 | 3.072823e-07 | 1.003235e-11 | 5.106381e-08 | 8.180532e-20 | 2.149304e-03 | 1.642292e-08 | 8.363430e-13 | 2.387654e-16 | ⋯ | 7.858560e-03 | 7.596886e-03 | 1.202411e-16 | 2.017054e-02 | 2.756694e-05 | 2.659602e-04 | 1.805244e-06 | 5.241958e-09 | 3.689306e-09 | -12.063160 |\n| view      | 2.137289e-03 | 1.861859e-06 | 1.250499e-11 | 9.038059e-07 | 1.571591e-03 | 7.419902e-04 | 5.373367e-04 | 3.270751e-04 | 1.807505e-03 | ⋯ | 6.923461e-04 | 1.088518e-03 | 3.847237e-03 | 1.440582e-03 | 4.460264e-05 | 6.093482e-05 | 3.793833e-08 | 1.183248e-04 | 3.579252e-04 | -10.164823 |\n| visit     | 2.854792e-03 | 2.164210e-06 | 4.855266e-11 | 6.607485e-07 | 7.448582e-04 | 1.570393e-03 | 1.437449e-03 | 5.028020e-04 | 8.788297e-04 | ⋯ | 7.055743e-04 | 1.130561e-03 | 1.799449e-03 | 6.167371e-04 | 3.092867e-05 | 5.634496e-05 | 4.870266e-08 | 9.697629e-05 | 2.095183e-04 | -10.365329 |\n\n",
            "text/latex": "A tibble: 150 × 22\n\\begin{tabular}{lllllllllllllllllllll}\n term & topic1 & topic2 & topic3 & topic4 & topic5 & topic6 & topic7 & topic8 & topic9 & ⋯ & topic12 & topic13 & topic14 & topic15 & topic16 & topic17 & topic18 & topic19 & topic20 & log\\_ratio\\\\\n <chr> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t abstract   & 9.891270e-04 & 2.183909e-02 & 8.046808e-06 & 6.637682e-03 & 6.036847e-06 & 1.077171e-03 & 8.071029e-06 & 1.530695e-04 & 6.162472e-04 & ⋯ & 1.102119e-04 & 1.924018e-03 & 2.689976e-04 & 9.583808e-04 & 1.170690e-02 & 2.898499e-02 & 1.853744e-02 & 1.111416e-02 & 1.387425e-02 &   4.464613\\\\\n\t access     & 1.120011e-03 & 2.391222e-02 & 1.711953e-03 & 6.193825e-03 & 8.722404e-06 & 7.095733e-04 & 2.523896e-05 & 1.172592e-02 & 3.055575e-03 & ⋯ & 8.936891e-05 & 1.200693e-03 & 1.136751e-02 & 2.171506e-03 & 7.584126e-03 & 2.464101e-02 & 4.074399e-02 & 2.737817e-02 & 3.289386e-02 &   4.416163\\\\\n\t ad         & 7.008042e-03 & 5.043826e-12 & 2.148814e-03 & 5.487457e-13 & 1.724233e-03 & 3.517472e-03 & 2.900027e-03 & 1.863381e-16 & 4.733777e-17 & ⋯ & 4.223413e-13 & 2.860197e-03 & 3.819004e-16 & 5.891280e-15 & 3.359898e-08 & 4.620764e-08 & 4.433034e-05 & 2.576116e-09 & 1.757723e-07 & -30.371846\\\\\n\t addit      & 2.818202e-03 & 1.401852e-07 & 6.693555e-03 & 4.732357e-08 & 6.800993e-04 & 6.487958e-03 & 1.077201e-03 & 6.914795e-03 & 5.576463e-03 & ⋯ & 3.139404e-03 & 3.081846e-03 & 2.100681e-03 & 6.077327e-03 & 1.709528e-06 & 8.510894e-07 & 4.164345e-06 & 7.519590e-06 & 7.008782e-06 & -14.295153\\\\\n\t address    & 2.030336e-12 & 7.448534e-03 & 1.353661e-44 & 6.197137e-17 & 4.158349e-06 & 4.409513e-03 & 3.340190e-03 & 1.090546e-15 & 1.321966e-03 & ⋯ & 1.656630e-25 & 6.748111e-30 & 8.039043e-26 & 3.299021e-10 & 2.190744e-04 & 5.100948e-05 & 8.407537e-04 & 6.037564e-04 & 5.438222e-02 &  31.772591\\\\\n\t al         & 4.959293e-02 & 2.594243e-05 & 8.723284e-03 & 7.642550e-03 & 5.579447e-06 & 1.218664e-02 & 3.652897e-03 & 1.043555e-02 & 9.920903e-03 & ⋯ & 1.277826e-02 & 1.803704e-02 & 8.846769e-03 & 6.283460e-03 & 9.834287e-05 & 7.495552e-04 & 9.761622e-04 & 3.656316e-04 & 1.152408e-04 & -10.900605\\\\\n\t algorithm  & 1.279238e-02 & 1.612707e-08 & 9.667328e-04 & 5.851164e-12 & 7.702624e-04 & 6.149898e-04 & 1.249067e-17 & 2.209886e-03 & 1.773114e-03 & ⋯ & 1.191969e-11 & 2.025265e-21 & 1.405831e-10 & 8.973717e-03 & 2.390390e-12 & 3.667118e-09 & 5.244659e-06 & 2.354098e-08 & 7.188320e-07 & -19.597369\\\\\n\t annot      & 4.419111e-03 & 5.661054e-07 & 3.126208e-03 & 8.181356e-10 & 4.977265e-03 & 3.620148e-02 & 2.941685e-17 & 1.356005e-04 & 2.501730e-03 & ⋯ & 1.191836e-10 & 8.179991e-18 & 1.638960e-11 & 6.306197e-03 & 1.034064e-10 & 2.362144e-07 & 2.603015e-04 & 1.320177e-06 & 1.644091e-05 & -12.930398\\\\\n\t appli      & 2.822480e-03 & 8.203205e-06 & 5.067956e-03 & 8.439849e-03 & 1.611016e-03 & 1.251382e-02 & 2.661850e-04 & 6.401412e-05 & 1.708201e-03 & ⋯ & 2.723833e-03 & 3.345135e-07 & 8.724387e-03 & 8.248182e-04 & 5.331917e-06 & 2.024520e-04 & 6.931348e-04 & 4.255744e-05 & 1.351996e-04 &  -8.426560\\\\\n\t appropri   & 4.287913e-04 & 2.916191e-02 & 2.452486e-04 & 3.578829e-10 & 2.675423e-07 & 3.091958e-04 & 4.699571e-03 & 2.994976e-05 & 4.784685e-04 & ⋯ & 3.279498e-05 & 3.134474e-04 & 1.993403e-03 & 1.590759e-03 & 1.957800e-02 & 3.009035e-03 & 7.550059e-03 & 1.535133e-02 & 1.301994e-02 &   6.087666\\\\\n\t articl     & 1.380737e-03 & 1.624354e-06 & 1.909452e-09 & 5.027072e-13 & 2.860082e-18 & 1.453109e-03 & 1.382428e-03 & 3.108460e-14 & 1.605046e-20 & ⋯ & 5.185167e-15 & 2.341135e-03 & 2.913642e-03 & 1.521318e-03 & 1.286733e-07 & 6.178286e-07 & 4.454018e-09 & 9.094123e-09 & 6.421294e-10 &  -9.731356\\\\\n\t assign     & 1.738337e-03 & 3.188525e-08 & 2.204132e-06 & 7.048900e-08 & 9.007766e-17 & 2.936961e-03 & 1.447034e-03 & 2.066484e-16 & 5.605659e-03 & ⋯ & 2.225758e-02 & 1.409597e-02 & 2.012521e-13 & 4.321236e-09 & 1.325097e-04 & 6.902069e-05 & 2.123401e-06 & 5.852219e-08 & 9.346156e-09 & -15.734459\\\\\n\t author     & 1.216551e-03 & 2.655127e-07 & 8.583378e-11 & 9.327456e-03 & 3.436140e-03 & 1.447600e-05 & 3.812620e-03 & 4.595319e-03 & 7.707470e-03 & ⋯ & 2.136899e-03 & 3.170465e-03 & 2.411198e-12 & 2.069869e-03 & 4.019256e-05 & 2.997288e-04 & 1.483614e-04 & 2.255188e-05 & 3.301625e-05 & -12.161729\\\\\n\t automat    & 4.661787e-04 & 3.513100e-02 & 3.728234e-03 & 4.774023e-12 & 3.419297e-16 & 2.968896e-03 & 7.310384e-20 & 1.229760e-03 & 7.195086e-04 & ⋯ & 1.349661e-03 & 1.870065e-07 & 8.974396e-07 & 1.359410e-03 & 2.135563e-03 & 7.105686e-04 & 1.554800e-04 & 1.807164e-03 & 3.178214e-05 &   6.235718\\\\\n\t avail      & 1.221211e-03 & 3.786471e-02 & 4.109698e-06 & 6.181862e-03 & 2.457682e-12 & 7.214460e-03 & 4.520363e-08 & 7.910794e-03 & 3.174459e-05 & ⋯ & 5.147275e-04 & 6.497022e-03 & 1.371174e-03 & 7.677596e-03 & 1.492809e-02 & 6.199535e-02 & 2.983071e-02 & 1.652672e-02 & 2.643197e-03 &   4.954470\\\\\n\t b          & 4.204775e-03 & 1.475797e-09 & 3.420375e-04 & 2.738781e-09 & 1.750221e-03 & 3.273561e-15 & 1.914965e-18 & 2.423580e-03 & 2.777781e-03 & ⋯ & 1.545185e-03 & 5.732180e-03 & 7.452978e-13 & 3.096485e-03 & 1.909632e-08 & 3.990144e-07 & 1.408433e-06 & 7.057816e-08 & 8.071677e-07 & -21.442103\\\\\n\t background & 7.698933e-04 & 1.363058e-02 & 5.793790e-04 & 2.459302e-02 & 3.917252e-04 & 5.651355e-04 & 3.028366e-04 & 8.151740e-05 & 1.522143e-03 & ⋯ & 4.805344e-04 & 2.861941e-03 & 3.360595e-04 & 1.419968e-04 & 1.362272e-02 & 3.847877e-02 & 5.600014e-02 & 1.832391e-02 & 2.103449e-02 &   4.146045\\\\\n\t becom      & 1.406732e-03 & 2.558069e-12 & 7.595363e-29 & 1.076617e-09 & 7.562309e-04 & 1.516971e-03 & 1.431094e-03 & 2.255005e-03 & 1.828404e-03 & ⋯ & 6.329347e-24 & 6.294256e-21 & 4.202434e-13 & 7.087722e-19 & 5.068777e-10 & 2.123746e-07 & 1.618801e-08 & 5.051627e-06 & 6.109471e-06 & -29.034645\\\\\n\t biomed     & 5.611050e-03 & 1.255468e-05 & 1.248233e-02 & 8.166679e-03 & 5.493058e-03 & 6.187196e-03 & 6.957069e-03 & 3.645285e-03 & 8.982907e-03 & ⋯ & 6.494097e-03 & 9.737253e-03 & 3.853707e-02 & 4.708152e-03 & 7.029111e-05 & 3.727022e-04 & 5.252596e-04 & 4.449118e-04 & 5.668793e-04 &  -8.803902\\\\\n\t c          & 8.515085e-02 & 1.761635e-07 & 2.583672e-03 & 2.535301e-08 & 2.231142e-03 & 5.441060e-07 & 2.643292e-16 & 1.848887e-03 & 4.091417e-03 & ⋯ & 8.976594e-04 & 3.676523e-15 & 7.780450e-10 & 2.590792e-03 & 2.234159e-08 & 4.866334e-06 & 4.215045e-05 & 2.434859e-06 & 1.507445e-05 & -18.882746\\\\\n\t care       & 2.459688e-03 & 5.620387e-05 & 2.121653e-10 & 1.859297e-06 & 3.613394e-16 & 6.871268e-04 & 2.514296e-02 & 1.833950e-05 & 2.095156e-16 & ⋯ & 1.138556e-02 & 1.552860e-03 & 3.350752e-03 & 1.211869e-03 & 2.237874e-04 & 8.938321e-04 & 2.818156e-10 & 2.098193e-05 & 2.922198e-07 &  -5.451662\\\\\n\t categori   & 2.511380e-03 & 6.525686e-07 & 3.513442e-15 & 2.747414e-09 & 8.082483e-04 & 9.346778e-17 & 6.904345e-04 & 6.951239e-15 & 8.414317e-23 & ⋯ & 1.436452e-03 & 8.013386e-18 & 1.544790e-03 & 6.898158e-04 & 2.600569e-07 & 2.391057e-05 & 6.734726e-13 & 8.029861e-09 & 2.100183e-05 & -11.910063\\\\\n\t classifi   & 1.220839e-03 & 2.318718e-09 & 2.781941e-02 & 2.573913e-11 & 7.062520e-04 & 1.461470e-03 & 1.255792e-19 & 1.314125e-17 & 9.075287e-04 & ⋯ & 1.299924e-02 & 6.865868e-07 & 3.103685e-16 & 6.577172e-03 & 3.194381e-09 & 9.360424e-09 & 2.981869e-08 & 2.677354e-13 & 7.416383e-08 & -19.006115\\\\\n\t clinic     & 6.730783e-03 & 1.958323e-01 & 4.197374e-21 & 7.556034e-10 & 3.518779e-30 & 4.057729e-03 & 2.582586e-13 & 9.191410e-18 & 8.083128e-08 & ⋯ & 4.306903e-08 & 3.294139e-07 & 9.185019e-11 & 2.027203e-08 & 6.063059e-02 & 1.543232e-01 & 8.026830e-08 & 2.792345e-06 & 2.339808e-06 &   4.862700\\\\\n\t code       & 2.073523e-03 & 6.254227e-07 & 7.819911e-13 & 5.860721e-08 & 7.322117e-18 & 1.433327e-03 & 2.655431e-03 & 5.776552e-13 & 2.236108e-17 & ⋯ & 2.180668e-03 & 6.954577e-03 & 4.557209e-13 & 1.147115e-02 & 6.200373e-05 & 3.149030e-04 & 6.170539e-08 & 3.199847e-08 & 2.631757e-08 & -11.694965\\\\\n\t common     & 3.473133e-03 & 7.735280e-06 & 3.159602e-03 & 1.450731e-06 & 6.645984e-03 & 3.826069e-03 & 1.238090e-02 & 4.337335e-03 & 1.311777e-02 & ⋯ & 4.595924e-03 & 1.053259e-02 & 1.023464e-02 & 5.555479e-03 & 5.358095e-05 & 7.741992e-05 & 1.224595e-04 & 2.873647e-04 & 4.451912e-04 &  -8.810568\\\\\n\t comput     & 2.819313e-03 & 6.211652e-07 & 1.603550e-03 & 6.152242e-11 & 2.567386e-03 & 7.209705e-15 & 2.405402e-06 & 4.202400e-05 & 5.578562e-03 & ⋯ & 6.546892e-13 & 2.728500e-18 & 7.221106e-03 & 1.096700e-02 & 2.337505e-10 & 2.747105e-07 & 6.406775e-05 & 4.368791e-06 & 5.358622e-05 & -12.148079\\\\\n\t concept    & 1.527003e-02 & 1.958033e-07 & 1.609858e-03 & 4.587695e-08 & 1.551003e-03 & 5.071337e-13 & 1.926047e-03 & 8.785473e-03 & 7.811333e-04 & ⋯ & 1.436969e-03 & 1.118669e-16 & 1.299925e-08 & 4.440545e-03 & 1.711155e-07 & 1.336094e-05 & 1.287919e-05 & 8.170237e-06 & 6.201736e-05 & -16.250938\\\\\n\t consist    & 2.534902e-03 & 2.012769e-09 & 2.837861e-03 & 3.373529e-09 & 8.270901e-04 & 1.567406e-03 & 6.090519e-04 & 1.431202e-03 & 5.652282e-03 & ⋯ & 2.361887e-03 & 4.879473e-03 & 1.436422e-10 & 2.276071e-03 & 1.566843e-07 & 9.141889e-08 & 2.644551e-08 & 3.189997e-07 & 6.931708e-07 & -20.264317\\\\\n\t contain    & 1.382895e-03 & 1.433249e-05 & 1.320740e-04 & 2.205983e-06 & 3.381046e-03 & 6.967243e-04 & 1.178004e-03 & 1.204953e-03 & 8.442831e-04 & ⋯ & 4.817845e-03 & 3.200685e-03 & 3.621419e-03 & 1.997298e-03 & 9.663356e-05 & 1.391307e-04 & 1.463067e-04 & 3.242602e-04 & 7.192874e-04 &  -6.592259\\\\\n\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋱ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n\t requir    & 2.651101e-13 & 5.433721e-03 & 7.053808e-12 & 2.145810e-07 & 1.635895e-02 & 3.769148e-03 & 9.784849e-18 & 5.037504e-03 & 1.168802e-03 & ⋯ & 2.297590e-03 & 3.170416e-03 & 4.743175e-05 & 7.436474e-03 & 3.533793e-02 & 7.542716e-05 & 1.259603e-05 & 5.893189e-03 & 4.764888e-02 &  34.254630\\\\\n\t resourc   & 9.563138e-26 & 2.991568e-02 & 3.196894e-06 & 2.042148e-08 & 2.130328e-08 & 2.640717e-04 & 3.530524e-07 & 2.306115e-03 & 4.363810e-20 & ⋯ & 8.668097e-06 & 6.345836e-07 & 2.583768e-02 & 1.693822e-02 & 1.231651e-02 & 1.631455e-05 & 2.517959e-04 & 7.876871e-03 & 5.553990e-03 &  78.049692\\\\\n\t review    & 7.780898e-03 & 2.583608e-07 & 4.528827e-04 & 5.217645e-08 & 2.797743e-14 & 2.102552e-03 & 1.393630e-03 & 5.804722e-16 & 2.738678e-03 & ⋯ & 2.157579e-03 & 1.574899e-15 & 1.291758e-03 & 4.963188e-16 & 4.160110e-06 & 5.245387e-05 & 4.980519e-04 & 4.278476e-07 & 3.997150e-08 & -14.878262\\\\\n\t s         & 1.073173e-01 & 5.905413e-06 & 3.209919e-03 & 4.677995e-06 & 1.162558e-02 & 6.919190e-03 & 1.568630e-02 & 2.030493e-02 & 3.064131e-02 & ⋯ & 1.361620e-02 & 3.087469e-05 & 6.546738e-03 & 1.484804e-05 & 5.490291e-05 & 1.609546e-04 & 2.289795e-03 & 5.131828e-04 & 3.988947e-04 & -14.149486\\\\\n\t search    & 2.729446e-03 & 2.574298e-08 & 5.038322e-04 & 1.640140e-14 & 1.271111e-11 & 7.019241e-18 & 5.191067e-05 & 1.532129e-12 & 1.162798e-16 & ⋯ & 3.583388e-10 & 2.613683e-04 & 7.210127e-16 & 1.601236e-02 & 1.686364e-08 & 5.831544e-07 & 5.474231e-04 & 8.443078e-10 & 8.052038e-10 & -16.694070\\\\\n\t select    & 2.071751e-03 & 1.317791e-07 & 1.888010e-03 & 2.237886e-07 & 7.299765e-04 & 6.575797e-04 & 1.172481e-03 & 8.356523e-03 & 1.544028e-16 & ⋯ & 6.275551e-03 & 6.968690e-17 & 5.809728e-11 & 4.368237e-03 & 1.093655e-07 & 9.925481e-06 & 3.003759e-03 & 1.473784e-06 & 4.679659e-06 & -13.940441\\\\\n\t semant    & 6.303475e-03 & 8.810193e-06 & 7.149435e-03 & 4.008171e-02 & 8.871820e-03 & 5.889596e-03 & 9.227885e-03 & 3.060020e-02 & 1.580750e-02 & ⋯ & 6.966860e-03 & 5.361345e-03 & 1.314493e-02 & 3.679974e-03 & 6.404972e-05 & 6.302600e-04 & 1.494241e-03 & 3.591847e-04 & 4.780490e-04 &  -9.482758\\\\\n\t sinc      & 1.491130e-03 & 9.928983e-13 & 8.211691e-05 & 4.402393e-08 & 6.988457e-03 & 3.540485e-15 & 5.066808e-04 & 5.726458e-03 & 2.820657e-03 & ⋯ & 3.173610e-03 & 1.713077e-18 & 4.401057e-11 & 7.697566e-15 & 8.329998e-09 & 4.168947e-07 & 2.018334e-04 & 1.594476e-06 & 9.534243e-07 & -30.484041\\\\\n\t sourc     & 2.046465e-03 & 9.204202e-06 & 2.094225e-03 & 2.489374e-02 & 1.437286e-05 & 6.256792e-03 & 1.726439e-03 & 6.165052e-03 & 4.610696e-03 & ⋯ & 3.668140e-03 & 6.278889e-03 & 4.960376e-03 & 2.393391e-03 & 4.983254e-05 & 6.721379e-04 & 6.806780e-04 & 2.850932e-04 & 2.017853e-04 &  -7.796626\\\\\n\t specif    & 1.399678e-03 & 4.200053e-06 & 5.405619e-03 & 6.274845e-07 & 3.200630e-03 & 1.536844e-03 & 6.870144e-03 & 1.849648e-03 & 8.361394e-04 & ⋯ & 2.346414e-03 & 1.982240e-03 & 2.615571e-03 & 2.364658e-02 & 3.394491e-05 & 2.841043e-05 & 1.028503e-04 & 1.551024e-04 & 2.696745e-04 &  -8.380472\\\\\n\t standard  & 3.554464e-03 & 4.671545e-07 & 4.974477e-12 & 8.714671e-07 & 2.396007e-03 & 3.965851e-03 & 4.482356e-03 & 2.969436e-03 & 8.533895e-04 & ⋯ & 7.531849e-03 & 7.952213e-03 & 1.797171e-09 & 2.600512e-03 & 4.430359e-05 & 9.311562e-05 & 5.160446e-08 & 4.028396e-05 & 1.532075e-04 & -12.893445\\\\\n\t step      & 5.165770e-14 & 1.146366e-02 & 1.164647e-21 & 3.503949e-07 & 5.810797e-08 & 7.889259e-13 & 1.058829e-05 & 5.215642e-03 & 1.684063e-18 & ⋯ & 1.218656e-02 & 2.414912e-19 & 2.081811e-05 & 3.527609e-06 & 3.789087e-03 & 8.476196e-04 & 2.344289e-05 & 2.534314e-02 & 9.894129e-03 &  37.691222\\\\\n\t structur  & 1.011567e-03 & 1.246816e-02 & 1.561459e-25 & 5.458288e-03 & 2.496536e-08 & 8.926302e-04 & 4.669665e-03 & 9.726700e-03 & 3.302975e-03 & ⋯ & 1.017776e-02 & 9.140531e-14 & 7.741066e-08 & 5.960010e-03 & 1.082417e-02 & 1.289781e-01 & 2.903461e-05 & 1.137516e-02 & 2.584095e-02 &   3.623585\\\\\n\t symptom   & 0.000000e+00 & 8.132030e-02 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & ⋯ & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 & 0.000000e+00 &        Inf\\\\\n\t system    & 1.211972e-03 & 2.797292e-07 & 2.986720e-02 & 7.730213e-03 & 1.564045e-04 & 1.622899e-02 & 1.185282e-02 & 7.639730e-03 & 1.701542e-03 & ⋯ & 3.944566e-02 & 1.356433e-10 & 1.766462e-07 & 2.498503e-03 & 2.247524e-07 & 6.450466e-05 & 8.240585e-04 & 1.388125e-05 & 4.972751e-06 & -12.081037\\\\\n\t t         & 5.551403e-02 & 2.330568e-10 & 1.985560e-03 & 2.757272e-13 & 6.693228e-14 & 6.545120e-06 & 1.298768e-03 & 5.444586e-16 & 9.950903e-03 & ⋯ & 2.623400e-13 & 4.009014e-04 & 2.403538e-14 & 7.076270e-15 & 1.852922e-07 & 3.326125e-07 & 9.465422e-05 & 2.739388e-08 & 6.418931e-10 & -27.827596\\\\\n\t tabl      & 1.285310e-03 & 1.406033e-08 & 1.138762e-02 & 8.056325e-09 & 4.328746e-03 & 9.736944e-03 & 2.704599e-03 & 3.305840e-17 & 5.568040e-03 & ⋯ & 9.103520e-03 & 7.633059e-03 & 2.583879e-04 & 7.353773e-16 & 9.518755e-06 & 1.354660e-06 & 1.842895e-05 & 3.175289e-07 & 3.417238e-06 & -16.480126\\\\\n\t task      & 3.137330e-03 & 1.561246e-04 & 1.415095e-03 & 6.038269e-07 & 5.405819e-03 & 2.393325e-02 & 1.417641e-04 & 1.082273e-07 & 1.039887e-02 & ⋯ & 1.023637e-02 & 1.188407e-03 & 2.405421e-03 & 1.253703e-04 & 1.169725e-04 & 5.465355e-05 & 4.403144e-04 & 9.379979e-05 & 5.343848e-04 &  -4.328768\\\\\n\t text      & 2.290442e-02 & 1.173857e-04 & 5.028892e-03 & 2.373132e-04 & 5.948906e-17 & 8.465194e-04 & 1.053856e-09 & 7.214198e-19 & 1.606346e-02 & ⋯ & 2.457642e-02 & 3.362304e-17 & 5.188903e-09 & 4.628823e-20 & 3.251529e-04 & 9.751288e-03 & 2.953765e-01 & 9.056318e-07 & 4.640419e-07 &  -7.608226\\\\\n\t therefor  & 3.693794e-03 & 1.800992e-14 & 6.230708e-03 & 4.264379e-11 & 8.643181e-04 & 1.517697e-03 & 1.571386e-18 & 6.254913e-04 & 1.803054e-17 & ⋯ & 1.119241e-12 & 2.654954e-03 & 1.828557e-12 & 7.835730e-16 & 1.197622e-10 & 1.630550e-10 & 5.969838e-07 & 2.073772e-07 & 2.348635e-09 & -37.577521\\\\\n\t tion      & 2.799772e-03 & 5.127146e-07 & 5.179386e-06 & 9.586946e-07 & 2.565223e-03 & 5.989842e-04 & 4.951837e-03 & 7.358868e-03 & 7.666712e-03 & ⋯ & 5.153927e-03 & 1.716977e-03 & 2.205865e-11 & 5.376250e-03 & 5.096748e-05 & 6.430693e-05 & 1.104739e-03 & 4.314718e-05 & 1.618196e-04 & -12.414866\\\\\n\t treatment & 4.081466e-25 & 8.341618e-03 & 3.006203e-11 & 1.184767e-07 & 7.806397e-22 & 3.489717e-04 & 2.365162e-04 & 1.173801e-06 & 5.348865e-03 & ⋯ & 5.110973e-05 & 2.139484e-03 & 4.936456e-03 & 1.443479e-07 & 5.309154e-02 & 5.456368e-05 & 4.789987e-08 & 8.885283e-03 & 3.507845e-05 &  74.113658\\\\\n\t type      & 2.123200e-03 & 1.198630e-06 & 3.607481e-03 & 3.334206e-07 & 5.207057e-03 & 4.006298e-03 & 3.876567e-03 & 7.505584e-03 & 3.934998e-16 & ⋯ & 2.293804e-03 & 1.925814e-03 & 6.562676e-03 & 1.289760e-02 & 1.568049e-05 & 1.725896e-05 & 3.461722e-05 & 4.182137e-05 & 3.990601e-05 & -10.790638\\\\\n\t under     & 9.428805e-39 & 9.865233e-03 & 1.171682e-10 & 8.158001e-03 & 2.474007e-09 & 2.315447e-17 & 1.432359e-22 & 3.344750e-03 & 5.831852e-25 & ⋯ & 2.914710e-09 & 3.357855e-03 & 9.886849e-04 & 2.756130e-07 & 4.836058e-03 & 1.851716e-05 & 9.752044e-08 & 1.067660e-02 & 9.645659e-03 & 119.654690\\\\\n\t unless    & 1.402495e-03 & 2.165140e-06 & 5.562675e-04 & 3.940094e-07 & 1.620809e-03 & 1.559653e-03 & 3.641730e-03 & 2.027132e-03 & 1.818337e-03 & ⋯ & 1.547843e-03 & 2.078652e-03 & 3.496030e-03 & 1.267791e-03 & 1.452777e-05 & 2.121317e-05 & 3.817902e-05 & 7.758486e-05 & 1.238108e-04 &  -9.339319\\\\\n\t usag      & 1.248938e-03 & 1.389423e-07 & 9.201359e-17 & 1.694846e-12 & 7.719359e-18 & 5.464123e-14 & 1.420730e-03 & 3.073837e-03 & 8.763122e-04 & ⋯ & 4.248518e-18 & 1.853693e-20 & 1.769525e-03 & 5.981724e-04 & 1.913948e-11 & 5.871247e-08 & 2.865082e-16 & 4.521851e-07 & 2.419262e-09 & -13.133928\\\\\n\t valid     & 5.634323e-03 & 2.342462e-08 & 4.637276e-03 & 2.703723e-10 & 2.062555e-13 & 1.505306e-03 & 5.817825e-04 & 2.308203e-03 & 3.742204e-03 & ⋯ & 1.446625e-11 & 3.275471e-03 & 6.182080e-11 & 1.427890e-03 & 3.151711e-08 & 1.098513e-07 & 2.875573e-06 & 1.678606e-06 & 4.656130e-09 & -17.875857\\\\\n\t valu      & 1.314954e-03 & 3.072823e-07 & 1.003235e-11 & 5.106381e-08 & 8.180532e-20 & 2.149304e-03 & 1.642292e-08 & 8.363430e-13 & 2.387654e-16 & ⋯ & 7.858560e-03 & 7.596886e-03 & 1.202411e-16 & 2.017054e-02 & 2.756694e-05 & 2.659602e-04 & 1.805244e-06 & 5.241958e-09 & 3.689306e-09 & -12.063160\\\\\n\t view      & 2.137289e-03 & 1.861859e-06 & 1.250499e-11 & 9.038059e-07 & 1.571591e-03 & 7.419902e-04 & 5.373367e-04 & 3.270751e-04 & 1.807505e-03 & ⋯ & 6.923461e-04 & 1.088518e-03 & 3.847237e-03 & 1.440582e-03 & 4.460264e-05 & 6.093482e-05 & 3.793833e-08 & 1.183248e-04 & 3.579252e-04 & -10.164823\\\\\n\t visit     & 2.854792e-03 & 2.164210e-06 & 4.855266e-11 & 6.607485e-07 & 7.448582e-04 & 1.570393e-03 & 1.437449e-03 & 5.028020e-04 & 8.788297e-04 & ⋯ & 7.055743e-04 & 1.130561e-03 & 1.799449e-03 & 6.167371e-04 & 3.092867e-05 & 5.634496e-05 & 4.870266e-08 & 9.697629e-05 & 2.095183e-04 & -10.365329\\\\\n\\end{tabular}\n",
            "text/plain": [
              "    term       topic1       topic2       topic3       topic4       topic5      \n",
              "1   abstract   9.891270e-04 2.183909e-02 8.046808e-06 6.637682e-03 6.036847e-06\n",
              "2   access     1.120011e-03 2.391222e-02 1.711953e-03 6.193825e-03 8.722404e-06\n",
              "3   ad         7.008042e-03 5.043826e-12 2.148814e-03 5.487457e-13 1.724233e-03\n",
              "4   addit      2.818202e-03 1.401852e-07 6.693555e-03 4.732357e-08 6.800993e-04\n",
              "5   address    2.030336e-12 7.448534e-03 1.353661e-44 6.197137e-17 4.158349e-06\n",
              "6   al         4.959293e-02 2.594243e-05 8.723284e-03 7.642550e-03 5.579447e-06\n",
              "7   algorithm  1.279238e-02 1.612707e-08 9.667328e-04 5.851164e-12 7.702624e-04\n",
              "8   annot      4.419111e-03 5.661054e-07 3.126208e-03 8.181356e-10 4.977265e-03\n",
              "9   appli      2.822480e-03 8.203205e-06 5.067956e-03 8.439849e-03 1.611016e-03\n",
              "10  appropri   4.287913e-04 2.916191e-02 2.452486e-04 3.578829e-10 2.675423e-07\n",
              "11  articl     1.380737e-03 1.624354e-06 1.909452e-09 5.027072e-13 2.860082e-18\n",
              "12  assign     1.738337e-03 3.188525e-08 2.204132e-06 7.048900e-08 9.007766e-17\n",
              "13  author     1.216551e-03 2.655127e-07 8.583378e-11 9.327456e-03 3.436140e-03\n",
              "14  automat    4.661787e-04 3.513100e-02 3.728234e-03 4.774023e-12 3.419297e-16\n",
              "15  avail      1.221211e-03 3.786471e-02 4.109698e-06 6.181862e-03 2.457682e-12\n",
              "16  b          4.204775e-03 1.475797e-09 3.420375e-04 2.738781e-09 1.750221e-03\n",
              "17  background 7.698933e-04 1.363058e-02 5.793790e-04 2.459302e-02 3.917252e-04\n",
              "18  becom      1.406732e-03 2.558069e-12 7.595363e-29 1.076617e-09 7.562309e-04\n",
              "19  biomed     5.611050e-03 1.255468e-05 1.248233e-02 8.166679e-03 5.493058e-03\n",
              "20  c          8.515085e-02 1.761635e-07 2.583672e-03 2.535301e-08 2.231142e-03\n",
              "21  care       2.459688e-03 5.620387e-05 2.121653e-10 1.859297e-06 3.613394e-16\n",
              "22  categori   2.511380e-03 6.525686e-07 3.513442e-15 2.747414e-09 8.082483e-04\n",
              "23  classifi   1.220839e-03 2.318718e-09 2.781941e-02 2.573913e-11 7.062520e-04\n",
              "24  clinic     6.730783e-03 1.958323e-01 4.197374e-21 7.556034e-10 3.518779e-30\n",
              "25  code       2.073523e-03 6.254227e-07 7.819911e-13 5.860721e-08 7.322117e-18\n",
              "26  common     3.473133e-03 7.735280e-06 3.159602e-03 1.450731e-06 6.645984e-03\n",
              "27  comput     2.819313e-03 6.211652e-07 1.603550e-03 6.152242e-11 2.567386e-03\n",
              "28  concept    1.527003e-02 1.958033e-07 1.609858e-03 4.587695e-08 1.551003e-03\n",
              "29  consist    2.534902e-03 2.012769e-09 2.837861e-03 3.373529e-09 8.270901e-04\n",
              "30  contain    1.382895e-03 1.433249e-05 1.320740e-04 2.205983e-06 3.381046e-03\n",
              "⋮   ⋮          ⋮            ⋮            ⋮            ⋮            ⋮           \n",
              "121 requir     2.651101e-13 5.433721e-03 7.053808e-12 2.145810e-07 1.635895e-02\n",
              "122 resourc    9.563138e-26 2.991568e-02 3.196894e-06 2.042148e-08 2.130328e-08\n",
              "123 review     7.780898e-03 2.583608e-07 4.528827e-04 5.217645e-08 2.797743e-14\n",
              "124 s          1.073173e-01 5.905413e-06 3.209919e-03 4.677995e-06 1.162558e-02\n",
              "125 search     2.729446e-03 2.574298e-08 5.038322e-04 1.640140e-14 1.271111e-11\n",
              "126 select     2.071751e-03 1.317791e-07 1.888010e-03 2.237886e-07 7.299765e-04\n",
              "127 semant     6.303475e-03 8.810193e-06 7.149435e-03 4.008171e-02 8.871820e-03\n",
              "128 sinc       1.491130e-03 9.928983e-13 8.211691e-05 4.402393e-08 6.988457e-03\n",
              "129 sourc      2.046465e-03 9.204202e-06 2.094225e-03 2.489374e-02 1.437286e-05\n",
              "130 specif     1.399678e-03 4.200053e-06 5.405619e-03 6.274845e-07 3.200630e-03\n",
              "131 standard   3.554464e-03 4.671545e-07 4.974477e-12 8.714671e-07 2.396007e-03\n",
              "132 step       5.165770e-14 1.146366e-02 1.164647e-21 3.503949e-07 5.810797e-08\n",
              "133 structur   1.011567e-03 1.246816e-02 1.561459e-25 5.458288e-03 2.496536e-08\n",
              "134 symptom    0.000000e+00 8.132030e-02 0.000000e+00 0.000000e+00 0.000000e+00\n",
              "135 system     1.211972e-03 2.797292e-07 2.986720e-02 7.730213e-03 1.564045e-04\n",
              "136 t          5.551403e-02 2.330568e-10 1.985560e-03 2.757272e-13 6.693228e-14\n",
              "137 tabl       1.285310e-03 1.406033e-08 1.138762e-02 8.056325e-09 4.328746e-03\n",
              "138 task       3.137330e-03 1.561246e-04 1.415095e-03 6.038269e-07 5.405819e-03\n",
              "139 text       2.290442e-02 1.173857e-04 5.028892e-03 2.373132e-04 5.948906e-17\n",
              "140 therefor   3.693794e-03 1.800992e-14 6.230708e-03 4.264379e-11 8.643181e-04\n",
              "141 tion       2.799772e-03 5.127146e-07 5.179386e-06 9.586946e-07 2.565223e-03\n",
              "142 treatment  4.081466e-25 8.341618e-03 3.006203e-11 1.184767e-07 7.806397e-22\n",
              "143 type       2.123200e-03 1.198630e-06 3.607481e-03 3.334206e-07 5.207057e-03\n",
              "144 under      9.428805e-39 9.865233e-03 1.171682e-10 8.158001e-03 2.474007e-09\n",
              "145 unless     1.402495e-03 2.165140e-06 5.562675e-04 3.940094e-07 1.620809e-03\n",
              "146 usag       1.248938e-03 1.389423e-07 9.201359e-17 1.694846e-12 7.719359e-18\n",
              "147 valid      5.634323e-03 2.342462e-08 4.637276e-03 2.703723e-10 2.062555e-13\n",
              "148 valu       1.314954e-03 3.072823e-07 1.003235e-11 5.106381e-08 8.180532e-20\n",
              "149 view       2.137289e-03 1.861859e-06 1.250499e-11 9.038059e-07 1.571591e-03\n",
              "150 visit      2.854792e-03 2.164210e-06 4.855266e-11 6.607485e-07 7.448582e-04\n",
              "    topic6       topic7       topic8       topic9       ⋯ topic12     \n",
              "1   1.077171e-03 8.071029e-06 1.530695e-04 6.162472e-04 ⋯ 1.102119e-04\n",
              "2   7.095733e-04 2.523896e-05 1.172592e-02 3.055575e-03 ⋯ 8.936891e-05\n",
              "3   3.517472e-03 2.900027e-03 1.863381e-16 4.733777e-17 ⋯ 4.223413e-13\n",
              "4   6.487958e-03 1.077201e-03 6.914795e-03 5.576463e-03 ⋯ 3.139404e-03\n",
              "5   4.409513e-03 3.340190e-03 1.090546e-15 1.321966e-03 ⋯ 1.656630e-25\n",
              "6   1.218664e-02 3.652897e-03 1.043555e-02 9.920903e-03 ⋯ 1.277826e-02\n",
              "7   6.149898e-04 1.249067e-17 2.209886e-03 1.773114e-03 ⋯ 1.191969e-11\n",
              "8   3.620148e-02 2.941685e-17 1.356005e-04 2.501730e-03 ⋯ 1.191836e-10\n",
              "9   1.251382e-02 2.661850e-04 6.401412e-05 1.708201e-03 ⋯ 2.723833e-03\n",
              "10  3.091958e-04 4.699571e-03 2.994976e-05 4.784685e-04 ⋯ 3.279498e-05\n",
              "11  1.453109e-03 1.382428e-03 3.108460e-14 1.605046e-20 ⋯ 5.185167e-15\n",
              "12  2.936961e-03 1.447034e-03 2.066484e-16 5.605659e-03 ⋯ 2.225758e-02\n",
              "13  1.447600e-05 3.812620e-03 4.595319e-03 7.707470e-03 ⋯ 2.136899e-03\n",
              "14  2.968896e-03 7.310384e-20 1.229760e-03 7.195086e-04 ⋯ 1.349661e-03\n",
              "15  7.214460e-03 4.520363e-08 7.910794e-03 3.174459e-05 ⋯ 5.147275e-04\n",
              "16  3.273561e-15 1.914965e-18 2.423580e-03 2.777781e-03 ⋯ 1.545185e-03\n",
              "17  5.651355e-04 3.028366e-04 8.151740e-05 1.522143e-03 ⋯ 4.805344e-04\n",
              "18  1.516971e-03 1.431094e-03 2.255005e-03 1.828404e-03 ⋯ 6.329347e-24\n",
              "19  6.187196e-03 6.957069e-03 3.645285e-03 8.982907e-03 ⋯ 6.494097e-03\n",
              "20  5.441060e-07 2.643292e-16 1.848887e-03 4.091417e-03 ⋯ 8.976594e-04\n",
              "21  6.871268e-04 2.514296e-02 1.833950e-05 2.095156e-16 ⋯ 1.138556e-02\n",
              "22  9.346778e-17 6.904345e-04 6.951239e-15 8.414317e-23 ⋯ 1.436452e-03\n",
              "23  1.461470e-03 1.255792e-19 1.314125e-17 9.075287e-04 ⋯ 1.299924e-02\n",
              "24  4.057729e-03 2.582586e-13 9.191410e-18 8.083128e-08 ⋯ 4.306903e-08\n",
              "25  1.433327e-03 2.655431e-03 5.776552e-13 2.236108e-17 ⋯ 2.180668e-03\n",
              "26  3.826069e-03 1.238090e-02 4.337335e-03 1.311777e-02 ⋯ 4.595924e-03\n",
              "27  7.209705e-15 2.405402e-06 4.202400e-05 5.578562e-03 ⋯ 6.546892e-13\n",
              "28  5.071337e-13 1.926047e-03 8.785473e-03 7.811333e-04 ⋯ 1.436969e-03\n",
              "29  1.567406e-03 6.090519e-04 1.431202e-03 5.652282e-03 ⋯ 2.361887e-03\n",
              "30  6.967243e-04 1.178004e-03 1.204953e-03 8.442831e-04 ⋯ 4.817845e-03\n",
              "⋮   ⋮            ⋮            ⋮            ⋮            ⋱ ⋮           \n",
              "121 3.769148e-03 9.784849e-18 5.037504e-03 1.168802e-03 ⋯ 2.297590e-03\n",
              "122 2.640717e-04 3.530524e-07 2.306115e-03 4.363810e-20 ⋯ 8.668097e-06\n",
              "123 2.102552e-03 1.393630e-03 5.804722e-16 2.738678e-03 ⋯ 2.157579e-03\n",
              "124 6.919190e-03 1.568630e-02 2.030493e-02 3.064131e-02 ⋯ 1.361620e-02\n",
              "125 7.019241e-18 5.191067e-05 1.532129e-12 1.162798e-16 ⋯ 3.583388e-10\n",
              "126 6.575797e-04 1.172481e-03 8.356523e-03 1.544028e-16 ⋯ 6.275551e-03\n",
              "127 5.889596e-03 9.227885e-03 3.060020e-02 1.580750e-02 ⋯ 6.966860e-03\n",
              "128 3.540485e-15 5.066808e-04 5.726458e-03 2.820657e-03 ⋯ 3.173610e-03\n",
              "129 6.256792e-03 1.726439e-03 6.165052e-03 4.610696e-03 ⋯ 3.668140e-03\n",
              "130 1.536844e-03 6.870144e-03 1.849648e-03 8.361394e-04 ⋯ 2.346414e-03\n",
              "131 3.965851e-03 4.482356e-03 2.969436e-03 8.533895e-04 ⋯ 7.531849e-03\n",
              "132 7.889259e-13 1.058829e-05 5.215642e-03 1.684063e-18 ⋯ 1.218656e-02\n",
              "133 8.926302e-04 4.669665e-03 9.726700e-03 3.302975e-03 ⋯ 1.017776e-02\n",
              "134 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 ⋯ 0.000000e+00\n",
              "135 1.622899e-02 1.185282e-02 7.639730e-03 1.701542e-03 ⋯ 3.944566e-02\n",
              "136 6.545120e-06 1.298768e-03 5.444586e-16 9.950903e-03 ⋯ 2.623400e-13\n",
              "137 9.736944e-03 2.704599e-03 3.305840e-17 5.568040e-03 ⋯ 9.103520e-03\n",
              "138 2.393325e-02 1.417641e-04 1.082273e-07 1.039887e-02 ⋯ 1.023637e-02\n",
              "139 8.465194e-04 1.053856e-09 7.214198e-19 1.606346e-02 ⋯ 2.457642e-02\n",
              "140 1.517697e-03 1.571386e-18 6.254913e-04 1.803054e-17 ⋯ 1.119241e-12\n",
              "141 5.989842e-04 4.951837e-03 7.358868e-03 7.666712e-03 ⋯ 5.153927e-03\n",
              "142 3.489717e-04 2.365162e-04 1.173801e-06 5.348865e-03 ⋯ 5.110973e-05\n",
              "143 4.006298e-03 3.876567e-03 7.505584e-03 3.934998e-16 ⋯ 2.293804e-03\n",
              "144 2.315447e-17 1.432359e-22 3.344750e-03 5.831852e-25 ⋯ 2.914710e-09\n",
              "145 1.559653e-03 3.641730e-03 2.027132e-03 1.818337e-03 ⋯ 1.547843e-03\n",
              "146 5.464123e-14 1.420730e-03 3.073837e-03 8.763122e-04 ⋯ 4.248518e-18\n",
              "147 1.505306e-03 5.817825e-04 2.308203e-03 3.742204e-03 ⋯ 1.446625e-11\n",
              "148 2.149304e-03 1.642292e-08 8.363430e-13 2.387654e-16 ⋯ 7.858560e-03\n",
              "149 7.419902e-04 5.373367e-04 3.270751e-04 1.807505e-03 ⋯ 6.923461e-04\n",
              "150 1.570393e-03 1.437449e-03 5.028020e-04 8.788297e-04 ⋯ 7.055743e-04\n",
              "    topic13      topic14      topic15      topic16      topic17     \n",
              "1   1.924018e-03 2.689976e-04 9.583808e-04 1.170690e-02 2.898499e-02\n",
              "2   1.200693e-03 1.136751e-02 2.171506e-03 7.584126e-03 2.464101e-02\n",
              "3   2.860197e-03 3.819004e-16 5.891280e-15 3.359898e-08 4.620764e-08\n",
              "4   3.081846e-03 2.100681e-03 6.077327e-03 1.709528e-06 8.510894e-07\n",
              "5   6.748111e-30 8.039043e-26 3.299021e-10 2.190744e-04 5.100948e-05\n",
              "6   1.803704e-02 8.846769e-03 6.283460e-03 9.834287e-05 7.495552e-04\n",
              "7   2.025265e-21 1.405831e-10 8.973717e-03 2.390390e-12 3.667118e-09\n",
              "8   8.179991e-18 1.638960e-11 6.306197e-03 1.034064e-10 2.362144e-07\n",
              "9   3.345135e-07 8.724387e-03 8.248182e-04 5.331917e-06 2.024520e-04\n",
              "10  3.134474e-04 1.993403e-03 1.590759e-03 1.957800e-02 3.009035e-03\n",
              "11  2.341135e-03 2.913642e-03 1.521318e-03 1.286733e-07 6.178286e-07\n",
              "12  1.409597e-02 2.012521e-13 4.321236e-09 1.325097e-04 6.902069e-05\n",
              "13  3.170465e-03 2.411198e-12 2.069869e-03 4.019256e-05 2.997288e-04\n",
              "14  1.870065e-07 8.974396e-07 1.359410e-03 2.135563e-03 7.105686e-04\n",
              "15  6.497022e-03 1.371174e-03 7.677596e-03 1.492809e-02 6.199535e-02\n",
              "16  5.732180e-03 7.452978e-13 3.096485e-03 1.909632e-08 3.990144e-07\n",
              "17  2.861941e-03 3.360595e-04 1.419968e-04 1.362272e-02 3.847877e-02\n",
              "18  6.294256e-21 4.202434e-13 7.087722e-19 5.068777e-10 2.123746e-07\n",
              "19  9.737253e-03 3.853707e-02 4.708152e-03 7.029111e-05 3.727022e-04\n",
              "20  3.676523e-15 7.780450e-10 2.590792e-03 2.234159e-08 4.866334e-06\n",
              "21  1.552860e-03 3.350752e-03 1.211869e-03 2.237874e-04 8.938321e-04\n",
              "22  8.013386e-18 1.544790e-03 6.898158e-04 2.600569e-07 2.391057e-05\n",
              "23  6.865868e-07 3.103685e-16 6.577172e-03 3.194381e-09 9.360424e-09\n",
              "24  3.294139e-07 9.185019e-11 2.027203e-08 6.063059e-02 1.543232e-01\n",
              "25  6.954577e-03 4.557209e-13 1.147115e-02 6.200373e-05 3.149030e-04\n",
              "26  1.053259e-02 1.023464e-02 5.555479e-03 5.358095e-05 7.741992e-05\n",
              "27  2.728500e-18 7.221106e-03 1.096700e-02 2.337505e-10 2.747105e-07\n",
              "28  1.118669e-16 1.299925e-08 4.440545e-03 1.711155e-07 1.336094e-05\n",
              "29  4.879473e-03 1.436422e-10 2.276071e-03 1.566843e-07 9.141889e-08\n",
              "30  3.200685e-03 3.621419e-03 1.997298e-03 9.663356e-05 1.391307e-04\n",
              "⋮   ⋮            ⋮            ⋮            ⋮            ⋮           \n",
              "121 3.170416e-03 4.743175e-05 7.436474e-03 3.533793e-02 7.542716e-05\n",
              "122 6.345836e-07 2.583768e-02 1.693822e-02 1.231651e-02 1.631455e-05\n",
              "123 1.574899e-15 1.291758e-03 4.963188e-16 4.160110e-06 5.245387e-05\n",
              "124 3.087469e-05 6.546738e-03 1.484804e-05 5.490291e-05 1.609546e-04\n",
              "125 2.613683e-04 7.210127e-16 1.601236e-02 1.686364e-08 5.831544e-07\n",
              "126 6.968690e-17 5.809728e-11 4.368237e-03 1.093655e-07 9.925481e-06\n",
              "127 5.361345e-03 1.314493e-02 3.679974e-03 6.404972e-05 6.302600e-04\n",
              "128 1.713077e-18 4.401057e-11 7.697566e-15 8.329998e-09 4.168947e-07\n",
              "129 6.278889e-03 4.960376e-03 2.393391e-03 4.983254e-05 6.721379e-04\n",
              "130 1.982240e-03 2.615571e-03 2.364658e-02 3.394491e-05 2.841043e-05\n",
              "131 7.952213e-03 1.797171e-09 2.600512e-03 4.430359e-05 9.311562e-05\n",
              "132 2.414912e-19 2.081811e-05 3.527609e-06 3.789087e-03 8.476196e-04\n",
              "133 9.140531e-14 7.741066e-08 5.960010e-03 1.082417e-02 1.289781e-01\n",
              "134 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
              "135 1.356433e-10 1.766462e-07 2.498503e-03 2.247524e-07 6.450466e-05\n",
              "136 4.009014e-04 2.403538e-14 7.076270e-15 1.852922e-07 3.326125e-07\n",
              "137 7.633059e-03 2.583879e-04 7.353773e-16 9.518755e-06 1.354660e-06\n",
              "138 1.188407e-03 2.405421e-03 1.253703e-04 1.169725e-04 5.465355e-05\n",
              "139 3.362304e-17 5.188903e-09 4.628823e-20 3.251529e-04 9.751288e-03\n",
              "140 2.654954e-03 1.828557e-12 7.835730e-16 1.197622e-10 1.630550e-10\n",
              "141 1.716977e-03 2.205865e-11 5.376250e-03 5.096748e-05 6.430693e-05\n",
              "142 2.139484e-03 4.936456e-03 1.443479e-07 5.309154e-02 5.456368e-05\n",
              "143 1.925814e-03 6.562676e-03 1.289760e-02 1.568049e-05 1.725896e-05\n",
              "144 3.357855e-03 9.886849e-04 2.756130e-07 4.836058e-03 1.851716e-05\n",
              "145 2.078652e-03 3.496030e-03 1.267791e-03 1.452777e-05 2.121317e-05\n",
              "146 1.853693e-20 1.769525e-03 5.981724e-04 1.913948e-11 5.871247e-08\n",
              "147 3.275471e-03 6.182080e-11 1.427890e-03 3.151711e-08 1.098513e-07\n",
              "148 7.596886e-03 1.202411e-16 2.017054e-02 2.756694e-05 2.659602e-04\n",
              "149 1.088518e-03 3.847237e-03 1.440582e-03 4.460264e-05 6.093482e-05\n",
              "150 1.130561e-03 1.799449e-03 6.167371e-04 3.092867e-05 5.634496e-05\n",
              "    topic18      topic19      topic20      log_ratio \n",
              "1   1.853744e-02 1.111416e-02 1.387425e-02   4.464613\n",
              "2   4.074399e-02 2.737817e-02 3.289386e-02   4.416163\n",
              "3   4.433034e-05 2.576116e-09 1.757723e-07 -30.371846\n",
              "4   4.164345e-06 7.519590e-06 7.008782e-06 -14.295153\n",
              "5   8.407537e-04 6.037564e-04 5.438222e-02  31.772591\n",
              "6   9.761622e-04 3.656316e-04 1.152408e-04 -10.900605\n",
              "7   5.244659e-06 2.354098e-08 7.188320e-07 -19.597369\n",
              "8   2.603015e-04 1.320177e-06 1.644091e-05 -12.930398\n",
              "9   6.931348e-04 4.255744e-05 1.351996e-04  -8.426560\n",
              "10  7.550059e-03 1.535133e-02 1.301994e-02   6.087666\n",
              "11  4.454018e-09 9.094123e-09 6.421294e-10  -9.731356\n",
              "12  2.123401e-06 5.852219e-08 9.346156e-09 -15.734459\n",
              "13  1.483614e-04 2.255188e-05 3.301625e-05 -12.161729\n",
              "14  1.554800e-04 1.807164e-03 3.178214e-05   6.235718\n",
              "15  2.983071e-02 1.652672e-02 2.643197e-03   4.954470\n",
              "16  1.408433e-06 7.057816e-08 8.071677e-07 -21.442103\n",
              "17  5.600014e-02 1.832391e-02 2.103449e-02   4.146045\n",
              "18  1.618801e-08 5.051627e-06 6.109471e-06 -29.034645\n",
              "19  5.252596e-04 4.449118e-04 5.668793e-04  -8.803902\n",
              "20  4.215045e-05 2.434859e-06 1.507445e-05 -18.882746\n",
              "21  2.818156e-10 2.098193e-05 2.922198e-07  -5.451662\n",
              "22  6.734726e-13 8.029861e-09 2.100183e-05 -11.910063\n",
              "23  2.981869e-08 2.677354e-13 7.416383e-08 -19.006115\n",
              "24  8.026830e-08 2.792345e-06 2.339808e-06   4.862700\n",
              "25  6.170539e-08 3.199847e-08 2.631757e-08 -11.694965\n",
              "26  1.224595e-04 2.873647e-04 4.451912e-04  -8.810568\n",
              "27  6.406775e-05 4.368791e-06 5.358622e-05 -12.148079\n",
              "28  1.287919e-05 8.170237e-06 6.201736e-05 -16.250938\n",
              "29  2.644551e-08 3.189997e-07 6.931708e-07 -20.264317\n",
              "30  1.463067e-04 3.242602e-04 7.192874e-04  -6.592259\n",
              "⋮   ⋮            ⋮            ⋮            ⋮         \n",
              "121 1.259603e-05 5.893189e-03 4.764888e-02  34.254630\n",
              "122 2.517959e-04 7.876871e-03 5.553990e-03  78.049692\n",
              "123 4.980519e-04 4.278476e-07 3.997150e-08 -14.878262\n",
              "124 2.289795e-03 5.131828e-04 3.988947e-04 -14.149486\n",
              "125 5.474231e-04 8.443078e-10 8.052038e-10 -16.694070\n",
              "126 3.003759e-03 1.473784e-06 4.679659e-06 -13.940441\n",
              "127 1.494241e-03 3.591847e-04 4.780490e-04  -9.482758\n",
              "128 2.018334e-04 1.594476e-06 9.534243e-07 -30.484041\n",
              "129 6.806780e-04 2.850932e-04 2.017853e-04  -7.796626\n",
              "130 1.028503e-04 1.551024e-04 2.696745e-04  -8.380472\n",
              "131 5.160446e-08 4.028396e-05 1.532075e-04 -12.893445\n",
              "132 2.344289e-05 2.534314e-02 9.894129e-03  37.691222\n",
              "133 2.903461e-05 1.137516e-02 2.584095e-02   3.623585\n",
              "134 0.000000e+00 0.000000e+00 0.000000e+00        Inf\n",
              "135 8.240585e-04 1.388125e-05 4.972751e-06 -12.081037\n",
              "136 9.465422e-05 2.739388e-08 6.418931e-10 -27.827596\n",
              "137 1.842895e-05 3.175289e-07 3.417238e-06 -16.480126\n",
              "138 4.403144e-04 9.379979e-05 5.343848e-04  -4.328768\n",
              "139 2.953765e-01 9.056318e-07 4.640419e-07  -7.608226\n",
              "140 5.969838e-07 2.073772e-07 2.348635e-09 -37.577521\n",
              "141 1.104739e-03 4.314718e-05 1.618196e-04 -12.414866\n",
              "142 4.789987e-08 8.885283e-03 3.507845e-05  74.113658\n",
              "143 3.461722e-05 4.182137e-05 3.990601e-05 -10.790638\n",
              "144 9.752044e-08 1.067660e-02 9.645659e-03 119.654690\n",
              "145 3.817902e-05 7.758486e-05 1.238108e-04  -9.339319\n",
              "146 2.865082e-16 4.521851e-07 2.419262e-09 -13.133928\n",
              "147 2.875573e-06 1.678606e-06 4.656130e-09 -17.875857\n",
              "148 1.805244e-06 5.241958e-09 3.689306e-09 -12.063160\n",
              "149 3.793833e-08 1.183248e-04 3.579252e-04 -10.164823\n",
              "150 4.870266e-08 9.697629e-05 2.095183e-04 -10.365329"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "library(tidytext)\n",
        "\n",
        "stm_topics <- tidy(stmFit, matrix = \"beta\")\n",
        "beta_wide <- stm_topics %>%\n",
        "  mutate(topic = paste0(\"topic\", topic)) %>%\n",
        "  pivot_wider(names_from = topic, values_from = beta) %>% \n",
        "  filter(topic1 > .001 | topic2 > .001) %>%\n",
        "  mutate(log_ratio = log2(topic2 / topic1))\n",
        "\n",
        "beta_wide\n",
        "write.csv(beta_wide,\"/content/321atext.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdxy0GoTH5Zu",
        "outputId": "c3915703-9613-4176-c98e-c6712b159da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependency ‘plyr’\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages('reshape2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unpiscXRHgqb",
        "outputId": "0f2fc35c-f175-4938-f88c-5323d86e04a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘hunspell’, ‘janeaustenr’, ‘tokenizers’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages('tidytext')\n",
        "install.packages('readr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmsswg4Fi3bh"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TopicModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}